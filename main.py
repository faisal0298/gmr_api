"""
docker build -t rdx.registry.easemyai.com/galasachin97/gmr_api:0.5.3 . --no-cache
docker push rdx.registry.easemyai.com/galasachin97/gmr_api:0.5.3
"""

import rdx
import os, sys
import mongoengine
from database.models import *
# from helpers.ai_metadata_handler import on_ai_call
from helpers.usecase_handler import load_params, pre_processing
# from helpers.widget_handler import on_widget_call
import helpers.usecase_handler as usecase_handler_object
from helpers.logger import console_logger
from fastapi import FastAPI, BackgroundTasks
import json

# from fastapi import Response
from fastapi import (
    APIRouter,
    HTTPException,
    Form,
    Query,
    File,
    Depends,
    UploadFile,
    Header,
    Request,
    Response,
    status,
)
from lxml import etree
import xml.etree.ElementTree as ET
import datetime
from fastapi.middleware.cors import CORSMiddleware
from fastapi import FastAPI
from helpers.serializer import *
from datetime import timedelta
import requests
from helpers.scheduler import backgroundTaskHandler
from dateutil.relativedelta import relativedelta
import copy
from helpers.read_timezone import read_timezone_from_file
from helpers.serializer import *
import xlsxwriter
from typing import Optional
from mongoengine.queryset.visitor import Q
from collections import defaultdict
import pandas as pd
import pytz
import shutil
from typing import List
from helpers.report_handler import generate_report
from helpers.coal_consumption_report import generate_report_consumption
from helpers.coal_gcv_comparision import generate_report_comparision
from helpers.bunker_report_handler import bunker_generate_report, bunker_single_generate_report
from helpers.form15_report_handler import single_generate_report_form15
from helpers.data_execution import DataExecutions
from service import host, db_port, username, password, ip
from helpers.mail import send_email, send_test_email, send_multiapproval_mail
import cryptocode
from mongoengine import MultipleObjectsReturned
from io import BytesIO
from pymongo import MongoClient
from dotenv import load_dotenv, dotenv_values
from bson.objectid import ObjectId
from fastapi.responses import JSONResponse
load_dotenv() 

# mahabal starts
import tabula
import math
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
import re
from collections import OrderedDict
import PyPDF2
import PyPDF3
# mahabal end


#### railway pdf upload start #####

import pdftotext
import re
import camelot
import json
import warnings

#### railway pdf upload end #####


#maps start
import googlemaps
import polyline
import json
from shapely.geometry import LineString, mapping
from shapely.geometry.polygon import Polygon
#maps end

from PyPDF2 import PdfReader

from pdfminer.high_level import extract_text
from pdfminer.pdfpage import PDFPage
from pdfminer.pdfinterp import PDFResourceManager
from pdfminer.pdfinterp import PDFPageInterpreter
from pdfminer.converter import TextConverter
from pdfminer.layout import LAParams
from io import StringIO

from requests.auth import HTTPBasicAuth
import calendar
import numpy as np
from helpers.service_handler import *

from pymongo.operations import UpdateOne, DeleteMany
import polars as pl

import pdfplumber

from calendar import monthrange
from math import isfinite
from bson import json_util
import json
import base64

from fastapi.responses import HTMLResponse

pd.options.mode.chained_assignment = None


### database setup
# host = os.environ.get("HOST", "192.168.1.57")
# db_port = int(os.environ.get("DB_PORT", 30000))
# username = os.environ.get("USERNAME", "gmr_api")
# password = os.environ.get("PASSWORD", "Q1hTpYkpYNRzsUVs")

### usecase setup
environment = os.environ.get("ENVIRONMENT", "test")
parent_ids = os.environ.get("PARENTS_IDS", ["gmr_ai"])
parent_ids = (
    parent_ids.strip("][").replace("'", "").split(", ")
    if type(parent_ids) == str
    else parent_ids
)

widget_ids = os.environ.get(
    "WIDGETS_IDS",
    [
        "gmr_table",
        "coal_test_table",
        "timestamp_wise"
    ],
)

service_id = os.environ.get("SERVICE_ID", "gmr_api")
server_ip = os.environ.get("IP", "192.168.1.57")
server_port = os.environ.get("PORT", "80")
db_name = os.environ.get("DB_NAME", "gmrDB")

client = MongoClient(f"mongodb://{host}:{db_port}/")
# client = MongoClient(f"mongodb://admin:%40ccessDenied321@192.168.1.42:27017/gmrdbprod1?authSource=admin")

# client = MongoClient("mongodb://gmr:Gmr%402026@192.168.1.42:27017/?authSource=gmrDB")
# print(f"--{client}")
db = client.gmrDB
gmrdata = db.gmrdata

db = client.gmrDB.get_collection("gmrdata")
short_mine_collection = db.short_mine

sapdb = client.gmrDB.get_collection("SapRecords")
sapraildb = client.gmrDB.get_collection("sapRecordsRail")
raildb = client.gmrDB.get_collection("raildata")
coaltestdb = client.gmrDB.get_collection("coaltesting")
receiptCoalQualityAnalysisdb = client.gmrDB.get_collection("RecieptCoalQualityAnalysis")

gmrDB = client.get_database("gmrDB")
collectionList = gmrDB.list_collection_names()
if "RecieptCoalQualityAnalysis" not in collectionList:
    RCA = gmrDB.create_collection("RecieptCoalQualityAnalysis")
else:
    RCA = gmrDB.get_collection("RecieptCoalQualityAnalysis")
    BQS = gmrDB.get_collection("BunkerQualitySummary")


proxies = {
    "http": None,
    "https": None
}

IST = pytz.timezone('Asia/Kolkata')

usecase_handler_object.handler = rdx.SocketHandler(
    service_id=service_id,
    parent_ids=parent_ids,
)

def convert_to_utc_format(date_time, format, timezone= "Asia/Kolkata",start = True):
    to_zone = tz.gettz(timezone)
    _datetime = datetime.datetime.strptime(date_time, format)

    if not start:
        _datetime =_datetime.replace(hour=23,minute=59)
    return _datetime.replace(tzinfo=to_zone).astimezone(datetime.timezone.utc).replace(tzinfo=None)


tags_meta = [{"name":"Coal Consumption",
              "description": "Coal Consumption Data"},
              {"name":"Coal Testing",
              "description": "Coal Testing And Sampling"},
              {"name":"Road Map",
              "description": "Road Map for Truck Journey"},
              {"name":"Road Map Request",
              "description": "Road Map for Request Journey"}]

router = FastAPI(title="GMR API's", description="Contains GMR Testing, Consumption, Roadmap and Roadmap Request Apis",
                 openapi_tags=tags_meta)

router.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

console_logger.debug(" ------ Usecase GMR API Started ! ------ ")

mongoengine.connect(
        db=db_name,
        host=host,
        port=db_port,
        authentication_source=db_name,
        alias="gmrDB-alias"
    )


@usecase_handler_object.handler.fetch_data
def subscribe_to_socket_server_data(data):    
    if "source" in data:
        if data["source"] in parent_ids:
            return on_ai_call(data)
        elif data["source"] in widget_ids:
            return on_widget_call(data)
    return 0


@usecase_handler_object.handler.add_camera_handler
def add_camera(data, *args, **kwargs):
    load_params(data)


@usecase_handler_object.handler.usecase_params_handler
def variable_initializer(data, *args, **kwargs):
    load_params(data)

timezone = read_timezone_from_file()


#  x------------------------------    Historian Api's for Coal Consumption    ------------------------------------x


#### railway pdf upload start #####

def extract_pdf_data(file_path):
    with open(file_path, "rb") as f:
        pdf = pdftotext.PDF(f, raw=True)
        output_string = pdf[0]

    patterns = {
        "RR_NO": r"RR NO\.\s*(\d+)",
        "RR_DATE": r"RR DATE\s*(\d{2}-\d{2}-\d{4})",
        "FREIGHT": r"FREIGHT:\s*([\d.]+)",
        "TOTAL_FREIGHT": r"TOTAL FREIGHT:\s*([\d.]+)",
        "SD": r"SD\s*([\d.]+)",
        "POLA": r"POLA\s*([\d.]+)",
        "GST": r"\*GST\s*(\d+)",
    }

    data_dictionary = {}
    for key, pattern in patterns.items():
        match = re.search(pattern, output_string)
        data_dictionary[key] = match.group(1) if match else "Not found"
    return data_dictionary


def outbond(pdf_path):

    abc = camelot.read_pdf(pdf_path, flavor="stream", compress=True, pages="all")
    filtered_tables_data = []
    for table in abc:
        df = table.df
        if df.applymap(lambda cell: "Wagon Details" in cell).any().any():
            filtered_tables_data.extend(df.to_dict(orient="records"))

    return filtered_tables_data


# ---------------------------------- Mahabal data start ----------------------------------------

def mahabal_rr_lot(pdf_path,page):
    try:
        flattened_table = []
        rake_and_lot = {}
        rake_and_lot["rake"] = None
        rake_and_lot["rr"] = None
        rake_and_lot["lot"] = None
        rake_and_lot["do"] = None

        area = [224.64, 174.24, 349.92, 293.76]
        tables = tabula.read_pdf(
            pdf_path,
            guess=False,
            lattice=False,
            stream=True,
            multiple_tables=False,
            area=area,
            pages=str(page),
        )

        for table in tables:

            flattened_table = [
                str(item)
                for sublist in table.values.tolist()
                for item in sublist
                if not (isinstance(item, float) and math.isnan(item))
            ]

            joined_string = " ".join(flattened_table)
            if "Rake" and "RR" in joined_string:
                match = re.search(r"Rake\s+(\d+)\s+RR\s+(\d+)", joined_string)
                if match:
                    rake = match.group(1)
                    rr = match.group(2)
                    rake_and_lot["rake"] = rake
                    rake_and_lot["rr"] = rr

            if "Lot" and "DO" in joined_string:
                match = re.search(r"Lot-\s?(\d+)\s+DO\s?(\d+)", joined_string)
                if match:
                    lot = match.group(1)
                    do = match.group(2)
                    rake_and_lot["lot"] = lot
                    rake_and_lot["do"] = do

    except Exception as e:
        print(e)
    return rake_and_lot


def mahabal_ulr(pdf_path,page):
    try:
        ulrtable = []
        date_and_report = {}
        date_and_report["date"] = None
        date_and_report["report_no"] = None

        ulr_area = [129.6, 174.24, 162, 553.68]
        tables = tabula.read_pdf(
            pdf_path,
            guess=False,
            lattice=False,
            stream=True,
            multiple_tables=False,
            area=ulr_area,
            pages=str(page),
        )

        for table in tables:
            report_no = ""
            date = ""

            for col in table.columns:
                if not col.startswith("Unnamed"):
                    ulrtable.append(col)

            for index, row in table.iterrows():
                if "Report No." in row.values:
                    for item in row.values:
                        if "Report No." in item:
                            report_no = item.split(":")[1].strip()
                if "Date" in row.values:
                    for item in row.values:
                        if "Date" in item:
                            date = item.split(":")[1].strip()

            ulrtable.append(report_no)
            ulrtable.append(date)

            for sublist in table.values.tolist():
                ulrtable.extend(sublist)

        joined_string = " ".join(str(v) for v in ulrtable)
        date_match = re.search(
            r"Date:\s*([^\d]*)(\d{2})[^\d]*(\d{2})[^\d]*(\d{4})", joined_string
        )
        report_no_match = re.search(
            r"Report\s+No\.\s*:\s*([A-Z\s-]+-\d+)", joined_string
        )
        if date_match:
            day = date_match.group(2)
            month = date_match.group(3)
            year = date_match.group(4)
            date = f"{day}.{month}.{year}"
            date_and_report["date"] = date

        if report_no_match:
            report_no = report_no_match.group(1)
            date_and_report["report_no"] = report_no

    except Exception as e:
        print(e)
    return date_and_report


def mahabal_parameter(pdf_path,page):
    try:
        para_table = []
        coal_data = {}
        total_moisture_adb = None
        total_moisture_arb = None
        moisture_inherent_adb = None
        moisture_inherent_arb = None
        ash_adb = None
        ash_arb = None
        volatile_adb = None
        volatile_arb = None
        fixed_carbon_adb = None
        fixed_carbon_arb = None
        gross_calorific_adb = None
        gross_calorific_arb = None

        area = [362.16, 64.8, 546.4, 552.96]
        tables = tabula.read_pdf(
            pdf_path,
            guess=True,
            stream=True,
            multiple_tables=False,
            area=area,
            pages=str(page),
        )

        for table in tables:
            para_table = [
                item
                for sublist in table.values.tolist()
                for item in sublist
                if not (isinstance(item, float) and math.isnan(item))
            ]

            joined_string = " ".join(para_table)
        para_table = list(joined_string.split(" "))

        if "Moisture" in para_table:
            total_moisture_index = para_table.index("Moisture")
            total_moisture_adb = para_table[total_moisture_index + 2]
            total_moisture_arb = para_table[total_moisture_index + 3]
        if "(Inherent)" in para_table:
            moisture_inherent_index = para_table.index("(Inherent)")
            moisture_inherent_adb = para_table[moisture_inherent_index + 2]
            moisture_inherent_arb = para_table[moisture_inherent_index + 3]
        if "Ash" in para_table:
            ash_index = para_table.index("Ash")
            ash_adb = para_table[ash_index + 2]
            ash_arb = para_table[ash_index + 3]
        if "Matter" in para_table:
            volatile_index = para_table.index("Matter")
            volatile_adb = para_table[volatile_index + 2]
            volatile_arb = para_table[volatile_index + 3]
        if "Carbon" in para_table:
            fixed_carbon_index = para_table.index("Carbon")
            fixed_carbon_adb = para_table[fixed_carbon_index + 2]
            fixed_carbon_arb = para_table[fixed_carbon_index + 3]
        if "Value" in para_table:
            gross_calorific_index = para_table.index("Value")
            gross_calorific_adb = para_table[gross_calorific_index + 2]
            gross_calorific_arb = para_table[gross_calorific_index + 3]

        coal_data["total_moisture_adb"] = total_moisture_adb
        coal_data["total_moisture_arb"] = total_moisture_arb
        coal_data["moisture_inherent_adb"] = moisture_inherent_adb
        coal_data["moisture_inherent_arb"] = moisture_inherent_arb
        coal_data["ash_adb"] = ash_adb
        coal_data["ash_arb"] = ash_arb
        coal_data["volatile_adb"] = volatile_adb
        coal_data["volatile_arb"] = volatile_arb
        coal_data["fixed_carbon_adb"] = fixed_carbon_adb
        coal_data["fixed_carbon_arb"] = fixed_carbon_arb
        coal_data["gross_calorific_adb"] = gross_calorific_adb
        coal_data["gross_calorific_arb"] = gross_calorific_arb

    except Exception as e:
        print(e)

    return coal_data
    

# @router.post("/pdf_data_upload", tags=["Extra"])
# async def extract_data_from_mahabal_pdf(response: Response, pdf_upload: Optional[UploadFile] = File(None)):
#     try:
#         if pdf_upload is None:
#             return {"error": "No file uploaded"}
#         contents = await pdf_upload.read()

#         # Check if the file is empty
#         if not contents:
#             return {"error": "Uploaded file is empty"}
        
#         # Verify file format (PDF)
#         if not pdf_upload.filename.endswith('.pdf'):
#             return {"error": "Uploaded file is not a PDF"}

#         file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
#         target_directory = f"static_server/gmr_ai/{file}"
#         os.umask(0)
#         os.makedirs(target_directory, exist_ok=True, mode=0o777)

#         file_extension = pdf_upload.filename.split(".")[-1]
#         file_name = f'pdf_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.{file_extension}'
#         full_path = os.path.join(os.getcwd(), target_directory, file_name)
#         with open(full_path, "wb") as file_object:
#             file_object.write(contents)
        
#         pdfReader = PyPDF2.PdfReader(full_path)
#         totalPages = len(pdfReader.pages)
        
#         listData = []
#         id = None
        
#         list_data = []
#         for page in range(1,totalPages+1):
#             rrLot = mahabal_rr_lot(full_path, page)
#             ulrData = mahabal_ulr(full_path, page)
#             parameterData = mahabal_parameter(full_path, page)

#             console_logger.debug(rrLot)
#             console_logger.debug(ulrData)
#             console_logger.debug(parameterData)

#             api_data = {
#                 "Total_Moisture_%": None,
#                 "Inherent_Moisture_(Adb)_%": None,
#                 "Ash_(Adb)_%": None,
#                 "Volatile_Matter_(Adb)_%": None,
#                 "Gross_Calorific_Value_(Adb)_Kcal/Kg": None,
#                 "Ash_(Arb)_%": None,
#                 "Volatile_Matter_(Arb)_%": None,
#                 "Fixed_Carbon_(Arb)_%": None,
#                 "Gross_Calorific_Value_(Arb)_Kcal/Kg": None,
#                 "DO_No": None,
#                 "Lot_No": None, 
#                 "RR_No": None,
#             }
#             pdf_data = {
#                 "Third_Party_Total_Moisture_%": None,
#                 "Third_Party_Total_Moisture(adb)_%": None,
#                 "Third_Party_Inherent_Moisture_(Adb)_%": None,
#                 "Third_Party_Inherent_Moisture_(Arb)_%": None,
#                 "Third_Party_Ash_(Adb)_%": None,
#                 "Third_Party_Volatile_Matter_(Adb)_%": None,
#                 "Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg": None,
#                 "Third_Party_Ash_(Arb)_%": None,
#                 "Third_Party_Volatile_Matter_(Arb)_%": None,
#                 "Third_Party_Fixed_Carbon_(Arb)_%": None,
#                 "Third_Party_Fixed_Carbon_(Adb)_%": None,
#                 "Third_Party_Gross_Calorific_Value_(Arb)_Kcal/Kg": None,
#                 "Third_Party_Report_No": None,
#             }

        
#             # rail data
#             if rrLot != None and parameterData != None:
#                 if rrLot.get("rake") != None and rrLot.get("rr") != None:
#                     try:
#                         # coalTrainData = CoalTestingTrain.objects.get(rake_no=f"{int(rrLot.get('rake'))}", rrNo=rrLot.get("rr"))
#                         querysetTrain = CoalTestingTrain.objects.filter(rake_no=f"{int(rrLot.get('rake'))}", rrNo=rrLot.get("rr"))
#                         if querysetTrain.count() == 0:
#                             console_logger.debug("no data available")
#                             continue
#                         if querysetTrain.count() == 1:
#                             coalTrainData = querysetTrain.get()
#                         else:
#                             coalTrainData = querysetTrain.first() 
#                         id = str(coalTrainData.id)
#                         if coalTrainData.rrNo:
#                             api_data["RR_No"] = coalTrainData.rrNo
#                         if coalTrainData.rake_no:
#                             api_data["Lot_No"] = coalTrainData.rake_no
#                         for single_data in coalTrainData.parameters:
#                             if single_data.get("parameter_Name") == "Total_Moisture":
#                                 api_data["Total_Moisture_%"] = single_data.get("val1")
#                             elif single_data.get("parameter_Name") == "Inherent_Moisture_(Adb)":
#                                 api_data["Inherent_Moisture_(Adb)_%"] = single_data.get("val1")
#                             elif single_data.get("parameter_Name") == "Ash_(Adb)":
#                                 api_data["Ash_(Adb)_%"] = single_data.get("val1")
#                             elif single_data.get("parameter_Name") == "Volatile_Matter_(Adb)":
#                                 api_data["Volatile_Matter_(Adb)_%"] = single_data.get("val1")
#                             elif single_data.get("parameter_Name") == "Gross_Calorific_Value_(Adb)":
#                                 api_data["Gross_Calorific_Value_(Adb)_Kcal/Kg"] = single_data.get(
#                                     "val1"
#                                 )
#                             elif single_data.get("parameter_Name") == "Ash_(Arb)":
#                                 api_data["Ash_(Arb)_%"] = single_data.get("val1")
#                             elif single_data.get("parameter_Name") == "Volatile_Matter_(Arb)":
#                                 api_data["Volatile_Matter_(Arb)_%"] = single_data.get("val1")
#                             elif single_data.get("parameter_Name") == "Fixed_Carbon_(Arb)":
#                                 api_data["Fixed_Carbon_(Arb)_%"] = single_data.get("val1")
#                             elif single_data.get("parameter_Name") == "Gross_Calorific_Value_(Arb)":
#                                 api_data["Gross_Calorific_Value_(Arb)_Kcal/Kg"] = single_data.get(
#                                     "val1"
#                                 )
#                     except DoesNotExist as e:
#                         pass
                    
#                     if ulrData.get("report_no"):
#                         pdf_data["Third_Party_Report_No"]= ulrData.get("report_no")
                    
#                     for key, value in parameterData.items():
#                         if value != '-':
#                             if key == 'total_moisture_adb':
#                                 pdf_data["Third_Party_Total_Moisture(adb)_%"] = value
#                             elif key == 'total_moisture_arb':
#                                 pdf_data["Third_Party_Total_Moisture_%"] = value
#                             elif key == 'moisture_inherent_adb':
#                                 pdf_data["Third_Party_Inherent_Moisture_(Adb)_%"] = value
#                             elif key == 'moisture_inherent_arb':
#                                 pdf_data["Third_Party_Inherent_Moisture_(Arb)_%"] = value
#                             elif key == "ash_adb":
#                                 pdf_data["Third_Party_Ash_(Adb)_%"] = value
#                             elif key == "ash_arb":
#                                 pdf_data["Third_Party_Ash_(Arb)_%"] = value
#                             elif key == "volatile_adb":
#                                 pdf_data["Third_Party_Volatile_Matter_(Adb)_%"] = value
#                             elif key == "volatile_arb":
#                                 pdf_data["Third_Party_Volatile_Matter_(Arb)_%"] = value
#                             elif key == "fixed_carbon_adb":
#                                 pdf_data["Third_Party_Fixed_Carbon_(Adb)_%"] = value
#                             elif key == "fixed_carbon_arb":
#                                 pdf_data["Third_Party_Fixed_Carbon_(Arb)_%"] = value
#                             elif key == "gross_calorific_adb":
#                                 pdf_data["Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg"] = value
#                             elif key == "gross_calorific_arb":
#                                 pdf_data["Third_Party_Gross_Calorific_Value_(Arb)_Kcal/Kg"] = value
#                     # dataDict = {"id": id, "api_data": api_data, "pdf_data": pdf_data}
#                     list_data.append({"id": id, "api_data": api_data, "pdf_data": pdf_data})
#                     # return list_data 
#                 # road data
#                 elif rrLot.get("lot") != None and rrLot.get("do") != None:
#                     try:
#                         # queryset = CoalTesting.objects.get(rake_no=f'LOT-{rrLot.get("lot")}', rrNo=rrLot.get("do"))
#                         queryset = CoalTesting.objects.filter(rake_no=f'LOT-{rrLot.get("lot")}', rrNo=rrLot.get("do"))
#                         if queryset.count() == 0:
#                             console_logger.debug("no data available")
#                             continue
#                         if queryset.count() == 1:
#                             coalRoadData = queryset.get()
#                         else:
#                             coalRoadData = queryset.first() 
#                         id = str(coalRoadData.id)
#                         if coalRoadData.rrNo:
#                             api_data["DO_No"] = coalRoadData.rrNo
#                         if coalRoadData.rake_no:
#                             api_data["Lot_No"] = coalRoadData.rake_no
#                         for single_data in coalRoadData.parameters:
#                             if single_data.get("parameter_Name") == "Total_Moisture":
#                                 api_data["Total_Moisture_%"] = single_data.get("val1")
#                             elif single_data.get("parameter_Name") == "Inherent_Moisture_(Adb)":
#                                 api_data["Inherent_Moisture_(Adb)_%"] = single_data.get("val1")
#                             elif single_data.get("parameter_Name") == "Ash_(Adb)":
#                                 api_data["Ash_(Adb)_%"] = single_data.get("val1")
#                             elif single_data.get("parameter_Name") == "Volatile_Matter_(Adb)":
#                                 api_data["Volatile_Matter_(Adb)_%"] = single_data.get("val1")
#                             elif single_data.get("parameter_Name") == "Gross_Calorific_Value_(Adb)":
#                                 api_data["Gross_Calorific_Value_(Adb)_Kcal/Kg"] = single_data.get(
#                                     "val1"
#                                 )
#                             elif single_data.get("parameter_Name") == "Ash_(Arb)":
#                                 api_data["Ash_(Arb)_%"] = single_data.get("val1")
#                             elif single_data.get("parameter_Name") == "Volatile_Matter_(Arb)":
#                                 api_data["Volatile_Matter_(Arb)_%"] = single_data.get("val1")
#                             elif single_data.get("parameter_Name") == "Fixed_Carbon_(Arb)":
#                                 api_data["Fixed_Carbon_(Arb)_%"] = single_data.get("val1")
#                             elif single_data.get("parameter_Name") == "Gross_Calorific_Value_(Arb)":
#                                 api_data["Gross_Calorific_Value_(Arb)_Kcal/Kg"] = single_data.get(
#                                     "val1"
#                                 )
#                     except DoesNotExist as e:
#                         pass

#                     if ulrData.get("report_no"):
#                         pdf_data["Third_Party_Report_No"]= ulrData.get("report_no")
                    
#                     for key, value in parameterData.items():
#                         if value != '-':
#                             if key == 'total_moisture_adb':
#                                 pdf_data["Third_Party_Total_Moisture(adb)_%"] = value
#                             elif key == 'total_moisture_arb':
#                                 pdf_data["Third_Party_Total_Moisture_%"] = value
#                             elif key == 'moisture_inherent_adb':
#                                 pdf_data["Third_Party_Inherent_Moisture_(Adb)_%"] = value
#                             elif key == 'moisture_inherent_arb':
#                                 pdf_data["Third_Party_Inherent_Moisture_(Arb)_%"] = value
#                             elif key == "ash_adb":
#                                 pdf_data["Third_Party_Ash_(Adb)_%"] = value
#                             elif key == "ash_arb":
#                                 pdf_data["Third_Party_Ash_(Arb)_%"] = value
#                             elif key == "volatile_adb":
#                                 pdf_data["Third_Party_Volatile_Matter_(Adb)_%"] = value
#                             elif key == "volatile_arb":
#                                 pdf_data["Third_Party_Volatile_Matter_(Arb)_%"] = value
#                             elif key == "fixed_carbon_adb":
#                                 pdf_data["Third_Party_Fixed_Carbon_(Adb)_%"] = value
#                             elif key == "fixed_carbon_arb":
#                                 pdf_data["Third_Party_Fixed_Carbon_(Arb)_%"] = value
#                             elif key == "gross_calorific_adb":
#                                 pdf_data["Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg"] = value
#                             elif key == "gross_calorific_arb":
#                                 pdf_data["Third_Party_Gross_Calorific_Value_(Arb)_Kcal/Kg"] = value
#                     # dataDict = {"id": id, "api_data": api_data, "pdf_data": pdf_data}
#                     list_data.append({"id": id, "api_data": api_data, "pdf_data": pdf_data})
#             else:
#                 console_logger.debug("data not found")  
#         return list_data      
#     except DoesNotExist as e:
#         console_logger.debug("No matching object found.")
#         return HTTPException(status_code="404", detail="No matching object found in db")
#     except MultipleObjectsReturned:
#         pass
#     #     console_logger.debug("multiple entry found for single rrno/dono")
#     #     return HTTPException(status_code="400", detail="multiple entry found for single rrno/dono")
#     except Exception as e:
#         console_logger.debug("----- Excel error -----", e)
#         response.status_code = 400
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#         console_logger.debug(
#             "Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno)
#         )
#         return e


def extract_text_by_page(pdf_path):
    with open(pdf_path, "rb") as fh:
        for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):
            resource_manager = PDFResourceManager()
            fake_file_handle = StringIO()
            converter = TextConverter(
                resource_manager, fake_file_handle, laparams=LAParams()
            )
            page_interpreter = PDFPageInterpreter(resource_manager, converter)
            page_interpreter.process_page(page)
            text = fake_file_handle.getvalue()
            yield text
            converter.close()
            fake_file_handle.close()


def extract_report_no(text):
    pattern = re.compile(r"Report\s+No\.?\s*:?\s*((?:ULR\s+No\.?\s*)?(\S+))")
    match = pattern.search(text)
    if match:
        full_match = match.group(1)
        potential_report_no = match.group(2)
        if "ULR No" in full_match:
            return potential_report_no
        elif not potential_report_no.startswith("ULR"):
            return potential_report_no
    return None


def extract_and_standardize_date(text):
    # Patterns to match various date formats
    patterns = [
        r"Date\s*:?\s*(\d{1,2}[\s.]\d{1,2}[\s.]\d{4})",  # Matches "15 06 2024" and "05.12.2010"
        r"Date\s*:?\s*(\d{2}\.\d{2}\.\d{4})",  # Matches "15.06.2024"
    ]

    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            date_str = match.group(1)
            # Standardize the date format
            parts = re.split(r"[\s.]", date_str)
            return f"{parts[0].zfill(2)}.{parts[1].zfill(2)}.{parts[2]}"

    return None


def extract_data_from_text(text):
    try:
        # print(text)
        extracted_data = {}
        patterns = {
            # "date": re.compile(r"Date\.?\s*:?\s*(\d{2}\.\d{2}\.\d{4})"),
            "lot": re.compile(r"Lot\-\s*(\d+)"),
            "do": re.compile(r"D[O0]\s+(\d+)"),
            "rake": re.compile(r"Rake\s+(?:No\.?\s+)?(\d+)", re.IGNORECASE),
            "rr": re.compile(r"RR\s*.*?(\d+)", re.DOTALL),
        }

        result_section_pattern = r"Discipline:.*?(?=END\s+OF\s+REPORT)"
        result_section = re.search(result_section_pattern, text, re.DOTALL)

        extracted_data["report_no"] = extract_report_no(text)
        extracted_data["date"] = extract_and_standardize_date(text)

        for key, pattern in patterns.items():
            match = pattern.search(text)
            if match:
                extracted_data[key] = match.group(1)
                if result_section:
                    results_text = result_section.group(0)

                    exclude_pattern = r"IS\s+\d{4}\s*\(Part\s+[IVXLCDM|]+\)\s*:\s*\d{4}"
                    cleaned_text = re.sub(exclude_pattern, "", results_text)

                    values_pattern = r"\b\d+\.\d+|\b\d+\b"
                    values = re.findall(values_pattern, cleaned_text)

                    filtered_values = [
                        value
                        for value in values
                        if value not in ["1", "2", "3", "4", "5", "6"]
                    ]
                    new_filter = filtered_values[:10]
                    extracted_data["total_moisture_adb"] = "-"
                    try:
                        extracted_data["moisture_inherent_adb"] = new_filter[0]
                    except IndexError:
                        extracted_data["moisture_inherent_adb"] = 0
                    try:
                        extracted_data["ash_adb"] = new_filter[1]
                    except IndexError:
                        extracted_data["ash_adb"] = 0
                    try:
                        extracted_data["volatile_adb"] = new_filter[2]
                    except IndexError:
                        extracted_data["volatile_adb"] = 0
                    try:
                        extracted_data["fixed_carbon_adb"] = new_filter[3]
                    except IndexError:
                        extracted_data["fixed_carbon_adb"] = 0
                    try:
                        extracted_data["gross_calorific_adb"] = new_filter[4]
                    except IndexError:
                        extracted_data["gross_calorific_adb"] = 0
                    try:
                        extracted_data["total_moisture_arb"] = new_filter[5]
                    except IndexError:
                        extracted_data["total_moisture_arb"] = 0
                    extracted_data["moisture_inherent_arb"] = "-"
                    try:
                        extracted_data["ash_arb"] = new_filter[6]
                    except IndexError:
                        extracted_data["ash_arb"] = 0
                    try:
                        extracted_data["volatile_arb"] = new_filter[7]
                    except IndexError:
                        extracted_data["volatile_arb"] = 0
                    try:
                        extracted_data["fixed_carbon_arb"] = new_filter[8]
                    except IndexError:
                        extracted_data["fixed_carbon_arb"] = 0
                    try:
                        extracted_data["gross_calorific_arb"] = new_filter[9]
                    except IndexError:
                        extracted_data["gross_calorific_arb"] = 0

        return extracted_data
    except Exception as e:
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/pdf_data_upload", tags=["Extra"])
async def extract_data_from_mahabal_pdf(response: Response, pdf_upload: Optional[UploadFile] = File(None)):
    try:
        if pdf_upload is None:
            return {"error": "No file uploaded"}
        contents = await pdf_upload.read()

        # Check if the file is empty
        if not contents:
            return {"error": "Uploaded file is empty"}
        
        # Verify file format (PDF)
        if not pdf_upload.filename.endswith('.pdf'):
            return {"error": "Uploaded file is not a PDF"}

        file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
        target_directory = f"static_server/gmr_ai/{file}"
        os.umask(0)
        os.makedirs(target_directory, exist_ok=True, mode=0o777)

        file_extension = pdf_upload.filename.split(".")[-1]
        file_name = f'pdf_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.{file_extension}'
        full_path = os.path.join(os.getcwd(), target_directory, file_name)
        with open(full_path, "wb") as file_object:
            file_object.write(contents)
        
        pdfReader = PyPDF2.PdfReader(full_path)
        totalPages = len(pdfReader.pages)

        extracted_data_per_page = []
        for page_number, text in enumerate(extract_text_by_page(full_path), start=1):
            extracted_data = extract_data_from_text(text)
            if extracted_data:
                extracted_data["page"] = page_number
                extracted_data_per_page.append(extracted_data)
        listData = []
        for data in extracted_data_per_page:
            dictData = {}
            # dictData[f"page_{data['page']}"] = {}
            print(f"Page {data['page']}:")
            for key, value in data.items():
                if key != "page":
                    dictData[key] = value
            listData.append(dictData)

        console_logger.debug(listData)

        mainListData = []
        for single_list in listData:
            api_data = {
                "Total_Moisture_%": None,
                "Inherent_Moisture_(Adb)_%": None,
                "Ash_(Adb)_%": None,
                "Volatile_Matter_(Adb)_%": None,
                "Gross_Calorific_Value_(Adb)_Kcal/Kg": None,
                "Ash_(Arb)_%": None,
                "Volatile_Matter_(Arb)_%": None,
                "Fixed_Carbon_(Arb)_%": None,
                "Gross_Calorific_Value_(Arb)_Kcal/Kg": None,
                "DO_No": None,
                "Lot_No": None, 
                "RR_No": None,
            }
            pdf_data = {
                "Third_Party_Total_Moisture_%": None,
                "Third_Party_Total_Moisture(adb)_%": None,
                "Third_Party_Inherent_Moisture_(Adb)_%": None,
                "Third_Party_Inherent_Moisture_(Arb)_%": None,
                "Third_Party_Ash_(Adb)_%": None,
                "Third_Party_Volatile_Matter_(Adb)_%": None,
                "Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg": None,
                "Third_Party_Ash_(Arb)_%": None,
                "Third_Party_Volatile_Matter_(Arb)_%": None,
                "Third_Party_Fixed_Carbon_(Arb)_%": None,
                "Third_Party_Fixed_Carbon_(Adb)_%": None,
                "Third_Party_Gross_Calorific_Value_(Arb)_Kcal/Kg": None,
                "Third_Party_Report_No": None,
            }

            # rail data
            if "rake" in single_list:
                try:
                    # coalTrainData = CoalTestingTrain.objects.get(rake_no=f"{int(rrLot.get('rake'))}", rrNo=rrLot.get("rr"))
                    # querysetTrain = CoalTestingTrain.objects.filter(rake_no=f"{int(single_list.get('rake'))}", rrNo=single_list.get('rr'))
                    querysetTrain = RecieptCoalQualityAnalysis.objects.filter(sample_no=single_list.get("rake"), sample_id=single_list.get("rr"))
                    if querysetTrain.count() == 0:
                        console_logger.debug("no data available")
                        continue
                    if querysetTrain.count() == 1:
                        coalTrainData = querysetTrain.get()
                    else:
                        coalTrainData = querysetTrain.first() 
                    id = str(coalTrainData.id)
                    api_data["RR_No"] = coalTrainData.sample_id
                    api_data["Lot_No"] = coalTrainData.sample_no
                    if "plant_arb_tm" in coalTrainData:
                        api_data["Total_Moisture_%"] = coalTrainData.plant_arb_tm
                    if "plant_adb_im" in coalTrainData:
                        api_data["Inherent_Moisture_(Adb)_%"] = coalTrainData.plant_adb_im
                    if "plant_adb_ash" in coalTrainData:
                        api_data["Ash_(Adb)_%"] = coalTrainData.plant_adb_ash
                    if "plant_adb_vm" in coalTrainData:
                        api_data["Volatile_Matter_(Adb)_%"] = coalTrainData.plant_adb_vm
                    if "plant_adb_gcv" in coalTrainData:
                        api_data["Gross_Calorific_Value_(Adb)_Kcal/Kg"] = coalTrainData.plant_adb_gcv
                    if "plant_arb_ash" in coalTrainData:
                        api_data["Ash_(Arb)_%"] = coalTrainData.plant_arb_ash
                    if "plant_arb_vm" in coalTrainData:
                        api_data["Volatile_Matter_(Arb)_%"] = coalTrainData.plant_arb_vm
                    if "plant_arb_fc" in coalTrainData:
                        api_data["Fixed_Carbon_(Arb)_%"] = coalTrainData.plant_arb_fc
                    if "plant_arb_gcv" in coalTrainData:
                        api_data["Gross_Calorific_Value_(Arb)_Kcal/Kg"] = coalTrainData.plant_arb_gcv
                except DoesNotExist as e:
                    continue
                
                if single_list.get("report_no"):
                    pdf_data["Third_Party_Report_No"]= single_list.get("report_no")
                if 'total_moisture_adb' in single_list:
                    pdf_data["Third_Party_Total_Moisture(adb)_%"] = single_list["total_moisture_adb"]
                if 'total_moisture_arb' in single_list:
                    pdf_data["Third_Party_Total_Moisture_%"] = single_list["total_moisture_arb"]
                if 'moisture_inherent_adb' in single_list:
                    pdf_data["Third_Party_Inherent_Moisture_(Adb)_%"] = single_list["moisture_inherent_adb"]
                if 'moisture_inherent_arb' in single_list:
                    pdf_data["Third_Party_Inherent_Moisture_(Arb)_%"] = single_list["moisture_inherent_arb"]
                if "ash_adb" in single_list:
                    pdf_data["Third_Party_Ash_(Adb)_%"] = single_list["ash_adb"]
                if "ash_arb" in single_list:
                    pdf_data["Third_Party_Ash_(Arb)_%"] = single_list["ash_arb"]
                if "volatile_adb" in single_list:
                    pdf_data["Third_Party_Volatile_Matter_(Adb)_%"] = single_list["volatile_adb"]
                if "volatile_arb" in single_list:
                    pdf_data["Third_Party_Volatile_Matter_(Arb)_%"] = single_list["volatile_arb"]
                if "fixed_carbon_adb" in single_list:
                    pdf_data["Third_Party_Fixed_Carbon_(Adb)_%"] = single_list["fixed_carbon_adb"]
                if "fixed_carbon_arb" in single_list:
                    pdf_data["Third_Party_Fixed_Carbon_(Arb)_%"] = single_list["fixed_carbon_arb"]
                if "gross_calorific_adb" in single_list:
                    pdf_data["Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg"] = single_list["gross_calorific_adb"]
                if "gross_calorific_arb" in single_list:
                    pdf_data["Third_Party_Gross_Calorific_Value_(Arb)_Kcal/Kg"] = single_list["gross_calorific_arb"]
                # dataDict = {"id": id, "api_data": api_data, "pdf_data": pdf_data}
                mainListData.append({"id": id, "api_data": api_data, "pdf_data": pdf_data})
            #     # return list_data 
                # console_logger.debug(mainListData)
            # road data
            elif "lot" in single_list:
                try:
                    # queryset = CoalTesting.objects.get(rake_no=f'LOT-{rrLot.get("lot")}', rrNo=rrLot.get("do"))
                    queryset = RecieptCoalQualityAnalysis.objects.filter(sample_no=single_list.get("lot"), sample_id=single_list.get("do"))
                    if queryset.count() == 0:
                        console_logger.debug("no data available")
                        continue
                    if queryset.count() == 1:
                        coalRoadData = queryset.get()
                    else:
                        coalRoadData = queryset.first() 
                    id = str(coalRoadData.id)
                    api_data["DO_No"] = coalRoadData.sample_id
                    api_data["Lot_No"] = coalRoadData.sample_no
                    # for single_data in coalRoadData.parameters:
                    if "plant_arb_tm" in coalRoadData:
                        api_data["Total_Moisture_%"] = coalRoadData.plant_arb_tm
                    if "plant_adb_im" in coalRoadData:
                        api_data["Inherent_Moisture_(Adb)_%"] = coalRoadData.plant_adb_im
                    if "plant_adb_ash" in coalRoadData:
                        api_data["Ash_(Adb)_%"] = coalRoadData.plant_adb_ash
                    if "plant_adb_vm" in coalRoadData:
                        api_data["Volatile_Matter_(Adb)_%"] = coalRoadData.plant_adb_vm
                    if "plant_adb_gcv" in coalRoadData:
                        api_data["Gross_Calorific_Value_(Adb)_Kcal/Kg"] = coalRoadData.plant_adb_gcv
                    if "plant_arb_ash" in coalRoadData:
                        api_data["Ash_(Arb)_%"] = coalRoadData.plant_arb_ash
                    if "plant_arb_vm" in coalRoadData:
                        api_data["Volatile_Matter_(Arb)_%"] = coalRoadData.plant_arb_vm
                    if "plant_arb_fc" in coalRoadData:
                        api_data["Fixed_Carbon_(Arb)_%"] = coalRoadData.plant_arb_fc
                    if "plant_arb_gcv" in coalRoadData:
                        api_data["Gross_Calorific_Value_(Arb)_Kcal/Kg"] = coalRoadData.plant_arb_gcv
                except DoesNotExist as e:
                    console_logger.debug("data not there")
                    continue

                if "report_no" in single_list:
                    pdf_data["Third_Party_Report_No"]= single_list.get("report_no")
                if 'total_moisture_adb' in single_list:
                    pdf_data["Third_Party_Total_Moisture(adb)_%"] = single_list["total_moisture_adb"]
                if 'total_moisture_arb' in single_list:
                    pdf_data["Third_Party_Total_Moisture_%"] = single_list["total_moisture_arb"]
                if 'moisture_inherent_adb' in single_list:
                    pdf_data["Third_Party_Inherent_Moisture_(Adb)_%"] = single_list["moisture_inherent_adb"]
                if 'moisture_inherent_arb' in single_list:
                    pdf_data["Third_Party_Inherent_Moisture_(Arb)_%"] = single_list["moisture_inherent_arb"]
                if "ash_adb" in single_list:
                    pdf_data["Third_Party_Ash_(Adb)_%"] = single_list["ash_adb"]
                if "ash_arb" in single_list:
                    pdf_data["Third_Party_Ash_(Arb)_%"] = single_list["ash_arb"]
                if "volatile_adb" in single_list:
                    pdf_data["Third_Party_Volatile_Matter_(Adb)_%"] = single_list["volatile_adb"]
                if "volatile_arb" in single_list:
                    pdf_data["Third_Party_Volatile_Matter_(Arb)_%"] = single_list["volatile_arb"]
                if "fixed_carbon_adb" in single_list:
                    pdf_data["Third_Party_Fixed_Carbon_(Adb)_%"] = single_list["fixed_carbon_adb"]
                if "fixed_carbon_arb" in single_list:
                    pdf_data["Third_Party_Fixed_Carbon_(Arb)_%"] = single_list["fixed_carbon_arb"]
                if "gross_calorific_adb" in single_list:
                    pdf_data["Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg"] = single_list["gross_calorific_adb"]
                if "gross_calorific_arb" in single_list:
                    pdf_data["Third_Party_Gross_Calorific_Value_(Arb)_Kcal/Kg"] = single_list["gross_calorific_arb"]
                # dataDict = {"id": id, "api_data": api_data, "pdf_data": pdf_data}
                mainListData.append({"id": id, "api_data": api_data, "pdf_data": pdf_data})
                # console_logger.debug(mainListData)
        if mainListData:
            return mainListData
        raise HTTPException(status_code=404, detail="No data available")
    except DoesNotExist as e:
        console_logger.debug("No matching object found.")
        raise HTTPException(status_code="404", detail="No matching object found in db")
    except MultipleObjectsReturned:
        pass
    #     console_logger.debug("multiple entry found for single rrno/dono")
    #     return HTTPException(status_code="400", detail="multiple entry found for single rrno/dono")
    except Exception as e:
        console_logger.debug(f"----- Excel error ----- {e}")
        response.status_code = 404
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        # console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug(
            "Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno)
        )
        return e



# ---------------------------------- Coal Consumption ----------------------------------------


consumption_headers = {
'ClientToken': 'Administrator',
'Content-Type': 'application/json'}

# @router.get("/load_historian_data", tags=["Coal Consumption"])                                    # coal consumption
# def extract_historian_data(start_date: Optional[str] = None, end_date: Optional[str] = None):
#     success = False
#     try:
#         global consumption_headers, proxies
#         entry = UsecaseParameters.objects.first()
#         historian_ip = entry.Parameters.get('gmr_api', {}).get('roi1', {}).get('Coal Consumption IP') if entry else None
#         historian_timer = entry.Parameters.get('gmr_api', {}).get('roi1', {}).get('Coal Consumption Duration') if entry else None

#         console_logger.debug(f"---- Coal Consumption IP ----        {historian_ip}")

#         if not end_date:
#             end_date = (datetime.datetime.now(IST).replace(minute=00,second=00,microsecond=00).strftime("%Y-%m-%dT%H:%M:%S"))
#         if not start_date:
#             start_date = (datetime.datetime.now(IST).replace(minute=0,second=0,microsecond=0) - datetime.timedelta(hours=1)).strftime("%Y-%m-%dT%H:%M:%S")

#         console_logger.debug(f" --- Consumption Start Date --- {start_date}")
#         console_logger.debug(f" --- Consumption End Date --- {end_date}")

#         payload = json.dumps({
#                     "StartTime": start_date,
#                     "EndTime": end_date, 
#                     "RetrievalType": "Aggregate", 
#                     "RetrievalMode": "History", 
#                     "TagID": ["2","3538","16","3536"],
#                     "RetrieveBy": "ID"
#                     })
        
#         consumption_url = f"http://{historian_ip}/api/REST/HistoryData/LoadTagData"
#         # consumption_url = "http://10.100.12.28:8093/api/REST/HistoryData/LoadTagData"
#         try:
#             response = requests.request("POST", url=consumption_url, headers=consumption_headers, data=payload, proxies=proxies)
#             data = json.loads(response.text)

#             for item in data["Data"]:
#                 tag_id = item["Data"]["TagID"]
#                 sum = item["Data"]["SUM"]
#                 avg = item["Data"]["AVG"]
#                 created_date = item["Data"]["CreatedDate"]

#                 if tag_id in [16, 3538]:
#                     sum_value = str(round(int(float(sum)) / 1000 , 2))
#                 elif tag_id in [2, 3536]:
#                     sum_value = avg

#                 if not Historian.objects.filter(tagid=tag_id, created_date=created_date):
#                     Historian(
#                         tagid = tag_id,
#                         sum = sum_value,
#                         created_date = created_date,
#                         ID=Historian.objects.count() + 1
#                     ).save()
#                 else:
#                     console_logger.debug("data already exists in historian")
                
#             success = "completed"
#         except requests.exceptions.Timeout:
#             console_logger.debug("Request timed out!")
#         except requests.exceptions.ConnectionError:
#             console_logger.debug("Connection error")
    
#     except Exception as e:
#         success = False
#         console_logger.debug("----- Coal Testing Error -----",e)
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#         console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#         success = e
        
#     finally:
#         console_logger.debug(f"success:{success}")
#         SchedulerResponse("save consumption data", f"{success}")
#         return {"message" : "Successful"} 


@router.get("/load_historian_data", tags=["Coal Consumption"])                                    # coal consumption
def extract_historian_data(start_date: Optional[str] = None, end_date: Optional[str] = None):
    success = False
    try:
        headers_data = {
            'accept': 'application/json',
        }
        params = {
            'start_date': start_date,
            'end_date': end_date,
        }
        try:
            response = requests.get(f'http://{ip}/api/v1/host/historian_extract_data', params=params, headers=headers_data)
            data = json.loads(response.text)
            if response.status_code == 200:
                for item in data.get("Data", []):
                    item_data = item.get("Data")
                    if item_data:
                        tag_id = item_data.get("TagID")
                        avg = item_data.get("AVG")
                        created_date = item_data.get("CreatedDate")

                        if tag_id in [16, 3538, 2, 3536] and avg is not None:
                            sum_value = avg

                        if int(float(sum_value)) > 0:
                            existing_records = (
                                    Historian.objects(tagid=tag_id, created_date=created_date)
                                    .order_by("-created_date")
                                    .limit(150)
                                )

                            if existing_records.count() > 1:
                                existing_records[1:].delete()

                            if existing_records:
                                latest_record = existing_records.first()
                                latest_record.sum = sum_value
                                latest_record.created_at = datetime.datetime.utcnow()
                                latest_record.save()
                            else:
                                new_record = Historian(
                                    tagid=tag_id,
                                    sum=sum_value,
                                    created_date=created_date,
                                    created_at=datetime.datetime.utcnow()
                                )
                                new_record.save()
            
                success = "completed"
        except requests.exceptions.Timeout:
            console_logger.debug("Request timed out!")
        except requests.exceptions.ConnectionError:
            console_logger.debug("Connection error")
    
    except Exception as e:
        success = False
        console_logger.debug("----- Coal Testing Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e
        
    finally:
        console_logger.debug(f"success:{success}")
        SchedulerResponse("save consumption data", f"{success}")
        return {"message" : "Successful"} 
    

# @router.get("/historian_data", tags=["Historian"])
# def fetch_historian_data():
#     historian_tag_count = "http://10.100.12.28:8093/api/REST/Tag/Search?tagCount=100&keywords=level"
#     response = requests.request("GET", url = historian_tag_count, headers=consumption_headers)
#     return Response(response.content)



# @router.post("/historian_latest_value", tags=["Historian"])
# def load_historian_value():
#     payload = json.dumps({
#                 "Timestamps": "2023-03-20 15:30:00",
#                 "TagID": ["2","3538"],
#                 "RetrieveBy": "ID"})
#     historain_latest_value = "http://10.100.12.28:8093/api/REST/HistoryData/LoadLatestValue"
#     response = requests.request("POST", url = historain_latest_value, headers=consumption_headers, data=payload)
#     return Response(response.text)


@router.get("/load_historian", tags=["Coal Consumption"])
def sync_historian_data(start_date: Optional[str] = None, end_date: Optional[str] = None):
    success = False
    try:
        headers_data = {
            'accept': 'application/json',
        }

        start_dt = datetime.datetime.fromisoformat(start_date)
        end_dt = datetime.datetime.now().replace(minute=0, second=0, microsecond=0)
        
        intervals = []
        current_dt = start_dt
        while current_dt < end_dt:
            next_dt = current_dt + timedelta(hours=1)
            if next_dt > end_dt:
                next_dt = end_dt
            intervals.append({"StartTime": current_dt.isoformat(), "EndTime": next_dt.isoformat()})
            current_dt = next_dt

        for interval in intervals:
            params = {
                'start_date': interval["StartTime"],
                'end_date': interval["EndTime"],
            }

            try:
                response = requests.get(f'http://{ip}/api/v1/host/historian_extract_data', params=params, headers=headers_data)
                data = json.loads(response.text)

                for item in data.get("Data", []):
                    item_data = item.get("Data")
                    if item_data:
                        tag_id = item_data.get("TagID")
                        avg = item_data.get("AVG")
                        created_date = item_data.get("CreatedDate")

                        if tag_id in [16, 3538, 2, 3536] and avg is not None:
                            sum_value = avg

                            if int(float(sum_value)) > 0:
                                existing_records = (
                                    Historian.objects(tagid=tag_id, created_date=created_date)
                                    .order_by("-created_date")
                                    .limit(150)
                                )

                                if existing_records.count() > 1:
                                    existing_records[1:].delete()

                                if existing_records:
                                    latest_record = existing_records.first()
                                    latest_record.sum = sum_value
                                    latest_record.created_at = datetime.datetime.utcnow()
                                    latest_record.save()
                                else:
                                    new_record = Historian(
                                        tagid=tag_id,
                                        sum=sum_value,
                                        created_date=created_date,
                                        created_at=datetime.datetime.utcnow()
                                    )
                                    new_record.save()

                success = "completed"
            except requests.exceptions.Timeout:
                console_logger.debug("Request timed out!")
            except requests.exceptions.ConnectionError:
                console_logger.debug("Connection error")

    except Exception as e:
        success = False
        console_logger.debug("----- Coal Testing Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e

    finally:
        console_logger.debug(f"success: {success}")
        SchedulerResponse("save consumption data", f"{success}")
        return {"message": "Successful"}


@router.delete("/delete_historian_duplicates", tags=["Coal Consumption"])
def delete_historian_duplicates():
    try:
        historian_records = Historian.objects.order_by("-created_date").limit(10000)

        if historian_records:
            duplicates = {}

            for record in historian_records:
                key = (record.tagid, record.created_date)
                if key not in duplicates:
                    duplicates[key] = []
                duplicates[key].append(record)

            delete_count = 0

            for records in duplicates.values():
                if len(records) > 1:
                    records.sort(key=lambda r: r.created_at, reverse=True)
                    for record in records[1:]:  # Delete all except the latest
                        record.delete()
                        delete_count += 1

    except Exception as e:
        console_logger.debug("----- Historian Deletion Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))


@router.get("/coal_generation_graph", tags=["Coal Consumption"])
def coal_generation_analysis(
    response: Response,
    type: Optional[str] = "Daily",
    Month: Optional[str] = None,
    Daily: Optional[str] = None,
    Year: Optional[str] = None
):
    try:
        UTC_OFFSET_TIMEDELTA = datetime.datetime.utcnow() - datetime.datetime.now()

        # Define base pipeline
        basePipeline = [
            {
                "$match": {"created_date": {"$gte": None}},
            },
            {"$sort": {"created_date": -1}},
            {
                "$group": {
                    "_id": {
                        "ts": None,
                        "tagid": "$tagid"
                    },
                    "latest_sum": {"$first": "$sum"}
                }
            },
            {
                "$project": {
                    "ts": "$_id.ts",
                    "tagid": "$_id.tagid",
                    "sum": "$latest_sum",
                    "_id": 0
                }
            },
            {
                "$group": {
                    "_id": {"ts": "$ts", "tagid": "$tagid"},
                    "data": {"$push": "$sum"}
                }
            }
        ]

        if type == "Daily":
            date = Daily
            start_date = datetime.datetime.strptime(f"{date} 00:00:00", "%Y-%m-%d %H:%M:%S")
            end_date = datetime.datetime.strptime(f"{date} 23:59:59", "%Y-%m-%d %H:%M:%S")
            basePipeline[0]["$match"]["created_date"].update({"$gte": start_date, "$lte": end_date})
            basePipeline[2]["$group"]["_id"]["ts"] = {"$hour": "$created_date"}
            result = {
                "data": {
                    "labels": [str(i) for i in range(24)],
                    "datasets": [
                        {"label": "Unit 1", "data": [0 for _ in range(24)]},  # tagid 2
                        {"label": "Unit 2", "data": [0 for _ in range(24)]}   # tagid 3536
                    ]
                }
            }


        elif type == "Week":
            basePipeline[0]["$match"]["created_date"]["$gte"] = (
                datetime.datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
                + UTC_OFFSET_TIMEDELTA
                - datetime.timedelta(days=7)
            )
            basePipeline[2]["$group"]["_id"]["ts"] = {"$dayOfMonth": "$created_date"}
            result = {
                "data": {
                    "labels": [
                        (
                            basePipeline[0]["$match"]["created_date"]["$gte"]
                            + datetime.timedelta(days=i + 1)
                        ).strftime("%d")
                        for i in range(1, 8)
                    ],
                    "datasets": [
                        {"label": "Unit 1", "data": [0 for i in range(1, 8)]},              # unit 1 = tagid_2
                        {"label": "Unit 2", "data": [0 for i in range(1, 8)]},              # unit 2 = tagid_3536
                    ],
                }
            }


        elif type == "Month":

            date=Month
            format_data = "%Y - %m-%d"

            start_date = f'{date}-01'
            startd_date=datetime.datetime.strptime(start_date,format_data)
            
            end_date = startd_date + relativedelta( day=31)
            end_label = (end_date).strftime("%d")

            basePipeline[0]["$match"]["created_date"]["$lte"] = (end_date)
            basePipeline[0]["$match"]["created_date"]["$gte"] = (startd_date)
            basePipeline[2]["$group"]["_id"]["ts"] = {"$dayOfMonth": "$created_date"}
            result = {
                "data": {
                    "labels": [
                        (
                            basePipeline[0]["$match"]["created_date"]["$gte"]
                            + datetime.timedelta(days=i + 1)
                        ).strftime("%d")
                        for i in range(-1, (int(end_label))-1)
                    ],
                    "datasets": [
                        {"label": "Unit 1", "data": [0 for i in range(-1, (int(end_label))-1)]},        # unit 1 = tagid_2
                        {"label": "Unit 2", "data": [0 for i in range(-1, (int(end_label))-1)]},        # unit 2 = tagid_3536
                    ],
                }
            }

        elif type == "Year":

            date=Year
            end_date =f'{date}-12-31 23:59:59'
            start_date = f'{date}-01-01 00:00:00'
            format_data = "%Y-%m-%d %H:%M:%S"
            endd_date=datetime.datetime.strptime(end_date,format_data)
            startd_date=datetime.datetime.strptime(start_date,format_data)

            basePipeline[0]["$match"]["created_date"]["$lte"] = (endd_date)
            basePipeline[0]["$match"]["created_date"]["$gte"] = (startd_date)

            basePipeline[2]["$group"]["_id"]["ts"] = {"$month": "$created_date"}
            result = {
                "data": {
                    "labels": [
                        (
                            basePipeline[0]["$match"]["created_date"]["$gte"]
                            + relativedelta(months=i)
                        ).strftime("%m")
                        for i in range(0, 12)
                    ],
                    "datasets": [
                        {"label": "Unit 1", "data": [0 for i in range(0, 12)]},                     # unit 1 = tagid_2
                        {"label": "Unit 2", "data": [0 for i in range(0, 12)]},                     # unit 2 = tagid_3536
                    ],
                }
            }

        output = Historian.objects().aggregate(basePipeline)
        outputDict = {}
        for data in output:
            ts = data["_id"]["ts"]
            tag_id = data["_id"]["tagid"]
            sum_list = [float(item) for item in data.get('data', []) if item]
            if sum_list:
                outputDict.setdefault(ts, {}).setdefault(tag_id, []).extend(sum_list)

        modified_labels = [i for i in range(0, 24)]

        for index, label in enumerate(result["data"]["labels"]):

            if type == "Week":
                modified_labels = [
                    (
                        basePipeline[0]["$match"]["created_date"]["$gte"]
                        + datetime.timedelta(days=i + 1)
                    ).strftime("%d-%m-%Y,%a")
                    for i in range(1, 8)
                ]
            
            elif type == "Month":
                modified_labels = [
                    (
                        basePipeline[0]["$match"]["created_date"]["$gte"]
                        + datetime.timedelta(days=i + 1)
                    ).strftime("%d/%m")
                    for i in range(-1, (int(end_label))-1)
                ]

            elif type == "Year":
                modified_labels = [
                    (
                        basePipeline[0]["$match"]["created_date"]["$gte"]
                        + relativedelta(months=i)
                    ).strftime("%b %y")
                    for i in range(0, 12)
                ]


            if int(label.split("-")[0]) in outputDict:
                for tag, values in outputDict[int(label.split("-")[0])].items():
                    if type in ["Week", "Month", "Year"]:
                        avg_val = sum(values) / len(values)
                        dataset_index = 0 if tag == 2 else 1 if tag == 3536 else None
                        if dataset_index is not None:
                            result["data"]["datasets"][dataset_index]["data"][index] = round(avg_val, 2)
                    elif type == "Daily":
                        total_sum = sum(values)
                        dataset_index = 0 if tag == 2 else 1 if tag == 3536 else None
                        if dataset_index is not None:
                            result["data"]["datasets"][dataset_index]["data"][index] = round(total_sum, 2)

        result["data"]["labels"] = copy.deepcopy(modified_labels)
        return result

    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e



@router.get("/coal_consumption_graph", tags=["Coal Consumption"])
def coal_consumption_analysis(response:Response,type: Optional[str] = "Daily",
                              Month: Optional[str] = None, 
                              Daily: Optional[str] = None, Year: Optional[str] = None):
    try:
        data={}
        UTC_OFFSET_TIMEDELTA = datetime.datetime.utcnow() - datetime.datetime.now()

        basePipeline = [
            {
                "$match": {
                    "created_date": {
                        "$gte": None,
                    },
                },
            },
            {
                "$sort": {
                    "created_date": -1
                }
            },
            {
                "$group": {
                    "_id": {
                        "ts": {"$hour": {"date": "$created_date"}},
                        "tagid": "$tagid",
                        "created_date": "$created_date"
                    },
                    "latest_sum": {"$first": "$sum"}
                }
            },
            {
                "$project": {
                    "ts": "$_id.ts",
                    "tagid": "$_id.tagid",
                    "sum": "$latest_sum",
                    "_id": 0
                }
            },
            {
                "$group": {
                    "_id": {
                        "ts": "$ts",
                        "tagid": "$tagid"
                    },
                    "data": {
                        "$push": "$sum"
                    }
                }
            }
        ]

        if type == "Daily":
            date = Daily
            start_date = datetime.datetime.strptime(f"{date} 00:00:00", "%Y-%m-%d %H:%M:%S")
            end_date = datetime.datetime.strptime(f"{date} 23:59:59", "%Y-%m-%d %H:%M:%S")
            basePipeline[0]["$match"]["created_date"].update({"$gte": start_date, "$lte": end_date})
            basePipeline[2]["$group"]["_id"]["ts"] = {"$hour": "$created_date"}
            result = {
                "data": {
                    "labels": [str(i) for i in range(24)],
                    "datasets": [
                        {"label": "Unit 1", "data": [0 for _ in range(24)]},  # tagid 2
                        {"label": "Unit 2", "data": [0 for _ in range(24)]}   # tagid 3536
                    ]
                }
            }

        elif type == "Week":
            basePipeline[0]["$match"]["created_date"]["$gte"] = (
                datetime.datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
                + UTC_OFFSET_TIMEDELTA
                - datetime.timedelta(days=7)
            )
            basePipeline[2]["$group"]["_id"]["ts"] = {"$dayOfMonth": "$created_date"}
            result = {
                "data": {
                    "labels": [
                        (
                            basePipeline[0]["$match"]["created_date"]["$gte"]
                            + datetime.timedelta(days=i + 1)
                        ).strftime("%d")
                        for i in range(1, 8)
                    ],
                    "datasets": [
                        {"label": "Unit 1", "data": [0 for i in range(1, 8)]},              # unit 1 = tagid_16
                        {"label": "Unit 2", "data": [0 for i in range(1, 8)]},              # unit 2 = tagid_3538
                    ],
                }
            }


        elif type == "Month":

            date=Month
            format_data = "%Y - %m-%d"

            start_date = f'{date}-01'
            startd_date=datetime.datetime.strptime(start_date,format_data)
            
            end_date = startd_date + relativedelta( day=31)
            end_label = (end_date).strftime("%d")

            basePipeline[0]["$match"]["created_date"]["$lte"] = (end_date)
            basePipeline[0]["$match"]["created_date"]["$gte"] = (startd_date)
            basePipeline[2]["$group"]["_id"]["ts"] = {"$dayOfMonth": "$created_date"}
            result = {
                "data": {
                    "labels": [
                        (
                            basePipeline[0]["$match"]["created_date"]["$gte"]
                            + datetime.timedelta(days=i + 1)
                        ).strftime("%d")
                        for i in range(-1, (int(end_label))-1)
                    ],
                    "datasets": [
                        {"label": "Unit 1", "data": [0 for i in range(-1, (int(end_label))-1)]},        # unit 1 = tagid_16
                        {"label": "Unit 2", "data": [0 for i in range(-1, (int(end_label))-1)]},        # unit 2 = tagid_3538
                    ],
                }
            }

        elif type == "Year":

            date=Year
            end_date =f'{date}-12-31 23:59:59'
            start_date = f'{date}-01-01 00:00:00'
            format_data = "%Y-%m-%d %H:%M:%S"
            endd_date=datetime.datetime.strptime(end_date,format_data)
            startd_date=datetime.datetime.strptime(start_date,format_data)

            basePipeline[0]["$match"]["created_date"]["$lte"] = (endd_date)
            basePipeline[0]["$match"]["created_date"]["$gte"] = (startd_date)

            basePipeline[2]["$group"]["_id"]["ts"] = {"$month": "$created_date"}
            result = {
                "data": {
                    "labels": [
                        (
                            basePipeline[0]["$match"]["created_date"]["$gte"]
                            + relativedelta(months=i)
                        ).strftime("%m")
                        for i in range(0, 12)
                    ],
                    "datasets": [
                        {"label": "Unit 1", "data": [0 for i in range(0, 12)]},                     # unit 1 = tagid_16
                        {"label": "Unit 2", "data": [0 for i in range(0, 12)]},                     # unit 2 = tagid_3538
                    ],
                }
            }


        output = Historian.objects().aggregate(basePipeline)
        outputDict = {}
        for data in output:
            ts = data["_id"]["ts"]
            tag_id = data["_id"]["tagid"]
            sum_list = [float(item) for item in data.get('data', []) if item]
            if sum_list:
                outputDict.setdefault(ts, {}).setdefault(tag_id, []).extend(sum_list)

        modified_labels = [i for i in range(0, 24)]

        for index, label in enumerate(result["data"]["labels"]):

            if type == "Week":
                modified_labels = [
                    (
                        basePipeline[0]["$match"]["created_date"]["$gte"]
                        + datetime.timedelta(days=i + 1)
                    ).strftime("%d-%m-%Y,%a")
                    for i in range(1, 8)
                ]
            
            elif type == "Month":
                modified_labels = [
                    (
                        basePipeline[0]["$match"]["created_date"]["$gte"]
                        + datetime.timedelta(days=i + 1)
                    ).strftime("%d/%m")
                    for i in range(-1, (int(end_label))-1)
                ]

            elif type == "Year":
                modified_labels = [
                    (
                        basePipeline[0]["$match"]["created_date"]["$gte"]
                        + relativedelta(months=i)
                    ).strftime("%b %y")
                    for i in range(0, 12)
                ]

            if int(label.split("-")[0]) in outputDict:
                for tag, values in outputDict[int(label.split("-")[0])].items():
                    if type in ["Week", "Month", "Year"]:
                        avg_val = sum(values) / len(values)
                        dataset_index = 0 if tag == 16 else 1 if tag == 3538 else None
                        if dataset_index is not None:
                            result["data"]["datasets"][dataset_index]["data"][index] = round(avg_val, 2)
                    elif type == "Daily":
                        total_sum = sum(values)
                        dataset_index = 0 if tag == 16 else 1 if tag == 3538 else None
                        if dataset_index is not None:
                            result["data"]["datasets"][dataset_index]["data"][index] = round(total_sum, 2)

        result["data"]["labels"] = copy.deepcopy(modified_labels)
        return result

    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e



#  x------------------------------    Coal Quality Testing Api's    ------------------------------------x


# @router.get("/extract_coal_test", tags=["Coal Testing"])
# def coal_test(start_date: Optional[str] = None, end_date: Optional[str] = None):
#     # entry = UsecaseParameters.objects.filter(Parameters__gmr_api__exists=True).first()
#     entry = UsecaseParameters.objects.first()
#     testing_ip = entry.Parameters.get('gmr_api', {}).get('roi1', {}).get('Coal Testing IP') if entry else None
#     testing_timer = entry.Parameters.get('gmr_api', {}).get('roi1', {}).get('Coal Testing Duration') if entry else None
    
#     console_logger.debug(f"---- Coal Testing IP ----            {testing_ip}")
#     console_logger.debug(f"---- Coal Testing Duration ----      {testing_timer}")

#     payload={}
#     headers = {}
#     if not end_date:
#         end_date = datetime.date.today()                                      #  end_date will always be the current date

#     if not start_date:
#         no_of_day = testing_timer.split(":")[0]
#         start_date = (end_date-timedelta(int(no_of_day))).__str__()
        
#     console_logger.debug(f" --- Test Start Date --- {start_date}")
#     console_logger.debug(f" --- Test End Date --- {end_date}")

#     coal_testing_url = f"http://{testing_ip}/limsapi/api/SampleDetails/GetSampleRecord/GetSampleRecord?Fromdate={start_date}&todate={end_date}"
#     # coal_testing_url = f"http://172.21.96.145/limsapi/api/SampleDetails/GetSampleRecord/GetSampleRecord?Fromdate={start_date}&todate={end_date}"

#     response = requests.request("GET", url = coal_testing_url,headers=headers, data=payload)
#     testing_data = json.loads(response.text)

#     wcl_extracted_data = []
#     secl_extracted_data = []

#     for entry in testing_data["responseData"]:
#         if entry["supplier"] == "WCL" and entry["rrNo"] != "" and entry["rrNo"] != "NA":

#             data = {
#                 "sample_Desc": entry["sample_Desc"],
#                 "rrNo": entry["rrNo"],
#                 "rR_Qty": entry["rR_Qty"],
#                 "rake_No": entry["rake_No"],
#                 "supplier": entry["supplier"],
#                 "receive_date": entry["sample_Received_Date"],
#                 "parameters": [] 
#             }

#             for param in entry["sample_Parameters"]:
#                 param_info = {
#                     "parameter_Name": param.get('parameter_Name').title().replace(" ","_"),
#                     "unit_Val": param["unit_Val"].title().replace(" ",""),
#                     "test_Method": param["test_Method"],
#                     "val1": param["val1"]
#                 }
#                 data["parameters"].append(param_info)
#             wcl_extracted_data.append(data)

        
#         if entry["supplier"] == "SECL" and entry["rrNo"] != "" and entry["rrNo"] != "NA":

#             secl_data = {
#                 "sample_Desc": entry["sample_Desc"],
#                 "rrNo": entry["rrNo"],
#                 "rR_Qty": entry["rR_Qty"],
#                 "rake_No": entry["rake_No"],
#                 "supplier": entry["supplier"],
#                 "receive_date": entry["sample_Received_Date"],
#                 "parameters": [] 
#             }

#             for secl_param in entry["sample_Parameters"]:
#                 param_info = {
#                     "parameter_Name": secl_param.get('parameter_Name').title().replace(" ","_"),
#                     "unit_Val": secl_param["unit_Val"].title().replace(" ",""),
#                     "test_Method": secl_param["test_Method"],
#                     "val1": secl_param["val1"]
#                 }
#                 secl_data["parameters"].append(param_info)
#             secl_extracted_data.append(secl_data)

#     for entry in wcl_extracted_data:
#         CoalTesting(
#             location = entry["sample_Desc"].upper(),
#             rrNo = entry["rrNo"],
#             rR_Qty = entry["rR_Qty"],
#             rake_no = entry["rake_No"],
#             supplier = entry["supplier"],
#             receive_date = entry["receive_date"],
#             parameters = entry["parameters"],
#             ID = CoalTesting.objects.count() + 1
#         ).save()

#     for secl_entry in secl_extracted_data:
#         CoalTestingTrain(
#             location = secl_entry["sample_Desc"].upper(),
#             rrNo = secl_entry["rrNo"],
#             rR_Qty = secl_entry["rR_Qty"],
#             rake_no = secl_entry["rake_No"],
#             supplier = secl_entry["supplier"],
#             receive_date = secl_entry["receive_date"],
#             parameters = secl_entry["parameters"],
#             ID = CoalTestingTrain.objects.count() + 1
#         ).save()
    
#     return {"message" : "Successful"}


def coal_grade_data():
    coalData = CoalGrades.objects()
    if coalData:
        coalData.delete()
    dict_data = [
        {
            "start_value": "7000",
            "end_value": "",
            "grade": "G-1",
        },
        {
            "start_value": "6700",
            "end_value": "7000",
            "grade": "G-2",
        },
        {
            "start_value": "6400",
            "end_value": "6700",
            "grade": "G-3",
        },
        {
            "start_value": "6100",
            "end_value": "6400",
            "grade": "G-4",
        },
        {
            "start_value": "5800",
            "end_value": "6100",
            "grade": "G-5",
        },
        {
            "start_value": "5500",
            "end_value": "5800",
            "grade": "G-6",
        },
        {
            "start_value": "5200",
            "end_value": "5500",
            "grade": "G-7",
        },
        {
            "start_value": "4900",
            "end_value": "5200",
            "grade": "G-8",
        },
        {
            "start_value": "4600",
            "end_value": "4900",
            "grade": "G-9",
        },
        {
            "start_value": "4300",
            "end_value": "4600",
            "grade": "G-10",
        },
        {
            "start_value": "4000",
            "end_value": "4300",
            "grade": "G-11",
        },
        {
            "start_value": "3700",
            "end_value": "4000",
            "grade": "G-12",
        },
        {
            "start_value": "3400",
            "end_value": "3700",
            "grade": "G-13",
        },
        {
            "start_value": "3100",
            "end_value": "3400",
            "grade": "G-14",
        },
        {
            "start_value": "2800",
            "end_value": "3100",
            "grade": "G-15",
        },
        {
            "start_value": "2500",
            "end_value": "2800",
            "grade": "G-16",
        },
        {
            "start_value": "2200",
            "end_value": "2500",
            "grade": "G-17",
        },
    ]

    for single_data in dict_data:
        coalgrade = CoalGrades(
            grade=single_data["grade"],
            start_value=single_data["start_value"],
            end_value=single_data["end_value"],
        )
        coalgrade.save()

    return {"detail": "success"}


@router.get("/fetchcoalgrades", tags=["Coal Testing"])
def endpoint_to_fetch_coal_grades(response: Response):
    try:
        fetchData = coal_grade_data()
        return fetchData
    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        return e


@router.on_event("startup")
async def startup_event(bg_task=BackgroundTasks()):
    bg_task.add_task(coal_grade_data())
    return


# def SchedulerResponse(job_id, status):
#     SchedulerError(JobId=job_id, ErrorMsg=status).save()
#     if len(SchedulerError.objects()) > 1000:
#         for i in SchedulerError.objects()[-1:100]:
#             i.delete()

def SchedulerResponse(job_id, status):
    SchedulerError(JobId=job_id, ErrorMsg=status).save()
    if SchedulerError.objects.count() > 1000:
        old_errors = SchedulerError.objects.order_by('Created_at')[:100]
        for error in old_errors:
            error.delete()



# @router.get("/extract_coal_test", tags=["Coal Testing"])
# def coal_test(start_date: Optional[str] = None, end_date: Optional[str] = None):
#     success = False
#     try:
#         console_logger.debug("hitted data")
#         global proxies
#         # entry = UsecaseParameters.objects.filter(Parameters__gmr_api__exists=True).first()
#         entry = UsecaseParameters.objects.first()
#         testing_ip = entry.Parameters.get("gmr_api", {}).get("roi1", {}).get("Coal Testing IP") if entry else None
#         testing_timer = entry.Parameters.get("gmr_api", {}).get("roi1", {}).get("Coal Testing Duration") if entry else None

#         console_logger.debug(f"---- Coal Testing IP ----            {testing_ip}")
#         console_logger.debug(f"---- Coal Testing Duration ----      {testing_timer}")

#         payload = {}
#         headers = {}
#         current_time = datetime.datetime.now(IST)
#         current_date = current_time.date()
#         if not end_date:
#             end_date = current_date.__str__()  #  end_date will always be the current date

#         if not start_date:
#             no_of_day = testing_timer.split(":")[0]
#             start_date = (current_date - timedelta(int(no_of_day))).__str__()

#         console_logger.debug(f" --- Test Start Date --- {start_date}")
#         console_logger.debug(f" --- Test End Date --- {end_date}")
#         console_logger.debug(ip)
#         coal_testing_url = f"http://{ip}/api/v1/host/coal_extract_data?start_date={start_date}&end_date={end_date}"
#         # coal_testing_url = f"http://{ip}/api/v1/host/coal_extract_data?start_date=2024-07-09&end_date=2024-08-09"
#         # coal_testing_url = f"http://{testing_ip}/limsapi/api/SampleDetails/GetSampleRecord/GetSampleRecord?Fromdate={start_date}&todate={end_date}"
#         # coal_testing_url = f"http://172.21.96.145/limsapi/api/SampleDetails/GetSampleRecord/GetSampleRecord?Fromdate={start_date}&todate={end_date}"
#         try:
#             response = requests.request("GET", url=coal_testing_url, headers=headers, data=payload, proxies=proxies)
#             testing_data = response.json()
#             wcl_extracted_data = []
#             secl_extracted_data = []
#             # console_logger.debug(testing_data)
#             if testing_data.get("statusCode") == 200:
#                 for entry in testing_data["responseData"]:
#                     if entry.get("supplier") == "WCL" and entry.get("rrNo") != "" and entry.get("rrNo") != "NA":
#                         data = {
#                             "sample_Desc": entry.get("sample_Desc"),
#                             "rrNo": entry.get("rrNo"),
#                             "rR_Qty": entry.get("rR_Qty"),
#                             "rake_No": entry.get("rake_No"),
#                             "supplier": entry.get("supplier"),
#                             "receive_date": entry.get("sample_Received_Date"),
#                             "parameters": [],
#                         }

#                         for param in entry.get("sample_Parameters"):
#                             param_info = {
#                                 "parameter_Name": param.get("parameter_Name")
#                                 .title()
#                                 .replace(" ", "_"),
#                                 "unit_Val": param.get("unit_Val").title().replace(" ",""),
#                                 "test_Method": param.get("test_Method"),
#                                 "val1": param.get("val1"),
#                             }

#                             if param.get("parameter_Name").title() == "Gross Calorific Value (Adb)":
#                                 if param.get("val1"):
#                                     fetchCoalGrades = CoalGrades.objects()
#                                     for single_coal_grades in fetchCoalGrades:
#                                         if (
#                                             single_coal_grades["start_value"]
#                                             <= param.get("val1")
#                                             <= single_coal_grades["end_value"]
#                                             and single_coal_grades["start_value"] != ""
#                                             and single_coal_grades["end_value"] != ""
#                                         ):
#                                             param_info["grade"] = single_coal_grades["grade"]
#                                         elif param.get("val1") > "7001":
#                                             param_info["grade"] = "G-1"
#                                             break
#                             #uncomment below and comment upper for using bombcalorimeter                     
#                             # if param.get("parameter_Name").title() == "Gross Calorific Value (Adb)":
#                             #     param_info = {
#                             #         "parameter_Name": param.get("parameter_Name")
#                             #         .title()
#                             #         .replace(" ", "_"),
#                             #         "unit_Val": param.get("unit_Val").title().replace(" ",""),
#                             #         "test_Method": param.get("test_Method"),
#                             #         "val1": "0",
#                             #     }
#                             #     if param.get("val1"):
#                             #         fetchCoalGrades = CoalGrades.objects()
#                             #         for single_coal_grades in fetchCoalGrades:
#                             #             if (
#                             #                 single_coal_grades["start_value"]
#                             #                 <= param.get("val1")
#                             #                 <= single_coal_grades["end_value"]
#                             #                 and single_coal_grades["start_value"] != ""
#                             #                 and single_coal_grades["end_value"] != ""
#                             #             ):
#                             #                 param_info["grade"] = single_coal_grades["grade"]
#                             #             elif param.get("val1") > "7001":
#                             #                 param_info["grade"] = "G-1"
#                             #                 break
#                             # else:
#                             #     param_info = {
#                             #         "parameter_Name": param.get("parameter_Name")
#                             #         .title()
#                             #         .replace(" ", "_"),
#                             #         "unit_Val": param.get("unit_Val").title().replace(" ",""),
#                             #         "test_Method": param.get("test_Method"),
#                             #         "val1": param.get("val1"),
#                             #     }

#                             data["parameters"].append(param_info)
#                         wcl_extracted_data.append(data)

#                 if (
#                     entry["supplier"] == "SECL"
#                     and entry["rrNo"] != ""
#                     and entry["rrNo"] != "NA"
#                 ):

#                     secl_data = {
#                         "sample_Desc": entry.get("sample_Desc"),
#                         "rrNo": entry.get("rrNo"),
#                         "rR_Qty": entry.get("rR_Qty"),
#                         "rake_No": entry.get("rake_No"),
#                         "supplier": entry.get("supplier"),
#                         "receive_date": entry.get("sample_Received_Date"),
#                         "parameters": [],
#                     }

#                     for secl_param in entry["sample_Parameters"]:
#                         param_info = {
#                             "parameter_Name": secl_param.get("parameter_Name")
#                             .title()
#                             .replace(" ", "_"),
#                             "unit_Val": secl_param.get("unit_Val").title().replace(" ",""),
#                             "test_Method": secl_param.get("test_Method"),
#                             "val1": secl_param.get("val1"),
#                         }
#                         secl_data["parameters"].append(param_info)
#                     secl_extracted_data.append(secl_data)

#                     #uncomment below and comment upper for using bombcalorimeter
#                     # for secl_param in entry["sample_Parameters"]:
#                     #     if secl_param.get("parameter_Name").title() == "Gross Calorific Value (Adb)":
#                     #         param_info = {
#                     #             "parameter_Name": secl_param.get("parameter_Name")
#                     #             .title()
#                     #             .replace(" ", "_"),
#                     #             "unit_Val": secl_param.get("unit_Val").title().replace(" ",""),
#                     #             "test_Method": secl_param.get("test_Method"),
#                     #             "val1": "0",
#                     #         }
#                     #     else:
#                     #         param_info = {
#                     #             "parameter_Name": secl_param.get("parameter_Name")
#                     #             .title()
#                     #             .replace(" ", "_"),
#                     #             "unit_Val": secl_param.get("unit_Val").title().replace(" ",""),
#                     #             "test_Method": secl_param.get("test_Method"),
#                     #             "val1": secl_param.get("val1"),
#                     #         }
#                     #     secl_data["parameters"].append(param_info)
#                     # secl_extracted_data.append(secl_data)

#             for entry in wcl_extracted_data:
#                 if re.sub(r'\t', '', entry.get("sample_Desc")) != "":
#                     try:
#                         coalTestRoadData = CoalTesting.objects.get(rrNo=entry.get("rrNo").strip(), rake_no=entry.get("rake_No").upper().strip())
#                     except DoesNotExist as e:
#                         # first re removes \t from string and second re will remove multiple space and will keep only single space
#                         CoalTesting(
#                             location=re.sub(r'\t', '', re.sub(' +', ' ', entry.get("sample_Desc").upper().strip())),
#                             rrNo=entry.get("rrNo").strip(),
#                             rR_Qty=entry.get("rR_Qty").strip(),
#                             rake_no=entry.get("rake_No").upper().strip(),
#                             supplier=entry.get("supplier").strip(),
#                             receive_date=entry.get("receive_date"),
#                             parameters=entry.get("parameters"),
#                             ID=CoalTesting.objects.count() + 1,
#                         ).save()

#             for secl_entry in secl_extracted_data:
#                 if re.sub(r'\t', '', secl_entry.get("sample_Desc")) != "":
#                     if "Rake" in secl_entry.get("rake_No").strip() or "RAKE" in secl_entry.get("rake_No").strip():
#                         rake_no = secl_entry.get("rake_No").strip()
#                     else:
#                         no_data = '{:02d}'.format(int(secl_entry.get("rake_No").strip()))
#                         # rake_no = f"Rake-{str(no_data)}"
#                         rake_no = f"{str(no_data)}"
#                     try:
#                         coalTestRailData = CoalTestingTrain.objects.get(rrNo=secl_entry.get("rrNo").strip(), rake_no=rake_no)
#                     except DoesNotExist as e:
#                         CoalTestingTrain(
#                             location=re.sub(r'\t', '', re.sub(' +', ' ', secl_entry.get("sample_Desc").strip())),
#                             rrNo=secl_entry.get("rrNo").strip(),
#                             rR_Qty=secl_entry.get("rR_Qty").strip(),
#                             # rake_no=secl_entry.get("rake_No").strip(),
#                             rake_no=rake_no,
#                             supplier=secl_entry.get("supplier").strip(),
#                             receive_date=secl_entry.get("receive_date"),
#                             parameters=secl_entry.get("parameters"),
#                             ID=CoalTestingTrain.objects.count() + 1,
#                         ).save()
#             success = "completed"
#         except requests.exceptions.Timeout:
#             console_logger.debug("Request timed out!")
#         except requests.exceptions.ConnectionError:
#             console_logger.debug("Connection error")
        
#     except Exception as e:
#         success = False
#         console_logger.debug("----- Coal Testing Error -----",e)
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#         console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#         success = e
#     finally:
#         SchedulerResponse("save testing data", f"{success}")
#         return {"message": "Successful"}


def get_val_from_sample_params(params, param_type):
        try:
            for param in params:
                if param['parameter_Name'] == param_type:
                    return float(param['val1'])
            return None
        except Exception as e:
            success = False
            console_logger.debug("----- Coal Testing Error -----",e)
            exc_type, exc_obj, exc_tb = sys.exc_info()
            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
            console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
            console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            success = e

def get_mine_name_from_sap_records(do_no, data_type):
        try:
            try:
                fetchMineNameSapRecords = SapRecords.objects.get(do_no=do_no)
                if data_type == "mine_name":
                    # console_logger.debug(fetchMineNameSapRecords.mine_name)
                    return fetchMineNameSapRecords.mine_name
                elif data_type == "grade":
                    # console_logger.debug(fetchMineNameSapRecords.grade)
                    return fetchMineNameSapRecords.grade.split("-")[0] if fetchMineNameSapRecords.grade else None
                elif data_type == "consumer_type":
                    return fetchMineNameSapRecords.consumer_type if fetchMineNameSapRecords.consumer_type else None
            except DoesNotExist as e:
                return None
        except Exception as e:
            success = False
            console_logger.debug("----- Coal Testing Error -----",e)
            exc_type, exc_obj, exc_tb = sys.exc_info()
            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
            console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
            console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            success = e


# def get_mine_name_from_sap_records_rail(rr_no, data_type):
#         try:
#             try:
#                 fetchMineNameSapRecords = RailData.objects.get(rr_no=rr_no)
#                 if data_type == "mine_name":
#                     # console_logger.debug(fetchMineNameSapRecords.mine_name)
#                     return fetchMineNameSapRecords.mine
#                 elif data_type == "grade":
#                     # console_logger.debug(fetchMineNameSapRecords.grade)
#                     return fetchMineNameSapRecords.grade.split("-")[0] if fetchMineNameSapRecords.grade else None
#                 elif data_type == "consumer_type":
#                     return fetchMineNameSapRecords.consumer_type if fetchMineNameSapRecords.consumer_type else None
#             except DoesNotExist as e:
#                 return None
#         except Exception as e:
#             success = False
#             console_logger.debug("----- Coal Testing Error -----",e)
#             exc_type, exc_obj, exc_tb = sys.exc_info()
#             fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#             console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#             console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#             success = e


def get_mine_name_from_railData(rr_no, data_type):
        try:
            try:
                fetchMineNameRailData = RailData.objects.get(rr_no=rr_no)
                if data_type == "mine_name":
                    return fetchMineNameRailData.source
                elif data_type == "type_consumer":
                    return fetchMineNameRailData.source_type
                elif data_type == "grade":
                    return fetchMineNameRailData.grade
            except DoesNotExist as e:
                return None
        except Exception as e:
            success = False
            console_logger.debug("----- Coal Testing Error -----",e)
            exc_type, exc_obj, exc_tb = sys.exc_info()
            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
            console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
            console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            success = e


@router.get("/add/gcvgrade", tags=["extra"])
def endpoint_to_add_grade(response: Response):
    try:
        coal_grades = CoalGrades.objects()
        fetchReceiptcoalQuality = RecieptCoalQualityAnalysis.objects()
        for single_data in fetchReceiptcoalQuality:
            if single_data.plant_adb_gcv is not None:
                for single_coal_grades in coal_grades:
                    if single_coal_grades["end_value"] != "":
                        if (int(single_coal_grades["start_value"]) <= int(float(single_data.plant_adb_gcv)) <= int(single_coal_grades["end_value"]) and single_coal_grades["start_value"] != "" and single_coal_grades["end_value"] != ""):
                            single_data.plant_gcv_grade = single_coal_grades["grade"]
                        elif int(single_data.plant_adb_gcv) > 7001:
                            single_data.plant_gcv_grade = "G-1"
                            # continue

            if single_data.thirdparty_adb_gcv is not None:
                for single_coal_grades in coal_grades:
                    if single_coal_grades["end_value"] != "":
                        if (int(single_coal_grades["start_value"]) <= int(float(single_data.thirdparty_adb_gcv)) <= int(single_coal_grades["end_value"]) and single_coal_grades["start_value"] != "" and single_coal_grades["end_value"] != ""):
                            single_data.thirdparty_gcv_grade = single_coal_grades["grade"]
                        elif int(single_data.thirdparty_adb_gcv) > 7001:
                            single_data.thirdparty_gcv_grade = "G-1"
                            # continue

            single_data.save()

        return {"details": "success"}
                            
    except Exception as e:
        success = False
        console_logger.debug("----- Gcv Grade Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e


@router.get("/add/igigcvgrade", tags=["extra"])
def endpoint_to_add_grade(response: Response):
    try:
        coal_grades = CoalGrades.objects()
        fetchReceiptcoalQuality = minesamplequalityanalysis.objects()
        for single_data in fetchReceiptcoalQuality:
            if single_data.plant_adb_gcv is not None:
                for single_coal_grades in coal_grades:
                    if single_coal_grades["end_value"] != "":
                        if (int(single_coal_grades["start_value"]) <= int(float(single_data.plant_adb_gcv)) <= int(single_coal_grades["end_value"]) and single_coal_grades["start_value"] != "" and single_coal_grades["end_value"] != ""):
                            single_data.plant_gcv_grade = single_coal_grades["grade"]
                        elif int(single_data.plant_adb_gcv) > 7001:
                            single_data.plant_gcv_grade = "G-1"
                            # continue

            if single_data.mine_thirdparty_adb_gcv is not None:
                for single_coal_grades in coal_grades:
                    if single_coal_grades["end_value"] != "":
                        if (int(single_coal_grades["start_value"]) <= int(float(single_data.mine_thirdparty_adb_gcv)) <= int(single_coal_grades["end_value"]) and single_coal_grades["start_value"] != "" and single_coal_grades["end_value"] != ""):
                            single_data.mine_thirdparty_gcv_grade = single_coal_grades["grade"]
                        elif int(single_data.mine_thirdparty_adb_gcv) > 7001:
                            single_data.mine_thirdparty_gcv_grade = "G-1"
                            # continue

            single_data.save()

        return {"details": "success"}
                            
    except Exception as e:
        success = False
        console_logger.debug("----- Gcv Grade Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e


#new as per tabl
@router.get("/extract_coal_test", tags=["Coal Testing"])
def coal_test(start_date: Optional[str] = None, end_date: Optional[str] = None):
    success = False
    try:
        global proxies
        # entry = UsecaseParameters.objects.filter(Parameters__gmr_api__exists=True).first()
        entry = UsecaseParameters.objects.first()
        testing_ip = entry.Parameters.get("gmr_api", {}).get("roi1", {}).get("Coal Testing IP") if entry else None
        testing_timer = entry.Parameters.get("gmr_api", {}).get("roi1", {}).get("Coal Testing Duration") if entry else None

        console_logger.debug(f"---- Coal Testing IP ----            {testing_ip}")
        console_logger.debug(f"---- Coal Testing Duration ----      {testing_timer}")

        payload = {}
        headers = {}
        current_time = datetime.datetime.now(IST)
        current_date = current_time.date()
        if not end_date:
            end_date = current_date.__str__()  #  end_date will always be the current date

        if not start_date:
            no_of_day = testing_timer.split(":")[0]
            start_date = (current_date - timedelta(int(no_of_day))).__str__()

        console_logger.debug(f" --- Test Start Date --- {start_date}")
        console_logger.debug(f" --- Test End Date --- {end_date}")
        console_logger.debug(ip)
        coal_testing_url = f"http://{ip}/api/v1/host/coal_extract_data?start_date={start_date}&end_date={end_date}"
        # coal_testing_url = f"http://{ip}/api/v1/host/coal_extract_data?start_date=2024-07-09&end_date=2024-08-09"
        # coal_testing_url = f"http://{testing_ip}/limsapi/api/SampleDetails/GetSampleRecord/GetSampleRecord?Fromdate={start_date}&todate={end_date}"
        # coal_testing_url = f"http://172.21.96.145/limsapi/api/SampleDetails/GetSampleRecord/GetSampleRecord?Fromdate={start_date}&todate={end_date}"
        try:
            response = requests.request("GET", url=coal_testing_url, headers=headers, data=payload, proxies=proxies)
            testing_data = response.json()
            if testing_data.get("statusCode") == 200:
                for entry in testing_data["responseData"]:
                    # console_logger.debug(entry)
                    if entry.get("supplier") == "WCL" and entry.get("rrNo") != "" and entry.get("rrNo") != "NA":
                        gcv_value = get_val_from_sample_params(entry['sample_Parameters'], "Gross calorific value (ADB)")
                        # console_logger.debug(entry.get("rake_No"))
                        # Fetch grade directly while inserting the RecieptCoalQualityAnalysis object
                        if gcv_value:
                            gcv_value_float = float(gcv_value)  # Ensure gcv_value is a float for comparison

                            # Find the appropriate grade from CoalGrades or default to G-1 for values above 7001
                            grade = next(
                                (
                                    single_coal_grades["grade"]
                                    for single_coal_grades in CoalGrades.objects()
                                    if int(single_coal_grades["start_value"]) <= int(gcv_value_float) <= int(single_coal_grades["end_value"])
                                    and single_coal_grades["start_value"] != ""
                                    and single_coal_grades["end_value"] != ""
                                ),
                                "G-1" if gcv_value_float > 7001 else None
                            )
                        else:
                            grade = None
                        fetchMineName = get_mine_name_from_sap_records(entry.get("rrNo"), "mine_name")
                        fetchMineGrade = get_mine_name_from_sap_records(entry.get("rrNo"), "grade")
                        fetchtypeConsumerRoad = get_mine_name_from_sap_records(entry.get("rrNo"), "consumer_type")
                        found_data = find_update_receipt_coal_quality(entry.get("rrNo"))
                        console_logger.debug(found_data)
                        try:
                            receipt = RecieptCoalQualityAnalysis.objects.get(
                                plant_sample_id=entry.get("sample_Id_No")
                            )
                            receipt.update(
                                plant_sample_id=entry.get("sample_Id_No"),
                                sample_no=str(int(entry.get("rake_No").split("-")[1])),
                                plant_sample_date=entry.get("sample_Date"),
                                plant_preperation_date=entry.get("sample_Received_Date"),
                                plant_analysis_date=entry.get("analysis_Date"),
                                sample_qty=float(entry.get("rR_Qty")),
                                # mine=fetchMineName or entry.get("supplier"),
                                mine=found_data.get("mine") if found_data else None,
                                mine_grade=fetchMineGrade.replace(" ", "") if fetchMineGrade else None,
                                mode="Road",
                                # type_consumer=fetchtypeConsumerRoad if fetchtypeConsumerRoad else None,
                                type_consumer=found_data.get("consumer_type") if found_data else None,
                                plant_lab_temp=float(entry.get("test_Temp")),
                                plant_lab_rh=float(entry.get("humidity")),
                                plant_adb_im=get_val_from_sample_params(entry['sample_Parameters'], "Inherent Moisture (ADB)"),
                                plant_adb_vm=get_val_from_sample_params(entry['sample_Parameters'], "Volatile Matter (ADB)"),
                                plant_adb_ash=get_val_from_sample_params(entry['sample_Parameters'], "ASH (ADB)"),
                                plant_adb_fc=0,
                                plant_adb_gcv=get_val_from_sample_params(entry['sample_Parameters'], "Gross calorific value (ADB)"),
                                plant_arb_tm=get_val_from_sample_params(entry['sample_Parameters'], "Total Moisture"),
                                plant_arb_vm=get_val_from_sample_params(entry['sample_Parameters'], "Volatile Matter (ARB)"),
                                plant_arb_ash=get_val_from_sample_params(entry['sample_Parameters'], "ASH (ARB)"),
                                plant_arb_fc=get_val_from_sample_params(entry['sample_Parameters'], "Fixed Carbon (ARB)"),
                                plant_arb_gcv=get_val_from_sample_params(entry['sample_Parameters'], "Gross Calorific Value (ARB)"),
                                plant_ulr_id=entry.get("ulrNo"),
                                plant_gcv_grade=grade
                            )
                        except DoesNotExist as e:
                            insertRoadReceipt = RecieptCoalQualityAnalysis(
                                plant_certificate_id = entry.get("test_Report_No"),
                                plant_sample_id = entry.get("sample_Id_No"),
                                sample_no = str(int(entry.get("rake_No").split("-")[1])),
                                sample_id = entry.get("rrNo"),
                                plant_sample_date = entry.get("sample_Date"),
                                plant_preperation_date = entry.get("sample_Received_Date"),
                                plant_analysis_date = entry.get("analysis_Date"),
                                sample_qty = float(entry.get("rR_Qty")),
                                mine = found_data.get("mine") if found_data else None,
                                mine_grade = fetchMineGrade.replace(" ", "") if fetchMineGrade else None,
                                mode = "Road",
                                # type_consumer=fetchtypeConsumerRoad if fetchtypeConsumerRoad else None,
                                type_consumer=found_data.get("consumer_type") if found_data else None,
                                plant_lab_temp = float(entry.get("test_Temp")),
                                plant_lab_rh = float(entry.get("humidity")),
                                plant_adb_im = get_val_from_sample_params(entry['sample_Parameters'], "Inherent Moisture (ADB)"),
                                plant_adb_vm = get_val_from_sample_params(entry['sample_Parameters'], "Volatile Matter (ADB)"),
                                plant_adb_ash = get_val_from_sample_params(entry['sample_Parameters'], "ASH (ADB)"),
                                plant_adb_fc = 0,
                                plant_adb_gcv = get_val_from_sample_params(entry['sample_Parameters'], "Gross calorific value (ADB)"),
                                plant_arb_tm = get_val_from_sample_params(entry['sample_Parameters'], "Total Moisture"),
                                plant_arb_vm = get_val_from_sample_params(entry['sample_Parameters'], "Volatile Matter (ARB)"),
                                plant_arb_ash = get_val_from_sample_params(entry['sample_Parameters'], "ASH (ARB)"),
                                plant_arb_fc = get_val_from_sample_params(entry['sample_Parameters'], "Fixed Carbon (ARB)"),
                                plant_arb_gcv = get_val_from_sample_params(entry['sample_Parameters'], "Gross Calorific Value (ARB)"),
                                plant_ulr_id = entry.get("ulrNo"),
                                plant_gcv_grade = grade
                            )
                            insertRoadReceipt.save()

                    if (
                        entry["supplier"] == "SECL"
                        and entry["rrNo"] != ""
                        and entry["rrNo"] != "NA"
                    ):
                        gcv_value = get_val_from_sample_params(entry['sample_Parameters'], "Gross calorific value (ADB)")

                        # Fetch grade directly while inserting the RecieptCoalQualityAnalysis object
                        if gcv_value:
                            gcv_value_float = float(gcv_value)  # Ensure gcv_value is a float for comparison

                            # Find the appropriate grade from CoalGrades or default to G-1 for values above 7001
                            grade = next(
                                (
                                    single_coal_grades["grade"]
                                    for single_coal_grades in CoalGrades.objects()
                                    if int(single_coal_grades["start_value"]) <= int(gcv_value_float) <= int(single_coal_grades["end_value"])
                                    and single_coal_grades["start_value"] != ""
                                    and single_coal_grades["end_value"] != ""
                                ),
                                "G-1" if gcv_value_float > 7001 else None
                            )
                        else:
                            grade = None
                        console_logger.debug(entry.get("rrNo"))
                        console_logger.debug(entry.get("sample_Id_No"))
                        # fetchMineName = get_mine_name_from_sap_records(entry.get("rrNo"), "mine_name")
                        # fetchMineGrade = get_mine_name_from_sap_records(entry.get("rrNo"), "grade")
                        fetchMineGrade = get_mine_name_from_railData(entry.get("rrNo"), "grade")
                        fetchMineName = get_mine_name_from_railData(entry.get("rrNo"), "mine_name")
                        fetchTypeConsumerRail = get_mine_name_from_railData(entry.get("rrNo"), "type_consumer")
                        found_data = find_update_receipt_coal_quality(entry.get("rrNo"))
                        try:
                            if len(entry.get("rake_No")) == 2:
                                rake_data = str(entry.get("rake_No"))
                            else:
                                rake_data = str(int(entry.get("rake_No").split("-")[1]))
                            receipt = RecieptCoalQualityAnalysis.objects.get(
                                plant_sample_id=entry.get("sample_Id_No")
                            )
                            receipt.update(
                                plant_sample_id=entry.get("sample_Id_No"),
                                sample_no = rake_data,
                                # sample_no=str(int(entry.get("rake_No").split("-")[1])),
                                plant_sample_date=entry.get("sample_Date"),
                                plant_preperation_date=entry.get("sample_Received_Date"),
                                plant_analysis_date=entry.get("analysis_Date"),
                                sample_qty=float(entry.get("rR_Qty")),
                                # mine=fetchMineName if fetchMineName else entry.get("supplier"),
                                mine=found_data.get("mine") if found_data else None,
                                mine_grade=fetchMineGrade.replace(" ", "") if fetchMineGrade else None,
                                mode="Rail",
                                # type_consumer=fetchTypeConsumerRail if fetchTypeConsumerRail else None,
                                type_consumer=found_data.get("consumer_type") if found_data else None,
                                plant_lab_temp=float(entry.get("test_Temp")),
                                plant_lab_rh=float(entry.get("humidity")),
                                plant_adb_im=get_val_from_sample_params(entry['sample_Parameters'], "Inherent Moisture (ADB)"),
                                plant_adb_vm=get_val_from_sample_params(entry['sample_Parameters'], "Volatile Matter (ADB)"),
                                plant_adb_ash=get_val_from_sample_params(entry['sample_Parameters'], "ASH (ADB)"),
                                plant_adb_fc=0,
                                plant_adb_gcv=get_val_from_sample_params(entry['sample_Parameters'], "Gross calorific value (ADB)"),
                                plant_arb_tm=get_val_from_sample_params(entry['sample_Parameters'], "Total Moisture"),
                                plant_arb_vm=get_val_from_sample_params(entry['sample_Parameters'], "Volatile Matter (ARB)"),
                                plant_arb_ash=get_val_from_sample_params(entry['sample_Parameters'], "ASH (ARB)"),
                                plant_arb_fc=get_val_from_sample_params(entry['sample_Parameters'], "Fixed Carbon (ARB)"),
                                plant_arb_gcv=get_val_from_sample_params(entry['sample_Parameters'], "Gross Calorific Value (ARB)"),
                                plant_ulr_id=entry.get("ulrNo"),
                                plant_gcv_grade=grade
                            )

                        except DoesNotExist as e:
                            console_logger.debug(entry.get("sample_Id_No"))
                            console_logger.debug(entry.get("rake_No"))
                            if len(entry.get("rake_No")) == 2:
                                rake_data = str(entry.get("rake_No"))
                            else:
                                rake_data = str(int(entry.get("rake_No").split("-")[1]))
                            insertRailReceipt = RecieptCoalQualityAnalysis(
                                plant_certificate_id = entry.get("test_Report_No"),
                                plant_sample_id = entry.get("sample_Id_No"),
                                sample_no = rake_data,
                                sample_id = entry.get("rrNo"),
                                plant_sample_date = entry.get("sample_Date"),
                                plant_preperation_date = entry.get("sample_Received_Date"),
                                plant_analysis_date = entry.get("analysis_Date"),
                                sample_qty = float(entry.get("rR_Qty")),
                                # mine = fetchMineName if fetchMineName else entry.get("supplier"),
                                mine = found_data.get("mine") if found_data else None,
                                mine_grade = fetchMineGrade.replace(" ", "") if fetchMineGrade else None,
                                mode = "Rail",
                                # type_consumer = fetchTypeConsumerRail if fetchTypeConsumerRail else None,
                                type_consumer=found_data.get("consumer_type") if found_data else None,
                                plant_lab_temp = float(entry.get("test_Temp")),
                                plant_lab_rh = float(entry.get("humidity")),
                                plant_adb_im = get_val_from_sample_params(entry['sample_Parameters'], "Inherent Moisture (ADB)"),
                                plant_adb_vm = get_val_from_sample_params(entry['sample_Parameters'], "Volatile Matter (ADB)"),
                                plant_adb_ash = get_val_from_sample_params(entry['sample_Parameters'], "ASH (ADB)"),
                                plant_adb_fc = 0,
                                plant_adb_gcv = get_val_from_sample_params(entry['sample_Parameters'], "Gross calorific value (ADB)"),
                                plant_arb_tm = get_val_from_sample_params(entry['sample_Parameters'], "Total Moisture"),
                                plant_arb_vm = get_val_from_sample_params(entry['sample_Parameters'], "Volatile Matter (ARB)"),
                                plant_arb_ash = get_val_from_sample_params(entry['sample_Parameters'], "ASH (ARB)"),
                                plant_arb_fc = get_val_from_sample_params(entry['sample_Parameters'], "Fixed Carbon (ARB)"),
                                plant_arb_gcv = get_val_from_sample_params(entry['sample_Parameters'], "Gross Calorific Value (ARB)"),
                                plant_ulr_id = entry.get("ulrNo"),
                                plant_gcv_grade = grade,
                            )
                            insertRailReceipt.save()

                    # update based on rr_No from Rcr_Data changes on 11-11-2004 on 04:21 pm (changes said by sachin bhai)
                    try:
                        # console_logger.debug(entry.get("sample_Id_No"))
                        # # console_logger.debug(entry.get("rake_No"))
                        # if len(entry.get("rake_No")) == 2:
                        #     rake_data = str(entry.get("rake_No"))
                        # else:
                        #     rake_data = str(int(entry.get("rake_No").split("-")[1]))
                        fetchRcrData = RcrData.objects.get(rr_no = entry.get("rrNo"))
                        # console_logger.debug(fetchRcrData.rr_no)
                        # updateDataReciept = RecieptCoalQualityAnalysis.objects.get(sample_id=fetchRcrData.rr_no, sample_no=str(int(entry.get("rake_No").split("-")[1])))
                        updateDataReciept = RecieptCoalQualityAnalysis.objects.get(sample_id=fetchRcrData.rr_no)
                        updateDataReciept.update(mine=fetchRcrData.source, type_consumer=fetchRcrData.source_type, mine_grade=fetchRcrData.grade)

                    except DoesNotExist as e:
                        continue
                    

            success = "completed"
        except requests.exceptions.Timeout:
            console_logger.debug("Request timed out!")
        except requests.exceptions.ConnectionError:
            console_logger.debug("Connection error")
        
    except Exception as e:
        success = False
        console_logger.debug("----- Coal Testing Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e
    finally:
        SchedulerResponse("save testing data", f"{success}")
        return {"message": "Successful"}

# delete duplicate from RecieptCoalQualityAnalysis
@router.get("/delete/duplicate/receiptcoalquality")
def endpoint_to_delete_duplicate_data_from_RecieptCoalQualityAnalysis(response: Response):
    try:
        pipeline = [
            {
                "$group": {
                    # "_id": { "sample_id": "$sample_id", "plant_certificate_id": "$plant_certificate_id", "sample_no": "$sample_no" },
                    "_id": { "sample_id": "$sample_id", "sample_no": "$sample_no" },
                    "ids": { "$push": "$_id" },
                    "count": { "$sum": 1 }
                }
            },
            {
                "$match": {
                    "count": { "$gt": 1 }
                }
            }
        ]

        # Step 2: Run aggregation pipeline to get duplicates
        duplicate_groups = list(RecieptCoalQualityAnalysis.objects.aggregate(pipeline))

        # Step 3: Delete only the first duplicate in each group
        for group in duplicate_groups:
            first_duplicate_id = group["ids"][0]  # Get the first document's ID in each duplicate group
            RecieptCoalQualityAnalysis.objects.filter(id=first_duplicate_id).delete()  # Delete only the first duplicate

        console_logger.debug("First duplicate in each group has been deleted.")

        return {"detail": "success"}

    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- Sap Excel Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return {"error": str(e)} 
    



@router.get("/extract_coal_test_old", tags=["Coal Testing"])
def coal_test_old(start_date: Optional[str] = None, end_date: Optional[str] = None):
    success = False
    try:
        global proxies
        # entry = UsecaseParameters.objects.filter(Parameters__gmr_api__exists=True).first()
        entry = UsecaseParameters.objects.first()
        testing_ip = entry.Parameters.get("gmr_api", {}).get("roi1", {}).get("Coal Testing IP") if entry else None
        testing_timer = entry.Parameters.get("gmr_api", {}).get("roi1", {}).get("Coal Testing Duration") if entry else None

        console_logger.debug(f"---- Coal Testing IP ----            {testing_ip}")
        console_logger.debug(f"---- Coal Testing Duration ----      {testing_timer}")

        payload = {}
        headers = {}
        current_time = datetime.datetime.now(IST)
        current_date = current_time.date()
        if not end_date:
            end_date = current_date.__str__()  #  end_date will always be the current date

        if not start_date:
            no_of_day = testing_timer.split(":")[0]
            start_date = (current_date - timedelta(int(no_of_day))).__str__()

        console_logger.debug(f" --- Test Start Date --- {start_date}")
        console_logger.debug(f" --- Test End Date --- {end_date}")
        console_logger.debug(ip)
        coal_testing_url = f"http://{ip}/api/v1/host/coal_extract_data?start_date={start_date}&end_date={end_date}"
        # coal_testing_url = f"http://{ip}/api/v1/host/coal_extract_data?start_date=2024-07-09&end_date=2024-08-09"
        # coal_testing_url = f"http://{testing_ip}/limsapi/api/SampleDetails/GetSampleRecord/GetSampleRecord?Fromdate={start_date}&todate={end_date}"
        # coal_testing_url = f"http://172.21.96.145/limsapi/api/SampleDetails/GetSampleRecord/GetSampleRecord?Fromdate={start_date}&todate={end_date}"
        try:
            response = requests.request("GET", url=coal_testing_url, headers=headers, data=payload, proxies=proxies)
            testing_data = response.json()
            if testing_data.get("statusCode") == 200:
                for entry in testing_data["responseData"]:
                    console_logger.debug(entry)
                    if entry.get("supplier") == "WCL" and entry.get("rrNo") != "" and entry.get("rrNo") != "NA":
                        gcv_value = get_val_from_sample_params(entry['sample_Parameters'], "Gross calorific value (ADB)")
                        # console_logger.debug(entry.get("rake_No"))
                        # Fetch grade directly while inserting the RecieptCoalQualityAnalysis object
                        if gcv_value:
                            gcv_value_float = float(gcv_value)  # Ensure gcv_value is a float for comparison

                            # Find the appropriate grade from CoalGrades or default to G-1 for values above 7001
                            grade = next(
                                (
                                    single_coal_grades["grade"]
                                    for single_coal_grades in CoalGrades.objects()
                                    if int(single_coal_grades["start_value"]) <= int(gcv_value_float) <= int(single_coal_grades["end_value"])
                                    and single_coal_grades["start_value"] != ""
                                    and single_coal_grades["end_value"] != ""
                                ),
                                "G-1" if gcv_value_float > 7001 else None
                            )
                        else:
                            grade = None
                        fetchMineName = get_mine_name_from_sap_records(entry.get("rrNo"), "mine_name")
                        fetchMineGrade = get_mine_name_from_sap_records(entry.get("rrNo"), "grade")
                        try:
                            # console_logger.debug(entry.get("rrNo"))
                            # console_logger.debug(entry.get("test_Report_No"))
                            # fetchReceiptCoalQualityAnalysis = RecieptCoalQualityAnalysis.objects.get(sample_id=entry.get("rrNo"), plant_certificate_id=entry.get("test_Report_No"))
                            # # fetchReceiptCoalQualityAnalysis.plant_certificate_id = entry.get("test_Report_No")
                            # fetchReceiptCoalQualityAnalysis.plant_sample_id=entry.get("sample_Id_No")
                            # fetchReceiptCoalQualityAnalysis.sample_no = str(int(entry.get("rake_No").split("-")[1]))
                            # fetchReceiptCoalQualityAnalysis.sample_id = entry.get("rrNo")
                            # fetchReceiptCoalQualityAnalysis.plant_sample_date = entry.get("sample_Date")
                            # fetchReceiptCoalQualityAnalysis.plant_preperation_date = entry.get("sample_Received_Date")
                            # fetchReceiptCoalQualityAnalysis.plant_analysis_date = entry.get("analysis_Date")
                            # fetchReceiptCoalQualityAnalysis.sample_qty = float(entry.get("rR_Qty"))
                            # fetchReceiptCoalQualityAnalysis.mine = fetchMineName if fetchMineName else entry.get("supplier")
                            # fetchReceiptCoalQualityAnalysis.mine_grade = fetchMineGrade.replace(" ", "") if fetchMineGrade else None
                            # fetchReceiptCoalQualityAnalysis.mode = "Road"
                            # fetchReceiptCoalQualityAnalysis.plant_lab_temp = float(entry.get("test_Temp"))
                            # fetchReceiptCoalQualityAnalysis.plant_lab_rh = float(entry.get("humidity"))
                            # fetchReceiptCoalQualityAnalysis.plant_adb_im = get_val_from_sample_params(entry['sample_Parameters'], "Inherent Moisture (ADB)")
                            # fetchReceiptCoalQualityAnalysis.plant_adb_vm = get_val_from_sample_params(entry['sample_Parameters'], "Volatile Matter (ADB)")
                            # fetchReceiptCoalQualityAnalysis.plant_adb_ash = get_val_from_sample_params(entry['sample_Parameters'], "ASH (ADB)")
                            # fetchReceiptCoalQualityAnalysis.plant_adb_fc = 0
                            # fetchReceiptCoalQualityAnalysis.plant_adb_gcv = get_val_from_sample_params(entry['sample_Parameters'], "Gross calorific value (ADB)")
                            # fetchReceiptCoalQualityAnalysis.plant_arb_tm = get_val_from_sample_params(entry['sample_Parameters'], "Total Moisture")
                            # fetchReceiptCoalQualityAnalysis.plant_arb_vm = get_val_from_sample_params(entry['sample_Parameters'], "Volatile Matter (ARB)")
                            # fetchReceiptCoalQualityAnalysis.plant_arb_ash = get_val_from_sample_params(entry['sample_Parameters'], "ASH (ARB)")
                            # fetchReceiptCoalQualityAnalysis.plant_arb_fc = get_val_from_sample_params(entry['sample_Parameters'], "Fixed Carbon (ARB)")
                            # fetchReceiptCoalQualityAnalysis.plant_arb_gcv = get_val_from_sample_params(entry['sample_Parameters'], "Gross Calorific Value (ARB)")
                            # fetchReceiptCoalQualityAnalysis.plant_ulr_id = entry.get("ulrNo")
                            # fetchReceiptCoalQualityAnalysis.plant_gcv_grade = grade
                            # fetchReceiptCoalQualityAnalysis.save()
                            receipt = RecieptCoalQualityAnalysis.objects.get(
                                sample_id=entry.get("rrNo"),
                                plant_certificate_id=entry.get("test_Report_No"),
                                sample_no=str(int(entry.get("rake_No").split("-")[1]))
                            )
                            receipt.update(
                                plant_sample_id=entry.get("sample_Id_No"),
                                sample_no=str(int(entry.get("rake_No").split("-")[1])),
                                plant_sample_date=entry.get("sample_Date"),
                                plant_preperation_date=entry.get("sample_Received_Date"),
                                plant_analysis_date=entry.get("analysis_Date"),
                                sample_qty=float(entry.get("rR_Qty")),
                                mine=fetchMineName or entry.get("supplier"),
                                mine_grade=fetchMineGrade.replace(" ", "") if fetchMineGrade else None,
                                mode="Road",
                                plant_lab_temp=float(entry.get("test_Temp")),
                                plant_lab_rh=float(entry.get("humidity")),
                                plant_adb_im=get_val_from_sample_params(entry['sample_Parameters'], "Inherent Moisture (ADB)"),
                                plant_adb_vm=get_val_from_sample_params(entry['sample_Parameters'], "Volatile Matter (ADB)"),
                                plant_adb_ash=get_val_from_sample_params(entry['sample_Parameters'], "ASH (ADB)"),
                                plant_adb_fc=0,
                                plant_adb_gcv=get_val_from_sample_params(entry['sample_Parameters'], "Gross calorific value (ADB)"),
                                plant_arb_tm=get_val_from_sample_params(entry['sample_Parameters'], "Total Moisture"),
                                plant_arb_vm=get_val_from_sample_params(entry['sample_Parameters'], "Volatile Matter (ARB)"),
                                plant_arb_ash=get_val_from_sample_params(entry['sample_Parameters'], "ASH (ARB)"),
                                plant_arb_fc=get_val_from_sample_params(entry['sample_Parameters'], "Fixed Carbon (ARB)"),
                                plant_arb_gcv=get_val_from_sample_params(entry['sample_Parameters'], "Gross Calorific Value (ARB)"),
                                plant_ulr_id=entry.get("ulrNo"),
                                plant_gcv_grade=grade
                            )
                        except DoesNotExist as e:
                            insertRoadReceipt = RecieptCoalQualityAnalysis(
                                plant_certificate_id = entry.get("test_Report_No"),
                                plant_sample_id = entry.get("sample_Id_No"),
                                sample_no = str(int(entry.get("rake_No").split("-")[1])),
                                sample_id = entry.get("rrNo"),
                                plant_sample_date = entry.get("sample_Date"),
                                plant_preperation_date = entry.get("sample_Received_Date"),
                                plant_analysis_date = entry.get("analysis_Date"),
                                sample_qty = entry.get("rR_Qty"),
                                mine = fetchMineName if fetchMineName else entry.get("supplier"),
                                mine_grade = fetchMineGrade.replace(" ", "") if fetchMineGrade else None,
                                mode = "Road",
                                plant_lab_temp = entry.get("test_Temp"),
                                plant_lab_rh = entry.get("humidity"),
                                plant_adb_im = get_val_from_sample_params(entry['sample_Parameters'], "Inherent Moisture (ADB)"),
                                plant_adb_vm = get_val_from_sample_params(entry['sample_Parameters'], "Volatile Matter (ADB)"),
                                plant_adb_ash = get_val_from_sample_params(entry['sample_Parameters'], "ASH (ADB)"),
                                plant_adb_fc = 0,
                                plant_adb_gcv = get_val_from_sample_params(entry['sample_Parameters'], "Gross calorific value (ADB)"),
                                plant_arb_tm = get_val_from_sample_params(entry['sample_Parameters'], "Total Moisture"),
                                plant_arb_vm = get_val_from_sample_params(entry['sample_Parameters'], "Volatile Matter (ARB)"),
                                plant_arb_ash = get_val_from_sample_params(entry['sample_Parameters'], "ASH (ARB)"),
                                plant_arb_fc = get_val_from_sample_params(entry['sample_Parameters'], "Fixed Carbon (ARB)"),
                                plant_arb_gcv = get_val_from_sample_params(entry['sample_Parameters'], "Gross Calorific Value (ARB)"),
                                plant_ulr_id = entry.get("ulrNo"),
                                plant_gcv_grade = grade
                            )
                            insertRoadReceipt.save()

                    if (
                        entry["supplier"] == "SECL"
                        and entry["rrNo"] != ""
                        and entry["rrNo"] != "NA"
                    ):
                        console_logger.debug(entry.get("rake_No"))
                        gcv_value = get_val_from_sample_params(entry['sample_Parameters'], "Gross calorific value (ADB)")

                        # Fetch grade directly while inserting the RecieptCoalQualityAnalysis object
                        if gcv_value:
                            gcv_value_float = float(gcv_value)  # Ensure gcv_value is a float for comparison

                            # Find the appropriate grade from CoalGrades or default to G-1 for values above 7001
                            grade = next(
                                (
                                    single_coal_grades["grade"]
                                    for single_coal_grades in CoalGrades.objects()
                                    if int(single_coal_grades["start_value"]) <= int(gcv_value_float) <= int(single_coal_grades["end_value"])
                                    and single_coal_grades["start_value"] != ""
                                    and single_coal_grades["end_value"] != ""
                                ),
                                "G-1" if gcv_value_float > 7001 else None
                            )
                        else:
                            grade = None
                        fetchMineName = get_mine_name_from_sap_records(entry.get("rrNo"), "mine_name")
                        fetchMineGrade = get_mine_name_from_sap_records(entry.get("rrNo"), "grade")
                        try:
                            receiptRail = RecieptCoalQualityAnalysis.objects.get(
                                sample_id=entry.get("rrNo"),
                                plant_certificate_id=entry.get("test_Report_No"),
                                sample_no=str(int(entry.get("rake_No").split("-")[1]))
                            )
                            receiptRail.update(
                                plant_sample_id=entry.get("sample_Id_No"),
                                sample_no=str(int(entry.get("rake_No").split("-")[1])),
                                plant_sample_date=entry.get("sample_Date"),
                                plant_preperation_date=entry.get("sample_Received_Date"),
                                plant_analysis_date=entry.get("analysis_Date"),
                                sample_qty=float(entry.get("rR_Qty")),
                                mine=fetchMineName or entry.get("supplier"),
                                mine_grade=fetchMineGrade.replace(" ", "") if fetchMineGrade else None,
                                mode="Rail",
                                plant_lab_temp=float(entry.get("test_Temp")),
                                plant_lab_rh=float(entry.get("humidity")),
                                plant_adb_im=get_val_from_sample_params(entry['sample_Parameters'], "Inherent Moisture (ADB)"),
                                plant_adb_vm=get_val_from_sample_params(entry['sample_Parameters'], "Volatile Matter (ADB)"),
                                plant_adb_ash=get_val_from_sample_params(entry['sample_Parameters'], "ASH (ADB)"),
                                plant_adb_fc=0,
                                plant_adb_gcv=get_val_from_sample_params(entry['sample_Parameters'], "Gross calorific value (ADB)"),
                                plant_arb_tm=get_val_from_sample_params(entry['sample_Parameters'], "Total Moisture"),
                                plant_arb_vm=get_val_from_sample_params(entry['sample_Parameters'], "Volatile Matter (ARB)"),
                                plant_arb_ash=get_val_from_sample_params(entry['sample_Parameters'], "ASH (ARB)"),
                                plant_arb_fc=get_val_from_sample_params(entry['sample_Parameters'], "Fixed Carbon (ARB)"),
                                plant_arb_gcv=get_val_from_sample_params(entry['sample_Parameters'], "Gross Calorific Value (ARB)"),
                                plant_ulr_id=entry.get("ulrNo"),
                                plant_gcv_grade=grade
                            )
                        except DoesNotExist as e:
                            insertRailReceipt = RecieptCoalQualityAnalysis(
                                plant_certificate_id = entry.get("test_Report_No"),
                                plant_sample_id = entry.get("sample_Id_No"),
                                sample_no = str(int(entry.get("rake_No").split("-")[1])),
                                sample_id = entry.get("rrNo"),
                                plant_sample_date = entry.get("sample_Date"),
                                plant_preperation_date = entry.get("sample_Received_Date"),
                                plant_analysis_date = entry.get("analysis_Date"),
                                sample_qty = entry.get("rR_Qty"),
                                mine = fetchMineName if fetchMineName else entry.get("supplier"),
                                mine_grade = fetchMineGrade.replace(" ", "") if fetchMineGrade else None,
                                mode = "Rail",
                                plant_lab_temp = entry.get("test_Temp"),
                                plant_lab_rh = entry.get("humidity"),
                                plant_adb_im = get_val_from_sample_params(entry['sample_Parameters'], "Inherent Moisture (ADB)"),
                                plant_adb_vm = get_val_from_sample_params(entry['sample_Parameters'], "Volatile Matter (ADB)"),
                                plant_adb_ash = get_val_from_sample_params(entry['sample_Parameters'], "ASH (ADB)"),
                                plant_adb_fc = 0,
                                plant_adb_gcv = get_val_from_sample_params(entry['sample_Parameters'], "Gross calorific value (ADB)"),
                                plant_arb_tm = get_val_from_sample_params(entry['sample_Parameters'], "Total Moisture"),
                                plant_arb_vm = get_val_from_sample_params(entry['sample_Parameters'], "Volatile Matter (ARB)"),
                                plant_arb_ash = get_val_from_sample_params(entry['sample_Parameters'], "ASH (ARB)"),
                                plant_arb_fc = get_val_from_sample_params(entry['sample_Parameters'], "Fixed Carbon (ARB)"),
                                plant_arb_gcv = get_val_from_sample_params(entry['sample_Parameters'], "Gross Calorific Value (ARB)"),
                                plant_ulr_id = entry.get("ulrNo"),
                                plant_gcv_grade = grade,
                            )
                            insertRailReceipt.save()
            success = "completed"
        except requests.exceptions.Timeout:
            console_logger.debug("Request timed out!")
        except requests.exceptions.ConnectionError:
            console_logger.debug("Connection error")
        
    except Exception as e:
        success = False
        console_logger.debug("----- Coal Testing Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e
    finally:
        SchedulerResponse("save testing data", f"{success}")
        return {"message": "Successful"}



# as per RecieptCoalQualityAnalysis wise
@router.get("/coal_gcv_table", tags=["Coal Testing"])
def coal_wcl_gcv_table(
    response: Response,
    currentPage: Optional[int] = None,
    perPage: Optional[int] = None,
    search_text: Optional[str] = None,
    start_timestamp: Optional[str] = None,
    end_timestamp: Optional[str] = None,
    month_date: Optional[str] = None,
    type: Optional[str] = "display"):
    try:
        result = {"labels": [], "datasets": [], "total": 0, "page_size": 15}

        if type and type == "display":
            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage

            data = Q()

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(plant_analysis_date__gte = start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(plant_analysis_date__lte = end_date)

            if search_text:
                if search_text.isdigit():
                    data &= (Q(sample_id__icontains=search_text))
                else:
                    data &= (Q(mine__icontains=search_text))

            offset = (page_no - 1) * page_len
            # logs = CoalTesting.objects(data).order_by("-ID").skip(offset).limit(page_len)
            logs = RecieptCoalQualityAnalysis.objects(data, mode="Road")

            if any(logs):
                aggregated_data = defaultdict(lambda: defaultdict(lambda: {"DO_Qty": 0, "Gross_Calorific_Value_(Adb)": 0, "Third_Party_Gross_Calorific_Value_(Adb)": 0, "Third_Party_Gross_Calorific_Value_(Adb)_count": 0, "count": 0}))

                for log in logs:
                    month = log.plant_analysis_date.strftime("%Y-%m")
                    payload = log.payload()
                    mine = payload["mine"]
                    if payload.get("sample_qty"):
                        aggregated_data[month][mine]["DO_Qty"] += float(payload["sample_qty"])
                        # if payload.get("DO_Qty").count('.') > 1:
                        #     aggregated_data[month][mine]["DO_Qty"] += float(payload.get("DO_Qty")[:5])
                        # else:
                        #     if "," in payload["DO_Qty"]:
                        #         aggregated_data[month][mine]["DO_Qty"] += float(payload["DO_Qty"].replace(",", ""))
                        #     else:
                        #         aggregated_data[month][mine]["DO_Qty"] += float(payload["DO_Qty"])
                    if payload.get("GWEL_ADB_GCV"):
                        aggregated_data[month][mine]["Gross_Calorific_Value_(Adb)"] += float(payload["GWEL_ADB_GCV"])
                    if payload.get("THIRDPARTY_ADB_GCV"):
                        aggregated_data[month][mine]["Third_Party_Gross_Calorific_Value_(Adb)"] += float(payload["THIRDPARTY_ADB_GCV"])
                        aggregated_data[month][mine]["Third_Party_Gross_Calorific_Value_(Adb)_count"] += 1
                    aggregated_data[month][mine]["count"] += 1

                dataList = [
                    {"month": month, "data": {
                        mine: {
                            # first it was average then we have changed to total on 08-11-2024 11:31 am
                            # "average_DO_Qty": data["DO_Qty"] / data["count"],
                            "average_DO_Qty": data["DO_Qty"],
                            "average_Gross_Calorific_Value_(Adb)": data["Gross_Calorific_Value_(Adb)"] / data["count"],
                            "average_Third_Party_Gross_Calorific_Value_(Adb)": data["Third_Party_Gross_Calorific_Value_(Adb)"] / data["Third_Party_Gross_Calorific_Value_(Adb)_count"] if data["Third_Party_Gross_Calorific_Value_(Adb)"] != 0 else "",
                        } for mine, data in aggregated_data[month].items()
                    }} for month in aggregated_data
                ]
                coal_grades = CoalGrades.objects()  # Fetch all coal grades from the database

                # Iterate through each month's data
                for month_data in dataList:
                    for key, mine_data in month_data["data"].items():
                        if mine_data["average_Gross_Calorific_Value_(Adb)"] is not None:
                            for single_coal_grades in coal_grades:
                                if single_coal_grades["end_value"] != "":
                                    if (int(single_coal_grades["start_value"]) <= int(float(mine_data.get("average_Gross_Calorific_Value_(Adb)"))) <= int(single_coal_grades["end_value"]) and single_coal_grades["start_value"] != "" and single_coal_grades["end_value"] != ""):
                                        mine_data["average_GCV_Grade"] = single_coal_grades["grade"]
                                    elif int(mine_data["average_Gross_Calorific_Value_(Adb)"]) > 7001:
                                        mine_data["average_GCV_Grade"] = "G-1"
                                        break

                        if mine_data["average_Third_Party_Gross_Calorific_Value_(Adb)"] != "":
                            for single_coal_grades in coal_grades:
                                if single_coal_grades["end_value"] != "":
                                    if (int(single_coal_grades["start_value"]) <= int(float(mine_data.get("average_Third_Party_Gross_Calorific_Value_(Adb)"))) <= int(single_coal_grades["end_value"]) and single_coal_grades["start_value"] != "" and single_coal_grades["end_value"] != ""):
                                        mine_data["average_Third_Party_GCV_Grade"] = single_coal_grades["grade"]
                                    elif int(mine_data["average_Third_Party_Gross_Calorific_Value_(Adb)"]) > 7001:
                                        mine_data["average_Third_Party_GCV_Grade"] = "G-1"
                                        break

                final_data = []
                if month_date:
                    filtered_data = [entry for entry in dataList if entry["month"] == month_date]
                    if filtered_data:
                        data = filtered_data[0]['data']  # Extracting the 'data' dictionary from the list
                        for mine, values in data.items():
                            dictData = {}
                            dictData['Mine'] = mine
                            dictData['DO_Qty'] = round(values['average_DO_Qty'], 2)
                            dictData['GWEL_Gross_Calorific_Value_(Adb)'] = round(values['average_Gross_Calorific_Value_(Adb)'], 2)
                            if values["average_Third_Party_Gross_Calorific_Value_(Adb)"] != "":
                                if values.get('average_GCV_Grade'):
                                    dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'] = values['average_GCV_Grade']
                                dictData["Third_Party_Gross_Calorific_Value_(Adb)"] = round(values["average_Third_Party_Gross_Calorific_Value_(Adb)"], 2)
                                if values.get("average_Third_Party_GCV_Grade"):
                                    dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"] = str(values["average_Third_Party_GCV_Grade"])
                                dictData["Difference_Gross_Calorific_Value_(Adb)"] = str(abs(int(float(dictData["GWEL_Gross_Calorific_Value_(Adb)"])) - int(float(dictData["Third_Party_Gross_Calorific_Value_(Adb)"]))))
                                if values.get("average_Third_Party_GCV_Grade"):
                                    dictData["Difference_Gross_Calorific_Value_Grade_(Adb)"] = str(abs(int(dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'].replace('G-', '')) - int(dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"].replace('G-', ''))))
                            final_data.append(dictData)
                    else:
                        console_logger.debug(
                            f"No data available for the given month: {month_date}"
                        )
                        return result
                else:
                    console_logger.debug("inside else")
                    filtered_data = [entry for entry in dataList]
                    for single_data in filtered_data:
                        for mine, values in single_data['data'].items():
                            dictData = {}
                            dictData['Mine'] = mine
                            dictData['DO_Qty'] = round(values['average_DO_Qty'], 2)
                            dictData['GWEL_Gross_Calorific_Value_(Adb)'] = round(values['average_Gross_Calorific_Value_(Adb)'], 2)
                            if values["average_Third_Party_Gross_Calorific_Value_(Adb)"] != "":
                                dictData['Gross_Calorific_Value_Grade_(Adb)'] = values['average_GCV_Grade']
                                dictData["Third_Party_Gross_Calorific_Value_(Adb)"] = round(values["average_Third_Party_Gross_Calorific_Value_(Adb)"], 2)
                                if values.get("average_Third_Party_GCV_Grade"):
                                    dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"] = str(values["average_Third_Party_GCV_Grade"])
                                dictData["Difference_Gross_Calorific_Value_(Adb)"] = str(abs(int(float(dictData["GWEL_Gross_Calorific_Value_(Adb)"])) - int(float(dictData["Third_Party_Gross_Calorific_Value_(Adb)"]))))
                                if values.get("average_Third_Party_GCV_Grade"):
                                    dictData["Difference_Gross_Calorific_Value_Grade_(Adb)"] = str(abs(int(dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'].replace('G-', '')) - int(dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"].replace('G-', ''))))

                            final_data.append(dictData)
                # Perform pagination here using list slicing
                start_index = (page_no - 1) * page_len
                end_index = start_index + page_len
                paginated_data = final_data[start_index:end_index]

                unique_keys = OrderedDict()

                for data in paginated_data:
                    for key in data.keys():
                        unique_keys[key] = None

                result["labels"] = list(unique_keys.keys())

                result["total"] = len(final_data)
                result["datasets"] = paginated_data
                return result
            else:
                return result
        
        elif type and type == "download":
            del type
            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)
            
            data = Q()

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(plant_analysis_date__gte = start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(plant_analysis_date__lte = end_date)
            
            if search_text:
                if search_text.isdigit():
                    data &= (Q(rrNo__icontains=search_text))
                else:
                    data &= Q(location__icontains=search_text) | Q(rake_no__icontains=search_text)

            # usecase_data = CoalTesting.objects(data).order_by("-receive_date")
            usecase_data = RecieptCoalQualityAnalysis.objects(data, mode="Road")
            count = len(usecase_data)
            path = None
            if usecase_data:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "Roadwise_GCV_Grade_Comparision_Report_{}.xlsx".format(
                            datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format()
                    cell_format2.set_bold()
                    cell_format2.set_font_size(10)
                    cell_format2.set_align("center")
                    cell_format2.set_align("vjustify")

                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)
                    cell_format = workbook.add_format()
                    cell_format.set_font_size(10)
                    cell_format.set_align("center")
                    cell_format.set_align("vcenter")

                    headers = [
                        "Sr.No",
                        "Mine",
                        "DO_Qty",
                        "GWEL_Gross_Calorific_Value_(Adb)",
                        "GWEL_Gross_Calorific_Value_Grade",
                        "Third_Party_Gross_Calorific_Value_(Adb)",
                        "Third_Party_Gross_Calorific_Value_(Adb)_grade",
                        "Difference_Gross_Calorific_Value(Adb)",
                        "Difference_Gross_Calorific_Value_Grade_(Adb)"
                    ]

                    for index, header in enumerate(headers):
                        worksheet.write(0, index, header, cell_format2)


                    if any(usecase_data):
                        aggregated_data = defaultdict(lambda: defaultdict(lambda: {"DO_Qty": 0, "Gross_Calorific_Value_(Adb)": 0, "Third_Party_Gross_Calorific_Value_(Adb)": 0, "Third_Party_Gross_Calorific_Value_(Adb)_count": 0, "count": 0}))

                        for log in usecase_data:
                            month = log.plant_analysis_date.strftime("%Y-%m")
                            payload = log.payload()
                            mine = payload["mine"]
                            if payload.get("sample_qty"):
                                aggregated_data[month][mine]["DO_Qty"] += float(payload["sample_qty"])
                                # if payload.get("DO_Qty").count('.') > 1:
                                #     aggregated_data[month][mine]["DO_Qty"] += float(payload.get("DO_Qty")[:5])
                                # else:
                                #     aggregated_data[month][mine]["DO_Qty"] += float(payload["DO_Qty"])
                            if payload.get("GWEL_ADB_GCV"):
                                aggregated_data[month][mine]["Gross_Calorific_Value_(Adb)"] += float(payload["GWEL_ADB_GCV"]) 
                            if payload.get("THIRDPARTY_ADB_GCV"):
                                aggregated_data[month][mine]["Third_Party_Gross_Calorific_Value_(Adb)"] += float(payload["THIRDPARTY_ADB_GCV"])
                                aggregated_data[month][mine]["Third_Party_Gross_Calorific_Value_(Adb)_count"] += 1
                
                            aggregated_data[month][mine]["count"] += 1

                        dataList = [
                            {"month": month, "data": {
                                mine: {
                                    # first it was average then we have changed to total on 08-11-2024 11:31 am
                                    # "average_DO_Qty": data["DO_Qty"] / data["count"],
                                    "average_DO_Qty": data["DO_Qty"],
                                    "average_Gross_Calorific_Value_(Adb)": data["Gross_Calorific_Value_(Adb)"] / data["count"],
                                    "average_Third_Party_Gross_Calorific_Value_(Adb)": data["Third_Party_Gross_Calorific_Value_(Adb)"] / data["Third_Party_Gross_Calorific_Value_(Adb)_count"] if data["Third_Party_Gross_Calorific_Value_(Adb)"] != 0 else "",
                                } for mine, data in aggregated_data[month].items()
                            }} for month in aggregated_data
                        ]
                        coal_grades = CoalGrades.objects()  # Fetch all coal grades from the database

                        for month_data in dataList:
                            for key, mine_data in month_data["data"].items():
                                if mine_data["average_Gross_Calorific_Value_(Adb)"] is not None:
                                    for single_coal_grades in coal_grades:
                                        if single_coal_grades["end_value"] != "":
                                            if (int(single_coal_grades["start_value"]) <= int(float(mine_data.get("average_Gross_Calorific_Value_(Adb)"))) <= int(single_coal_grades["end_value"]) and single_coal_grades["start_value"] != "" and single_coal_grades["end_value"] != ""):
                                                mine_data["average_GCV_Grade"] = single_coal_grades["grade"]
                                            elif int(mine_data["average_Gross_Calorific_Value_(Adb)"]) > 7001:
                                                mine_data["average_GCV_Grade"] = "G-1"
                                                break

                                if mine_data["average_Third_Party_Gross_Calorific_Value_(Adb)"] != "":
                                    for single_coal_grades in coal_grades:
                                        if single_coal_grades["end_value"] != "":
                                            if (int(single_coal_grades["start_value"]) <= int(float(mine_data.get("average_Third_Party_Gross_Calorific_Value_(Adb)"))) <= int(single_coal_grades["end_value"]) and single_coal_grades["start_value"] != "" and single_coal_grades["end_value"] != ""):
                                                mine_data["average_Third_Party_GCV_Grade"] = single_coal_grades["grade"]
                                            elif int(mine_data["average_Gross_Calorific_Value_(Adb)"]) > 7001:
                                                mine_data["average_Third_Party_GCV_Grade"] = "G-1"
                                                break
                    final_data = []
                    if month_date:
                        filtered_data = [entry for entry in dataList if entry["month"] == month_date]
                        if filtered_data:
                            data = filtered_data[0]['data']  # Extracting the 'data' dictionary from the list
                            for mine, values in data.items():
                                dictData = {}
                                dictData['Mine'] = mine
                                dictData['DO_Qty'] = round(values['average_DO_Qty'], 2)
                                dictData['GWEL_Gross_Calorific_Value_(Adb)'] = round(values['average_Gross_Calorific_Value_(Adb)'], 2)
                                if values["average_Third_Party_Gross_Calorific_Value_(Adb)"] != "":    
                                    dictData['Gross_Calorific_Value_Grade_(Adb)'] = values['average_GCV_Grade']
                                    dictData["Third_Party_Gross_Calorific_Value_(Adb)"] = round(values["average_Third_Party_Gross_Calorific_Value_(Adb)"], 2)
                                    if values.get("average_Third_Party_GCV_Grade"):
                                        dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"] = str(values["average_Third_Party_GCV_Grade"])
                                    dictData["Difference_Gross_Calorific_Value_(Adb)"] = str(abs(int(float(dictData["GWEL_Gross_Calorific_Value_(Adb)"])) - int(float(dictData["Third_Party_Gross_Calorific_Value_(Adb)"]))))
                                    if values.get("average_Third_Party_GCV_Grade"):
                                        dictData["Difference_Gross_Calorific_Value_Grade_(Adb)"] = str(abs(int(dictData['GWEL_Gross_Calorific_Value_(Adb)'].replace('G-', '')) - int(dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"].replace('G-', ''))))
                                final_data.append(dictData)
                        else:
                            console_logger.debug(
                                f"No data available for the given month: {month_date}"
                            )
                            return {"message": f"No data available for the given month: {month_date}"}
                    else:
                        filtered_data = [entry for entry in dataList]
                        # data = filtered_data[0]['data']  # Extracting the 'data' dictionary from the list
                        for single_data in filtered_data:
                            for mine, values in single_data['data'].items():
                                dictData = {}
                                dictData['Mine'] = mine
                                dictData['DO_Qty'] = round(values['average_DO_Qty'], 2)
                                dictData['GWEL_Gross_Calorific_Value_(Adb)'] = round(values['average_Gross_Calorific_Value_(Adb)'], 2)
                                if values["average_Third_Party_Gross_Calorific_Value_(Adb)"] != "":
                                    dictData['Gross_Calorific_Value_Grade_(Adb)'] = values['average_GCV_Grade']
                                    dictData["Third_Party_Gross_Calorific_Value_(Adb)"] = round(values["average_Third_Party_Gross_Calorific_Value_(Adb)"],2)
                                    if values.get("average_Third_Party_GCV_Grade"):
                                        dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"] = str(values["average_Third_Party_GCV_Grade"])
                                    dictData["Difference_Gross_Calorific_Value_(Adb)"] = str(abs(int(float(dictData["GWEL_Gross_Calorific_Value_(Adb)"])) - int(float(dictData["Third_Party_Gross_Calorific_Value_(Adb)"]))))
                                    if values.get("average_Third_Party_GCV_Grade"):
                                        dictData["Difference_Gross_Calorific_Value_Grade_(Adb)"] = str(abs(int(dictData['GWEL_Gross_Calorific_Value_(Adb)'].replace('G-', '')) - int(dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"].replace('G-', ''))))
                                final_data.append(dictData)
                    result["labels"] = list(final_data[0].keys())
                    result["total"] = len(final_data)
                    result["datasets"] = final_data

                    row = 1
                    for single_data in result["datasets"]:
                        worksheet.write(row, 0, count, cell_format)
                        worksheet.write(row, 1, single_data["Mine"])
                        worksheet.write(row, 2, single_data["DO_Qty"])
                        worksheet.write(row, 3, single_data["GWEL_Gross_Calorific_Value_(Adb)"])
                        if single_data.get("Gross_Calorific_Value_Grade_(Adb)") != "" and single_data.get("Gross_Calorific_Value_Grade_(Adb)") != None:
                            worksheet.write(row, 4, str(single_data["Gross_Calorific_Value_Grade_(Adb)"]), cell_format)
                        if single_data.get("Third_Party_Gross_Calorific_Value_(Adb)") != "" and single_data.get("Third_Party_Gross_Calorific_Value_(Adb)") != None:
                            worksheet.write(row, 5, str(single_data["Third_Party_Gross_Calorific_Value_(Adb)"]), cell_format)
                        if single_data.get("Third_Party_Gross_Calorific_Value_(Adb)_grade") != "" and single_data.get("Third_Party_Gross_Calorific_Value_(Adb)_grade") != None:
                            worksheet.write(row, 6, str(single_data["Third_Party_Gross_Calorific_Value_(Adb)_grade"]), cell_format)
                        if single_data.get("Difference_Gross_Calorific_Value_(Adb)") != "" and single_data.get("Difference_Gross_Calorific_Value_(Adb)") != None:
                            worksheet.write(row, 7, str(single_data["Difference_Gross_Calorific_Value_(Adb)"]), cell_format)
                        if single_data.get("Difference_Gross_Calorific_Value_Grade_(Adb)") != "" and single_data.get("Difference_Gross_Calorific_Value_Grade_(Adb)") != None:
                            worksheet.write(row, 8, str(single_data["Difference_Gross_Calorific_Value_Grade_(Adb)"]), cell_format)
                        count -= 1
                        row += 1
                    workbook.close()

                    # console_logger.debug("Successfully {} report generated".format(service_id))
                    # console_logger.debug("sent data {}".format(path))
                    return {
                            "Type": "coal_test_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                        }
                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            else:
                console_logger.error("No data found")
                return {
                            "Type": "coal_test_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                        }
    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e



@router.get("/coal_gcv_table_aggregation", tags=["Coal Testing"])
def coal_wcl_gcv_table_update(
    response: Response,
    currentPage: Optional[int] = None,
    perPage: Optional[int] = None,
    search_text: Optional[str] = None,
    start_timestamp: Optional[str] = None,
    end_timestamp: Optional[str] = None,
    month_date: Optional[str] = None,
    type: Optional[str] = "display"):
    try:
        result = {"labels": [], "datasets": [], "total": 0, "page_size": 15}
        if type and type == "display":

            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage
            
            skip_value = (page_no - 1) * page_len
            
            if search_text:
                if search_text.isdigit():
                    search_filter = {'$match': {'sample_id': {'$regex': f'{search_text}', '$options': 'i'}}}
                else:
                    search_filter = {'$match': {'mine': {'$regex': f'{search_text}', '$options': 'i'}}}
            else:
                search_filter = {}

            basePipeline = [
                {
                    '$match': {
                        'plant_analysis_date': {
                            '$ne': None
                        }, 
                        'mode': 'Road'
                    }
                },
                {
                    '$addFields': {
                        'month': {
                            '$dateToString': {
                                'format': '%Y-%m', 
                                'date': '$plant_analysis_date'
                            }
                        }
                    }
                }
            ]

            # Conditionally add `search_filter` if it is defined and not empty
            if search_filter:
                basePipeline.append(search_filter)

            # Conditionally add `month_filter` only if month_date is provided
            if month_date:
                month_filter = {
                    '$match': {
                        'month': month_date
                    }
                }
                basePipeline.append(month_filter)

            # Add the remaining stages
            basePipeline.extend([
                {
                    '$group': {
                        '_id': {
                            'month': '$month', 
                            'mine': '$mine'
                        }, 
                        'DO_Qty_Total': {
                            '$sum': {
                                '$cond': [
                                    {
                                        '$and': [
                                            { '$ifNull': ['$sample_qty', False] },
                                            { '$lte': [ { '$strLenCP': { '$toString': '$sample_qty' } }, 5 ] }
                                        ]
                                    },
                                    { '$toDouble': { '$substrCP': [{ '$toString': '$sample_qty' }, 0, 5] } },
                                    {
                                        '$cond': [
                                            { '$regexMatch': { 'input': { '$toString': '$sample_qty' }, 'regex': "," } },
                                            {
                                                '$toDouble': {
                                                    '$replaceAll': {
                                                        'input': { '$toString': '$sample_qty' },
                                                        'find': ',', 
                                                        'replacement': ''
                                                    }
                                                }
                                            },
                                            { '$toDouble': '$sample_qty' }
                                        ]
                                    }
                                ]
                            }
                        }, 
                        'Gross_Calorific_Value_Adb_Total': {
                            '$sum': { '$ifNull': ['$plant_adb_gcv', 0] }
                        }, 
                        'Third_Party_Gross_Calorific_Value_Adb_Total': {
                            '$sum': { '$ifNull': ['$thirdparty_adb_gcv', 0] }
                        }, 
                        'count': { '$sum': 1 }
                    }
                },
                {
                    '$project': {
                        'month': '$_id.month', 
                        'mine': '$_id.mine', 
                        'Avg_DO_Qty': {
                            '$divide': ['$DO_Qty_Total', '$count']
                        }, 
                        'Avg_Gross_Calorific_Value_Adb': {
                            '$divide': ['$Gross_Calorific_Value_Adb_Total', '$count']
                        }, 
                        'Avg_Third_Party_Gross_Calorific_Value_Adb': {
                            '$divide': ['$Third_Party_Gross_Calorific_Value_Adb_Total', '$count']
                        }, 
                        '_id': 0
                    }
                }
            ])
            
            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                basePipeline[0]["$match"]["plant_analysis_date"]["$gte"] = start_date
            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")
                basePipeline[0]["$match"]["plant_analysis_date"]["$lte"] = end_date
            
            
            fetchGmrData = RecieptCoalQualityAnalysis.objects.aggregate(basePipeline)
            coal_grades = CoalGrades.objects()
            final_data = []
            for single_data in fetchGmrData:
                dictData = {}
                dictData["Mine"] = single_data.get("mine")
                dictData["DO_Qty"] = round(single_data.get("Avg_DO_Qty"), 2)
                dictData["GWEL_Gross_Calorific_Value_(Adb)"] = round(single_data.get("Avg_Gross_Calorific_Value_Adb"), 2)
                
                # Determine GWEL GCV Grade
                if single_data.get("Avg_Gross_Calorific_Value_Adb") is not None:
                    for single_coal_grade in coal_grades:
                        start_value = single_coal_grade["start_value"]
                        end_value = single_coal_grade["end_value"]
                        if end_value != "":
                            if start_value and end_value and int(start_value) <= int(single_data["Avg_Gross_Calorific_Value_Adb"]) <= int(end_value):
                                dictData["average_GCV_Grade"] = single_coal_grade["grade"]
                                break
                            elif single_data["Avg_Gross_Calorific_Value_Adb"] > 7001:
                                dictData["average_GCV_Grade"] = "G-1"
                                break
                
                # Determine Third Party GCV Grade
                if single_data.get("Avg_Third_Party_Gross_Calorific_Value_Adb") is not None:
                    for single_coal_grade in coal_grades:
                        start_value = single_coal_grade["start_value"]
                        end_value = single_coal_grade["end_value"]
                        if end_value != "":
                            if start_value and end_value and int(start_value) <= int(single_data["Avg_Third_Party_Gross_Calorific_Value_Adb"]) <= int(end_value):
                                console_logger.debug(single_coal_grade["grade"])
                                dictData["average_Third_Party_GCV_Grade"] = single_coal_grade["grade"]
                                break
                            elif single_data["Avg_Third_Party_Gross_Calorific_Value_Adb"] > 7001:
                                dictData["average_Third_Party_GCV_Grade"] = "G-1"
                                break
                
                # console_logger.debug(dictData.get("average_Third_Party_GCV_Grade"))
                # Final Calculations and formatting
                if dictData.get("average_GCV_Grade"):
                    dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'] = dictData.get('average_GCV_Grade')
                else:
                    dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'] = 0
                if single_data.get("Avg_Third_Party_Gross_Calorific_Value_Adb"):
                    dictData["Third_Party_Gross_Calorific_Value_(Adb)"] = round(single_data.get("Avg_Third_Party_Gross_Calorific_Value_Adb", 0), 2)
                else:
                    dictData["Third_Party_Gross_Calorific_Value_(Adb)"] = 0
                if dictData.get("average_Third_Party_GCV_Grade"):
                    dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"] = dictData.get("average_Third_Party_GCV_Grade")
                else:
                    dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"] = 0
                # Difference Calculations
                if dictData.get("GWEL_Gross_Calorific_Value_(Adb)") and dictData.get("Third_Party_Gross_Calorific_Value_(Adb)"):
                    dictData["Difference_Gross_Calorific_Value_(Adb)"] = abs(
                        int(float(dictData["GWEL_Gross_Calorific_Value_(Adb)"])) - int(float(dictData["Third_Party_Gross_Calorific_Value_(Adb)"]))
                    )
                if dictData.get("GWEL_Gross_Calorific_Value_Grade_(Adb)") and dictData.get("Third_Party_Gross_Calorific_Value_(Adb)_grade"):
                    dictData["Difference_Gross_Calorific_Value_Grade_(Adb)"] = abs(
                        int(dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'].replace('G-', '')) - 
                        int(dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"].replace('G-', ''))
                    )

                # console_logger.debug(dictData)
                final_data.append(dictData)
            start_index = (page_no - 1) * page_len
            end_index = start_index + page_len
            paginated_data = final_data[start_index:end_index]

            unique_keys = OrderedDict()

            for data in paginated_data:
                for key in data.keys():
                    unique_keys[key] = 0

            result["labels"] = list(unique_keys.keys())
            # result["labels"] = [
            #             "Mine",
            #             "DO_Qty",
            #             "GWEL_Gross_Calorific_Value_(Adb)",
            #             "GWEL_Gross_Calorific_Value_Grade_(Adb)",
            #             "Third_Party_Gross_Calorific_Value_(Adb)",
            #             "Third_Party_Gross_Calorific_Value_(Adb)_grade",
            #             "Difference_Gross_Calorific_Value_(Adb)",
            #             "Difference_Gross_Calorific_Value_Grade_(Adb)"
            #         ]
            result["total"] = len(final_data)
            result["datasets"] = paginated_data
            return result
        elif type and type == "download":
            del type
            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)
            
            try:
                path = os.path.join(
                    "static_server",
                    "gmr_ai",
                    file,
                    "Roadwise_GCV_Grade_Comparision_Report_{}.xlsx".format(
                        datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                    ),
                )
                filename = os.path.join(os.getcwd(), path)
                workbook = xlsxwriter.Workbook(filename)
                workbook.use_zip64()
                cell_format2 = workbook.add_format()
                cell_format2.set_bold()
                cell_format2.set_font_size(10)
                cell_format2.set_align("center")
                cell_format2.set_align("vjustify")

                worksheet = workbook.add_worksheet()
                worksheet.set_column("A:AZ", 20)
                worksheet.set_default_row(50)
                cell_format = workbook.add_format()
                cell_format.set_font_size(10)
                cell_format.set_align("center")
                cell_format.set_align("vcenter")

                headers = [
                    "Sr.No",
                    "Mine",
                    "DO_Qty",
                    "GWEL_Gross_Calorific_Value_(Adb)",
                    "GWEL_Gross_Calorific_Value_Grade",
                    "Third_Party_Gross_Calorific_Value_(Adb)",
                    "Third_Party_Gross_Calorific_Value_(Adb)_grade",
                    "Difference_Gross_Calorific_Value(Adb)",
                    "Difference_Gross_Calorific_Value_Grade_(Adb)"
                ]

                for index, header in enumerate(headers):
                    worksheet.write(0, index, header, cell_format2)

                # basePipeline = [
                #     {
                #         '$match': {
                #             'plant_analysis_date': {
                #                 '$ne': None, 
                #                 '$gte': start_date, 
                #                 '$lte': end_date
                #             }, 
                #             'mode': 'Road'
                #         }
                #     },
                #     {
                #         '$addFields': {
                #             'month': {
                #                 '$dateToString': {
                #                     'format': '%Y-%m', 
                #                     'date': '$plant_analysis_date'
                #                 }
                #             }
                #         }
                #     },
                #     {
                #         '$group': {
                #             '_id': {
                #                 'month': '$month', 
                #                 'mine': '$mine'
                #             }, 
                #             'DO_Qty_Total': {
                #                 '$sum': {
                #                     '$cond': [
                #                         {
                #                             '$and': [
                #                                 {
                #                                     '$ifNull': ['$sample_qty', False]
                #                                 }, 
                #                                 {
                #                                     '$lte': [
                #                                         { '$strLenCP': { '$toString': '$sample_qty' } }, 5
                #                                     ]
                #                                 }
                #                             ]
                #                         },
                #                         {
                #                             '$toDouble': {
                #                                 '$substrCP': [{ '$toString': '$sample_qty' }, 0, 5]
                #                             }
                #                         },
                #                         {
                #                             '$cond': [
                #                                 {
                #                                     '$regexMatch': {
                #                                         'input': { '$toString': '$sample_qty' },
                #                                         'regex': ","
                #                                     }
                #                                 },
                #                                 {
                #                                     '$toDouble': {
                #                                         '$replaceAll': {
                #                                             'input': { '$toString': '$sample_qty' },
                #                                             'find': ',', 
                #                                             'replacement': ''
                #                                         }
                #                                     }
                #                                 },
                #                                 { '$toDouble': '$sample_qty' }
                #                             ]
                #                         }
                #                     ]
                #                 }
                #             }, 
                #             'Gross_Calorific_Value_Adb_Total': {
                #                 '$sum': { '$ifNull': ['$plant_adb_gcv', 0] }
                #             }, 
                #             'Third_Party_Gross_Calorific_Value_Adb_Total': {
                #                 '$sum': { '$ifNull': ['$thirdparty_adb_gcv', 0] }
                #             }, 
                #             'count': { '$sum': 1 }
                #         }
                #     },
                #     {
                #         '$project': {
                #             'month': '$_id.month', 
                #             'mine': '$_id.mine', 
                #             'Avg_DO_Qty': {
                #                 '$divide': ['$DO_Qty_Total', '$count']
                #             }, 
                #             'Avg_Gross_Calorific_Value_Adb': {
                #                 '$divide': ['$Gross_Calorific_Value_Adb_Total', '$count']
                #             }, 
                #             'Avg_Third_Party_Gross_Calorific_Value_Adb': {
                #                 '$divide': ['$Third_Party_Gross_Calorific_Value_Adb_Total', '$count']
                #             }, 
                #             '_id': 0
                #         }
                #     }
                # ]

                basePipeline = [
                    {
                        '$match': {
                            'plant_analysis_date': {
                                '$ne': None
                            }, 
                            'mode': 'Road'
                        }
                    },
                    {
                        '$addFields': {
                            'month': {
                                '$dateToString': {
                                    'format': '%Y-%m', 
                                    'date': '$plant_analysis_date'
                                }
                            }
                        }
                    }
                ]

                # Conditionally add `search_filter` if it is defined and not empty
                # if search_filter:
                #     basePipeline.append(search_filter)

                # Conditionally add `month_filter` only if month_date is provided
                if month_date:
                    month_filter = {
                        '$match': {
                            'month': month_date
                        }
                    }
                    basePipeline.append(month_filter)

                # Add the remaining stages
                basePipeline.extend([
                    {
                        '$group': {
                            '_id': {
                                'month': '$month', 
                                'mine': '$mine'
                            }, 
                            'DO_Qty_Total': {
                                '$sum': {
                                    '$cond': [
                                        {
                                            '$and': [
                                                { '$ifNull': ['$sample_qty', False] },
                                                { '$lte': [ { '$strLenCP': { '$toString': '$sample_qty' } }, 5 ] }
                                            ]
                                        },
                                        { '$toDouble': { '$substrCP': [{ '$toString': '$sample_qty' }, 0, 5] } },
                                        {
                                            '$cond': [
                                                { '$regexMatch': { 'input': { '$toString': '$sample_qty' }, 'regex': "," } },
                                                {
                                                    '$toDouble': {
                                                        '$replaceAll': {
                                                            'input': { '$toString': '$sample_qty' },
                                                            'find': ',', 
                                                            'replacement': ''
                                                        }
                                                    }
                                                },
                                                { '$toDouble': '$sample_qty' }
                                            ]
                                        }
                                    ]
                                }
                            }, 
                            'Gross_Calorific_Value_Adb_Total': {
                                '$sum': { '$ifNull': ['$plant_adb_gcv', 0] }
                            }, 
                            'Third_Party_Gross_Calorific_Value_Adb_Total': {
                                '$sum': { '$ifNull': ['$thirdparty_adb_gcv', 0] }
                            }, 
                            'count': { '$sum': 1 }
                        }
                    },
                    {
                        '$project': {
                            'month': '$_id.month', 
                            'mine': '$_id.mine', 
                            'Avg_DO_Qty': {
                                '$divide': ['$DO_Qty_Total', '$count']
                            }, 
                            'Avg_Gross_Calorific_Value_Adb': {
                                '$divide': ['$Gross_Calorific_Value_Adb_Total', '$count']
                            }, 
                            'Avg_Third_Party_Gross_Calorific_Value_Adb': {
                                '$divide': ['$Third_Party_Gross_Calorific_Value_Adb_Total', '$count']
                            }, 
                            '_id': 0
                        }
                    }
                ])

                if start_timestamp:
                    start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                    basePipeline[0]["$match"]["plant_analysis_date"]["$gte"] = start_date
                if end_timestamp:
                    end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")
                    basePipeline[0]["$match"]["plant_analysis_date"]["$lte"] = end_date

                fetchGmrData = RecieptCoalQualityAnalysis.objects.aggregate(basePipeline)
                coal_grades = CoalGrades.objects()
                final_data = []
                for single_data in fetchGmrData:
                    dictData = {}
                    dictData["Mine"] = single_data.get("mine")
                    dictData["DO_Qty"] = single_data.get("Avg_DO_Qty")
                    dictData["GWEL_Gross_Calorific_Value_(Adb)"] = single_data.get("Avg_Gross_Calorific_Value_Adb")
                    
                    # Determine GWEL GCV Grade
                    if single_data.get("Avg_Gross_Calorific_Value_Adb") is not None:
                        for single_coal_grade in coal_grades:
                            start_value = single_coal_grade["start_value"]
                            end_value = single_coal_grade["end_value"]
                            if end_value != "":
                                if start_value and end_value and int(start_value) <= int(single_data["Avg_Gross_Calorific_Value_Adb"]) <= int(end_value):
                                    dictData["average_GCV_Grade"] = single_coal_grade["grade"]
                                    break
                                elif single_data["Avg_Gross_Calorific_Value_Adb"] > 7001:
                                    dictData["average_GCV_Grade"] = "G-1"
                                    break
                    
                    # Determine Third Party GCV Grade
                    if single_data.get("Avg_Third_Party_Gross_Calorific_Value_Adb") is not None:
                        for single_coal_grade in coal_grades:
                            start_value = single_coal_grade["start_value"]
                            end_value = single_coal_grade["end_value"]
                            if end_value != "":
                                if start_value and end_value and int(start_value) <= int(single_data["Avg_Third_Party_Gross_Calorific_Value_Adb"]) <= int(end_value):
                                    dictData["average_Third_Party_GCV_Grade"] = single_coal_grade["grade"]
                                    break
                                elif single_data["Avg_Third_Party_Gross_Calorific_Value_Adb"] > 7001:
                                    dictData["average_Third_Party_GCV_Grade"] = "G-1"
                                    break
                    
                    console_logger.debug(dictData.get("average_Third_Party_GCV_Grade"))
                    # Final Calculations and formatting
                    dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'] = dictData.get('average_GCV_Grade')
                    dictData["Third_Party_Gross_Calorific_Value_(Adb)"] = round(single_data.get("Avg_Third_Party_Gross_Calorific_Value_Adb", 0), 2)
                    dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"] = dictData.get("average_Third_Party_GCV_Grade")
                    
                    # Difference Calculations
                    if dictData.get("GWEL_Gross_Calorific_Value_(Adb)") and dictData.get("Third_Party_Gross_Calorific_Value_(Adb)"):
                        dictData["Difference_Gross_Calorific_Value_(Adb)"] = abs(
                            int(float(dictData["GWEL_Gross_Calorific_Value_(Adb)"])) - int(float(dictData["Third_Party_Gross_Calorific_Value_(Adb)"]))
                        )
                    if dictData.get("GWEL_Gross_Calorific_Value_Grade_(Adb)") and dictData.get("Third_Party_Gross_Calorific_Value_(Adb)_grade"):
                        dictData["Difference_Gross_Calorific_Value_Grade_(Adb)"] = abs(
                            int(dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'].replace('G-', '')) - 
                            int(dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"].replace('G-', ''))
                        )

                    # console_logger.debug(dictData)
                    final_data.append(dictData)
                result["labels"] = list(final_data[0].keys())
                result["total"] = len(final_data)
                result["datasets"] = final_data
                count = len(final_data)
                row = 1 
                for single_data in result["datasets"]:
                    worksheet.write(row, 0, count, cell_format)
                    worksheet.write(row, 1, single_data["Mine"], cell_format)
                    worksheet.write(row, 2, round(single_data["DO_Qty"], 2), cell_format)
                    worksheet.write(row, 3, round(single_data["GWEL_Gross_Calorific_Value_(Adb)"], 2), cell_format)
                    if single_data.get("Gross_Calorific_Value_Grade_(Adb)") != "" and single_data.get("Gross_Calorific_Value_Grade_(Adb)") != None:
                        worksheet.write(row, 4, str(single_data["Gross_Calorific_Value_Grade_(Adb)"]), cell_format)
                    else:
                        worksheet.write(row, 4, 0, cell_format)
                    if single_data.get("Third_Party_Gross_Calorific_Value_(Adb)") != "" and single_data.get("Third_Party_Gross_Calorific_Value_(Adb)") != None:
                        worksheet.write(row, 5, str(single_data["Third_Party_Gross_Calorific_Value_(Adb)"]), cell_format)
                    else:
                        worksheet.write(row, 5, 0, cell_format)
                    if single_data.get("Third_Party_Gross_Calorific_Value_(Adb)_grade") != "" and single_data.get("Third_Party_Gross_Calorific_Value_(Adb)_grade") != None:
                        worksheet.write(row, 6, str(single_data["Third_Party_Gross_Calorific_Value_(Adb)_grade"]), cell_format)
                    else:
                        worksheet.write(row, 6, 0, cell_format)
                    if single_data.get("Difference_Gross_Calorific_Value_(Adb)") != "" and single_data.get("Difference_Gross_Calorific_Value_(Adb)") != None:
                        worksheet.write(row, 7, str(single_data["Difference_Gross_Calorific_Value_(Adb)"]), cell_format)
                    else:
                        worksheet.write(row, 7, 0, cell_format)
                    if single_data.get("Difference_Gross_Calorific_Value_Grade_(Adb)") != "" and single_data.get("Difference_Gross_Calorific_Value_Grade_(Adb)") != None:
                        worksheet.write(row, 8, str(single_data["Difference_Gross_Calorific_Value_Grade_(Adb)"]), cell_format)
                    else:
                        worksheet.write(row, 8, 0, cell_format)
                    count -= 1
                    row += 1
                workbook.close()

                # console_logger.debug("Successfully {} report generated".format(service_id))
                # console_logger.debug("sent data {}".format(path))
                return {
                        "Type": "coal_test_download_event",
                        "Datatype": "Report",
                        "File_Path": path,
                    }
            except Exception as e:
                console_logger.debug(e)
                exc_type, exc_obj, exc_tb = sys.exc_info()
                fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            
    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/coal_gcv_table_train_aggregation", tags=["Coal Testing"])
def coal_wcl_gcv_table_update_aggregation(
    response: Response,
    currentPage: Optional[int] = None,
    perPage: Optional[int] = None,
    search_text: Optional[str] = None,
    start_timestamp: Optional[str] = None,
    end_timestamp: Optional[str] = None,
    month_date: Optional[str] = None,
    type: Optional[str] = "display"):
    try:
        result = {"labels": [], "datasets": [], "total": 0, "page_size": 15}
        if type and type == "display":

            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage
            
            skip_value = (page_no - 1) * page_len
            
            if search_text:
                if search_text.isdigit():
                    search_filter = {'$match': {'sample_id': {'$regex': f'{search_text}', '$options': 'i'}}}
                else:
                    search_filter = {'$match': {'mine': {'$regex': f'{search_text}', '$options': 'i'}}}
            else:
                search_filter = {}

            basePipeline = [
                {
                    '$match': {
                        'plant_analysis_date': {
                            '$ne': None
                        }, 
                        'mode': 'Rail'
                    }
                },
                {
                    '$addFields': {
                        'month': {
                            '$dateToString': {
                                'format': '%Y-%m', 
                                'date': '$plant_analysis_date'
                            }
                        }
                    }
                }
            ]

            # Conditionally add `search_filter` if it is defined and not empty
            if search_filter:
                basePipeline.append(search_filter)

            # Conditionally add `month_filter` only if month_date is provided
            if month_date:
                month_filter = {
                    '$match': {
                        'month': month_date
                    }
                }
                basePipeline.append(month_filter)

            # Add the remaining stages
            basePipeline.extend([
                {
                    '$group': {
                        '_id': {
                            'month': '$month', 
                            'mine': '$mine'
                        }, 
                        'DO_Qty_Total': {
                            '$sum': {
                                '$cond': [
                                    {
                                        '$and': [
                                            { '$ifNull': ['$sample_qty', False] },
                                            { '$lte': [ { '$strLenCP': { '$toString': '$sample_qty' } }, 5 ] }
                                        ]
                                    },
                                    { '$toDouble': { '$substrCP': [{ '$toString': '$sample_qty' }, 0, 5] } },
                                    {
                                        '$cond': [
                                            { '$regexMatch': { 'input': { '$toString': '$sample_qty' }, 'regex': "," } },
                                            {
                                                '$toDouble': {
                                                    '$replaceAll': {
                                                        'input': { '$toString': '$sample_qty' },
                                                        'find': ',', 
                                                        'replacement': ''
                                                    }
                                                }
                                            },
                                            { '$toDouble': '$sample_qty' }
                                        ]
                                    }
                                ]
                            }
                        }, 
                        'Gross_Calorific_Value_Adb_Total': {
                            '$sum': { '$ifNull': ['$plant_adb_gcv', 0] }
                        }, 
                        'Third_Party_Gross_Calorific_Value_Adb_Total': {
                            '$sum': { '$ifNull': ['$thirdparty_adb_gcv', 0] }
                        }, 
                        'count': { '$sum': 1 }
                    }
                },
                {
                    '$project': {
                        'month': '$_id.month', 
                        'mine': '$_id.mine', 
                        'Avg_DO_Qty': {
                            '$divide': ['$DO_Qty_Total', '$count']
                        }, 
                        'Avg_Gross_Calorific_Value_Adb': {
                            '$divide': ['$Gross_Calorific_Value_Adb_Total', '$count']
                        }, 
                        'Avg_Third_Party_Gross_Calorific_Value_Adb': {
                            '$divide': ['$Third_Party_Gross_Calorific_Value_Adb_Total', '$count']
                        }, 
                        '_id': 0
                    }
                }
            ])
            
            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                basePipeline[0]["$match"]["plant_analysis_date"]["$gte"] = start_date
            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")
                basePipeline[0]["$match"]["plant_analysis_date"]["$lte"] = end_date
            
            
            fetchGmrData = RecieptCoalQualityAnalysis.objects.aggregate(basePipeline)
            coal_grades = CoalGrades.objects()
            final_data = []
            for single_data in fetchGmrData:
                dictData = {}
                dictData["Mine"] = single_data.get("mine")
                dictData["DO_Qty"] = single_data.get("Avg_DO_Qty")
                dictData["GWEL_Gross_Calorific_Value_(Adb)"] = single_data.get("Avg_Gross_Calorific_Value_Adb")
                
                # Determine GWEL GCV Grade
                if single_data.get("Avg_Gross_Calorific_Value_Adb") is not None:
                    for single_coal_grade in coal_grades:
                        start_value = single_coal_grade["start_value"]
                        end_value = single_coal_grade["end_value"]
                        if end_value != "":
                            if start_value and end_value and int(start_value) <= int(single_data["Avg_Gross_Calorific_Value_Adb"]) <= int(end_value):
                                dictData["average_GCV_Grade"] = single_coal_grade["grade"]
                                break
                            elif single_data["Avg_Gross_Calorific_Value_Adb"] > 7001:
                                dictData["average_GCV_Grade"] = "G-1"
                                break
                
                # Determine Third Party GCV Grade
                if single_data.get("Avg_Third_Party_Gross_Calorific_Value_Adb") is not None:
                    for single_coal_grade in coal_grades:
                        start_value = single_coal_grade["start_value"]
                        end_value = single_coal_grade["end_value"]
                        if end_value != "":
                            if start_value and end_value and int(start_value) <= int(single_data["Avg_Third_Party_Gross_Calorific_Value_Adb"]) <= int(end_value):
                                console_logger.debug(single_coal_grade["grade"])
                                dictData["average_Third_Party_GCV_Grade"] = single_coal_grade["grade"]
                                break
                            elif single_data["Avg_Third_Party_Gross_Calorific_Value_Adb"] > 7001:
                                dictData["average_Third_Party_GCV_Grade"] = "G-1"
                                break
                
                # console_logger.debug(dictData.get("average_Third_Party_GCV_Grade"))
                # Final Calculations and formatting
                if dictData.get("average_GCV_Grade"):
                    dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'] = dictData.get('average_GCV_Grade')
                else:
                    dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'] = 0
                if single_data.get("Avg_Third_Party_Gross_Calorific_Value_Adb"):
                    dictData["Third_Party_Gross_Calorific_Value_(Adb)"] = round(single_data.get("Avg_Third_Party_Gross_Calorific_Value_Adb", 0), 2)
                else:
                    dictData["Third_Party_Gross_Calorific_Value_(Adb)"] = 0
                if dictData.get("average_Third_Party_GCV_Grade"):
                    dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"] = dictData.get("average_Third_Party_GCV_Grade")
                else:
                    dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"] = 0
                
                # Difference Calculations
                if dictData.get("GWEL_Gross_Calorific_Value_(Adb)") and dictData.get("Third_Party_Gross_Calorific_Value_(Adb)"):
                    dictData["Difference_Gross_Calorific_Value_(Adb)"] = abs(
                        int(float(dictData["GWEL_Gross_Calorific_Value_(Adb)"])) - int(float(dictData["Third_Party_Gross_Calorific_Value_(Adb)"]))
                    )
                if dictData.get("GWEL_Gross_Calorific_Value_Grade_(Adb)") and dictData.get("Third_Party_Gross_Calorific_Value_(Adb)_grade"):
                    dictData["Difference_Gross_Calorific_Value_Grade_(Adb)"] = abs(
                        int(dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'].replace('G-', '')) - 
                        int(dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"].replace('G-', ''))
                    )

                # console_logger.debug(dictData)
                final_data.append(dictData)
            start_index = (page_no - 1) * page_len
            end_index = start_index + page_len
            paginated_data = final_data[start_index:end_index]

            unique_keys = OrderedDict()

            for data in paginated_data:
                for key in data.keys():
                    unique_keys[key] = 0

            result["labels"] = list(unique_keys.keys())
            # result["labels"] = [
            #             "Mine",
            #             "DO_Qty",
            #             "GWEL_Gross_Calorific_Value_(Adb)",
            #             "GWEL_Gross_Calorific_Value_Grade_(Adb)",
            #             "Third_Party_Gross_Calorific_Value_(Adb)",
            #             "Third_Party_Gross_Calorific_Value_(Adb)_grade",
            #             "Difference_Gross_Calorific_Value_(Adb)",
            #             "Difference_Gross_Calorific_Value_Grade_(Adb)"
            #         ]
            result["total"] = len(final_data)
            result["datasets"] = paginated_data
            return result
        elif type and type == "download":
            del type
            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)
            
            try:
                path = os.path.join(
                    "static_server",
                    "gmr_ai",
                    file,
                    "Roadwise_GCV_Grade_Comparision_Report_{}.xlsx".format(
                        datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                    ),
                )
                filename = os.path.join(os.getcwd(), path)
                workbook = xlsxwriter.Workbook(filename)
                workbook.use_zip64()
                cell_format2 = workbook.add_format()
                cell_format2.set_bold()
                cell_format2.set_font_size(10)
                cell_format2.set_align("center")
                cell_format2.set_align("vjustify")

                worksheet = workbook.add_worksheet()
                worksheet.set_column("A:AZ", 20)
                worksheet.set_default_row(50)
                cell_format = workbook.add_format()
                cell_format.set_font_size(10)
                cell_format.set_align("center")
                cell_format.set_align("vcenter")

                headers = [
                    "Sr.No",
                    "Mine",
                    "DO_Qty",
                    "GWEL_Gross_Calorific_Value_(Adb)",
                    "GWEL_Gross_Calorific_Value_Grade",
                    "Third_Party_Gross_Calorific_Value_(Adb)",
                    "Third_Party_Gross_Calorific_Value_(Adb)_grade",
                    "Difference_Gross_Calorific_Value(Adb)",
                    "Difference_Gross_Calorific_Value_Grade_(Adb)"
                ]

                for index, header in enumerate(headers):
                    worksheet.write(0, index, header, cell_format2)

                basePipeline = [
                    {
                        '$match': {
                            'plant_analysis_date': {
                                '$ne': None
                            }, 
                            'mode': 'Rail'
                        }
                    },
                    {
                        '$addFields': {
                            'month': {
                                '$dateToString': {
                                    'format': '%Y-%m', 
                                    'date': '$plant_analysis_date'
                                }
                            }
                        }
                    }
                ]

                # Conditionally add `search_filter` if it is defined and not empty
                # if search_filter:
                #     basePipeline.append(search_filter)

                # Conditionally add `month_filter` only if month_date is provided
                if month_date:
                    month_filter = {
                        '$match': {
                            'month': month_date
                        }
                    }
                    basePipeline.append(month_filter)

                # Add the remaining stages
                basePipeline.extend([
                    {
                        '$group': {
                            '_id': {
                                'month': '$month', 
                                'mine': '$mine'
                            }, 
                            'DO_Qty_Total': {
                                '$sum': {
                                    '$cond': [
                                        {
                                            '$and': [
                                                { '$ifNull': ['$sample_qty', False] },
                                                { '$lte': [ { '$strLenCP': { '$toString': '$sample_qty' } }, 5 ] }
                                            ]
                                        },
                                        { '$toDouble': { '$substrCP': [{ '$toString': '$sample_qty' }, 0, 5] } },
                                        {
                                            '$cond': [
                                                { '$regexMatch': { 'input': { '$toString': '$sample_qty' }, 'regex': "," } },
                                                {
                                                    '$toDouble': {
                                                        '$replaceAll': {
                                                            'input': { '$toString': '$sample_qty' },
                                                            'find': ',', 
                                                            'replacement': ''
                                                        }
                                                    }
                                                },
                                                { '$toDouble': '$sample_qty' }
                                            ]
                                        }
                                    ]
                                }
                            }, 
                            'Gross_Calorific_Value_Adb_Total': {
                                '$sum': { '$ifNull': ['$plant_adb_gcv', 0] }
                            }, 
                            'Third_Party_Gross_Calorific_Value_Adb_Total': {
                                '$sum': { '$ifNull': ['$thirdparty_adb_gcv', 0] }
                            }, 
                            'count': { '$sum': 1 }
                        }
                    },
                    {
                        '$project': {
                            'month': '$_id.month', 
                            'mine': '$_id.mine', 
                            'Avg_DO_Qty': {
                                '$divide': ['$DO_Qty_Total', '$count']
                            }, 
                            'Avg_Gross_Calorific_Value_Adb': {
                                '$divide': ['$Gross_Calorific_Value_Adb_Total', '$count']
                            }, 
                            'Avg_Third_Party_Gross_Calorific_Value_Adb': {
                                '$divide': ['$Third_Party_Gross_Calorific_Value_Adb_Total', '$count']
                            }, 
                            '_id': 0
                        }
                    }
                ])

                if start_timestamp:
                    start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                    basePipeline[0]["$match"]["plant_analysis_date"]["$gte"] = start_date
                if end_timestamp:
                    end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")
                    basePipeline[0]["$match"]["plant_analysis_date"]["$lte"] = end_date

                fetchGmrData = RecieptCoalQualityAnalysis.objects.aggregate(basePipeline)
                coal_grades = CoalGrades.objects()
                final_data = []
                for single_data in fetchGmrData:
                    dictData = {}
                    dictData["Mine"] = single_data.get("mine")
                    dictData["DO_Qty"] = single_data.get("Avg_DO_Qty")
                    dictData["GWEL_Gross_Calorific_Value_(Adb)"] = single_data.get("Avg_Gross_Calorific_Value_Adb")
                    
                    # Determine GWEL GCV Grade
                    if single_data.get("Avg_Gross_Calorific_Value_Adb") is not None:
                        for single_coal_grade in coal_grades:
                            start_value = single_coal_grade["start_value"]
                            end_value = single_coal_grade["end_value"]
                            if end_value != "":
                                if start_value and end_value and int(start_value) <= int(single_data["Avg_Gross_Calorific_Value_Adb"]) <= int(end_value):
                                    dictData["average_GCV_Grade"] = single_coal_grade["grade"]
                                    break
                                elif single_data["Avg_Gross_Calorific_Value_Adb"] > 7001:
                                    dictData["average_GCV_Grade"] = "G-1"
                                    break
                    
                    # Determine Third Party GCV Grade
                    if single_data.get("Avg_Third_Party_Gross_Calorific_Value_Adb") is not None:
                        for single_coal_grade in coal_grades:
                            start_value = single_coal_grade["start_value"]
                            end_value = single_coal_grade["end_value"]
                            if end_value != "":
                                if start_value and end_value and int(start_value) <= int(single_data["Avg_Third_Party_Gross_Calorific_Value_Adb"]) <= int(end_value):
                                    console_logger.debug(single_coal_grade["grade"])
                                    dictData["average_Third_Party_GCV_Grade"] = single_coal_grade["grade"]
                                    break
                                elif single_data["Avg_Third_Party_Gross_Calorific_Value_Adb"] > 7001:
                                    dictData["average_Third_Party_GCV_Grade"] = "G-1"
                                    break
                    
                    console_logger.debug(dictData.get("average_Third_Party_GCV_Grade"))
                    # Final Calculations and formatting
                    dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'] = dictData.get('average_GCV_Grade')
                    dictData["Third_Party_Gross_Calorific_Value_(Adb)"] = round(single_data.get("Avg_Third_Party_Gross_Calorific_Value_Adb", 0), 2)
                    dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"] = dictData.get("average_Third_Party_GCV_Grade")
                    
                    # Difference Calculations
                    if dictData.get("GWEL_Gross_Calorific_Value_(Adb)") and dictData.get("Third_Party_Gross_Calorific_Value_(Adb)"):
                        dictData["Difference_Gross_Calorific_Value_(Adb)"] = abs(
                            int(float(dictData["GWEL_Gross_Calorific_Value_(Adb)"])) - int(float(dictData["Third_Party_Gross_Calorific_Value_(Adb)"]))
                        )
                    if dictData.get("GWEL_Gross_Calorific_Value_Grade_(Adb)") and dictData.get("Third_Party_Gross_Calorific_Value_(Adb)_grade"):
                        dictData["Difference_Gross_Calorific_Value_Grade_(Adb)"] = abs(
                            int(dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'].replace('G-', '')) - 
                            int(dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"].replace('G-', ''))
                        )

                    # console_logger.debug(dictData)
                    final_data.append(dictData)
                result["labels"] = list(final_data[0].keys())
                result["total"] = len(final_data)
                result["datasets"] = final_data
                count = len(final_data)
                row = 1
                for single_data in result["datasets"]:
                    worksheet.write(row, 0, count, cell_format)
                    worksheet.write(row, 1, single_data["Mine"], cell_format)
                    worksheet.write(row, 2, single_data["DO_Qty"], cell_format)
                    worksheet.write(row, 3, round(single_data["GWEL_Gross_Calorific_Value_(Adb)"], 2), cell_format)
                    if single_data.get("Gross_Calorific_Value_Grade_(Adb)") != "" and single_data.get("Gross_Calorific_Value_Grade_(Adb)") != None:
                        worksheet.write(row, 4, str(single_data["Gross_Calorific_Value_Grade_(Adb)"]), cell_format)
                    else:
                        worksheet.write(row, 4, 0, cell_format)
                    if single_data.get("Third_Party_Gross_Calorific_Value_(Adb)") != "" and single_data.get("Third_Party_Gross_Calorific_Value_(Adb)") != None:
                        worksheet.write(row, 5, str(single_data["Third_Party_Gross_Calorific_Value_(Adb)"]), cell_format)
                    else:
                        worksheet.write(row, 5, 0, cell_format)
                    if single_data.get("Third_Party_Gross_Calorific_Value_(Adb)_grade") != "" and single_data.get("Third_Party_Gross_Calorific_Value_(Adb)_grade") != None:
                        worksheet.write(row, 6, str(single_data["Third_Party_Gross_Calorific_Value_(Adb)_grade"]), cell_format)
                    else:
                        worksheet.write(row, 6, 0, cell_format)
                    if single_data.get("Difference_Gross_Calorific_Value_(Adb)") != "" and single_data.get("Difference_Gross_Calorific_Value_(Adb)") != None:
                        worksheet.write(row, 7, str(single_data["Difference_Gross_Calorific_Value_(Adb)"]), cell_format)
                    else:
                        worksheet.write(row, 7, 0, cell_format)
                    if single_data.get("Difference_Gross_Calorific_Value_Grade_(Adb)") != "" and single_data.get("Difference_Gross_Calorific_Value_Grade_(Adb)") != None:
                        worksheet.write(row, 8, str(single_data["Difference_Gross_Calorific_Value_Grade_(Adb)"]), cell_format)
                    else:
                        worksheet.write(row, 8, 0, cell_format)
                    count -= 1
                    row += 1
                workbook.close()

                # console_logger.debug("Successfully {} report generated".format(service_id))
                # console_logger.debug("sent data {}".format(path))
                return {
                        "Type": "coal_test_download_event",
                        "Datatype": "Report",
                        "File_Path": path,
                    }
            except Exception as e:
                console_logger.debug(e)
                exc_type, exc_obj, exc_tb = sys.exc_info()
                fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            
    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e
    


@router.get("/coal_gcv_table_old", tags=["Coal Testing"])
def coal_wcl_gcv_table(
    response: Response,
    currentPage: Optional[int] = None,
    perPage: Optional[int] = None,
    search_text: Optional[str] = None,
    start_timestamp: Optional[str] = None,
    end_timestamp: Optional[str] = None,
    month_date: Optional[str] = None,
    type: Optional[str] = "display"):
    try:
        result = {"labels": [], "datasets": [], "total": 0, "page_size": 15}

        if type and type == "display":
            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage

            data = Q()

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(receive_date__gte = start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(receive_date__lte = end_date)

            if search_text:
                if search_text.isdigit():
                    data &= (Q(rrNo__icontains=search_text))
                else:
                    data &= Q(location__icontains=search_text) | Q(rake_no__icontains=search_text)

            offset = (page_no - 1) * page_len
            # logs = CoalTesting.objects(data).order_by("-ID").skip(offset).limit(page_len)
            logs = CoalTesting.objects(data).order_by("-ID")

            if any(logs):
                aggregated_data = defaultdict(lambda: defaultdict(lambda: {"DO_Qty": 0, "Gross_Calorific_Value_(Adb)": 0, "Third_Party_Gross_Calorific_Value_(Adb)": 0, "Third_Party_Gross_Calorific_Value_(Adb)_count": 0, "count": 0}))

                for log in logs:
                    month = log.receive_date.strftime("%Y-%m")
                    payload = log.gradepayload()
                    mine = payload["Mine"]
                    if payload.get("DO_Qty"):
                        if payload.get("DO_Qty").count('.') > 1:
                            aggregated_data[month][mine]["DO_Qty"] += float(payload.get("DO_Qty")[:5])
                        else:
                            if "," in payload["DO_Qty"]:
                                aggregated_data[month][mine]["DO_Qty"] += float(payload["DO_Qty"].replace(",", ""))
                            else:
                                aggregated_data[month][mine]["DO_Qty"] += float(payload["DO_Qty"])
                    if payload.get("Gross_Calorific_Value_(Adb)"):
                        aggregated_data[month][mine]["Gross_Calorific_Value_(Adb)"] += float(payload["Gross_Calorific_Value_(Adb)"])
                    if payload.get("Third_Party_Gross_Calorific_Value_(Adb)"):
                        aggregated_data[month][mine]["Third_Party_Gross_Calorific_Value_(Adb)"] += float(payload["Third_Party_Gross_Calorific_Value_(Adb)"])
                        aggregated_data[month][mine]["Third_Party_Gross_Calorific_Value_(Adb)_count"] += 1
                    aggregated_data[month][mine]["count"] += 1

                dataList = [
                    {"month": month, "data": {
                        mine: {
                            "average_DO_Qty": data["DO_Qty"] / data["count"],
                            "average_Gross_Calorific_Value_(Adb)": data["Gross_Calorific_Value_(Adb)"] / data["count"],
                            "average_Third_Party_Gross_Calorific_Value_(Adb)": data["Third_Party_Gross_Calorific_Value_(Adb)"] / data["Third_Party_Gross_Calorific_Value_(Adb)_count"] if data["Third_Party_Gross_Calorific_Value_(Adb)"] != 0 else "",
                        } for mine, data in aggregated_data[month].items()
                    }} for month in aggregated_data
                ]
                coal_grades = CoalGrades.objects()  # Fetch all coal grades from the database

                # Iterate through each month's data
                for month_data in dataList:
                    for key, mine_data in month_data["data"].items():
                        if mine_data["average_Gross_Calorific_Value_(Adb)"] is not None:
                            for single_coal_grades in coal_grades:
                                if single_coal_grades["end_value"] != "":
                                    if (int(single_coal_grades["start_value"]) <= int(float(mine_data.get("average_Gross_Calorific_Value_(Adb)"))) <= int(single_coal_grades["end_value"]) and single_coal_grades["start_value"] != "" and single_coal_grades["end_value"] != ""):
                                        mine_data["average_GCV_Grade"] = single_coal_grades["grade"]
                                    elif int(mine_data["average_Gross_Calorific_Value_(Adb)"]) > 7001:
                                        mine_data["average_GCV_Grade"] = "G-1"
                                        break

                        if mine_data["average_Third_Party_Gross_Calorific_Value_(Adb)"] != "":
                            for single_coal_grades in coal_grades:
                                if single_coal_grades["end_value"] != "":
                                    if (int(single_coal_grades["start_value"]) <= int(float(mine_data.get("average_Third_Party_Gross_Calorific_Value_(Adb)"))) <= int(single_coal_grades["end_value"]) and single_coal_grades["start_value"] != "" and single_coal_grades["end_value"] != ""):
                                        mine_data["average_Third_Party_GCV_Grade"] = single_coal_grades["grade"]
                                    elif int(mine_data["average_Third_Party_Gross_Calorific_Value_(Adb)"]) > 7001:
                                        mine_data["average_Third_Party_GCV_Grade"] = "G-1"
                                        break

                final_data = []
                if month_date:
                    filtered_data = [entry for entry in dataList if entry["month"] == month_date]
                    if filtered_data:
                        data = filtered_data[0]['data']  # Extracting the 'data' dictionary from the list
                        for mine, values in data.items():
                            dictData = {}
                            dictData['Mine'] = mine
                            dictData['DO_Qty'] = round(values['average_DO_Qty'], 2)
                            dictData['GWEL_Gross_Calorific_Value_(Adb)'] = round(values['average_Gross_Calorific_Value_(Adb)'], 2)
                            if values["average_Third_Party_Gross_Calorific_Value_(Adb)"] != "":
                                if values.get('average_GCV_Grade'):
                                    dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'] = values['average_GCV_Grade']
                                dictData["Third_Party_Gross_Calorific_Value_(Adb)"] = round(values["average_Third_Party_Gross_Calorific_Value_(Adb)"], 2)
                                if values.get("average_Third_Party_GCV_Grade"):
                                    dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"] = str(values["average_Third_Party_GCV_Grade"])
                                dictData["Difference_Gross_Calorific_Value_(Adb)"] = str(abs(int(float(dictData["GWEL_Gross_Calorific_Value_(Adb)"])) - int(float(dictData["Third_Party_Gross_Calorific_Value_(Adb)"]))))
                                if values.get("average_Third_Party_GCV_Grade"):
                                    dictData["Difference_Gross_Calorific_Value_Grade_(Adb)"] = str(abs(int(dictData['Gross_Calorific_Value_Grade_(Adb)'].replace('G-', '')) - int(dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"].replace('G-', ''))))
                            final_data.append(dictData)
                    else:
                        console_logger.debug(
                            f"No data available for the given month: {month_date}"
                        )
                        return result
                else:
                    console_logger.debug("inside else")
                    filtered_data = [entry for entry in dataList]
                    for single_data in filtered_data:
                        for mine, values in single_data['data'].items():
                            dictData = {}
                            dictData['Mine'] = mine
                            dictData['DO_Qty'] = round(values['average_DO_Qty'], 2)
                            dictData['GWEL_Gross_Calorific_Value_(Adb)'] = round(values['average_Gross_Calorific_Value_(Adb)'], 2)
                            if values["average_Third_Party_Gross_Calorific_Value_(Adb)"] != "":
                                dictData['Gross_Calorific_Value_Grade_(Adb)'] = values['average_GCV_Grade']
                                dictData["Third_Party_Gross_Calorific_Value_(Adb)"] = round(values["average_Third_Party_Gross_Calorific_Value_(Adb)"], 2)
                                if values.get("average_Third_Party_GCV_Grade"):
                                    dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"] = str(values["average_Third_Party_GCV_Grade"])
                                dictData["Difference_Gross_Calorific_Value_(Adb)"] = str(abs(int(float(dictData["GWEL_Gross_Calorific_Value_(Adb)"])) - int(float(dictData["Third_Party_Gross_Calorific_Value_(Adb)"]))))
                                if values.get("average_Third_Party_GCV_Grade"):
                                    dictData["Difference_Gross_Calorific_Value_Grade_(Adb)"] = str(abs(int(dictData['Gross_Calorific_Value_Grade_(Adb)'].replace('G-', '')) - int(dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"].replace('G-', ''))))

                            final_data.append(dictData)
                # Perform pagination here using list slicing
                start_index = (page_no - 1) * page_len
                end_index = start_index + page_len
                paginated_data = final_data[start_index:end_index]

                unique_keys = OrderedDict()

                for data in paginated_data:
                    for key in data.keys():
                        unique_keys[key] = None

                result["labels"] = list(unique_keys.keys())

                result["total"] = len(final_data)
                result["datasets"] = paginated_data
                return result
            else:
                return result
        
        elif type and type == "download":
            del type
            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)
            
            data = Q()

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(receive_date__gte = start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(receive_date__lte = end_date)
            
            if search_text:
                if search_text.isdigit():
                    data &= (Q(rrNo__icontains=search_text))
                else:
                    data &= Q(location__icontains=search_text) | Q(rake_no__icontains=search_text)

            usecase_data = CoalTesting.objects(data).order_by("-receive_date")
            count = len(usecase_data)
            path = None
            logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
            if usecase_data:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "Roadwise_GCV_Grade_Comparision_Report_{}.xlsx".format(
                            datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format()
                    cell_format2.set_bold()
                    cell_format2.set_font_size(10)
                    cell_format2.set_align("center")
                    cell_format2.set_align("vcenter")
                    cell_format2.set_text_wrap(True)
                    cell_format2.set_border(1)

                    header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
                    date_format = workbook.add_format({'align': 'center', 'font_size': 12, "bold": True})
                    report_name_format = workbook.add_format({'align': 'center', 'font_size': 15, "bold": True})
                    # report_name_format.set_text_wrap(True)

                    header_format.set_align("vcenter")
                    date_format.set_align("vcenter")
                    report_name_format.set_align("vcenter")
                    header_format.set_border(1)
                    date_format.set_border(1)
                    report_name_format.set_border(1)

                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)
                    cell_format = workbook.add_format()
                    cell_format.set_font_size(10)
                    cell_format.set_align("center")
                    cell_format.set_align("vcenter")
                    cell_format.set_text_wrap(True)
                    cell_format.set_border(1)

                    worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                    
                    # Merge cells for the main header and place it in the center
                    main_header = "GMR Warora Energy Limited"  # Set your main header text here
                    worksheet.merge_range("A1:I1", main_header, header_format)  # Merge cells A1 to H1 for the header
                    
                    # Write the current date on the left side (A2)
                    # worksheet.write("A2", f"Date: {datetime.datetime.now().strftime('%d-%m-%Y')}", date_format)
                    worksheet.merge_range("A2:B2", f"Date: {datetime.datetime.now().strftime('%d-%m-%Y')}", date_format)
                    worksheet.merge_range("C2:I2", f"Road - GCV Wise Comparision Report", report_name_format)

                    headers = [
                        "Sr.No",
                        "Mine",
                        "DO_Qty",
                        "GWEL_Gross_Calorific_Value_(Adb)",
                        "GWEL_Gross_Calorific_Value_Grade",
                        "Third_Party_Gross_Calorific_Value_(Adb)",
                        "Third_Party_Gross_Calorific_Value_(Adb)_grade",
                        "Difference_Gross_Calorific_Value(Adb)",
                        "Difference_Gross_Calorific_Value_Grade_(Adb)"
                    ]

                    for index, header in enumerate(headers):
                        worksheet.write(2, index, header, cell_format2)


                    if any(usecase_data):
                        aggregated_data = defaultdict(lambda: defaultdict(lambda: {"DO_Qty": 0, "Gross_Calorific_Value_(Adb)": 0, "Third_Party_Gross_Calorific_Value_(Adb)": 0, "Third_Party_Gross_Calorific_Value_(Adb)_count": 0, "count": 0}))

                        for log in usecase_data:
                            month = log.receive_date.strftime("%Y-%m")
                            payload = log.gradepayload()
                            mine = payload["Mine"]
                            if payload.get("DO_Qty"):
                                if payload.get("DO_Qty").count('.') > 1:
                                    aggregated_data[month][mine]["DO_Qty"] += float(payload.get("DO_Qty")[:5])
                                else:
                                    aggregated_data[month][mine]["DO_Qty"] += float(payload["DO_Qty"])
                            if payload.get("Gross_Calorific_Value_(Adb)"):
                                aggregated_data[month][mine]["Gross_Calorific_Value_(Adb)"] += float(payload["Gross_Calorific_Value_(Adb)"]) 
                            if payload.get("Third_Party_Gross_Calorific_Value_(Adb)"):
                                aggregated_data[month][mine]["Third_Party_Gross_Calorific_Value_(Adb)"] += float(payload["Third_Party_Gross_Calorific_Value_(Adb)"])
                                aggregated_data[month][mine]["Third_Party_Gross_Calorific_Value_(Adb)_count"] += 1
                
                            aggregated_data[month][mine]["count"] += 1

                        dataList = [
                            {"month": month, "data": {
                                mine: {
                                    "average_DO_Qty": data["DO_Qty"] / data["count"],
                                    "average_Gross_Calorific_Value_(Adb)": data["Gross_Calorific_Value_(Adb)"] / data["count"],
                                    "average_Third_Party_Gross_Calorific_Value_(Adb)": data["Third_Party_Gross_Calorific_Value_(Adb)"] / data["Third_Party_Gross_Calorific_Value_(Adb)_count"] if data["Third_Party_Gross_Calorific_Value_(Adb)"] != 0 else "",
                                } for mine, data in aggregated_data[month].items()
                            }} for month in aggregated_data
                        ]
                        coal_grades = CoalGrades.objects()  # Fetch all coal grades from the database

                        for month_data in dataList:
                            for key, mine_data in month_data["data"].items():
                                if mine_data["average_Gross_Calorific_Value_(Adb)"] is not None:
                                    for single_coal_grades in coal_grades:
                                        if single_coal_grades["end_value"] != "":
                                            if (int(single_coal_grades["start_value"]) <= int(float(mine_data.get("average_Gross_Calorific_Value_(Adb)"))) <= int(single_coal_grades["end_value"]) and single_coal_grades["start_value"] != "" and single_coal_grades["end_value"] != ""):
                                                mine_data["average_GCV_Grade"] = single_coal_grades["grade"]
                                            elif int(mine_data["average_Gross_Calorific_Value_(Adb)"]) > 7001:
                                                mine_data["average_GCV_Grade"] = "G-1"
                                                break

                                if mine_data["average_Third_Party_Gross_Calorific_Value_(Adb)"] != "":
                                    for single_coal_grades in coal_grades:
                                        if single_coal_grades["end_value"] != "":
                                            if (int(single_coal_grades["start_value"]) <= int(float(mine_data.get("average_Third_Party_Gross_Calorific_Value_(Adb)"))) <= int(single_coal_grades["end_value"]) and single_coal_grades["start_value"] != "" and single_coal_grades["end_value"] != ""):
                                                mine_data["average_Third_Party_GCV_Grade"] = single_coal_grades["grade"]
                                            elif int(mine_data["average_Gross_Calorific_Value_(Adb)"]) > 7001:
                                                mine_data["average_Third_Party_GCV_Grade"] = "G-1"
                                                break
                    final_data = []
                    if month_date:
                        filtered_data = [entry for entry in dataList if entry["month"] == month_date]
                        if filtered_data:
                            data = filtered_data[0]['data']  # Extracting the 'data' dictionary from the list
                            for mine, values in data.items():
                                dictData = {}
                                dictData['Mine'] = mine
                                dictData['DO_Qty'] = round(values['average_DO_Qty'], 2)
                                dictData['Gross_Calorific_Value_(Adb)'] = round(values['average_Gross_Calorific_Value_(Adb)'], 2)
                                if values["average_Third_Party_Gross_Calorific_Value_(Adb)"] != "":    
                                    dictData['Gross_Calorific_Value_Grade_(Adb)'] = values['average_GCV_Grade']
                                    dictData["Third_Party_Gross_Calorific_Value_(Adb)"] = round(values["average_Third_Party_Gross_Calorific_Value_(Adb)"], 2)
                                    if values.get("average_Third_Party_GCV_Grade"):
                                        dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"] = str(values["average_Third_Party_GCV_Grade"])
                                    dictData["Difference_Gross_Calorific_Value_(Adb)"] = str(abs(int(float(dictData["Gross_Calorific_Value_(Adb)"])) - int(float(dictData["Third_Party_Gross_Calorific_Value_(Adb)"]))))
                                    if values.get("average_Third_Party_GCV_Grade"):
                                        dictData["Difference_Gross_Calorific_Value_Grade_(Adb)"] = str(abs(int(dictData['Gross_Calorific_Value_Grade_(Adb)'].replace('G-', '')) - int(dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"].replace('G-', ''))))
                                final_data.append(dictData)
                        else:
                            console_logger.debug(
                                f"No data available for the given month: {month_date}"
                            )
                            return {"message": f"No data available for the given month: {month_date}"}
                    else:
                        filtered_data = [entry for entry in dataList]
                        # data = filtered_data[0]['data']  # Extracting the 'data' dictionary from the list
                        for single_data in filtered_data:
                            for mine, values in single_data['data'].items():
                                dictData = {}
                                dictData['Mine'] = mine
                                dictData['DO_Qty'] = round(values['average_DO_Qty'], 2)
                                dictData['Gross_Calorific_Value_(Adb)'] = round(values['average_Gross_Calorific_Value_(Adb)'], 2)
                                if values["average_Third_Party_Gross_Calorific_Value_(Adb)"] != "":
                                    dictData['Gross_Calorific_Value_Grade_(Adb)'] = values['average_GCV_Grade']
                                    dictData["Third_Party_Gross_Calorific_Value_(Adb)"] = round(values["average_Third_Party_Gross_Calorific_Value_(Adb)"],2)
                                    if values.get("average_Third_Party_GCV_Grade"):
                                        dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"] = str(values["average_Third_Party_GCV_Grade"])
                                    dictData["Difference_Gross_Calorific_Value_(Adb)"] = str(abs(int(float(dictData["Gross_Calorific_Value_(Adb)"])) - int(float(dictData["Third_Party_Gross_Calorific_Value_(Adb)"]))))
                                    if values.get("average_Third_Party_GCV_Grade"):
                                        dictData["Difference_Gross_Calorific_Value_Grade_(Adb)"] = str(abs(int(dictData['Gross_Calorific_Value_Grade_(Adb)'].replace('G-', '')) - int(dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"].replace('G-', ''))))
                                final_data.append(dictData)
                    result["labels"] = list(final_data[0].keys())
                    result["total"] = len(final_data)
                    result["datasets"] = final_data

                    row = 3
                    for single_data in result["datasets"]:
                        worksheet.write(row, 0, count, cell_format)
                        worksheet.write(row, 1, single_data["Mine"])
                        worksheet.write(row, 2, single_data["DO_Qty"])
                        worksheet.write(row, 3, single_data["Gross_Calorific_Value_(Adb)"])
                        if single_data.get("Gross_Calorific_Value_Grade_(Adb)") != "" and single_data.get("Gross_Calorific_Value_Grade_(Adb)") != None:
                            worksheet.write(row, 4, str(single_data["Gross_Calorific_Value_Grade_(Adb)"]), cell_format)
                        if single_data.get("Third_Party_Gross_Calorific_Value_(Adb)") != "" and single_data.get("Third_Party_Gross_Calorific_Value_(Adb)") != None:
                            worksheet.write(row, 5, str(single_data["Third_Party_Gross_Calorific_Value_(Adb)"]), cell_format)
                        if single_data.get("Third_Party_Gross_Calorific_Value_(Adb)_grade") != "" and single_data.get("Third_Party_Gross_Calorific_Value_(Adb)_grade") != None:
                            worksheet.write(row, 6, str(single_data["Third_Party_Gross_Calorific_Value_(Adb)_grade"]), cell_format)
                        if single_data.get("Difference_Gross_Calorific_Value_(Adb)") != "" and single_data.get("Difference_Gross_Calorific_Value_(Adb)") != None:
                            worksheet.write(row, 7, str(single_data["Difference_Gross_Calorific_Value_(Adb)"]), cell_format)
                        if single_data.get("Difference_Gross_Calorific_Value_Grade_(Adb)") != "" and single_data.get("Difference_Gross_Calorific_Value_Grade_(Adb)") != None:
                            worksheet.write(row, 8, str(single_data["Difference_Gross_Calorific_Value_Grade_(Adb)"]), cell_format)
                        count -= 1
                        row += 1
                    workbook.close()

                    # console_logger.debug("Successfully {} report generated".format(service_id))
                    # console_logger.debug("sent data {}".format(path))
                    return {
                            "Type": "coal_test_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                        }
                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            else:
                console_logger.error("No data found")
                return {
                            "Type": "coal_test_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                        }
    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


# as per RecieptCoalQualityAnalysis wise
@router.get("/coal_gcv_table_train", tags=["Coal Testing"])
def coal_wcl_gcv_table_train(
    response: Response,
    currentPage: Optional[int] = None,
    perPage: Optional[int] = None,
    search_text: Optional[str] = None,
    start_timestamp: Optional[str] = None,
    end_timestamp: Optional[str] = None,
    month_date: Optional[str] = None,
    type: Optional[str] = "display"):
    try:
        result = {"labels": [], "datasets": [], "total": 0, "page_size": 15}

        if type and type == "display":
            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage

            data = Q()

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(plant_analysis_date__gte = start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(plant_analysis_date__lte = end_date)

            if search_text:
                if search_text.isdigit():
                    data &= (Q(sample_id__icontains=search_text))
                else:
                    data &= (Q(mine__icontains=search_text))

            offset = (page_no - 1) * page_len
            # logs = CoalTesting.objects(data).order_by("-ID").skip(offset).limit(page_len)
            logs = RecieptCoalQualityAnalysis.objects(data, mode="Rail")

            if any(logs):
                aggregated_data = defaultdict(lambda: defaultdict(lambda: {"DO_Qty": 0, "Gross_Calorific_Value_(Adb)": 0, "Third_Party_Gross_Calorific_Value_(Adb)": 0, "Third_Party_Gross_Calorific_Value_(Adb)_count": 0, "count": 0}))

                for log in logs:
                    month = log.plant_analysis_date.strftime("%Y-%m")
                    payload = log.payload()
                    mine = payload["mine"]
                    if payload.get("sample_qty"):
                        aggregated_data[month][mine]["DO_Qty"] += float(payload["sample_qty"])
                        # if payload.get("DO_Qty").count('.') > 1:
                        #     aggregated_data[month][mine]["DO_Qty"] += float(payload.get("DO_Qty")[:5])
                        # else:
                        #     if "," in payload["DO_Qty"]:
                        #         aggregated_data[month][mine]["DO_Qty"] += float(payload["DO_Qty"].replace(",", ""))
                        #     else:
                        #         aggregated_data[month][mine]["DO_Qty"] += float(payload["DO_Qty"])
                    if payload.get("GWEL_ADB_GCV"):
                        aggregated_data[month][mine]["Gross_Calorific_Value_(Adb)"] += float(payload["GWEL_ADB_GCV"])
                    if payload.get("THIRDPARTY_ADB_GCV"):
                        aggregated_data[month][mine]["Third_Party_Gross_Calorific_Value_(Adb)"] += float(payload["THIRDPARTY_ADB_GCV"])
                        aggregated_data[month][mine]["Third_Party_Gross_Calorific_Value_(Adb)_count"] += 1
                    aggregated_data[month][mine]["count"] += 1

                dataList = [
                    {"month": month, "data": {
                        mine: {
                            # first it was average then we have changed to total on 08-11-2024 11:31 am
                            # "average_DO_Qty": data["DO_Qty"] / data["count"],
                            "average_DO_Qty": data["DO_Qty"],
                            "average_Gross_Calorific_Value_(Adb)": data["Gross_Calorific_Value_(Adb)"] / data["count"],
                            "average_Third_Party_Gross_Calorific_Value_(Adb)": data["Third_Party_Gross_Calorific_Value_(Adb)"] / data["Third_Party_Gross_Calorific_Value_(Adb)_count"] if data["Third_Party_Gross_Calorific_Value_(Adb)"] != 0 else "",
                        } for mine, data in aggregated_data[month].items()
                    }} for month in aggregated_data
                ]
                coal_grades = CoalGrades.objects()  # Fetch all coal grades from the database

                # Iterate through each month's data
                for month_data in dataList:
                    for key, mine_data in month_data["data"].items():
                        if mine_data["average_Gross_Calorific_Value_(Adb)"] is not None:
                            for single_coal_grades in coal_grades:
                                if single_coal_grades["end_value"] != "":
                                    if (int(single_coal_grades["start_value"]) <= int(float(mine_data.get("average_Gross_Calorific_Value_(Adb)"))) <= int(single_coal_grades["end_value"]) and single_coal_grades["start_value"] != "" and single_coal_grades["end_value"] != ""):
                                        mine_data["average_GCV_Grade"] = single_coal_grades["grade"]
                                    elif int(mine_data["average_Gross_Calorific_Value_(Adb)"]) > 7001:
                                        mine_data["average_GCV_Grade"] = "G-1"
                                        break

                        if mine_data["average_Third_Party_Gross_Calorific_Value_(Adb)"] != "":
                            for single_coal_grades in coal_grades:
                                if single_coal_grades["end_value"] != "":
                                    if (int(single_coal_grades["start_value"]) <= int(float(mine_data.get("average_Third_Party_Gross_Calorific_Value_(Adb)"))) <= int(single_coal_grades["end_value"]) and single_coal_grades["start_value"] != "" and single_coal_grades["end_value"] != ""):
                                        mine_data["average_Third_Party_GCV_Grade"] = single_coal_grades["grade"]
                                    elif int(mine_data["average_Third_Party_Gross_Calorific_Value_(Adb)"]) > 7001:
                                        mine_data["average_Third_Party_GCV_Grade"] = "G-1"
                                        break

                final_data = []
                if month_date:
                    filtered_data = [entry for entry in dataList if entry["month"] == month_date]
                    if filtered_data:
                        data = filtered_data[0]['data']  # Extracting the 'data' dictionary from the list
                        for mine, values in data.items():
                            dictData = {}
                            dictData['Mine'] = mine
                            dictData['DO_Qty'] = round(values['average_DO_Qty'], 2)
                            dictData['GWEL_Gross_Calorific_Value_(Adb)'] = round(values['average_Gross_Calorific_Value_(Adb)'], 2)
                            if values["average_Third_Party_Gross_Calorific_Value_(Adb)"] != "":
                                if values.get('average_GCV_Grade'):
                                    dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'] = values['average_GCV_Grade']
                                dictData["Third_Party_Gross_Calorific_Value_(Adb)"] = round(values["average_Third_Party_Gross_Calorific_Value_(Adb)"], 2)
                                if values.get("average_Third_Party_GCV_Grade"):
                                    dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"] = str(values["average_Third_Party_GCV_Grade"])
                                dictData["Difference_Gross_Calorific_Value_(Adb)"] = str(abs(int(float(dictData["GWEL_Gross_Calorific_Value_(Adb)"])) - int(float(dictData["Third_Party_Gross_Calorific_Value_(Adb)"]))))
                                if values.get("average_Third_Party_GCV_Grade"):
                                    dictData["Difference_Gross_Calorific_Value_Grade_(Adb)"] = str(abs(int(dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'].replace('G-', '')) - int(dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"].replace('G-', ''))))
                            final_data.append(dictData)
                    else:
                        console_logger.debug(
                            f"No data available for the given month: {month_date}"
                        )
                        return result
                else:
                    console_logger.debug("inside else")
                    filtered_data = [entry for entry in dataList]
                    for single_data in filtered_data:
                        for mine, values in single_data['data'].items():
                            dictData = {}
                            dictData['Mine'] = mine
                            dictData['DO_Qty'] = round(values['average_DO_Qty'], 2)
                            dictData['GWEL_Gross_Calorific_Value_(Adb)'] = round(values['average_Gross_Calorific_Value_(Adb)'], 2)
                            if values.get('average_GCV_Grade'):
                                dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'] = values['average_GCV_Grade']
                            if values["average_Third_Party_Gross_Calorific_Value_(Adb)"] != "":
                                dictData["Third_Party_Gross_Calorific_Value_(Adb)"] = round(values["average_Third_Party_Gross_Calorific_Value_(Adb)"], 2)
                                if values.get("average_Third_Party_GCV_Grade"):
                                    dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"] = str(values["average_Third_Party_GCV_Grade"])
                                dictData["Difference_Gross_Calorific_Value_(Adb)"] = str(abs(int(float(dictData["GWEL_Gross_Calorific_Value_(Adb)"])) - int(float(dictData["Third_Party_Gross_Calorific_Value_(Adb)"]))))
                                if values.get("average_Third_Party_GCV_Grade"):
                                    dictData["Difference_Gross_Calorific_Value_Grade_(Adb)"] = str(abs(int(dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'].replace('G-', '')) - int(dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"].replace('G-', ''))))

                            final_data.append(dictData)
                # Perform pagination here using list slicing
                start_index = (page_no - 1) * page_len
                end_index = start_index + page_len
                paginated_data = final_data[start_index:end_index]

                unique_keys = OrderedDict()

                for data in paginated_data:
                    for key in data.keys():
                        unique_keys[key] = None

                result["labels"] = list(unique_keys.keys())

                result["total"] = len(final_data)
                result["datasets"] = paginated_data
                return result
            else:
                return result
        
        elif type and type == "download":
            del type
            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)
            
            data = Q()

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(plant_analysis_date__gte = start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(plant_analysis_date__lte = end_date)
            
            if search_text:
                if search_text.isdigit():
                    data &= (Q(rrNo__icontains=search_text))
                else:
                    data &= Q(mine__icontains=search_text)

            # usecase_data = CoalTesting.objects(data).order_by("-receive_date")
            usecase_data = RecieptCoalQualityAnalysis.objects(data, mode="Rail")
            count = len(usecase_data)
            path = None
            logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
            if usecase_data:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "Railwise_GCV_Grade_Comparision_Report_{}.xlsx".format(
                            datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format()
                    cell_format2.set_bold()
                    cell_format2.set_font_size(10)
                    cell_format2.set_align("center")
                    cell_format2.set_align("vcenter")
                    cell_format2.set_text_wrap(True)
                    cell_format2.set_border(1)

                    header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
                    date_format = workbook.add_format({'align': 'center', 'font_size': 12, "bold": True})
                    report_name_format = workbook.add_format({'align': 'center', 'font_size': 15, "bold": True})
                    # report_name_format.set_text_wrap(True)

                    header_format.set_align("vcenter")
                    date_format.set_align("vcenter")
                    report_name_format.set_align("vcenter")
                    header_format.set_border(1)
                    date_format.set_border(1)
                    report_name_format.set_border(1)

                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)
                    cell_format = workbook.add_format()
                    cell_format.set_font_size(10)
                    cell_format.set_align("center")
                    cell_format.set_align("vcenter")
                    cell_format.set_text_wrap(True)
                    cell_format.set_border(1)

                    worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                    
                    # Merge cells for the main header and place it in the center
                    main_header = "GMR Warora Energy Limited"  # Set your main header text here
                    worksheet.merge_range("A1:I1", main_header, header_format)  # Merge cells A1 to H1 for the header
                    
                    # Write the current date on the left side (A2)
                    # worksheet.write("A2", f"Date: {datetime.datetime.now().strftime('%d-%m-%Y')}", date_format)
                    worksheet.merge_range("A2:B2", f"Date: {datetime.datetime.now().strftime('%d-%m-%Y')}", date_format)
                    worksheet.merge_range("C2:I2", f"Rail - GCV Wise Comparision Report", report_name_format)

                    headers = [
                        "Sr.No",
                        "Mine",
                        "DO_Qty",
                        "GWEL_Gross_Calorific_Value_(Adb)",
                        "GWEL_Gross_Calorific_Value_Grade",
                        "Third_Party_Gross_Calorific_Value_(Adb)",
                        "Third_Party_Gross_Calorific_Value_(Adb)_grade",
                        "Difference_Gross_Calorific_Value(Adb)",
                        "Difference_Gross_Calorific_Value_Grade_(Adb)"
                    ]

                    for index, header in enumerate(headers):
                        worksheet.write(2, index, header, cell_format2)


                    if any(usecase_data):
                        aggregated_data = defaultdict(lambda: defaultdict(lambda: {"DO_Qty": 0, "Gross_Calorific_Value_(Adb)": 0, "Third_Party_Gross_Calorific_Value_(Adb)": 0, "Third_Party_Gross_Calorific_Value_(Adb)_count": 0, "count": 0}))

                        for log in usecase_data:
                            month = log.plant_analysis_date.strftime("%Y-%m")
                            payload = log.payload()
                            mine = payload["mine"]
                            if payload.get("sample_qty"):
                                aggregated_data[month][mine]["DO_Qty"] += float(payload["sample_qty"])
                            if payload.get("GWEL_ADB_GCV"):
                                aggregated_data[month][mine]["Gross_Calorific_Value_(Adb)"] += float(payload["GWEL_ADB_GCV"]) 
                            if payload.get("THIRDPARTY_ADB_GCV"):
                                aggregated_data[month][mine]["Third_Party_Gross_Calorific_Value_(Adb)"] += float(payload["THIRDPARTY_ADB_GCV"])
                                aggregated_data[month][mine]["Third_Party_Gross_Calorific_Value_(Adb)_count"] += 1
                
                            aggregated_data[month][mine]["count"] += 1

                        dataList = [
                            {"month": month, "data": {
                                mine: {
                                    # first it was average then we have changed to total on 08-11-2024 11:31 am
                                    # "average_DO_Qty": data["DO_Qty"] / data["count"],
                                    "average_DO_Qty": data["DO_Qty"],
                                    "average_Gross_Calorific_Value_(Adb)": data["Gross_Calorific_Value_(Adb)"] / data["count"],
                                    "average_Third_Party_Gross_Calorific_Value_(Adb)": data["Third_Party_Gross_Calorific_Value_(Adb)"] / data["Third_Party_Gross_Calorific_Value_(Adb)_count"] if data["Third_Party_Gross_Calorific_Value_(Adb)"] != 0 else "",
                                } for mine, data in aggregated_data[month].items()
                            }} for month in aggregated_data
                        ]
                        coal_grades = CoalGrades.objects()  # Fetch all coal grades from the database

                        for month_data in dataList:
                            for key, mine_data in month_data["data"].items():
                                if mine_data["average_Gross_Calorific_Value_(Adb)"] is not None:
                                    for single_coal_grades in coal_grades:
                                        if single_coal_grades["end_value"] != "":
                                            if (int(single_coal_grades["start_value"]) <= int(float(mine_data.get("average_Gross_Calorific_Value_(Adb)"))) <= int(single_coal_grades["end_value"]) and single_coal_grades["start_value"] != "" and single_coal_grades["end_value"] != ""):
                                                mine_data["average_GCV_Grade"] = single_coal_grades["grade"]
                                            elif int(mine_data["average_Gross_Calorific_Value_(Adb)"]) > 7001:
                                                mine_data["average_GCV_Grade"] = "G-1"
                                                break

                                if mine_data["average_Third_Party_Gross_Calorific_Value_(Adb)"] != "":
                                    for single_coal_grades in coal_grades:
                                        if single_coal_grades["end_value"] != "":
                                            if (int(single_coal_grades["start_value"]) <= int(float(mine_data.get("average_Third_Party_Gross_Calorific_Value_(Adb)"))) <= int(single_coal_grades["end_value"]) and single_coal_grades["start_value"] != "" and single_coal_grades["end_value"] != ""):
                                                mine_data["average_Third_Party_GCV_Grade"] = single_coal_grades["grade"]
                                            elif int(mine_data["average_Gross_Calorific_Value_(Adb)"]) > 7001:
                                                mine_data["average_Third_Party_GCV_Grade"] = "G-1"
                                                break
                    final_data = []
                    if month_date:
                        filtered_data = [entry for entry in dataList if entry["month"] == month_date]
                        if filtered_data:
                            data = filtered_data[0]['data']  # Extracting the 'data' dictionary from the list
                            for mine, values in data.items():
                                dictData = {}
                                dictData['Mine'] = mine
                                dictData['DO_Qty'] = round(values['average_DO_Qty'], 2)
                                dictData['GWEL_Gross_Calorific_Value_(Adb)'] = round(values['average_Gross_Calorific_Value_(Adb)'], 2)
                                if values.get('average_GCV_Grade'):
                                    dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'] = values['average_GCV_Grade']
                                if values["average_Third_Party_Gross_Calorific_Value_(Adb)"] != "":    
                                    dictData["Third_Party_Gross_Calorific_Value_(Adb)"] = round(values["average_Third_Party_Gross_Calorific_Value_(Adb)"], 2)
                                    if values.get("average_Third_Party_GCV_Grade"):
                                        dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"] = str(values["average_Third_Party_GCV_Grade"])
                                    dictData["Difference_Gross_Calorific_Value_(Adb)"] = str(abs(int(float(dictData["GWEL_Gross_Calorific_Value_(Adb)"])) - int(float(dictData["Third_Party_Gross_Calorific_Value_(Adb)"]))))
                                    if values.get("average_Third_Party_GCV_Grade"):
                                        dictData["Difference_Gross_Calorific_Value_Grade_(Adb)"] = str(abs(int(dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'].replace('G-', '')) - int(dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"].replace('G-', ''))))
                                final_data.append(dictData)
                        else:
                            console_logger.debug(
                                f"No data available for the given month: {month_date}"
                            )
                            return {"message": f"No data available for the given month: {month_date}"}
                    else:
                        filtered_data = [entry for entry in dataList]
                        # data = filtered_data[0]['data']  # Extracting the 'data' dictionary from the list
                        for single_data in filtered_data:
                            for mine, values in single_data['data'].items():
                                dictData = {}
                                dictData['Mine'] = mine
                                dictData['DO_Qty'] = round(values['average_DO_Qty'], 2)
                                dictData['GWEL_Gross_Calorific_Value_(Adb)'] = round(values['average_Gross_Calorific_Value_(Adb)'], 2)
                                if values.get('average_GCV_Grade'):
                                    dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'] = values['average_GCV_Grade']
                                if values["average_Third_Party_Gross_Calorific_Value_(Adb)"] != "":
                                    dictData["Third_Party_Gross_Calorific_Value_(Adb)"] = round(values["average_Third_Party_Gross_Calorific_Value_(Adb)"],2)
                                    if values.get("average_Third_Party_GCV_Grade"):
                                        dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"] = str(values["average_Third_Party_GCV_Grade"])
                                    dictData["Difference_Gross_Calorific_Value_(Adb)"] = str(abs(int(float(dictData["GWEL_Gross_Calorific_Value_(Adb)"])) - int(float(dictData["Third_Party_Gross_Calorific_Value_(Adb)"]))))
                                    if values.get("average_Third_Party_GCV_Grade"):
                                        dictData["Difference_Gross_Calorific_Value_Grade_(Adb)"] = str(abs(int(dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'].replace('G-', '')) - int(dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"].replace('G-', ''))))
                                final_data.append(dictData)
                    result["labels"] = list(final_data[0].keys())
                    result["total"] = len(final_data)
                    result["datasets"] = final_data

                    row = 3
                    for single_data in result["datasets"]:
                        worksheet.write(row, 0, count, cell_format)
                        worksheet.write(row, 1, single_data["Mine"])
                        worksheet.write(row, 2, single_data["DO_Qty"])
                        worksheet.write(row, 3, single_data["GWEL_Gross_Calorific_Value_(Adb)"])
                        if single_data.get("GWEL_Gross_Calorific_Value_Grade_(Adb)") != "" and single_data.get("GWEL_Gross_Calorific_Value_Grade_(Adb)") != None:
                            worksheet.write(row, 4, str(single_data["GWEL_Gross_Calorific_Value_Grade_(Adb)"]), cell_format)
                        if single_data.get("Third_Party_Gross_Calorific_Value_(Adb)") != "" and single_data.get("Third_Party_Gross_Calorific_Value_(Adb)") != None:
                            worksheet.write(row, 5, str(single_data["Third_Party_Gross_Calorific_Value_(Adb)"]), cell_format)
                        if single_data.get("Third_Party_Gross_Calorific_Value_(Adb)_grade") != "" and single_data.get("Third_Party_Gross_Calorific_Value_(Adb)_grade") != None:
                            worksheet.write(row, 6, str(single_data["Third_Party_Gross_Calorific_Value_(Adb)_grade"]), cell_format)
                        if single_data.get("Difference_Gross_Calorific_Value_(Adb)") != "" and single_data.get("Difference_Gross_Calorific_Value_(Adb)") != None:
                            worksheet.write(row, 7, str(single_data["Difference_Gross_Calorific_Value_(Adb)"]), cell_format)
                        if single_data.get("Difference_Gross_Calorific_Value_Grade_(Adb)") != "" and single_data.get("Difference_Gross_Calorific_Value_Grade_(Adb)") != None:
                            worksheet.write(row, 8, str(single_data["Difference_Gross_Calorific_Value_Grade_(Adb)"]), cell_format)
                        count -= 1
                        row += 1
                    workbook.close()

                    # console_logger.debug("Successfully {} report generated".format(service_id))
                    # console_logger.debug("sent data {}".format(path))
                    return {
                            "Type": "coal_test_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                        }
                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            else:
                console_logger.error("No data found")
                return {
                            "Type": "coal_test_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                        }
    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e



@router.get("/coal_gcv_table_train_old", tags=["Coal Testing"])
def coal_wcl_gcv_table_old(
    response: Response,
    currentPage: Optional[int] = None,
    perPage: Optional[int] = None,
    search_text: Optional[str] = None,
    start_timestamp: Optional[str] = None,
    end_timestamp: Optional[str] = None,
    month_date: Optional[str] = None,
    type: Optional[str] = "display"):
    try:
        result = {"labels": [], "datasets": [], "total": 0, "page_size": 15}

        if type and type == "display":
            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage

            data = Q()

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(receive_date__gte = start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(receive_date__lte = end_date)

            if search_text:
                if search_text.isdigit():
                    data &= (Q(rrNo__icontains=search_text))
                else:
                    data &= Q(location__icontains=search_text) | Q(rake_no__icontains=search_text)

            offset = (page_no - 1) * page_len
            # logs = CoalTestingTrain.objects(data).order_by("-ID").skip(offset).limit(page_len)
            logs = CoalTestingTrain.objects(data).order_by("-ID")

            if any(logs):
                aggregated_data = defaultdict(lambda: defaultdict(lambda: {"RR_Qty": 0, "Gross_Calorific_Value_(Adb)": 0, "Third_Party_Gross_Calorific_Value_(Adb)": 0, "Third_Party_Gross_Calorific_Value_(Adb)_count": 0, "count": 0}))

                for log in logs:
                    month = log.receive_date.strftime("%Y-%m")
                    payload = log.gradepayload()
                    mine = payload["Mine"]
                    if payload.get("RR_Qty"):
                        if payload.get("RR_Qty").count('.') > 1:
                            aggregated_data[month][mine]["RR_Qty"] += float(payload.get("RR_Qty")[:5])
                        else:
                            aggregated_data[month][mine]["RR_Qty"] += float(payload.get("RR_Qty"))
                    if payload.get("Gross_Calorific_Value_(Adb)"):
                        aggregated_data[month][mine]["Gross_Calorific_Value_(Adb)"] += float(payload["Gross_Calorific_Value_(Adb)"])
                    if payload.get("Third_Party_Gross_Calorific_Value_(Adb)"):
                        aggregated_data[month][mine]["Third_Party_Gross_Calorific_Value_(Adb)"] += float(payload["Third_Party_Gross_Calorific_Value_(Adb)"])
                        aggregated_data[month][mine]["Third_Party_Gross_Calorific_Value_(Adb)_count"] += 1
                    aggregated_data[month][mine]["count"] += 1

                dataList = [
                    {"month": month, "data": {
                        mine: {
                            "average_RR_Qty": data["RR_Qty"] / data["count"],
                            "average_Gross_Calorific_Value_(Adb)": data["Gross_Calorific_Value_(Adb)"] / data["count"],
                            "average_Third_Party_Gross_Calorific_Value_(Adb)": data["Third_Party_Gross_Calorific_Value_(Adb)"] / data["Third_Party_Gross_Calorific_Value_(Adb)_count"] if data["Third_Party_Gross_Calorific_Value_(Adb)"] != 0 else "",
                        } for mine, data in aggregated_data[month].items()
                    }} for month in aggregated_data
                ]
                coal_grades = CoalGrades.objects()  # Fetch all coal grades from the database

                # Iterate through each month's data
                for month_data in dataList:
                    for key, mine_data in month_data["data"].items():
                        if mine_data["average_Gross_Calorific_Value_(Adb)"] is not None:
                            for single_coal_grades in coal_grades:
                                if single_coal_grades["end_value"] != "":
                                    if (int(single_coal_grades["start_value"]) <= int(float(mine_data.get("average_Gross_Calorific_Value_(Adb)"))) <= int(single_coal_grades["end_value"]) and single_coal_grades["start_value"] != "" and single_coal_grades["end_value"] != ""):
                                        mine_data["average_GCV_Grade"] = single_coal_grades["grade"]
                                    elif int(mine_data["average_Gross_Calorific_Value_(Adb)"]) > 7001:
                                        mine_data["average_GCV_Grade"] = "G-1"
                                        break

                        if mine_data["average_Third_Party_Gross_Calorific_Value_(Adb)"] != "":
                            for single_coal_grades in coal_grades:
                                if single_coal_grades["end_value"] != "":
                                    if (int(single_coal_grades["start_value"]) <= int(float(mine_data.get("average_Third_Party_Gross_Calorific_Value_(Adb)"))) <= int(single_coal_grades["end_value"]) and single_coal_grades["start_value"] != "" and single_coal_grades["end_value"] != ""):
                                        mine_data["average_Third_Party_GCV_Grade"] = single_coal_grades["grade"]
                                    elif int(mine_data["average_Gross_Calorific_Value_(Adb)"]) > 7001:
                                        mine_data["average_Third_Party_GCV_Grade"] = "G-1"
                                        break
                final_data = []
                if month_date:
                    filtered_data = [entry for entry in dataList if entry["month"] == month_date]
                    if filtered_data:
                        data = filtered_data[0]['data']  # Extracting the 'data' dictionary from the list
                        for mine, values in data.items():
                            dictData = {}
                            dictData['Mine'] = mine
                            dictData['RR_Qty'] = round(values['average_RR_Qty'], 2)
                            dictData['GWEL_Gross_Calorific_Value_(Adb)'] = round(values['average_Gross_Calorific_Value_(Adb)'], 2)

                            if values["average_Third_Party_Gross_Calorific_Value_(Adb)"] != "":
                                # dictData['Gross_Calorific_Value_Grade_(Adb)'] = values['average_Third_Party_GCV_Grade']
                                dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'] = values['average_GCV_Grade']
                                dictData["Third_Party_Gross_Calorific_Value_(Adb)"] = round(values["average_Third_Party_Gross_Calorific_Value_(Adb)"], 2)
                                if values.get("average_Third_Party_GCV_Grade"):
                                    dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"] = str(values["average_Third_Party_GCV_Grade"])
                                dictData["Difference_Gross_Calorific_Value_(Adb)"] = str(abs(int(float(dictData["GWEL_Gross_Calorific_Value_(Adb)"])) - int(float(dictData["Third_Party_Gross_Calorific_Value_(Adb)"]))))
                                if values.get("average_Third_Party_GCV_Grade"):
                                    dictData["Difference_Gross_Calorific_Value_Grade_(Adb)"] = str(abs(int(dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'].replace('G-', ''))) - int(dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"].replace('G-', '')))
                            final_data.append(dictData)
                    else:
                        # console_logger.debug("No data available for the given month:", month_date)
                        return result
                else:
                    console_logger.debug("inside else")
                    filtered_data = [entry for entry in dataList]
                    for single_data in filtered_data:
                        for mine, values in single_data['data'].items():
                            dictData = {}
                            dictData['Mine'] = mine
                            dictData['RR_Qty'] = round(values['average_RR_Qty'], 2)
                            dictData['GWEL_Gross_Calorific_Value_(Adb)'] = round(values['average_Gross_Calorific_Value_(Adb)'], 2)
                            if values["average_Third_Party_Gross_Calorific_Value_(Adb)"] != "":
                                # dictData['Gross_Calorific_Value_Grade_(Adb)'] = values['average_Third_Party_GCV_Grade']
                                dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'] = values['average_GCV_Grade']
                                dictData["Third_Party_Gross_Calorific_Value_(Adb)"] = round(values["average_Third_Party_Gross_Calorific_Value_(Adb)"], 2)
                                if values.get("average_Third_Party_GCV_Grade"):
                                    dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"] = str(values["average_Third_Party_GCV_Grade"])
                                dictData["Difference_Gross_Calorific_Value_(Adb)"] = str(abs(int(float(dictData["GWEL_Gross_Calorific_Value_(Adb)"])) - int(float(dictData["Third_Party_Gross_Calorific_Value_(Adb)"]))))
                                if values.get("average_Third_Party_GCV_Grade"):
                                    dictData["Difference_Gross_Calorific_Value_Grade_(Adb)"] = str(abs(int(dictData['GWEL_Gross_Calorific_Value_Grade_(Adb)'].replace('G-', ''))) - int(dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"].replace('G-', '')))
                           
                            final_data.append(dictData)

                start_index = (page_no - 1) * page_len
                end_index = start_index + page_len
                paginated_data = final_data[start_index:end_index]

                # result["labels"] = list(final_data[0].keys())
                unique_keys = OrderedDict()

                for data in paginated_data:
                    for key in data.keys():
                        unique_keys[key] = None

                result["labels"] = [
                        "Mine",
                        "RR_Qty",
                        "GWEL_Gross_Calorific_Value_(Adb)",
                        "GWEL_Gross_Calorific_Value_Grade_(Adb)",
                        "Third_Party_Gross_Calorific_Value_(Adb)",
                        "Third_Party_Gross_Calorific_Value_(Adb)_grade",
                        "Difference_Gross_Calorific_Value_(Adb)",
                        "Difference_Gross_Calorific_Value_Grade_(Adb)"
                    ]
                result["total"] = len(final_data)
                result["datasets"] = paginated_data

                return result
            else:
                return result
        
        elif type and type == "download":
            del type
            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)
            
            data = Q()

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(receive_date__gte = start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(receive_date__lte = end_date)

            if search_text:
                if search_text.isdigit():
                    data &= (Q(rrNo__icontains=search_text))
                else:
                    data &= Q(location__icontains=search_text) | Q(rake_no__icontains=search_text)

            usecase_data = CoalTestingTrain.objects(data).order_by("-receive_date")
            count = len(usecase_data)
            path = None
            if usecase_data:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "Railwise_GCV_Grade_Comparision_Report_{}.xlsx".format(
                            datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format()
                    cell_format2.set_bold()
                    cell_format2.set_font_size(10)
                    cell_format2.set_align("center")
                    cell_format2.set_align("vjustify")

                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)
                    cell_format = workbook.add_format()
                    cell_format.set_font_size(10)
                    cell_format.set_align("center")
                    cell_format.set_align("vcenter")

                    headers = [
                        "Sr.No",
                        "Mine",
                        "RR_Qty",
                        "GWEL_Gross_Calorific_Value_(Adb)",
                        "GWEL_Gross_Calorific_Value_Grade",
                        "Third_Party_Gross_Calorific_Value_(Adb)",
                        "Third_Party_Gross_Calorific_Value_(Adb)_grade",
                        "Difference_Gross_Calorific_Value_(Adb)",
                        "Difference_Gross_Calorific_Value_Grade_(Adb)"
                    ]

                    for index, header in enumerate(headers):
                        worksheet.write(0, index, header, cell_format2)

                    if any(usecase_data):
                        aggregated_data = defaultdict(lambda: defaultdict(lambda: {"RR_Qty": 0, "Gross_Calorific_Value_(Adb)": 0, "Third_Party_Gross_Calorific_Value_(Adb)": 0, "Third_Party_Gross_Calorific_Value_(Adb)_count": 0, "count": 0}))

                        for log in usecase_data:
                            month = log.receive_date.strftime("%Y-%m")
                            payload = log.gradepayload()
                            mine = payload["Mine"]
                            if payload.get("RR_Qty"):
                                if payload.get("RR_Qty").count('.') > 1:
                                    aggregated_data[month][mine]["RR_Qty"] += float(payload.get("RR_Qty")[:5])
                                else:
                                    aggregated_data[month][mine]["RR_Qty"] += float(payload.get("RR_Qty"))
                            if payload.get("Gross_Calorific_Value_(Adb)"):
                                aggregated_data[month][mine]["Gross_Calorific_Value_(Adb)"] += float(payload["Gross_Calorific_Value_(Adb)"])

                            if payload.get("Third_Party_Gross_Calorific_Value_(Adb)"):
                                aggregated_data[month][mine]["Third_Party_Gross_Calorific_Value_(Adb)"] += float(payload["Third_Party_Gross_Calorific_Value_(Adb)"])
                                aggregated_data[month][mine]["Third_Party_Gross_Calorific_Value_(Adb)_count"] += 1
                            aggregated_data[month][mine]["count"] += 1


                        dataList = [
                            {"month": month, "data": {
                                mine: {
                                    "average_RR_Qty": data["RR_Qty"] / data["count"],
                                    "average_Gross_Calorific_Value_(Adb)": data["Gross_Calorific_Value_(Adb)"] / data["count"],
                                    "average_Third_Party_Gross_Calorific_Value_(Adb)": data["Third_Party_Gross_Calorific_Value_(Adb)"] / data["Third_Party_Gross_Calorific_Value_(Adb)_count"] if data["Third_Party_Gross_Calorific_Value_(Adb)"] != 0 else "",
                                } for mine, data in aggregated_data[month].items()
                            }} for month in aggregated_data
                        ]
                        coal_grades = CoalGrades.objects()  # Fetch all coal grades from the database

                        # Iterate through each month's data
                        for month_data in dataList:
                            for key, mine_data in month_data["data"].items():
                                if mine_data["average_Gross_Calorific_Value_(Adb)"] is not None:
                                    for single_coal_grades in coal_grades:
                                        if single_coal_grades["end_value"] != "":
                                            if (int(single_coal_grades["start_value"]) <= int(float(mine_data.get("average_Gross_Calorific_Value_(Adb)"))) <= int(single_coal_grades["end_value"]) and single_coal_grades["start_value"] != "" and single_coal_grades["end_value"] != ""):
                                                mine_data["average_GCV_Grade"] = single_coal_grades["grade"]
                                            elif int(mine_data["average_Gross_Calorific_Value_(Adb)"]) > 7001:
                                                mine_data["average_GCV_Grade"] = "G-1"
                                                break
                        

                                if mine_data["average_Third_Party_Gross_Calorific_Value_(Adb)"] != "":
                                    for single_coal_grades in coal_grades:
                                        if single_coal_grades["end_value"] != "":
                                            if (int(single_coal_grades["start_value"]) <= int(float(mine_data.get("average_Third_Party_Gross_Calorific_Value_(Adb)"))) <= int(single_coal_grades["end_value"]) and single_coal_grades["start_value"] != "" and single_coal_grades["end_value"] != ""):
                                                mine_data["average_Third_Party_GCV_Grade"] = single_coal_grades["grade"]
                                            elif int(mine_data["average_Gross_Calorific_Value_(Adb)"]) > 7001:
                                                mine_data["average_Third_Party_GCV_Grade"] = "G-1"
                                                break
                    final_data = []
                    if month_date:
                        filtered_data = [entry for entry in dataList if entry["month"] == month_date]
                        if filtered_data:
                            data = filtered_data[0]['data']  # Extracting the 'data' dictionary from the list
                            for mine, values in data.items():
                                dictData = {}
                                dictData['Mine'] = mine
                                dictData['RR_Qty'] = str(values['average_RR_Qty'])
                                dictData['Gross_Calorific_Value_(Adb)'] = str(values['average_Gross_Calorific_Value_(Adb)'])
                                if values["average_Third_Party_Gross_Calorific_Value_(Adb)"] != "":    
                                    dictData['Gross_Calorific_Value_Grade_(Adb)'] = values['average_GCV_Grade']
                                    dictData["Third_Party_Gross_Calorific_Value_(Adb)"] = str(values["average_Third_Party_Gross_Calorific_Value_(Adb)"])
                                    if values.get("average_Third_Party_GCV_Grade"):
                                        dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"] = str(values["average_Third_Party_GCV_Grade"])
                                        dictData["Difference_Gross_Calorific_Value_Grade_(Adb)"] = str(abs(int(dictData['Gross_Calorific_Value_Grade_(Adb)'].replace('G-', ''))) - int(dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"].replace('G-', '')))
                                    dictData["Difference_Gross_Calorific_Value_(Adb)"] = str(abs(int(float(dictData["Gross_Calorific_Value_(Adb)"])) - int(float(dictData["Third_Party_Gross_Calorific_Value_(Adb)"]))))
                                final_data.append(dictData)
                        else:
                            console_logger.debug("No data available for the given month:", month_date)
                            return {"message": f"No data available for the given month: {month_date}"}
                    else:
                        console_logger.debug("inside else")
                        filtered_data = [entry for entry in dataList]
                        # data = filtered_data[0]['data']  # Extracting the 'data' dictionary from the list
                        for single_data in filtered_data:
                            for mine, values in single_data['data'].items():
                                dictData = {}
                                dictData['Mine'] = mine
                                dictData['RR_Qty'] = str(values['average_RR_Qty'])
                                dictData['Gross_Calorific_Value_(Adb)'] = str(values['average_Gross_Calorific_Value_(Adb)'])
                                if values["average_Third_Party_Gross_Calorific_Value_(Adb)"] != "":
                                    dictData['Gross_Calorific_Value_Grade_(Adb)'] = values['average_GCV_Grade']
                                    dictData["Third_Party_Gross_Calorific_Value_(Adb)"] = str(values["average_Third_Party_Gross_Calorific_Value_(Adb)"])
                                    if values.get("average_Third_Party_GCV_Grade"):
                                        dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"] = str(values["average_Third_Party_GCV_Grade"])
                                        dictData["Difference_Gross_Calorific_Value_Grade_(Adb)"] = str(abs(int(dictData['Gross_Calorific_Value_Grade_(Adb)'].replace('G-', ''))) - int(dictData["Third_Party_Gross_Calorific_Value_(Adb)_grade"].replace('G-', '')))
                                    dictData["Difference_Gross_Calorific_Value_(Adb)"] = str(abs(int(float(dictData["Gross_Calorific_Value_(Adb)"])) - int(float(dictData["Third_Party_Gross_Calorific_Value_(Adb)"]))))
                                final_data.append(dictData)
                    result["labels"] = list(final_data[0].keys())
                    result["total"] = len(final_data)
                    result["datasets"] = final_data

                    row = 1
                    for single_data in result["datasets"]:
                        worksheet.write(row, 0, count, cell_format)
                        worksheet.write(row, 1, single_data["Mine"])
                        worksheet.write(row, 2, single_data["RR_Qty"])
                        worksheet.write(row, 3, single_data["Gross_Calorific_Value_(Adb)"])
                        if single_data.get("Gross_Calorific_Value_Grade_(Adb)") != "" and single_data.get("Gross_Calorific_Value_Grade_(Adb)") != None:
                            worksheet.write(row, 4, str(single_data["Gross_Calorific_Value_Grade_(Adb)"]), cell_format)
                        if single_data.get("Third_Party_Gross_Calorific_Value_(Adb)") != "" and single_data.get("Third_Party_Gross_Calorific_Value_(Adb)") != None:
                            worksheet.write(row, 5, str(single_data["Third_Party_Gross_Calorific_Value_(Adb)"]), cell_format)
                        if single_data.get("Third_Party_Gross_Calorific_Value_(Adb)_grade") != "" and single_data.get("Third_Party_Gross_Calorific_Value_(Adb)_grade") != None:
                            worksheet.write(row, 6, str(single_data["Third_Party_Gross_Calorific_Value_(Adb)_grade"]), cell_format)
                        if single_data.get("Difference_Gross_Calorific_Value_(Adb)") != "" and single_data.get("Difference_Gross_Calorific_Value_(Adb)") != None:
                            worksheet.write(row, 7, str(single_data["Difference_Gross_Calorific_Value_(Adb)"]), cell_format)
                        if single_data.get("Difference_Gross_Calorific_Value_Grade_(Adb)") != "" and single_data.get("Difference_Gross_Calorific_Value_Grade_(Adb)") != None:
                            worksheet.write(row, 8, str(single_data["Difference_Gross_Calorific_Value_Grade_(Adb)"]), cell_format)
                        count -= 1
                        row += 1
                    workbook.close()

                    console_logger.debug("Successfully {} report generated".format(service_id))
                    console_logger.debug("sent data {}".format(path))
                    return {
                            "Type": "coal_test_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                        }
                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            else:
                console_logger.error("No data found")
                return {
                            "Type": "coal_test_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                        }
    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/road/fetchyearmonth", tags=["Coal Testing"])
def endpoint_to_fetch_road_year_month(response:Response):
    try:
        dataList = []
        fetchCoaldates = CoalTesting.objects()
        for fetchCoaldate in fetchCoaldates:
            yearMonth = datetime.datetime.strptime(str(fetchCoaldate.receive_date),'%Y-%m-%d %H:%M:%S').strftime('%Y-%m')
            if yearMonth not in dataList:
                dataList.append(yearMonth)
        return dataList
    except Exception as e:
        console_logger.debug("----- Road Vehicle Count Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e
    

@router.get("/rail/fetchyearmonth", tags=["Coal Testing"])
def endpoint_to_fetch_road_year_month(response:Response):
    try:
        dataList = []
        fetchCoaldates = CoalTestingTrain.objects()
        for fetchCoaldate in fetchCoaldates:
            yearMonth = datetime.datetime.strptime(str(fetchCoaldate.receive_date),'%Y-%m-%d %H:%M:%S').strftime('%Y-%m')
            if yearMonth not in dataList:
                dataList.append(yearMonth)
        return dataList
    except Exception as e:
        console_logger.debug("----- Road Vehicle Count Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/update_wcl/testing", tags=["Coal Testing"])
def update_wcl_testing(response:Response,data: wclData):
    try:
        dataLoad = data.dict()
        fetchCoaltesting = CoalTesting.objects.get(id=dataLoad.get("id"))
        if fetchCoaltesting:
            for param in fetchCoaltesting.parameters:
                param_name = f"{param.get('parameter_Name')}_{param.get('unit_Val')}"
                if dataLoad.get("coal_data").get(param_name) is not None:
                    param["val1"] = dataLoad.get("coal_data").get(param_name)
        fetchCoaltesting.save()

        return {"detail": "success"}

    except Exception as e:
        console_logger.debug("----- Error updating WCL testing -----", e)
        response.status_code = 400
        return e


@router.post("/update_secl/testing", tags=["Coal Testing"])
def update_secl_testing(response:Response,data: seclData):
    try:
        dataLoad = data.dict()
        fetchCoaltesting = CoalTestingTrain.objects.get(id=dataLoad.get("id"))
        if fetchCoaltesting:
            for param in fetchCoaltesting.parameters:
                param_name = f"{param.get('parameter_Name')}_{param.get('unit_Val').replace(' ', '')}"
                if dataLoad.get("coal_data").get(param_name) is not None:
                    param["val1"] = dataLoad.get("coal_data").get(param_name)
        fetchCoaltesting.save()

        return {"detail": "success"}

    except Exception as e:
        console_logger.debug("----- Error updating SECL testing -----", e)
        response.status_code = 400
        return e


@router.post("/coal_test_wcl_addon", tags=["Coal Testing"])
def wcl_addon_data(response: Response, paydata: WCLtestMain):
    try:
        multyData = paydata.dict()
        for dataLoad in multyData["data"]:
            fetchCoaltesting = CoalTesting.objects.get(id=dataLoad.get("id"))
            fetchCoaltesting.third_party_report_no = dataLoad.get("coal_data").get("Third_Party_Report_No")
            fetchCoaltesting.third_party_upload_date = datetime.datetime.today()
            
            if fetchCoaltesting:
                for param_name, param_value in dataLoad.get("coal_data").items():
                    # Check if the parameter exists already
                    if not any(param['parameter_Name'] == param_name.rsplit('_', 1)[0] for param in fetchCoaltesting.parameters):
                        # if param_name != "Third_Party_Gcv":
                        single_data = {
                            "parameter_Name": param_name.rsplit('_', 1)[0],
                            "unit_Val": param_name.rsplit('_', 1)[1],  # Add the unit value if available
                            "test_Method": "",  # Add the test method if available
                            "val1": param_value
                        }
                        fetchCoaltesting.parameters.append(single_data)

                for single_data in fetchCoaltesting.parameters:
                    param_name = f"{single_data.get('parameter_Name')}_{single_data.get('unit_Val').replace(' ', '')}"
                    if dataLoad.get("coal_data").get(param_name) is not None:
                        single_data["val1"] = dataLoad.get("coal_data").get(param_name)
                    
                    if single_data["parameter_Name"] == "Gross_Calorific_Value_(Adb)":
                        single_data["Third_Party_Gross_Calorific_Value_(Adb)"] = dataLoad.get("coal_data").get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg")
                        if dataLoad.get("coal_data").get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg"):
                            single_data["Gcv_Difference"] = str(abs(float(single_data["val1"]) - float(dataLoad.get("coal_data").get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg"))))
        
                            fetchCoalGrades = CoalGrades.objects()
                            for single_coal_grades in fetchCoalGrades:
                                if (
                                    single_coal_grades["start_value"]
                                    <= dataLoad.get("coal_data").get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg")
                                    <= single_coal_grades["end_value"]
                                    and single_coal_grades["start_value"] != ""
                                    and single_coal_grades["end_value"] != ""
                                ):
                                    single_data["Third_Party_Grade"] = single_coal_grades["grade"]
                                elif dataLoad.get("coal_data").get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg") > "7001":
                                    single_data["Third_Party_Grade"] = single_coal_grades["grade"]
                                    break
                            if single_data.get("grade"):
                                grade_diff = str(abs(int(single_coal_grades["grade"].replace('G-', '')) - int(single_data["grade"].replace('G-', ''))))
                                single_data["Grade_Diff"] = grade_diff

                    else:
                        single_data["thrdgcv"] = None
                        single_data["gcv_difference"] = None
                        single_data["thrd_grade"] = None
                        single_data["grade_diff"] = None

            fetchCoaltesting.save()
        return {"detail": "success"}
    
    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/coal_test_wcl_train_addon", tags=["Coal Testing"])
def wcl_addon_data(response: Response, paydata: WCLtestMain):
    try:
        multyData = paydata.dict()
        for dataLoad in multyData["data"]:
            # fetchCoaltesting = CoalTestingTrain.objects.get(id=dataLoad.get("id"))
            fetchCoaltesting = RecieptCoalQualityAnalysis.objects.get(id=dataLoad.get("id"))
            # fetchCoaltesting.thirdparty_reference_no = dataLoad.get("coal_data").get("Third_Party_Report_No")
            # fetchCoaltesting.thirdparty_report_date = str(datetime.datetime.today())
            fetchCoaltesting.thirdparty_created_date = str(datetime.datetime.today().strftime("%Y-%m-%d"))
            if fetchCoaltesting:
                for param_name, param_value in dataLoad.get("coal_data").items():
                    # Check if the parameter exists already
                    if param_name == "Third_Party_Total_Moisture_%":
                        fetchCoaltesting.thirdparty_arb_tm = str(param_value)
                    if param_name == "Third_Party_Inherent_Moisture_(Adb)_%":
                        fetchCoaltesting.thirdparty_adb_im = str(param_value)
                    if param_name == "Third_Party_Ash_(Adb)_%":
                        fetchCoaltesting.thirdparty_adb_ash = str(param_value)
                    if param_name == "Third_Party_Volatile_Matter_(Adb)_%":
                        fetchCoaltesting.thirdparty_adb_vm = str(param_value)
                    if param_name == "Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg":
                        fetchCoaltesting.thirdparty_adb_gcv = str(param_value)
                    if param_name == "Third_Party_Ash_(Arb)_%":
                        fetchCoaltesting.thirdparty_arb_ash = str(param_value)
                    if param_name == "Third_Party_Volatile_Matter_(Arb)_%":
                        fetchCoaltesting.thirdparty_arb_vm = str(param_value)
                    if param_name == "Third_Party_Fixed_Carbon_(Arb)_%":
                        fetchCoaltesting.thirdparty_arb_fc = str(param_value)
                    if param_name == "Third_Party_Gross_Calorific_Value_(Arb)_Kcal/Kg":
                        fetchCoaltesting.thirdparty_arb_gcv = str(param_value)
                    if param_name == "Third_Party_Report_No":
                        fetchCoaltesting.thirdparty_reference_no = str(param_value)
                    
                # for single_data in fetchCoaltesting.parameters:
                #     param_name = f"{single_data.get('parameter_Name')}_{single_data.get('unit_Val').replace(' ', '')}"
                #     if dataLoad.get("coal_data").get(param_name) is not None:
                #         single_data["val1"] = dataLoad.get("coal_data").get(param_name)
                    
                #     if single_data["parameter_Name"] == "Gross_Calorific_Value_(Adb)":
                #         single_data["Third_Party_Gross_Calorific_Value_(Adb)"] = dataLoad.get("coal_data").get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg")
                #         if dataLoad.get("coal_data").get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg"):
                #             single_data["Gcv_Difference"] = str(abs(float(single_data["val1"]) - float(dataLoad.get("coal_data").get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg"))))


                #             fetchCoalGrades = CoalGrades.objects()
                #             for single_coal_grades in fetchCoalGrades:
                #                 if (
                #                     single_coal_grades["start_value"]
                #                     <= dataLoad.get("coal_data").get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg")
                #                     <= single_coal_grades["end_value"]
                #                     and single_coal_grades["start_value"] != ""
                #                     and single_coal_grades["end_value"] != ""
                #                 ):
                #                     single_data["Third_Party_Grade"] = single_coal_grades["grade"]
                #                 elif dataLoad.get("coal_data").get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg") > "7001":
                #                     single_data["Third_Party_Grade"] = single_coal_grades["grade"]
                #                     break
                #             if single_data.get("grade"):
                #                 grade_diff = str(abs(int(single_coal_grades["grade"].replace('G-', '')) - int(single_data["grade"].replace('G-', ''))))
                #                 single_data["Grade_Diff"] = grade_diff

                #     else:
                #         single_data["thrdgcv"] = None
                #         single_data["gcv_difference"] = None
                #         single_data["thrd_grade"] = None
                #         single_data["grade_diff"] = None

            fetchCoaltesting.save()
        return {"detail": "success"}
    
    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


# @router.get("/coal_test_table", tags=["Coal Testing"])
# def coal_wcl_test_table(response:Response,currentPage: Optional[int] = None, perPage: Optional[int] = None,
#                     search_text: Optional[str] = None,
#                     start_timestamp: Optional[str] = None,
#                     end_timestamp: Optional[str] = None,
#                     month_date: Optional[str] = None,
#                     filter_type: Optional[str] = None, 
#                     type: Optional[str] = "display"):
#     try:
#         result = {        
#                 "labels": [],
#                 "datasets": [],
#                 "total" : 0,
#                 "page_size": 15
#         }
        
#         if type and type == "display":

#             data = Q()
#             page_no = 1
#             page_len = result["page_size"]

#             if currentPage:
#                 page_no = currentPage

#             if perPage:
#                 page_len = perPage
#                 result["page_size"] = perPage

#             offset = (page_no - 1) * page_len

            
#             if month_date:
#                 start_date = f'{month_date}-01'
#                 startd_date=datetime.datetime.strptime(start_date,"%Y-%m-%d")
#                 end_date = startd_date + relativedelta(day=31)
#                 data &= Q(receive_date__gte = startd_date)
#                 data &= Q(receive_date__lte = end_date)

#             if search_text:
#                 if search_text.isdigit():
#                     data &= (Q(rrNo__icontains=search_text))
#                 else:
#                     data &= Q(location__icontains=search_text) | Q(rake_no__icontains=search_text)

#             if start_timestamp:
#                 start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
#                 data &= Q(receive_date__gte = start_date)
#             # else:
#             #     start_timestamp = (datetime.datetime.now() - datetime.timedelta(days=31)).strftime("%Y-%m-%d")
#             #     data &= Q(receive_date__gte = convert_to_utc_format(start_timestamp,"%Y-%m-%d").strftime("%Y-%m-%d"))

#             if end_timestamp:
#                 end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")
#                 data &= Q(receive_date__lte = end_date)
#             # else:
#             #     end_timestamp = datetime.datetime.now().strftime("%Y-%m-%d")
#             #     data &= Q(receive_date__lte = convert_to_utc_format(end_timestamp,"%Y-%m-%d").strftime("%Y-%m-%d"))


#             logs = (
#                 CoalTesting.objects(data)
#                 .order_by("-ID")
#                 .skip(offset)
#                 .limit(page_len)                  
#             )        

#             if any(logs):
#                 for log in logs:
#                     # result["labels"] = list(log.payload().keys())
#                     result["labels"] = ["Sr.No","Mine","Lot_No","DO_No","DO_Qty", "Supplier", "Date", "Time","Id", "GWEL_Total_Moisture_%", 
#                                         "GWEL_Inherent_Moisture_(Adb)_%", "GWEL_Ash_(Adb)_%", "GWEL_Volatile_Matter_(Adb)_%", "GWEL_Gross_Calorific_Value_(Adb)_Kcal/Kg", 
#                                         "Ash_(Arb)_%", "GWEL_Volatile_Matter_(Arb)_%", "GWEL_Fixed_Carbon_(Arb)_%", "GWEL_Gross_Calorific_Value_(Arb)_Kcal/Kg",
#                                         "Third_Party_Total_Moisture_%", "Third_Party_Inherent_Moisture_(Adb)_%", "Third_Party_Ash_(Adb)_%",
#                                         "Third_Party_Volatile_Matter_(Adb)_%", "Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg",
#                                         "Third_Party_Ash_(Arb)_%", "Third_Party_Volatile_Matter_(Arb)_%",
#                                         "Third_Party_Fixed_Carbon_(Arb)_%", "Third_Party_Gross_Calorific_Value_(Arb)_Kcal/Kg", "Third_Party_Report_No"]
#                     result["datasets"].append(log.payload())
#             result["total"] = (len(CoalTesting.objects(data)))
#             return result

#         elif type and type == "download":
#             del type

#             file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
#             target_directory = f"static_server/gmr_ai/{file}"
#             os.umask(0)
#             os.makedirs(target_directory, exist_ok=True, mode=0o777)
            
#             data = Q()

#             if start_timestamp:
#                 start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
#                 data &= Q(receive_date__gte = start_date)
           
#             if end_timestamp:
#                 end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")
#                 data &= Q(receive_date__lte = end_date)
            
#             if search_text:
#                 if search_text.isdigit():
#                     data &= (Q(rrNo__icontains=search_text))
#                 else:
#                     data &= Q(location__icontains=search_text) | Q(rake_no__icontains=search_text)

#             usecase_data = CoalTesting.objects(data).order_by("-receive_date")
#             count = len(usecase_data)
#             path = None
#             if usecase_data:
#                 try:
#                     path = os.path.join(
#                         "static_server",
#                         "gmr_ai",
#                         file,

#                         "Roadwise_Coal_Lab_Test_Report_{}.xlsx".format(
#                             datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
#                         ),
#                     )
#                     filename = os.path.join(os.getcwd(), path)
#                     workbook = xlsxwriter.Workbook(filename)
#                     workbook.use_zip64()
#                     cell_format2 = workbook.add_format()
#                     cell_format2.set_bold()
#                     cell_format2.set_font_size(10)
#                     cell_format2.set_align("center")
#                     cell_format2.set_align("vjustify")

#                     worksheet = workbook.add_worksheet()
#                     worksheet.set_column("A:AZ", 20)
#                     worksheet.set_default_row(50)
#                     cell_format = workbook.add_format()
#                     cell_format.set_font_size(10)
#                     cell_format.set_align("center")
#                     cell_format.set_align("vcenter")

#                     if filter_type == "gwel":
#                         headers = ["Sr.No",
#                                    "Mine",
#                                    "Lot No",
#                                    "DO No",
#                                    "DO Qty", 
#                                    "Supplier",
#                                    "Date", 
#                                    "Time",
#                                    "GWEL Total Moisture%", 
#                                    "GWEL Inherent Moisture (Adb)%", 
#                                    "GWEL Ash (Adb)%", 
#                                    "GWEL Volatile Matter (Adb)%", 
#                                    "GWEL Gross Calorific Value (Adb) Kcal/Kg", 
#                                    "GWEL Ash (Arb)%", 
#                                    "GWEL Volatile Matter (Arb)%", 
#                                    "GWEL Fixed Carbon (Arb)%", 
#                                    "GWEL Gross Calorific Value (Arb) Kcal/Kg",
#                                    "GWEL Grade (Adb)",
#                                    ]
#                     elif filter_type == "third_party":
#                         headers = ["Sr.No",
#                                    "Mine",
#                                    "Lot No",
#                                    "DO No",
#                                    "DO Qty", 
#                                    "Supplier", 
#                                    "Date", 
#                                    "Time",
#                                    "Third Party Report No", 
#                                    "Third Party Total Moisture%", 
#                                    "Third Party Inherent Moisture (Adb)%", 
#                                    "Third Party Ash (Adb)%", 
#                                    "Third Party Volatile Matter (Adb)%", 
#                                    "Third Party Gross Calorific Value (Adb) Kcal/Kg",  
#                                    "Third Party Ash (Arb)%", 
#                                    "Third Party Volatile Matter (Arb)%", 
#                                    "Third Party Fixed Carbon (Arb)%", 
#                                    "Third Party Gross Calorific Value (Arb) Kcal/Kg", 
#                                    "Third Party Grade (Adb)",
#                                     ]
#                     elif filter_type == "all":
#                         headers = ["Sr.No",
#                                     "Mine",
#                                     "Lot No",
#                                     "DO No",
#                                     "DO Qty", 
#                                     "Supplier",
#                                     "Date", 
#                                     "Time",
#                                     "GWEL Total Moisture%", 
#                                     "GWEL Inherent Moisture (Adb)%", 
#                                     "GWEL Ash (Adb)%", 
#                                     "GWEL Volatile Matter (Adb)%", 
#                                     "GWEL Gross Calorific Value (Adb) Kcal/Kg", 
#                                     "GWEL Ash (Arb)%", 
#                                     "GWEL Volatile Matter (Arb)%", 
#                                     "GWEL Fixed Carbon (Arb)%", 
#                                     "GWEL Gross Calorific Value (Arb) Kcal/Kg",
#                                     "GWEL Grade (Adb)",
#                                     "Third Party Report No", 
#                                     "Third Party Total Moisture%", 
#                                     "Third Party Inherent Moisture (Adb)%", 
#                                     "Third Party Ash (Adb)%", 
#                                     "Third Party Volatile Matter (Adb)%", 
#                                     "Third Party Gross Calorific Value (Adb) Kcal/Kg",  
#                                     "Third Party Ash (Arb)%", 
#                                     "Third Party Volatile Matter (Arb)%", 
#                                     "Third Party Fixed Carbon (Arb)%", 
#                                     "Third Party Gross Calorific Value (Arb) Kcal/Kg",
#                                     "Third Party Grade (Adb)",
#                                     ]
#                     else:
#                         headers = ["Sr.No",
#                                 "Mine",
#                                 "Lot No",
#                                 "DO No",
#                                 "DO Qty", 
#                                 "Supplier", 
#                                 "Date", 
#                                 "Time"]

#                     for index, header in enumerate(headers):
#                         worksheet.write(0, index, header, cell_format2)

#                     fetchCoalGrades = CoalGrades.objects()

#                     for row, query in enumerate(usecase_data,start=1):
#                         result = query.payload()
#                         worksheet.write(row, 0, count, cell_format)
#                         if filter_type == "gwel":
#                             worksheet.write(row, 1, str(result["Mine"]), cell_format)
#                             worksheet.write(row, 2, str(result["Lot_No"]), cell_format)
#                             worksheet.write(row, 3, str(result["DO_No"]), cell_format)
#                             worksheet.write(row, 4, str(result["DO_Qty"]), cell_format)
#                             worksheet.write(row, 5, str(result["Supplier"]), cell_format)
#                             worksheet.write(row, 6, str(result["Date"]), cell_format)
#                             worksheet.write(row, 7, str(result["Time"]), cell_format)
#                             worksheet.write(row, 8, str(result["GWEL_Total_Moisture_%"]), cell_format)
#                             worksheet.write(row, 9, str(result["GWEL_Inherent_Moisture_(Adb)_%"]), cell_format)
#                             worksheet.write(row, 10, str(result["GWEL_Ash_(Adb)_%"]), cell_format)
#                             worksheet.write(row, 11, str(result["GWEL_Volatile_Matter_(Adb)_%"]), cell_format)
#                             worksheet.write(row, 12, str(result["GWEL_Gross_Calorific_Value_(Adb)_Kcal/Kg"]), cell_format)
#                             worksheet.write(row, 13, str(result["GWEL_Ash_(Arb)_%"]), cell_format)
#                             worksheet.write(row, 14, str(result["GWEL_Volatile_Matter_(Arb)_%"]), cell_format)
#                             worksheet.write(row, 15, str(result["GWEL_Fixed_Carbon_(Arb)_%"]), cell_format)
#                             worksheet.write(row, 16, str(result["GWEL_Gross_Calorific_Value_(Arb)_Kcal/Kg"]), cell_format)
#                             if result.get("GWEL_Gross_Calorific_Value_(Adb)_Kcal/Kg"):
#                                 for single_coal_grades in fetchCoalGrades:
#                                     if (
#                                         int(single_coal_grades["start_value"])
#                                         <= int(result.get("GWEL_Gross_Calorific_Value_(Adb)_Kcal/Kg"))
#                                         <= int(single_coal_grades["end_value"])
#                                         and single_coal_grades["start_value"] != ""
#                                         and single_coal_grades["end_value"] != ""
#                                     ):
#                                         worksheet.write(row, 17, str(single_coal_grades["grade"]), cell_format)
#                                     elif int(result.get("GWEL_Gross_Calorific_Value_(Adb)_Kcal/Kg")) > 7001:
#                                         worksheet.write(row, 17, "G-1", cell_format)

#                         elif filter_type == "third_party":
#                             worksheet.write(row, 1, str(result["Mine"]), cell_format)
#                             worksheet.write(row, 2, str(result["Lot_No"]), cell_format)
#                             worksheet.write(row, 3, str(result["DO_No"]), cell_format)
#                             worksheet.write(row, 4, str(result["DO_Qty"]), cell_format)
#                             worksheet.write(row, 5, str(result["Supplier"]), cell_format)
#                             worksheet.write(row, 6, str(result["Date"]), cell_format)
#                             worksheet.write(row, 7, str(result["Time"]), cell_format)
#                             if result.get("Third_Party_Report_No"):
#                                 worksheet.write(row, 8, str(result["Third_Party_Report_No"]), cell_format)
#                             if result.get("Third_Party_Total_Moisture_%"):
#                                 worksheet.write(row, 9, str(result["Third_Party_Total_Moisture_%"]), cell_format)
#                             if result.get("Third_Party_Inherent_Moisture_(Adb)_%"):
#                                 worksheet.write(row, 10, str(result["Third_Party_Inherent_Moisture_(Adb)_%"]), cell_format)
#                             if result.get("Third_Party_Ash_(Adb)_%"):
#                                 worksheet.write(row, 11, str(result["Third_Party_Ash_(Adb)_%"]), cell_format)
#                             if result.get("Third_Party_Volatile_Matter_(Adb)_%"):
#                                 worksheet.write(row, 12, str(result["Third_Party_Volatile_Matter_(Adb)_%"]), cell_format)
#                             if result.get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg"):
#                                 worksheet.write(row, 13, str(result["Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg"]), cell_format)
#                             if result.get("Third_Party_Ash_(Arb)_%"):
#                                 worksheet.write(row, 14, str(result["Third_Party_Ash_(Arb)_%"]), cell_format)
#                             if result.get("Third_Party_Volatile_Matter_(Arb)_%"):
#                                 worksheet.write(row, 15, str(result["Third_Party_Volatile_Matter_(Arb)_%"]), cell_format)
#                             if result.get("Third_Party_Fixed_Carbon_(Arb)_%"):
#                                 worksheet.write(row, 16, str(result["Third_Party_Fixed_Carbon_(Arb)_%"]), cell_format)
#                             if result.get("Third_Party_Gross_Calorific_Value_(Arb)_Kcal/Kg"):
#                                 worksheet.write(row, 17, str(result["Third_Party_Gross_Calorific_Value_(Arb)_Kcal/Kg"]), cell_format)
#                             if result.get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg"):
#                                 for single_coal_grades in fetchCoalGrades:
#                                     if (
#                                         int(single_coal_grades["start_value"])
#                                         <= float(result.get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg"))
#                                         <= int(single_coal_grades["end_value"])
#                                         and single_coal_grades["start_value"] != ""
#                                         and single_coal_grades["end_value"] != ""
#                                     ):
#                                         worksheet.write(row, 18, str(single_coal_grades["grade"]), cell_format)
#                                     elif float(result.get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg")) > 7001:
#                                         worksheet.write(row, 18, "G-1", cell_format)
#                         elif filter_type == "all":
#                             worksheet.write(row, 1, str(result["Mine"]), cell_format)
#                             worksheet.write(row, 2, str(result["Lot_No"]), cell_format)
#                             worksheet.write(row, 3, str(result["DO_No"]), cell_format)
#                             worksheet.write(row, 4, str(result["DO_Qty"]), cell_format)
#                             worksheet.write(row, 5, str(result["Supplier"]), cell_format)
#                             worksheet.write(row, 6, str(result["Date"]), cell_format)
#                             worksheet.write(row, 7, str(result["Time"]), cell_format)
#                             worksheet.write(row, 8, str(result["GWEL_Total_Moisture_%"]), cell_format)
#                             worksheet.write(row, 9, str(result["GWEL_Inherent_Moisture_(Adb)_%"]), cell_format)
#                             worksheet.write(row, 10, str(result["GWEL_Ash_(Adb)_%"]), cell_format)
#                             worksheet.write(row, 11, str(result["GWEL_Volatile_Matter_(Adb)_%"]), cell_format)
#                             worksheet.write(row, 12, str(result["GWEL_Gross_Calorific_Value_(Adb)_Kcal/Kg"]), cell_format)
#                             worksheet.write(row, 13, str(result["GWEL_Ash_(Arb)_%"]), cell_format)
#                             worksheet.write(row, 14, str(result["GWEL_Volatile_Matter_(Arb)_%"]), cell_format)
#                             worksheet.write(row, 15, str(result["GWEL_Fixed_Carbon_(Arb)_%"]), cell_format)
#                             worksheet.write(row, 16, str(result["GWEL_Gross_Calorific_Value_(Arb)_Kcal/Kg"]), cell_format)
#                             if result.get("GWEL_Gross_Calorific_Value_(Adb)_Kcal/Kg"):
#                                 for single_coal_grades in fetchCoalGrades:
#                                     if (
#                                         int(single_coal_grades["start_value"])
#                                         <= float(result.get("GWEL_Gross_Calorific_Value_(Adb)_Kcal/Kg"))
#                                         <= int(single_coal_grades["end_value"])
#                                         and single_coal_grades["start_value"] != ""
#                                         and single_coal_grades["end_value"] != ""
#                                     ):
#                                         worksheet.write(row, 17, str(single_coal_grades["grade"]), cell_format)
#                                     elif float(result.get("GWEL_Gross_Calorific_Value_(Adb)_Kcal/Kg")) > 7001:
#                                         worksheet.write(row, 17, "G-1", cell_format)
#                             if result.get("Third_Party_Report_No"):
#                                 worksheet.write(row, 17, str(result["Third_Party_Report_No"]), cell_format)
#                             if result.get("Third_Party_Total_Moisture_%"):
#                                 worksheet.write(row, 18, str(result["Third_Party_Total_Moisture_%"]), cell_format)
#                             if result.get("Third_Party_Inherent_Moisture_(Adb)_%"):
#                                 worksheet.write(row, 19, str(result["Third_Party_Inherent_Moisture_(Adb)_%"]), cell_format)
#                             if result.get("Third_Party_Ash_(Adb)_%"):
#                                 worksheet.write(row, 20, str(result["Third_Party_Ash_(Adb)_%"]), cell_format)
#                             if result.get("Third_Party_Volatile_Matter_(Adb)_%"):
#                                 worksheet.write(row, 21, str(result["Third_Party_Volatile_Matter_(Adb)_%"]), cell_format)
#                             if result.get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg"):
#                                 worksheet.write(row, 22, str(result["Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg"]), cell_format)
#                             if result.get("Third_Party_Ash_(Arb)_%"):
#                                 worksheet.write(row, 23, str(result["Third_Party_Ash_(Arb)_%"]), cell_format)
#                             if result.get("Third_Party_Volatile_Matter_(Arb)_%"):
#                                 worksheet.write(row, 24, str(result["Third_Party_Volatile_Matter_(Arb)_%"]), cell_format)
#                             if result.get("Third_Party_Fixed_Carbon_(Arb)_%"):
#                                 worksheet.write(row, 25, str(result["Third_Party_Fixed_Carbon_(Arb)_%"]), cell_format)
#                             if result.get("Third_Party_Gross_Calorific_Value_(Arb)_Kcal/Kg"):
#                                 worksheet.write(row, 26, str(result["Third_Party_Gross_Calorific_Value_(Arb)_Kcal/Kg"]), cell_format)
#                             if result.get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg"):
#                                 for single_coal_grades in fetchCoalGrades:
#                                     if (
#                                         int(single_coal_grades["start_value"])
#                                         <= float(result.get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg"))
#                                         <= int(single_coal_grades["end_value"])
#                                         and single_coal_grades["start_value"] != ""
#                                         and single_coal_grades["end_value"] != ""
#                                     ):
#                                         worksheet.write(row, 27, str(single_coal_grades["grade"]), cell_format)
#                                     elif float(result.get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg")) > 7001:
#                                         worksheet.write(row, 27, "G-1", cell_format)
#                         else:
#                             worksheet.write(row, 1, str(result["Mine"]), cell_format)
#                             worksheet.write(row, 2, str(result["Lot_No"]), cell_format)
#                             worksheet.write(row, 3, str(result["DO_No"]), cell_format)
#                             worksheet.write(row, 4, str(result["DO_Qty"]), cell_format)
#                             worksheet.write(row, 5, str(result["Supplier"]), cell_format)
#                             worksheet.write(row, 6, str(result["Date"]), cell_format)
#                             worksheet.write(row, 7, str(result["Time"]), cell_format)
#                         count -= 1
#                     workbook.close()

#                     console_logger.debug("Successfully {} report generated".format(service_id))
#                     console_logger.debug("sent data {}".format(path))
#                     return {
#                             "Type": "coal_test_download_event",
#                             "Datatype": "Report",
#                             "File_Path": path,
#                         }
#                 except Exception as e:
#                     console_logger.debug(e)
#                     exc_type, exc_obj, exc_tb = sys.exc_info()
#                     fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#                     console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#                     console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#             else:
#                 return {
#                             "Type": "coal_test_download_event",
#                             "Datatype": "Report",
#                             "File_Path": path,
#                         }

#     except Exception as e:
#         response.status_code = 400
#         console_logger.debug(e)
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#         console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#         return e


# new table as per db
@router.get("/coal_test_table", tags=["Coal Testing"])
def coal_wcl_test_table(response:Response,currentPage: Optional[int] = None, perPage: Optional[int] = None,
                    search_text: Optional[str] = None,
                    start_timestamp: Optional[str] = None,
                    end_timestamp: Optional[str] = None,
                    month_date: Optional[str] = None,
                    filter_type: Optional[str] = None, 
                    type: Optional[str] = "display"):
    try:
        # pdf name starts with mahabal or REPORT-GMR-CO
        result = {        
                "labels": [],
                "datasets": [],
                "total" : 0,
                "page_size": 15
        }
        
        if type and type == "display":

            data = Q()
            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage

            offset = (page_no - 1) * page_len

            
            if month_date:
                start_date = f'{month_date}-01'
                startd_date=datetime.datetime.strptime(start_date,"%Y-%m-%d")
                end_date = startd_date + relativedelta(day=31)
                data &= Q(plant_analysis_date__gte = startd_date)
                data &= Q(plant_analysis_date__lte = end_date)

            if search_text:
                if search_text.isdigit():
                    data &= (Q(sample_id__icontains=search_text))
                else:
                    data &= Q(mine__icontains=search_text) | Q(mode__icontains=search_text)

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(plant_analysis_date__gte = start_date)
            # else:
            #     start_timestamp = (datetime.datetime.now() - datetime.timedelta(days=31)).strftime("%Y-%m-%d")
            #     data &= Q(plant_analysis_date__gte = convert_to_utc_format(start_timestamp,"%Y-%m-%d").strftime("%Y-%m-%d"))

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(plant_analysis_date__lte = end_date)
            # else:
            #     end_timestamp = datetime.datetime.now().strftime("%Y-%m-%d")
            #     data &= Q(receive_date__lte = convert_to_utc_format(end_timestamp,"%Y-%m-%d").strftime("%Y-%m-%d"))


            logs = (
                RecieptCoalQualityAnalysis.objects(data, mode="Road")
                .order_by("-plant_analysis_date")
                .skip(offset)
                .limit(page_len)                  
            )        

            if any(logs):
                for log in logs:
                    result["labels"] = list(log.payload().keys())
                    # result["labels"] = ["Sr.No","Mine","Lot_No","DO_No","DO_Qty", "Supplier", "Date", "Time","Id", "GWEL_Total_Moisture_%", 
                                        # "GWEL_Inherent_Moisture_(Adb)_%", "GWEL_Ash_(Adb)_%", "GWEL_Volatile_Matter_(Adb)_%", "GWEL_Gross_Calorific_Value_(Adb)_Kcal/Kg", 
                                        # "Ash_(Arb)_%", "GWEL_Volatile_Matter_(Arb)_%", "GWEL_Fixed_Carbon_(Arb)_%", "GWEL_Gross_Calorific_Value_(Arb)_Kcal/Kg",
                                        # "Third_Party_Total_Moisture_%", "Third_Party_Inherent_Moisture_(Adb)_%", "Third_Party_Ash_(Adb)_%",
                                        # "Third_Party_Volatile_Matter_(Adb)_%", "Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg",
                                        # "Third_Party_Ash_(Arb)_%", "Third_Party_Volatile_Matter_(Arb)_%",
                                        # "Third_Party_Fixed_Carbon_(Arb)_%", "Third_Party_Gross_Calorific_Value_(Arb)_Kcal/Kg", "Third_Party_Report_No"]
                    result["datasets"].append(log.payload())
            result["total"] = (len(RecieptCoalQualityAnalysis.objects(data, mode="Road")))
            return result

        elif type and type == "download":
            del type

            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)
            
            data = Q()

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(plant_analysis_date__gte = start_date)
           
            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(plant_analysis_date__lte = end_date)
            
            if search_text:
                if search_text.isdigit():
                    data &= (Q(rrNo__icontains=search_text))
                else:
                    data &= Q(location__icontains=search_text) | Q(rake_no__icontains=search_text)

            usecase_data = RecieptCoalQualityAnalysis.objects(data, mode="Road").order_by("-plant_analysis_date")
            count = len(usecase_data)
            path = None
            logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
            if usecase_data:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,

                        "Roadwise_Coal_Lab_Test_Report_{}.xlsx".format(
                            datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format()
                    cell_format2.set_bold()
                    cell_format2.set_font_size(10)
                    cell_format2.set_align("center")
                    cell_format2.set_align("vcenter")
                    cell_format2.set_text_wrap(True)
                    cell_format2.set_border(1)

                    header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
                    date_format = workbook.add_format({'align': 'center', 'font_size': 12, "bold": True})
                    report_name_format = workbook.add_format({'align': 'center', 'font_size': 15, "bold": True})

                    header_format.set_align("vcenter")
                    date_format.set_align("vcenter")
                    report_name_format.set_align("vcenter")
                    header_format.set_border(1)
                    date_format.set_border(1)
                    report_name_format.set_border(1)
                    
                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)
                    cell_format = workbook.add_format()
                    cell_format.set_font_size(10)
                    cell_format.set_align("center")
                    cell_format.set_align("vcenter")
                    cell_format.set_text_wrap(True)
                    cell_format.set_border(1)

                    worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                    
                    # Merge cells for the main header and place it in the center
                    main_header = "GMR Warora Energy Limited"  # Set your main header text here
                    
                    # Write the current date on the left side (A2)
                    worksheet.write("A2", f"Date: {end_date.strftime('%d-%m-%Y')}", date_format)


                    if filter_type == "gwel":
                        worksheet.merge_range("A1:Y1", main_header, header_format)  # Merge cells A1 to H1 for the header
                        worksheet.merge_range("C2:Y2", f"GWEL Receipt Quality Analysis (Road)", report_name_format)
                        headers = [
                            "Sr.No.",
                            "Plant Certificate ID",
                            "Sample No",
                            "Do No",
                            "Plant Sample Date",
                            "Plant Preparation Date",
                            "Plant Analysis Date",
                            "Sample Qty",
                            "Mine",
                            "Mine Grade",
                            "Mode",
                            "GWEL LAB TEMP",
                            "GWEL LAB RH",
                            "GWEL ARB TM",
                            "GWEL ARB VM",
                            "GWEL ARB ASH",
                            "GWEL ARB FC",
                            "GWEL ARB GCV",
                            "GWEL ADB IM",
                            "GWEL ADB VM",
                            "GWEL ADB ASH",
                            "GWEL ADB FC",
                            "GWEL ADB GCV",
                            "GWEL ULR ID",
                            "GWEL GRADE"
                        ]
                    elif filter_type == "third_party":
                        worksheet.merge_range("A1:Y1", main_header, header_format)  # Merge cells A1 to H1 for the header
                        worksheet.merge_range("C2:Y2", f"ThirdParty Receipt Quality Analysis (Road)", report_name_format)
                        headers = [
                            "Sr.No.",
                            "Plant Certificate ID",
                            "Sample No",
                            "Do No",
                            "Plant Sample Date",
                            "Plant Preparation Date",
                            "Plant Analysis Date",
                            "Sample Qty",
                            "Mine",
                            "Mine Grade",
                            "Mode",
                            "THIRDPARTY Report Date",
                            "THIRDPARTY Reference No",
                            "THIRDPARTY Sample Date",
                            "THIRDPARTY ARB TM",
                            "THIRDPARTY ARB VM",
                            "THIRDPARTY ARB ASH",
                            "THIRDPARTY ARB FC",
                            "THIRDPARTY ARB GCV",
                            "THIRDPARTY ADB IM",
                            "THIRDPARTY ADB VM",
                            "THIRDPARTY ADB ASH",
                            "THIRDPARTY ADB FC",
                            "THIRDPARTY ADB GCV",
                            "THIRDPARTY GRADE"
                        ]
                    elif filter_type == "all":
                        worksheet.merge_range("A1:AM1", main_header, header_format)  # Merge cells A1 to H1 for the header
                        worksheet.merge_range("C2:AM2", f"Receipt Quality Analysis (Road)", report_name_format)
                        headers = [
                            "Sr.No.",
                            "Plant Certificate Id",
                            "Sample No",
                            "Do No",
                            "Plant Sample Date",
                            "Plant Preparation Date",
                            "Plant Analysis Date",
                            "Sample Qty",
                            "Mine",
                            "Mine Grade",
                            "Mode",
                            "GWEL Lab Temp",
                            "GWEL Lab RH",
                            "GWEL ARB TM",
                            "GWEL ARB VM",
                            "GWEL ARB ASH",
                            "GWEL ARB FC",
                            "GWEL ARB GCV",
                            "GWEL ADB IM",
                            "GWEL ADB VM",
                            "GWEL ADB ASH",
                            "GWEL ADB FC",
                            "GWEL ADB GCV",
                            "GWEL ULR ID",
                            "GWEL GRADE",
                            "THIRDPARTY REPORT DATE",
                            "THIRDPARTY REFERENCE NO",
                            "THIRDPARTY SAMPLE DATE",
                            "THIRDPARTY ARB TM",
                            "THIRDPARTY ARB VM",
                            "THIRDPARTY ARB ASH",
                            "THIRDPARTY ARB FC",
                            "THIRDPARTY ARB GCV",
                            "THIRDPARTY ADB IM",
                            "THIRDPARTY ADB VM",
                            "THIRDPARTY ADB Ash",
                            "THIRDPARTY ADB FC",
                            "THIRDPARTY ADB GCV",
                            "THIRRDPARTY GRADE",
                        ]
                    else:
                        worksheet.merge_range("A1:AM1", main_header, header_format)  # Merge cells A1 to H1 for the header
                        worksheet.merge_range("C2:AM2", f"Receipt Quality Analysis (Road)", report_name_format)
                        headers = [
                            "Sr.No.",
                            "Plant Certificate Id",
                            "Sample No",
                            "Do No",
                            "Plant Sample Date",
                            "Plant Preparation Date",
                            "Plant Analysis Date",
                            "Sample Qty",
                            "Mine",
                            "Mine Grade",
                            "Mode"]

                    for index, header in enumerate(headers):
                        worksheet.write(2, index, header, cell_format2)

                    fetchCoalGrades = CoalGrades.objects()

                    for row, query in enumerate(usecase_data,start=3):
                        result = query.payload()
                        worksheet.write(row, 0, count, cell_format)
                        if filter_type == "gwel":
                            worksheet.write(row, 1, str(result["plant_certificate_id"]), cell_format)
                            worksheet.write(row, 2, str(result["sample_no"]), cell_format)
                            worksheet.write(row, 3, str(result["do_no"]), cell_format)
                            worksheet.write(row, 4, str(result["GWEL_sample_date"]), cell_format)
                            worksheet.write(row, 5, str(result["GWEL_preparation_date"]), cell_format)
                            worksheet.write(row, 6, str(result["GWEL_analysis_date"]), cell_format)
                            worksheet.write(row, 7, float(result["sample_qty"]) if result.get("sample_qty") else 0, cell_format)
                            worksheet.write(row, 8, str(result["mine"]), cell_format)
                            worksheet.write(row, 9, str(result["mine_grade"]), cell_format)
                            worksheet.write(row, 10, str(result["mode"]), cell_format)
                            worksheet.write(row, 11, float(result["GWEL_LAB_TEMP"]) if result.get("GWEL_LAB_TEMP") else 0, cell_format)
                            worksheet.write(row, 12, float(result["GWEL_LAB_RH"]) if result.get("GWEL_LAB_RH") else 0, cell_format)
                            worksheet.write(row, 13, float(result["GWEL_ARB_TM"]) if result.get("GWEL_ARB_TM") else 0, cell_format)
                            worksheet.write(row, 14, float(result["GWEL_ARB_VM"]) if result.get("GWEL_ARB_VM") else 0, cell_format)
                            worksheet.write(row, 15, float(result["GWEL_ARB_ASH"]) if result.get("GWEL_ARB_ASH") else 0, cell_format)
                            worksheet.write(row, 16, float(result["GWEL_ARB_FC"]) if result.get("GWEL_ARB_FC") else 0, cell_format)
                            worksheet.write(row, 17, float(result["GWEL_ARB_GCV"]) if result.get("GWEL_ARB_GCV") else 0, cell_format)
                            worksheet.write(row, 18, float(result["GWEL_ADB_IM"]) if result.get("GWEL_ADB_IM") else 0, cell_format)
                            worksheet.write(row, 19, float(result["GWEL_ADB_VM"]) if result.get("GWEL_ADB_VM") else 0, cell_format)
                            worksheet.write(row, 20, float(result["GWEL_ADB_ASH"]) if result.get("GWEL_ADB_ASH") else 0, cell_format)
                            worksheet.write(row, 21, float(result["GWEL_ADB_FC"]) if result.get("GWEL_ADB_FC") else 0, cell_format)
                            worksheet.write(row, 22, float(result["GWEL_ADB_GCV"]) if result.get("GWEL_ADB_GCV") else 0, cell_format)
                            worksheet.write(row, 23, str(result["GWEL_ULR_ID"]), cell_format)
                            if result.get("GWEL_ADB_GCV"):
                                for single_coal_grades in fetchCoalGrades:
                                    if (
                                        int(single_coal_grades["start_value"])
                                        <= int(result.get("GWEL_ADB_GCV"))
                                        <= int(single_coal_grades["end_value"])
                                        and single_coal_grades["start_value"] != ""
                                        and single_coal_grades["end_value"] != ""
                                    ):
                                        worksheet.write(row, 24, str(single_coal_grades["grade"]), cell_format)
                                    elif int(result.get("GWEL_ADB_GCV")) > 7001:
                                        worksheet.write(row, 24, "G-1", cell_format)

                        elif filter_type == "third_party":
                            worksheet.write(row, 1, str(result["plant_certificate_id"]), cell_format)
                            worksheet.write(row, 2, str(result["sample_no"]), cell_format)
                            worksheet.write(row, 3, str(result["do_no"]), cell_format)
                            worksheet.write(row, 4, str(result["GWEL_sample_date"]), cell_format)
                            worksheet.write(row, 5, str(result["GWEL_preparation_date"]), cell_format)
                            worksheet.write(row, 6, str(result["GWEL_analysis_date"]), cell_format)
                            worksheet.write(row, 7, float(result["sample_qty"]) if result.get("sample_qty") else 0, cell_format)
                            worksheet.write(row, 8, str(result["mine"]), cell_format)
                            worksheet.write(row, 9, str(result["mine_grade"]), cell_format)
                            worksheet.write(row, 10, str(result["mode"]), cell_format)
                            if result.get("THIRDPARTY_REPORT_DATE"):
                                worksheet.write(row, 11, str(result["THIRDPARTY_REPORT_DATE"]), cell_format)
                            if result.get("THIRDPARTY_REFERENCE_NO"):
                                worksheet.write(row, 12, str(result["THIRDPARTY_REFERENCE_NO"]), cell_format)
                            if result.get("THIRDPARTY_SAMPLE_DATE"):
                                worksheet.write(row, 13, str(result["THIRDPARTY_SAMPLE_DATE"]), cell_format)
                            if result.get("THIRDPARTY_ARB_TM"):
                                worksheet.write(row, 14, float(result["THIRDPARTY_ARB_TM"]), cell_format)
                            if result.get("THIRDPARTY_ARB_VM"):
                                worksheet.write(row, 15, float(result["THIRDPARTY_ARB_VM"]), cell_format)
                            if result.get("THIRDPARTY_ARB_ASH"):
                                worksheet.write(row, 16, float(result["THIRDPARTY_ARB_ASH"]), cell_format)
                            if result.get("THIRDPARTY_ARB_FC"):
                                worksheet.write(row, 17, float(result["THIRDPARTY_ARB_FC"]), cell_format)
                            if result.get("THIRDPARTY_ARB_GCV"):
                                worksheet.write(row, 18, float(result["THIRDPARTY_ARB_GCV"]), cell_format)
                            if result.get("THIRDPARTY_ADB_IM"):
                                worksheet.write(row, 19, float(result["THIRDPARTY_ADB_IM"]), cell_format)
                            if result.get("THIRDPARTY_ADB_VM"):
                                worksheet.write(row, 20, float(result["THIRDPARTY_ADB_VM"]), cell_format)
                            if result.get("THIRDPARTY_ADB_ASH"):
                                worksheet.write(row, 21, float(result["THIRDPARTY_ADB_ASH"]), cell_format)
                            if result.get("THIRDPARTY_ADB_FC"):
                                worksheet.write(row, 22, float(result["THIRDPARTY_ADB_FC"]), cell_format)
                            if result.get("THIRDPARTY_ADB_GCV"):
                                worksheet.write(row, 23, float(result["THIRDPARTY_ADB_GCV"]), cell_format)
                            if result.get("THIRDPARTY_ADB_GCV"):
                                for single_coal_grades in fetchCoalGrades:
                                    if (
                                        int(single_coal_grades["start_value"])
                                        <= float(result.get("THIRDPARTY_ADB_GCV"))
                                        <= int(single_coal_grades["end_value"])
                                        and single_coal_grades["start_value"] != ""
                                        and single_coal_grades["end_value"] != ""
                                    ):
                                        worksheet.write(row, 24, str(single_coal_grades["grade"]), cell_format)
                                    elif float(result.get("THIRDPARTY_ADB_GCV")) > 7001:
                                        worksheet.write(row, 24, "G-1", cell_format)
                        elif filter_type == "all":
                            worksheet.write(row, 1, str(result["plant_certificate_id"]), cell_format)
                            worksheet.write(row, 2, str(result["sample_no"]), cell_format)
                            worksheet.write(row, 3, str(result["do_no"]), cell_format)
                            worksheet.write(row, 4, str(result["GWEL_sample_date"]), cell_format)
                            worksheet.write(row, 5, str(result["GWEL_preparation_date"]), cell_format)
                            worksheet.write(row, 6, str(result["GWEL_analysis_date"]), cell_format)
                            worksheet.write(row, 7, float(result["sample_qty"]) if result.get("sample_qty") else 0, cell_format)
                            worksheet.write(row, 8, str(result["mine"]), cell_format)
                            worksheet.write(row, 9, str(result["mine_grade"]), cell_format)
                            worksheet.write(row, 10, str(result["mode"]), cell_format)
                            worksheet.write(row, 11, float(result["GWEL_LAB_TEMP"]) if result.get("GWEL_LAB_TEMP") else 0, cell_format)
                            worksheet.write(row, 12, float(result["GWEL_LAB_RH"]) if result.get("GWEL_LAB_RH") else 0, cell_format)
                            worksheet.write(row, 13, float(result["GWEL_ARB_TM"]) if result.get("GWEL_ARB_TM") else 0, cell_format)
                            worksheet.write(row, 14, float(result["GWEL_ARB_VM"]) if result.get("GWEL_ARB_VM") else 0, cell_format)
                            worksheet.write(row, 15, float(result["GWEL_ARB_ASH"]) if result.get("GWEL_ARB_ASH") else 0, cell_format)
                            worksheet.write(row, 16, float(result["GWEL_ARB_FC"]) if result.get("GWEL_ARB_FC") else 0, cell_format)
                            worksheet.write(row, 17, float(result["GWEL_ARB_GCV"]) if result.get("GWEL_ARB_GCV") else 0, cell_format)
                            worksheet.write(row, 18, float(result["GWEL_ADB_IM"]) if result.get("GWEL_ADB_IM") else 0, cell_format)
                            worksheet.write(row, 19, float(result["GWEL_ADB_VM"]) if result.get("GWEL_ADB_VM") else 0, cell_format)
                            worksheet.write(row, 20, float(result["GWEL_ADB_ASH"]) if result.get("GWEL_ADB_ASH") else 0, cell_format)
                            worksheet.write(row, 21, float(result["GWEL_ADB_FC"]) if result.get("GWEL_ADB_FC") else 0, cell_format)
                            worksheet.write(row, 22, float(result["GWEL_ADB_GCV"]) if result.get("GWEL_ADB_GCV") else 0, cell_format)
                            worksheet.write(row, 23, str(result["GWEL_ULR_ID"]), cell_format)
                            if result.get("GWEL_ADB_GCV"):
                                for single_coal_grades in fetchCoalGrades:
                                    if (
                                        int(single_coal_grades["start_value"])
                                        <= float(result.get("GWEL_ADB_GCV"))
                                        <= int(single_coal_grades["end_value"])
                                        and single_coal_grades["start_value"] != ""
                                        and single_coal_grades["end_value"] != ""
                                    ):
                                        worksheet.write(row, 24, str(single_coal_grades["grade"]), cell_format)
                                    elif float(result.get("GWEL_ADB_GCV")) > 7001:
                                        worksheet.write(row, 24, "G-1", cell_format)
                            if result.get("THIRDPARTY_REPORT_DATE"):
                                worksheet.write(row, 25, str(result["THIRDPARTY_REPORT_DATE"]), cell_format)
                            if result.get("THIRDPARTY_REFERENCE_NO"):
                                worksheet.write(row, 26, str(result["THIRDPARTY_REFERENCE_NO"]), cell_format)
                            if result.get("THIRDPARTY_SAMPLE_DATE"):
                                worksheet.write(row, 27, str(result["THIRDPARTY_SAMPLE_DATE"]), cell_format)
                            if result.get("THIRDPARTY_ARB_TM"):
                                worksheet.write(row, 28, float(result["THIRDPARTY_ARB_TM"]), cell_format)
                            if result.get("THIRDPARTY_ARB_VM"):
                                worksheet.write(row, 29, float(result["THIRDPARTY_ARB_VM"]), cell_format)
                            if result.get("THIRDPARTY_ARB_ASH"):
                                worksheet.write(row, 30, float(result["THIRDPARTY_ARB_ASH"]), cell_format)
                            if result.get("THIRDPARTY_ARB_FC"):
                                worksheet.write(row, 31, float(result["THIRDPARTY_ARB_FC"]), cell_format)
                            if result.get("THIRDPARTY_ARB_GCV"):
                                worksheet.write(row, 32, float(result["THIRDPARTY_ARB_GCV"]), cell_format)
                            if result.get("THIRDPARTY_ADB_IM"):
                                worksheet.write(row, 33, float(result["THIRDPARTY_ADB_IM"]), cell_format)
                            if result.get("THIRDPARTY_ADB_VM"):
                                worksheet.write(row, 34, float(result["THIRDPARTY_ADB_VM"]), cell_format)
                            if result.get("THIRDPARTY_ADB_ASH"):
                                worksheet.write(row, 35, float(result["THIRDPARTY_ADB_ASH"]), cell_format)
                            if result.get("THIRDPARTY_ADB_FC"):
                                worksheet.write(row, 36, float(result["THIRDPARTY_ADB_FC"]), cell_format)
                            if result.get("THIRDPARTY_ADB_GCV"):
                                worksheet.write(row, 37, float(result["THIRDPARTY_ADB_GCV"]), cell_format)
                            if result.get("THIRDPARTY_ADB_GCV"):
                                for single_coal_grades in fetchCoalGrades:
                                    if (
                                        int(single_coal_grades["start_value"])
                                        <= float(result.get("THIRDPARTY_ADB_GCV"))
                                        <= int(single_coal_grades["end_value"])
                                        and single_coal_grades["start_value"] != ""
                                        and single_coal_grades["end_value"] != ""
                                    ):
                                        worksheet.write(row, 38, str(single_coal_grades["grade"]), cell_format)
                                    elif float(result.get("THIRDPARTY_ADB_GCV")) > 7001:
                                        worksheet.write(row, 38, "G-1", cell_format)
                        else:
                            worksheet.write(row, 1, str(result["plant_certificate_id"]), cell_format)
                            worksheet.write(row, 2, str(result["sample_no"]), cell_format)
                            worksheet.write(row, 3, str(result["do_no"]), cell_format)
                            worksheet.write(row, 4, str(result["GWEL_sample_date"]), cell_format)
                            worksheet.write(row, 5, str(result["GWEL_preparation_date"]), cell_format)
                            worksheet.write(row, 6, str(result["GWEL_analysis_date"]), cell_format)
                            worksheet.write(row, 7, float(result["sample_qty"]) if result.get("sample_qty") else 0, cell_format)
                            worksheet.write(row, 8, str(result["mine"]), cell_format)
                            worksheet.write(row, 9, str(result["mine_grade"]), cell_format)
                            worksheet.write(row, 10, str(result["mode"]), cell_format)
                        count -= 1
                    workbook.close()

                    console_logger.debug("Successfully {} report generated".format(service_id))
                    console_logger.debug("sent data {}".format(path))
                    return {
                            "Type": "coal_test_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                        }
                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            else:
                return {
                            "Type": "coal_test_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                        }

    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


# @router.get("/coal_train_test_table", tags=["Coal Testing"])
# def coal_secl_test_table(response:Response,currentPage: Optional[int] = None, perPage: Optional[int] = None,
#                     search_text: Optional[str] = None,
#                     start_timestamp: Optional[str] = None,
#                     end_timestamp: Optional[str] = None,
#                     month_date: Optional[str] = None,
#                     filter_type: Optional[str] = None,
#                     type: Optional[str] = "display"):
#     try:
#         result = {        
#                 "labels": [],
#                 "datasets": [],
#                 "total" : 0,
#                 "page_size": 15
#         }
        
#         if type and type == "display":

#             data = Q()
#             page_no = 1
#             page_len = result["page_size"]

#             if currentPage:
#                 page_no = currentPage

#             if perPage:
#                 page_len = perPage
#                 result["page_size"] = perPage
            
#             offset = (page_no - 1) * page_len

#             if month_date:
#                 start_date = f'{month_date}-01'
#                 startd_date=datetime.datetime.strptime(start_date,"%Y-%m-%d")
#                 end_date = startd_date + relativedelta( day=31)
#                 data &= Q(receive_date__gte = startd_date)
#                 data &= Q(receive_date__lte = end_date)

#             if search_text:
#                 if search_text.isdigit():
#                     data &= (Q(rrNo__icontains=search_text))
#                 else:
#                     data &= Q(location__icontains=search_text) | Q(rake_no__icontains=search_text)

#             if start_timestamp:
#                 start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
#                 data &= Q(receive_date__gte = start_date)
            

#             if end_timestamp:
#                 end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")
#                 data &= Q(receive_date__lte = end_date)


            
#             logs = (
#                 CoalTestingTrain.objects(data)
#                 .order_by("-ID")
#                 .skip(offset)
#                 .limit(page_len)                  
#             )        

#             if any(logs):
#                 for log in logs:
#                     # result["labels"] = list(log.payload().keys())
#                     result["labels"] = [
#                     "Sr.No", 
#                     "Mine", 
#                     "Lot_No", 
#                     "RR_No", 
#                     "RR_Qty", 
#                     "Supplier", 
#                     "Date", 
#                     "Time", 
#                     "Id", 
#                     "GWEL_Total_Moisture_%", 
#                     "GWEL_Inherent_Moisture_(Adb)_%", 
#                     "GWEL_Ash_(Adb)_%", 
#                     "GWEL_Volatile_Matter_(Adb)_%", 
#                     "GWEL_Gross_Calorific_Value_(Adb)_Kcal/Kg", 
#                     "GWEL_Ash_(Arb)_%", 
#                     "GWEL_Volatile_Matter_(Arb)_%", 
#                     "GWEL_Fixed_Carbon_(Arb)_%", 
#                     "GWEL_Gross_Calorific_Value_(Arb)_Kcal/Kg", 
#                     "Third_Party_Report_No", 
#                     "Third_Party_Total_Moisture_%", 
#                     "Third_Party_Inherent_Moisture_(Adb)_%", 
#                     "Third_Party_Ash_(Adb)_%", 
#                     "Third_Party_Volatile_Matter_(Adb)_%", 
#                     "Third_Party_Ash_(Arb)_%", 
#                     "Third_Party_Volatile_Matter_(Arb)_%",
#                     "Third_Party_Fixed_Carbon_(Arb)_%",
#                     "Third_Party_Gross_Calorific_Value_(Arb)_Kcal/Kg"]
#                     result["datasets"].append(log.payload())

#             result["total"] = (len(CoalTestingTrain.objects(data)))
#             # console_logger.debug(f"-------- Rail Coal Testing Response -------- {result}")
#             return result

#         elif type and type == "download":
#             del type

#             file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
#             target_directory = f"static_server/gmr_ai/{file}"
#             os.umask(0)
#             os.makedirs(target_directory, exist_ok=True, mode=0o777)

#             data = Q()

#             if start_timestamp:
#                 start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
#                 data &= Q(receive_date__gte = start_date)

#             if end_timestamp:
#                 end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")
#                 data &= Q(receive_date__lte = end_date)

#             if search_text:
#                 if search_text.isdigit():
#                     data &= (Q(rrNo__icontains=search_text))
#                 else:
#                     data &= Q(location__icontains=search_text) | Q(rake_no__icontains=search_text)

#             usecase_data = CoalTestingTrain.objects(data).order_by("-receive_date")
#             count = len(usecase_data)
#             path = None
#             if usecase_data:
#                 try:
#                     path = os.path.join(
#                         "static_server",
#                         "gmr_ai",
#                         file,
#                         "Railwise_Coal_Lab_Test_Report_{}.xlsx".format(
#                             datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
#                         ),
#                     )
#                     filename = os.path.join(os.getcwd(), path)
#                     workbook = xlsxwriter.Workbook(filename)
#                     workbook.use_zip64()
#                     cell_format2 = workbook.add_format()
#                     cell_format2.set_bold()
#                     cell_format2.set_font_size(10)
#                     cell_format2.set_align("center")
#                     cell_format2.set_align("vjustify")

#                     worksheet = workbook.add_worksheet()
#                     worksheet.set_column("A:AZ", 20)
#                     worksheet.set_default_row(50)
#                     cell_format = workbook.add_format()
#                     cell_format.set_font_size(10)
#                     cell_format.set_align("center")
#                     cell_format.set_align("vcenter")

#                     if filter_type == "gwel":
#                         headers = ["Sr.No",
#                                 "Mine",
#                                 "Lot No", 
#                                 "RR No", 
#                                 "RR Qty", 
#                                 "Supplier", 
#                                 "Date", 
#                                 "Time",
#                                 "GWEL Total Moisture%", 
#                                 "GWEL Inherent Moisture (Adb)%", 
#                                 "GWEL Ash (Adb)%", 
#                                 "GWEL Volatile Matter (Adb)%", 
#                                 "GWEL Gross Calorific Value (Adb) Kcal/Kg", 
#                                 "GWEL Ash (Arb)%", 
#                                 "GWEL Volatile Matter (Arb)%", 
#                                 "GWEL Fixed Carbon (Arb)%", 
#                                 "GWEL Gross Calorific Value (Arb) Kcal/Kg",
#                                 "GWEL Grade (Adb)",
#                                 ]
#                     elif filter_type == "third_party":
#                         headers = [
#                                "Sr.No",
#                                "Mine",
#                                "Lot No", 
#                                "RR No", 
#                                "RR Qty", 
#                                "Supplier",
#                                "Date", 
#                                "Time",
#                                "Third Party Report No", 
#                                "Third Party Total Moisture%", 
#                                "Third Party Inherent Moisture (Adb)%", 
#                                "Third Party Ash (Adb)%", 
#                                "Third Party Volatile Matter (Adb)%", 
#                                "Third Party Gross Calorific Value (Adb) Kcal/Kg",
#                                "Third Party Ash (Arb)%", 
#                                "Third Party Volatile Matter (Arb)%", 
#                                "Third Party Fixed Carbon (Arb)%", 
#                                "Third Party Gross Calorific Value (Arb) Kcal/Kg",
#                                "Third Party Grade (Adb)",
#                                ]
#                     elif filter_type == "all":
#                         headers = ["Sr.No",
#                                 "Mine",
#                                 "Lot No", 
#                                 "RR No", 
#                                 "RR Qty", 
#                                 "Supplier", 
#                                 "Date", 
#                                 "Time",
#                                 "GWEL Total Moisture%", 
#                                 "GWEL Inherent Moisture (Adb)%", 
#                                 "GWEL Ash (Adb)%", 
#                                 "GWEL Volatile Matter (Adb)%", 
#                                 "GWEL Gross Calorific Value (Adb) Kcal/Kg", 
#                                 "GWEL Ash (Arb)%", 
#                                 "GWEL Volatile Matter (Arb)%", 
#                                 "GWEL Fixed Carbon (Arb)%", 
#                                 "GWEL Gross Calorific Value (Arb) Kcal/Kg",
#                                 "GWEL Grade (Adb)",
#                                 "Third Party Report No", 
#                                 "Third Party Total Moisture%", 
#                                 "Third Party Inherent Moisture (Adb)%", 
#                                 "Third Party Ash (Adb)%", 
#                                 "Third Party Volatile Matter (Adb)%", 
#                                 "Third Party Gross Calorific Value (Adb) Kcal/Kg",
#                                 "Third Party Ash (Arb)%", 
#                                 "Third Party Volatile Matter (Arb)%", 
#                                 "Third Party Fixed Carbon (Arb)%", 
#                                 "Third Party Gross Calorific Value (Arb) Kcal/Kg",
#                                 "Third Party Grade (Adb)",
#                                 ]
#                     else:
#                         headers = [
#                                 "Sr.No",
#                                 "Mine",
#                                 "Lot No", 
#                                 "RR No", 
#                                 "RR Qty", 
#                                 "Supplier",
#                                 "Date", 
#                                 "Time"]

#                     for index, header in enumerate(headers):
#                         worksheet.write(0, index, header, cell_format2)
#                     fetchCoalGrades = CoalGrades.objects()
#                     for row, query in enumerate(usecase_data,start=1):
#                         result = query.payload()
#                         worksheet.write(row, 0, count, cell_format)
#                         if filter_type == "gwel":
#                             worksheet.write(row, 1, str(result["Mine"]), cell_format)
#                             worksheet.write(row, 2, str(result["Lot_No"]), cell_format)
#                             worksheet.write(row, 3, str(result["RR_No"]), cell_format)
#                             worksheet.write(row, 4, str(result["RR_Qty"]), cell_format)
#                             worksheet.write(row, 5, str(result["Supplier"]), cell_format)
#                             worksheet.write(row, 6, str(result["Date"]), cell_format)
#                             worksheet.write(row, 7, str(result["Time"]), cell_format)
#                             worksheet.write(row, 8, str(result["GWEL_Total_Moisture_%"]), cell_format)
#                             worksheet.write(row, 9, str(result["GWEL_Inherent_Moisture_(Adb)_%"]), cell_format)
#                             worksheet.write(row, 10, str(result["GWEL_Ash_(Adb)_%"]), cell_format)
#                             worksheet.write(row, 11, str(result["GWEL_Volatile_Matter_(Adb)_%"]), cell_format)
#                             worksheet.write(row, 12, str(result["GWEL_Gross_Calorific_Value_(Adb)_Kcal/Kg"]), cell_format)
#                             worksheet.write(row, 13, str(result["GWEL_Ash_(Arb)_%"]), cell_format)
#                             worksheet.write(row, 14, str(result["GWEL_Volatile_Matter_(Arb)_%"]), cell_format)
#                             worksheet.write(row, 15, str(result["GWEL_Fixed_Carbon_(Arb)_%"]), cell_format)
#                             worksheet.write(row, 16, str(result["GWEL_Gross_Calorific_Value_(Arb)_Kcal/Kg"]), cell_format)
#                             if result.get("GWEL_Gross_Calorific_Value_(Adb)_Kcal/Kg"):
#                                 for single_coal_grades in fetchCoalGrades:
#                                     if (
#                                         int(single_coal_grades["start_value"])
#                                         <= float(result.get("GWEL_Gross_Calorific_Value_(Adb)_Kcal/Kg"))
#                                         <= int(single_coal_grades["end_value"])
#                                         and single_coal_grades["start_value"] != ""
#                                         and single_coal_grades["end_value"] != ""
#                                     ):
#                                         worksheet.write(row, 17, str(single_coal_grades["grade"]), cell_format)
#                                     elif float(result.get("GWEL_Gross_Calorific_Value_(Adb)_Kcal/Kg")) > 7001:
#                                         worksheet.write(row, 17, "G-1", cell_format)
                            
#                         elif filter_type == "third_party":
#                             worksheet.write(row, 1, str(result["Mine"]), cell_format)
#                             worksheet.write(row, 2, str(result["Lot_No"]), cell_format)
#                             worksheet.write(row, 3, str(result["RR_No"]), cell_format)
#                             worksheet.write(row, 4, str(result["RR_Qty"]), cell_format)
#                             worksheet.write(row, 5, str(result["Supplier"]), cell_format)
#                             worksheet.write(row, 6, str(result["Date"]), cell_format)
#                             worksheet.write(row, 7, str(result["Time"]), cell_format)
#                             if result.get("Third_Party_Report_No"):
#                                 worksheet.write(row, 8, str(result["Third_Party_Report_No"]), cell_format)
#                             if result.get("Third_Party_Total_Moisture_%"):
#                                 worksheet.write(row, 9, str(result["Third_Party_Total_Moisture_%"]), cell_format)
#                             if result.get("Third_Party_Inherent_Moisture_(Adb)_%"):
#                                 worksheet.write(row, 10, str(result["Third_Party_Inherent_Moisture_(Adb)_%"]), cell_format)
#                             if result.get("Third_Party_Ash_(Adb)_%"):
#                                 worksheet.write(row, 11, str(result["Third_Party_Ash_(Adb)_%"]), cell_format)
#                             if result.get("Third_Party_Volatile_Matter_(Adb)_%"):
#                                 worksheet.write(row, 12, str(result["Third_Party_Volatile_Matter_(Adb)_%"]), cell_format)
#                             if result.get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg"):
#                                 worksheet.write(row, 13, str(result["Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg"]), cell_format)
#                             if result.get("Third_Party_Ash_(Arb)_%"):
#                                 worksheet.write(row, 14, str(result["Third_Party_Ash_(Arb)_%"]), cell_format)
#                             if result.get("Third_Party_Volatile_Matter_(Arb)_%"):
#                                 worksheet.write(row, 15, str(result["Third_Party_Volatile_Matter_(Arb)_%"]), cell_format)
#                             if result.get("Third_Party_Fixed_Carbon_(Arb)_%"):
#                                 worksheet.write(row, 16, str(result["Third_Party_Fixed_Carbon_(Arb)_%"]), cell_format)
#                             if result.get("Third_Party_Gross_Calorific_Value_(Arb)_Kcal/Kg"):
#                                 worksheet.write(row, 17, str(result["Third_Party_Gross_Calorific_Value_(Arb)_Kcal/Kg"]), cell_format)
#                             if result.get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg"):
#                                 for single_coal_grades in fetchCoalGrades:
#                                     if (
#                                         int(single_coal_grades["start_value"])
#                                         <= int(result.get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg"))
#                                         <= int(single_coal_grades["end_value"])
#                                         and single_coal_grades["start_value"] != ""
#                                         and single_coal_grades["end_value"] != ""
#                                     ):
#                                         worksheet.write(row, 18, str(single_coal_grades["grade"]), cell_format)
#                                     elif int(result.get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg")) > 7001:
#                                         worksheet.write(row, 18, "G-1", cell_format)
#                         elif filter_type == "all":
#                             worksheet.write(row, 1, str(result["Mine"]), cell_format)
#                             worksheet.write(row, 2, str(result["Lot_No"]), cell_format)
#                             worksheet.write(row, 3, str(result["RR_No"]), cell_format)
#                             worksheet.write(row, 4, str(result["RR_Qty"]), cell_format)
#                             worksheet.write(row, 5, str(result["Supplier"]), cell_format)
#                             worksheet.write(row, 6, str(result["Date"]), cell_format)
#                             worksheet.write(row, 7, str(result["Time"]), cell_format)
#                             worksheet.write(row, 8, str(result["GWEL_Total_Moisture_%"]), cell_format)
#                             worksheet.write(row, 9, str(result["GWEL_Inherent_Moisture_(Adb)_%"]), cell_format)
#                             worksheet.write(row, 10, str(result["GWEL_Ash_(Adb)_%"]), cell_format)
#                             worksheet.write(row, 11, str(result["GWEL_Volatile_Matter_(Adb)_%"]), cell_format)
#                             worksheet.write(row, 12, str(result["GWEL_Gross_Calorific_Value_(Adb)_Kcal/Kg"]), cell_format)
#                             worksheet.write(row, 13, str(result["GWEL_Ash_(Arb)_%"]), cell_format)
#                             worksheet.write(row, 14, str(result["GWEL_Volatile_Matter_(Arb)_%"]), cell_format)
#                             worksheet.write(row, 15, str(result["GWEL_Fixed_Carbon_(Arb)_%"]), cell_format)
#                             worksheet.write(row, 16, str(result["GWEL_Gross_Calorific_Value_(Arb)_Kcal/Kg"]), cell_format)
#                             if result.get("GWEL_Gross_Calorific_Value_(Adb)_Kcal/Kg"):
#                                 for single_coal_grades in fetchCoalGrades:
#                                     if (
#                                         int(single_coal_grades["start_value"])
#                                         <= float(result.get("GWEL_Gross_Calorific_Value_(Adb)_Kcal/Kg"))
#                                         <= int(single_coal_grades["end_value"])
#                                         and single_coal_grades["start_value"] != ""
#                                         and single_coal_grades["end_value"] != ""
#                                     ):
#                                         worksheet.write(row, 17, str(single_coal_grades["grade"]), cell_format)
#                                     elif float(result.get("GWEL_Gross_Calorific_Value_(Adb)_Kcal/Kg")) > 7001:
#                                         worksheet.write(row, 17, "G-1", cell_format)
#                             if result.get("Third_Party_Report_No"):
#                                 worksheet.write(row, 17, str(result["Third_Party_Report_No"]), cell_format)
#                             if result.get("Third_Party_Total_Moisture_%"):
#                                 worksheet.write(row, 18, str(result["Third_Party_Total_Moisture_%"]), cell_format)
#                             if result.get("Third_Party_Inherent_Moisture_(Adb)_%"):
#                                 worksheet.write(row, 19, str(result["Third_Party_Inherent_Moisture_(Adb)_%"]), cell_format)
#                             if result.get("Third_Party_Ash_(Adb)_%"):
#                                 worksheet.write(row, 20, str(result["Third_Party_Ash_(Adb)_%"]), cell_format)
#                             if result.get("Third_Party_Volatile_Matter_(Adb)_%"):
#                                 worksheet.write(row, 21, str(result["Third_Party_Volatile_Matter_(Adb)_%"]), cell_format)
#                             if result.get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg"):
#                                 worksheet.write(row, 22, str(result["Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg"]), cell_format)
#                             if result.get("Third_Party_Ash_(Arb)_%"):
#                                 worksheet.write(row, 23, str(result["Third_Party_Ash_(Arb)_%"]), cell_format)
#                             if result.get("Third_Party_Volatile_Matter_(Arb)_%"):
#                                 worksheet.write(row, 24, str(result["Third_Party_Volatile_Matter_(Arb)_%"]), cell_format)
#                             if result.get("Third_Party_Fixed_Carbon_(Arb)_%"):
#                                 worksheet.write(row, 25, str(result["Third_Party_Fixed_Carbon_(Arb)_%"]), cell_format)
#                             if result.get("Third_Party_Gross_Calorific_Value_(Arb)_Kcal/Kg"):
#                                 worksheet.write(row, 26, str(result["Third_Party_Gross_Calorific_Value_(Arb)_Kcal/Kg"]), cell_format)
#                             if result.get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg"):
#                                 for single_coal_grades in fetchCoalGrades:
#                                     if (
#                                         int(single_coal_grades["start_value"])
#                                         <= int(result.get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg"))
#                                         <= int(single_coal_grades["end_value"])
#                                         and single_coal_grades["start_value"] != ""
#                                         and single_coal_grades["end_value"] != ""
#                                     ):
#                                         worksheet.write(row, 27, str(single_coal_grades["grade"]), cell_format)
#                                     elif int(result.get("Third_Party_Gross_Calorific_Value_(Adb)_Kcal/Kg")) > 7001:
#                                         worksheet.write(row, 27, "G-1", cell_format)
#                         else:
#                             worksheet.write(row, 1, str(result["Mine"]), cell_format)
#                             worksheet.write(row, 2, str(result["Lot_No"]), cell_format)
#                             worksheet.write(row, 3, str(result["RR_No"]), cell_format)
#                             worksheet.write(row, 4, str(result["RR_Qty"]), cell_format)
#                             worksheet.write(row, 5, str(result["Supplier"]), cell_format)
#                             worksheet.write(row, 6, str(result["Date"]), cell_format)
#                             worksheet.write(row, 7, str(result["Time"]), cell_format)
#                         count -= 1
#                     workbook.close()


#                     console_logger.debug("Successfully {} report generated".format(service_id))
#                     console_logger.debug("sent data {}".format(path))
#                     return {
#                             "Type": "coal_test_download_event",
#                             "Datatype": "Report",
#                             "File_Path": path,
#                         }
#                 except Exception as e:
#                     console_logger.debug(e)
#                     exc_type, exc_obj, exc_tb = sys.exc_info()
#                     fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#                     console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#                     console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#             else:
#                 console_logger.error("No data found")
#                 return {
#                             "Type": "coal_test_download_event",
#                             "Datatype": "Report",
#                             "File_Path": path,
#                         }

#     except Exception as e:
#         response.status_code = 400
#         console_logger.debug(e)
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#         console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#         return e

# new as per table
@router.get("/coal_train_test_table", tags=["Coal Testing"])
def coal_secl_test_table(response:Response,currentPage: Optional[int] = None, perPage: Optional[int] = None,
                    search_text: Optional[str] = None,
                    start_timestamp: Optional[str] = None,
                    end_timestamp: Optional[str] = None,
                    month_date: Optional[str] = None,
                    filter_type: Optional[str] = None,
                    type: Optional[str] = "display"):
    try:
        result = {        
                "labels": [],
                "datasets": [],
                "total" : 0,
                "page_size": 15
        }
        
        if type and type == "display":

            data = Q()
            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage
            
            offset = (page_no - 1) * page_len

            if month_date:
                start_date = f'{month_date}-01'
                startd_date=datetime.datetime.strptime(start_date,"%Y-%m-%d")
                end_date = startd_date + relativedelta( day=31)
                data &= Q(plant_analysis_date__gte = startd_date)
                data &= Q(plant_analysis_date__lte = end_date)

            if search_text:
                if search_text.isdigit():
                    data &= (Q(sample_id__icontains=search_text))
                else:
                    data &= Q(mine__icontains=search_text) | Q(sample_no__icontains=search_text)

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(plant_analysis_date__gte = start_date)
            
            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(plant_analysis_date__lte = end_date)

            logs = (
                RecieptCoalQualityAnalysis.objects(data, mode="Rail")
                .order_by("-plant_analysis_date")
                .skip(offset)
                .limit(page_len)                  
            )        

            if any(logs):
                for log in logs:
                    result["labels"] = list(log.payload().keys())
                    # result["labels"] = [
                    # "Sr.No", 
                    # "Mine", 
                    # "Lot_No", 
                    # "RR_No", 
                    # "RR_Qty", 
                    # "Supplier", 
                    # "Date", 
                    # "Time", 
                    # "Id", 
                    # "GWEL_Total_Moisture_%", 
                    # "GWEL_Inherent_Moisture_(Adb)_%", 
                    # "GWEL_Ash_(Adb)_%", 
                    # "GWEL_Volatile_Matter_(Adb)_%", 
                    # "GWEL_Gross_Calorific_Value_(Adb)_Kcal/Kg", 
                    # "GWEL_Ash_(Arb)_%", 
                    # "GWEL_Volatile_Matter_(Arb)_%", 
                    # "GWEL_Fixed_Carbon_(Arb)_%", 
                    # "GWEL_Gross_Calorific_Value_(Arb)_Kcal/Kg", 
                    # "Third_Party_Report_No", 
                    # "Third_Party_Total_Moisture_%", 
                    # "Third_Party_Inherent_Moisture_(Adb)_%", 
                    # "Third_Party_Ash_(Adb)_%", 
                    # "Third_Party_Volatile_Matter_(Adb)_%", 
                    # "Third_Party_Ash_(Arb)_%", 
                    # "Third_Party_Volatile_Matter_(Arb)_%",
                    # "Third_Party_Fixed_Carbon_(Arb)_%",
                    # "Third_Party_Gross_Calorific_Value_(Arb)_Kcal/Kg"]
                    result["datasets"].append(log.payload())

            result["total"] = (len(RecieptCoalQualityAnalysis.objects(data, mode="Rail")))
            # console_logger.debug(f"-------- Rail Coal Testing Response -------- {result}")
            return result

        elif type and type == "download":
            del type

            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            data = Q()

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(plant_analysis_date__gte = start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(plant_analysis_date__lte = end_date)

            if search_text:
                if search_text.isdigit():
                    data &= (Q(sample_id__icontains=search_text))
                else:
                    data &= Q(mine__icontains=search_text) | Q(sample_no__icontains=search_text)
            
            usecase_data = RecieptCoalQualityAnalysis.objects(data, mode="Rail").order_by("-plant_analysis_date")
            count = len(usecase_data)
            path = None
            logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
            if usecase_data:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "Railwise_Coal_Lab_Test_Report_{}.xlsx".format(
                            datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format()
                    cell_format2.set_bold()
                    cell_format2.set_font_size(10)
                    cell_format2.set_align("center")
                    cell_format2.set_align("vcenter")
                    cell_format2.set_text_wrap(True)
                    cell_format2.set_border(1)

                    header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
                    date_format = workbook.add_format({'align': 'center', 'font_size': 12, "bold": True})
                    report_name_format = workbook.add_format({'align': 'center', 'font_size': 15, "bold": True})

                    header_format.set_align("vcenter")
                    date_format.set_align("vcenter")
                    report_name_format.set_align("vcenter")
                    header_format.set_border(1)
                    date_format.set_border(1)
                    report_name_format.set_border(1)

                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)
                    cell_format = workbook.add_format()
                    cell_format.set_font_size(10)
                    cell_format.set_align("center")
                    cell_format.set_align("vcenter")
                    cell_format.set_text_wrap(True)
                    cell_format.set_border(1)

                    worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                    
                    main_header = "GMR Warora Energy Limited" 
                    
                    worksheet.write("A2", f"Date: {end_date.strftime('%d-%m-%Y')}", date_format)

                    if filter_type == "gwel":
                        worksheet.merge_range("A1:Y1", main_header, header_format)  # Merge cells A1 to H1 for the header
                        worksheet.merge_range("C2:Y2", f"GWEL Receipt Quality Analysis (Road)", report_name_format)
                        headers = [
                            "Sr.No.",
                            "Plant Certificate ID",
                            "Sample No",
                            "Do No",
                            "Plant Sample Date",
                            "Plant Preparation Date",
                            "Plant Analysis Date",
                            "Sample Qty",
                            "Mine",
                            "Mine Grade",
                            "Mode",
                            "GWEL LAB TEMP",
                            "GWEL LAB RH",
                            "GWEL ARB TM",
                            "GWEL ARB VM",
                            "GWEL ARB ASH",
                            "GWEL ARB FC",
                            "GWEL ARB GCV",
                            "GWEL ADB IM",
                            "GWEL ADB VM",
                            "GWEL ADB ASH",
                            "GWEL ADB FC",
                            "GWEL ADB GCV",
                            "GWEL ULR ID",
                            "GWEL GRADE"
                        ]
                    elif filter_type == "third_party":
                        worksheet.merge_range("A1:Y1", main_header, header_format)  # Merge cells A1 to H1 for the header
                        worksheet.merge_range("C2:Y2", f"ThirdParty Receipt Quality Analysis (Road)", report_name_format)
                        headers = [
                            "Sr.No.",
                            "Plant Certificate ID",
                            "Sample No",
                            "Do No",
                            "Plant Sample Date",
                            "Plant Preparation Date",
                            "Plant Analysis Date",
                            "Sample Qty",
                            "Mine",
                            "Mine Grade",
                            "Mode",
                            "THIRDPARTY Report Date",
                            "THIRDPARTY Reference No",
                            "THIRDPARTY Sample Date",
                            "THIRDPARTY ARB TM",
                            "THIRDPARTY ARB VM",
                            "THIRDPARTY ARB ASH",
                            "THIRDPARTY ARB FC",
                            "THIRDPARTY ARB GCV",
                            "THIRDPARTY ADB IM",
                            "THIRDPARTY ADB VM",
                            "THIRDPARTY ADB ASH",
                            "THIRDPARTY ADB FC",
                            "THIRDPARTY ADB GCV",
                            "THIRDPARTY GRADE"
                        ]
                    elif filter_type == "all":
                        worksheet.merge_range("A1:AM1", main_header, header_format)  # Merge cells A1 to H1 for the header
                        worksheet.merge_range("C2:AM2", f"Receipt Quality Analysis (Road)", report_name_format)
                        headers = [
                            "Sr.No.",
                            "Plant Certificate Id",
                            "Sample No",
                            "Do No",
                            "Plant Sample Date",
                            "Plant Preparation Date",
                            "Plant Analysis Date",
                            "Sample Qty",
                            "Mine",
                            "Mine Grade",
                            "Mode",
                            "GWEL Lab Temp",
                            "GWEL Lab RH",
                            "GWEL ARB TM",
                            "GWEL ARB VM",
                            "GWEL ARB ASH",
                            "GWEL ARB FC",
                            "GWEL ARB GCV",
                            "GWEL ADB IM",
                            "GWEL ADB VM",
                            "GWEL ADB ASH",
                            "GWEL ADB FC",
                            "GWEL ADB GCV",
                            "GWEL ULR ID",
                            "GWEL GRADE",
                            "THIRDPARTY REPORT DATE",
                            "THIRDPARTY REFERENCE NO",
                            "THIRDPARTY SAMPLE DATE",
                            "THIRDPARTY ARB TM",
                            "THIRDPARTY ARB VM",
                            "THIRDPARTY ARB ASH",
                            "THIRDPARTY ARB FC",
                            "THIRDPARTY ARB GCV",
                            "THIRDPARTY ADB IM",
                            "THIRDPARTY ADB VM",
                            "THIRDPARTY ADB ASH",
                            "THIRDPARTY ADB FC",
                            "THIRDPARTY ADB GCV",
                            "THIRRDPARTY GRADE",
                        ]
                    else:
                        worksheet.merge_range("A1:AM1", main_header, header_format)  # Merge cells A1 to H1 for the header
                        worksheet.merge_range("C2:AM2", f"Receipt Quality Analysis (Road)", report_name_format)
                        headers = [
                            "Sr.No.",
                            "Plant Certificate Id",
                            "Sample No",
                            "Do No",
                            "Plant Sample Date",
                            "Plant Preparation Date",
                            "Plant Analysis Date",
                            "Sample Qty",
                            "Mine",
                            "Mine Grade",
                            "Mode"]

                    for index, header in enumerate(headers):
                        worksheet.write(2, index, header, cell_format2)
                    fetchCoalGrades = CoalGrades.objects()

                    for row, query in enumerate(usecase_data,start=3):
                        result = query.payload()
                        worksheet.write(row, 0, count, cell_format)
                        if filter_type == "gwel":
                            worksheet.write(row, 1, str(result["plant_certificate_id"]), cell_format)
                            worksheet.write(row, 2, str(result["sample_no"]), cell_format)
                            worksheet.write(row, 3, str(result["do_no"]), cell_format)
                            worksheet.write(row, 4, str(result["GWEL_sample_date"]), cell_format)
                            worksheet.write(row, 5, str(result["GWEL_preparation_date"]), cell_format)
                            worksheet.write(row, 6, str(result["GWEL_analysis_date"]), cell_format)
                            worksheet.write(row, 7, float(result["sample_qty"]) if result.get("sample_qty") else 0, cell_format)
                            worksheet.write(row, 8, str(result["mine"]), cell_format)
                            worksheet.write(row, 9, str(result["mine_grade"]), cell_format)
                            worksheet.write(row, 10, str(result["mode"]), cell_format)
                            worksheet.write(row, 11, float(result["GWEL_LAB_TEMP"]) if result.get("GWEL_LAB_TEMP") else 0, cell_format)
                            worksheet.write(row, 12, float(result["GWEL_LAB_RH"]) if result.get("GWEL_LAB_RH") else 0, cell_format)
                            worksheet.write(row, 13, float(result["GWEL_ARB_TM"]) if result.get("GWEL_ARB_TM") else 0, cell_format)
                            worksheet.write(row, 14, float(result["GWEL_ARB_VM"]) if result.get("GWEL_ARB_VM") else 0, cell_format)
                            worksheet.write(row, 15, float(result["GWEL_ARB_ASH"]) if result.get("GWEL_ARB_ASH") else 0, cell_format)
                            worksheet.write(row, 16, float(result["GWEL_ARB_FC"]) if result.get("GWEL_ARB_FC") else 0, cell_format)
                            worksheet.write(row, 17, float(result["GWEL_ARB_GCV"]) if result.get("GWEL_ARB_GCV") else 0, cell_format)
                            worksheet.write(row, 18, float(result["GWEL_ADB_IM"]) if result.get("GWEL_ADB_IM") else 0, cell_format)
                            worksheet.write(row, 19, float(result["GWEL_ADB_VM"]) if result.get("GWEL_ADB_VM") else 0, cell_format)
                            worksheet.write(row, 20, float(result["GWEL_ADB_ASH"]) if result.get("GWEL_ADB_ASH") else 0, cell_format)
                            worksheet.write(row, 21, float(result["GWEL_ADB_FC"]) if result.get("GWEL_ADB_FC") else 0, cell_format)
                            worksheet.write(row, 22, float(result["GWEL_ADB_GCV"]) if result.get("GWEL_ADB_GCV") else 0, cell_format)
                            worksheet.write(row, 23, str(result["GWEL_ULR_ID"]), cell_format)
                            if result.get("GWEL_ADB_GCV"):
                                for single_coal_grades in fetchCoalGrades:
                                    if (
                                        int(single_coal_grades["start_value"])
                                        <= int(result.get("GWEL_ADB_GCV"))
                                        <= int(single_coal_grades["end_value"])
                                        and single_coal_grades["start_value"] != ""
                                        and single_coal_grades["end_value"] != ""
                                    ):
                                        console_logger.debug("inside if")
                                        worksheet.write(row, 24, str(single_coal_grades["grade"]), cell_format)
                                    elif int(result.get("GWEL_ADB_GCV")) > 7001:
                                        worksheet.write(row, 24, "G-1", cell_format)
                                        console_logger.debug("inside else")

                        elif filter_type == "third_party":
                            worksheet.write(row, 1, str(result["plant_certificate_id"]), cell_format)
                            worksheet.write(row, 2, str(result["sample_no"]), cell_format)
                            worksheet.write(row, 3, str(result["do_no"]), cell_format)
                            worksheet.write(row, 4, str(result["GWEL_sample_date"]), cell_format)
                            worksheet.write(row, 5, str(result["GWEL_preparation_date"]), cell_format)
                            worksheet.write(row, 6, str(result["GWEL_analysis_date"]), cell_format)
                            worksheet.write(row, 7, str(result["sample_qty"]), cell_format)
                            worksheet.write(row, 8, str(result["mine"]), cell_format)
                            worksheet.write(row, 9, str(result["mine_grade"]), cell_format)
                            worksheet.write(row, 10, str(result["mode"]), cell_format)
                            if result.get("THIRDPARTY_REPORT_DATE"):
                                worksheet.write(row, 11, str(result["THIRDPARTY_REPORT_DATE"]), cell_format)
                            if result.get("THIRDPARTY_REFERENCE_NO"):
                                worksheet.write(row, 12, str(result["THIRDPARTY_REFERENCE_NO"]), cell_format)
                            if result.get("THIRDPARTY_SAMPLE_DATE"):
                                worksheet.write(row, 13, str(result["THIRDPARTY_SAMPLE_DATE"]), cell_format)
                            if result.get("THIRDPARTY_ARB_TM"):
                                worksheet.write(row, 14, float(result["THIRDPARTY_ARB_TM"]), cell_format)
                            if result.get("THIRDPARTY_ARB_VM"):
                                worksheet.write(row, 15, float(result["THIRDPARTY_ARB_VM"]), cell_format)
                            if result.get("THIRDPARTY_ARB_ASH"):
                                worksheet.write(row, 16, float(result["THIRDPARTY_ARB_ASH"]), cell_format)
                            if result.get("THIRDPARTY_ARB_FC"):
                                worksheet.write(row, 17, float(result["THIRDPARTY_ARB_FC"]), cell_format)
                            if result.get("THIRDPARTY_ARB_GCV"):
                                worksheet.write(row, 18, float(result["THIRDPARTY_ARB_GCV"]), cell_format)
                            if result.get("THIRDPARTY_ADB_IM"):
                                worksheet.write(row, 19, float(result["THIRDPARTY_ADB_IM"]), cell_format)
                            if result.get("THIRDPARTY_ADB_VM"):
                                worksheet.write(row, 20, float(result["THIRDPARTY_ADB_VM"]), cell_format)
                            if result.get("THIRDPARTY_ADB_ASH"):
                                worksheet.write(row, 21, float(result["THIRDPARTY_ADB_ASH"]), cell_format)
                            if result.get("THIRDPARTY_ADB_FC"):
                                worksheet.write(row, 22, float(result["THIRDPARTY_ADB_FC"]), cell_format)
                            if result.get("THIRDPARTY_ADB_GCV"):
                                worksheet.write(row, 23, float(result["THIRDPARTY_ADB_GCV"]), cell_format)
                            if result.get("THIRDPARTY_ADB_GCV"):
                                for single_coal_grades in fetchCoalGrades:
                                    if (
                                        int(single_coal_grades["start_value"])
                                        <= float(result.get("THIRDPARTY_ADB_GCV"))
                                        <= int(single_coal_grades["end_value"])
                                        and single_coal_grades["start_value"] != ""
                                        and single_coal_grades["end_value"] != ""
                                    ):
                                        worksheet.write(row, 24, str(single_coal_grades["grade"]), cell_format)
                                    elif float(result.get("THIRDPARTY_ADB_GCV")) > 7001:
                                        worksheet.write(row, 24, "G-1", cell_format)
                        elif filter_type == "all":
                            worksheet.write(row, 1, str(result["plant_certificate_id"]), cell_format)
                            worksheet.write(row, 2, str(result["sample_no"]), cell_format)
                            worksheet.write(row, 3, str(result["do_no"]), cell_format)
                            worksheet.write(row, 4, str(result["GWEL_sample_date"]), cell_format)
                            worksheet.write(row, 5, str(result["GWEL_preparation_date"]), cell_format)
                            worksheet.write(row, 6, str(result["GWEL_analysis_date"]), cell_format)
                            worksheet.write(row, 7, float(result["sample_qty"]) if result.get("sample_qty") else 0, cell_format)
                            worksheet.write(row, 8, str(result["mine"]), cell_format)
                            worksheet.write(row, 9, str(result["mine_grade"]), cell_format)
                            worksheet.write(row, 10, str(result["mode"]), cell_format)
                            worksheet.write(row, 11, float(result["GWEL_LAB_TEMP"]) if result.get("GWEL_LAB_TEMP") else 0, cell_format)
                            worksheet.write(row, 12, float(result["GWEL_LAB_RH"]) if result.get("GWEL_LAB_RH") else 0, cell_format)
                            worksheet.write(row, 13, float(result["GWEL_ARB_TM"]) if result.get("GWEL_ARB_TM") else 0, cell_format)
                            worksheet.write(row, 14, float(result["GWEL_ARB_VM"]) if result.get("GWEL_ARB_VM") else 0, cell_format)
                            worksheet.write(row, 15, float(result["GWEL_ARB_ASH"]) if result.get("GWEL_ARB_ASH") else 0, cell_format)
                            worksheet.write(row, 16, float(result["GWEL_ARB_FC"]) if result.get("GWEL_ARB_FC") else 0, cell_format)
                            worksheet.write(row, 17, float(result["GWEL_ARB_GCV"]) if result.get("GWEL_ARB_GCV") else 0, cell_format)
                            worksheet.write(row, 18, float(result["GWEL_ADB_IM"]) if result.get("GWEL_ADB_IM") else 0, cell_format)
                            worksheet.write(row, 19, float(result["GWEL_ADB_VM"]) if result.get("GWEL_ADB_VM") else 0, cell_format)
                            worksheet.write(row, 20, float(result["GWEL_ADB_ASH"]) if result.get("GWEL_ADB_ASH") else 0, cell_format)
                            worksheet.write(row, 21, float(result["GWEL_ADB_FC"]) if result.get("GWEL_ADB_FC") else 0, cell_format)
                            worksheet.write(row, 22, float(result["GWEL_ADB_GCV"]) if result.get("GWEL_ADB_GCV") else 0, cell_format)
                            worksheet.write(row, 23, str(result["GWEL_ULR_ID"]), cell_format)
                            if result.get("GWEL_ADB_GCV"):
                                for single_coal_grades in fetchCoalGrades:
                                    if (
                                        int(single_coal_grades["start_value"])
                                        <= float(result.get("GWEL_ADB_GCV"))
                                        <= int(single_coal_grades["end_value"])
                                        and single_coal_grades["start_value"] != ""
                                        and single_coal_grades["end_value"] != ""
                                    ):
                                        worksheet.write(row, 24, str(single_coal_grades["grade"]), cell_format)
                                    elif float(result.get("GWEL_ADB_GCV")) > 7001:
                                        worksheet.write(row, 24, "G-1", cell_format)
                            if result.get("THIRDPARTY_REPORT_DATE"):
                                worksheet.write(row, 25, str(result["THIRDPARTY_REPORT_DATE"]), cell_format)
                            if result.get("THIRDPARTY_REFERENCE_NO"):
                                worksheet.write(row, 26, str(result["THIRDPARTY_REFERENCE_NO"]), cell_format)
                            if result.get("THIRDPARTY_SAMPLE_DATE"):
                                worksheet.write(row, 27, str(result["THIRDPARTY_SAMPLE_DATE"]), cell_format)
                            if result.get("THIRDPARTY_ARB_TM"):
                                worksheet.write(row, 28, float(result["THIRDPARTY_ARB_TM"]), cell_format)
                            if result.get("THIRDPARTY_ARB_VM"):
                                worksheet.write(row, 29, float(result["THIRDPARTY_ARB_VM"]), cell_format)
                            if result.get("THIRDPARTY_ARB_ASH"):
                                worksheet.write(row, 30, float(result["THIRDPARTY_ARB_ASH"]), cell_format)
                            if result.get("THIRDPARTY_ARB_FC"):
                                worksheet.write(row, 31, float(result["THIRDPARTY_ARB_FC"]), cell_format)
                            if result.get("THIRDPARTY_ARB_GCV"):
                                worksheet.write(row, 32, float(result["THIRDPARTY_ARB_GCV"]), cell_format)
                            if result.get("THIRDPARTY_ADB_IM"):
                                worksheet.write(row, 33, float(result["THIRDPARTY_ADB_IM"]), cell_format)
                            if result.get("THIRDPARTY_ADB_VM"):
                                worksheet.write(row, 34, float(result["THIRDPARTY_ADB_VM"]), cell_format)
                            if result.get("THIRDPARTY_ADB_ASH"):
                                worksheet.write(row, 35, float(result["THIRDPARTY_ADB_ASH"]), cell_format)
                            if result.get("THIRDPARTY_ADB_FC"):
                                worksheet.write(row, 36, float(result["THIRDPARTY_ADB_FC"]), cell_format)
                            if result.get("THIRDPARTY_ADB_GCV"):
                                worksheet.write(row, 37, float(result["THIRDPARTY_ADB_GCV"]), cell_format)
                            if result.get("THIRDPARTY_ADB_GCV"):
                                for single_coal_grades in fetchCoalGrades:
                                    if (
                                        int(single_coal_grades["start_value"])
                                        <= float(result.get("THIRDPARTY_ADB_GCV"))
                                        <= int(single_coal_grades["end_value"])
                                        and single_coal_grades["start_value"] != ""
                                        and single_coal_grades["end_value"] != ""
                                    ):
                                        worksheet.write(row, 38, str(single_coal_grades["grade"]), cell_format)
                                    elif float(result.get("THIRDPARTY_ADB_GCV")) > 7001:
                                        worksheet.write(row, 38, "G-1", cell_format)
                        else:
                            worksheet.write(row, 1, str(result["plant_certificate_id"]), cell_format)
                            worksheet.write(row, 2, str(result["sample_no"]), cell_format)
                            worksheet.write(row, 3, str(result["do_no"]), cell_format)
                            worksheet.write(row, 4, str(result["GWEL_sample_date"]), cell_format)
                            worksheet.write(row, 5, str(result["GWEL_preparation_date"]), cell_format)
                            worksheet.write(row, 6, str(result["GWEL_analysis_date"]), cell_format)
                            worksheet.write(row, 7, float(result["sample_qty"]) if result.get("sample_qty") else 0, cell_format)
                            worksheet.write(row, 8, str(result["mine"]), cell_format)
                            worksheet.write(row, 9, str(result["mine_grade"]), cell_format)
                            worksheet.write(row, 10, str(result["mode"]), cell_format)
                        count -= 1
                    workbook.close()

                    console_logger.debug("Successfully {} report generated".format(service_id))
                    console_logger.debug("sent data {}".format(path))
                    return {
                            "Type": "coal_test_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                        }
                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            else:
                console_logger.error("No data found")
                return {
                            "Type": "coal_test_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                        }

    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


#  x------------------------------   Road Coal Journey API    ------------------------------------x



@router.get("/road_journey_table_new_filter", tags=["Road Map"])
def gmr_table_new_filter(response:Response, filter_data: Optional[List[str]] = Query([]), 
              currentPage: Optional[int] = None, perPage: Optional[int] = None, 
              date: Optional[str] = None, search_text: Optional[str] = None,
              start_timestamp: Optional[str] = None, end_timestamp: Optional[str] = None, 
              type: Optional[str] = "display", show_type: Optional[str] = "challan_wise",
              consumer_type: Optional[str] = "All"):
    try:
        result = {        
                "labels": [],
                "datasets": [],
                "total" : 0,
                "page_size": 15
        }
        
        if type and type == "display":

            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage

            if show_type == "challan_wise":

                data = Q()
                
                data &= Q(GWEL_Tare_Time__ne=None)
                # data &= Q(GWEL_Tare_Time__ne=None) & Q(vehicle_out_time__ne=None)

                if date:
                    end =f'{date} 23:59:59'
                    start = f'{date} 00:00:00'

                    start_date = convert_to_utc_format(start, "%Y-%m-%d %H:%M:%S")
                    end_date = convert_to_utc_format(end, "%Y-%m-%d %H:%M:%S")

                    data &= Q(GWEL_Tare_Time__gte = start_date)
                    data &= Q(GWEL_Tare_Time__lte = end_date)

                if start_timestamp:
                    start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                    data &= Q(GWEL_Tare_Time__gte = start_date)

                if end_timestamp:
                    end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M","Asia/Kolkata",False)
                    data &= Q(GWEL_Tare_Time__lte = end_date)

                if search_text:
                    if search_text.isdigit():
                        data &= Q(arv_cum_do_number__icontains=search_text) | Q(delivery_challan_number__icontains=search_text)
                    else:
                        data &= (Q(vehicle_number__icontains=search_text))
                
                if consumer_type and consumer_type != "All":
                    data &= Q(type_consumer__icontains=consumer_type)
                
                offset = (page_no - 1) * page_len
                
                logs = (
                    Gmrdata.objects(data)
                    .order_by("-GWEL_Tare_Time")
                    .skip(offset)
                    .limit(page_len)
                )        
                if any(logs):
                    for log in logs:
                        result["labels"] = list(log.payload().keys())
                        result["datasets"].append(log.payload())

                result["total"]= len(Gmrdata.objects(data))
                return result
            elif show_type == "do_wise":
                skip_value = (page_no - 1) * page_len
                created_at_date = datetime.datetime(2024, 9, 23, 19, 50, 51, 572000)
                
                # if search_text:
                #     query["$or"] = [
                #         {"type_consumer": {"$regex": f"{search_text}", "$options": "i"}},
                #         {"arv_cum_do_number": {"$regex": f"{search_text}", "$options": "i"}}]
                
                pipeline = [
                # {"$match": query},
                {
                    '$match': {
                        'created_at': {
                            '$gt': created_at_date,  # Match records created after this date
                        }
                    }
                },
                {
                    '$group': {
                        '_id': {
                            'do_number': '$arv_cum_do_number', 
                            'mine': '$mine', 
                            'month': {
                                '$dateToString': {
                                    'format': '%Y-%m', 
                                    'date': '$GWEL_Gross_Time'
                                }
                            },
                            'source': '$source', 
                            'line_item': '$line_item', 
                            'consumer_type': '$type_consumer', 
                            'start_date': '$start_date', 
                            'end_date': '$end_date',
                            'po_no': '$po_no',
                            'po_date': '$po_date',
                        }, 
                        'challan_gross_qty': {
                            '$sum': {
                                '$toDouble': '$gross_qty'
                            }
                        }, 
                        'challan_tare_qty': {
                            '$sum': {
                                '$toDouble': '$tare_qty'
                            }
                        }, 
                        'challan_net_qty': {
                            '$sum': {
                                '$toDouble': '$net_qty'
                            }
                        }, 
                        'gwel_actual_gross_qty': {
                            '$sum': {
                                '$toDouble': '$actual_gross_qty'
                            }
                        }, 
                        'gwel_actual_tare_qty': {
                            '$sum': {
                                '$toDouble': '$actual_tare_qty'
                            }
                        }, 
                        'gwel_actual_net_qty': {
                            '$sum': {
                                '$toDouble': '$actual_net_qty'
                            }
                        }, 
                        'do_qty': {
                            '$last': '$po_qty'
                        }
                    }
                },
                {
                    '$project': {
                        '_id': 0, 
                        'do_number': '$_id.do_number', 
                        'mine': '$_id.mine', 
                        'source': '$_id.source', 
                        'month': '$_id.month', 
                        'line_item': '$_id.line_item', 
                        'consumer_type': '$_id.consumer_type', 
                        'start_date': '$_id.start_date', 
                        'end_date': '$_id.end_date',
                        'challan_gross_qty': 1, 
                        'challan_tare_qty': 1, 
                        'challan_net_qty': 1, 
                        'gwel_actual_gross_qty': 1, 
                        'gwel_actual_tare_qty': 1,
                        'gwel_actual_net_qty': 1, 
                        'do_qty': 1,
                        'po_no': '$_id.po_no',
                        'po_date': '$_id.po_date',
                    }
                },
                {
                    "$facet": {
                        # "totalData": [
                        #     { "$skip": skip_value },  # Apply skip based on calculated skip_value
                        #     { "$limit": page_len }    # Apply limit based on page_len
                        # ],
                        "totalData": [],
                        "totalCount": [
                            { "$count": "count" }  # Get total count of documents
                        ]
                    }
                }
            ]

            gmHistoricPipeline = [
                # {"$match": query},
                {
                    '$group': {
                        '_id': {
                            'do_number': '$arv_cum_do_number', 
                            'mine': '$mine', 
                            'month': {
                                '$dateToString': {
                                    'format': '%Y-%m', 
                                    'date': '$GWEL_Gross_Time'
                                }
                            },
                            'source': '$source', 
                            'line_item': '$line_item', 
                            'consumer_type': '$type_consumer', 
                            'start_date': '$start_date', 
                            'end_date': '$end_date',
                            'po_no': '$po_no',
                            'po_date': '$po_date',
                        }, 
                        'challan_gross_qty': {
                            '$sum': {
                                '$toDouble': '$gross_qty'
                            }
                        }, 
                        'challan_tare_qty': {
                            '$sum': {
                                '$toDouble': '$tare_qty'
                            }
                        }, 
                        'challan_net_qty': {
                            '$sum': {
                                '$toDouble': '$net_qty'
                            }
                        }, 
                        'gwel_actual_gross_qty': {
                            '$sum': {
                                '$toDouble': '$actual_gross_qty'
                            }
                        }, 
                        'gwel_actual_tare_qty': {
                            '$sum': {
                                '$toDouble': '$actual_tare_qty'
                            }
                        }, 
                        'gwel_actual_net_qty': {
                            '$sum': {
                                '$toDouble': '$actual_net_qty'
                            }
                        }, 
                        'do_qty': {
                            '$last': '$po_qty'
                        }
                    }
                },
                {
                    '$project': {
                        '_id': 0, 
                        'do_number': '$_id.do_number', 
                        'mine': '$_id.mine', 
                        'source': '$_id.source', 
                        'month': '$_id.month', 
                        'line_item': '$_id.line_item', 
                        'consumer_type': '$_id.consumer_type', 
                        'start_date': '$_id.start_date', 
                        'end_date': '$_id.end_date',
                        'challan_gross_qty': 1, 
                        'challan_tare_qty': 1, 
                        'challan_net_qty': 1, 
                        'gwel_actual_gross_qty': 1, 
                        'gwel_actual_tare_qty': 1,
                        'gwel_actual_net_qty': 1, 
                        'do_qty': 1,
                        'po_no': '$_id.po_no',
                        'po_date': '$_id.po_date',
                    }
                },
                {
                    "$facet": {
                        # "totalData": [
                        #     { "$skip": skip_value },  # Apply skip based on calculated skip_value
                        #     { "$limit": page_len }    # Apply limit based on page_len
                        # ],
                        "totalData": [],
                        "totalCount": [
                            { "$count": "count" }  # Get total count of documents
                        ]
                    }
                }
            ]

            saprecordsPipeline = [
                {
                    '$group': {
                        '_id': {
                            'do_number': '$do_no', 
                            'mine': '$mine_name', 
                            'source': '$source', 
                            'month': '$slno', 
                            'line_item': '$line_item', 
                            'consumer_type': '$consumer_type', 
                            'start_date': '$start_date', 
                            'end_date': '$end_date',
                            'po_no': '$sap_po',
                            'po_date': '$po_date',
                        }, 
                        'do_qty': {
                            '$last': '$do_qty'
                        }
                    }
                }, {
                    '$project': {
                        '_id': 0, 
                        'do_number': '$_id.do_number', 
                        'mine': '$_id.mine', 
                        'source': '$_id.source', 
                        'quota': '$_id.month', 
                        'line_item': '$_id.line_item', 
                        'consumer_type': '$_id.consumer_type', 
                        'start_date': '$_id.start_date', 
                        'end_date': '$_id.end_date', 
                        'do_qty': 1
                    }
                },
                {
                    "$facet": {
                        # "totalData": [
                        #     { "$skip": skip_value },  
                        #     { "$limit": page_len }  
                        # ],
                        "totalData": [],
                        "totalCount": [
                            { "$count": "count" } 
                        ]
                    }
                }
            ]
            
            output = Gmrdata.objects().aggregate(pipeline)
            gmrHistoricoutput = gmrdataHistoric.objects().aggregate(gmHistoricPipeline)
            sapRecordsOutput = SapRecords.objects().aggregate(saprecordsPipeline)
            countPipeline = pipeline.copy()
            results = list(Gmrdata.objects.aggregate(countPipeline))

            historiccountPipeline = gmHistoricPipeline.copy()
            historicgmrresults = list(gmrdataHistoric.objects.aggregate(historiccountPipeline))

            sapRecordscountPipeline = saprecordsPipeline.copy()
            saprecordsResults = list(SapRecords.objects.aggregate(sapRecordscountPipeline))

            outputDict = {}
            listData = []
            for singleData in output:
                for dataload in singleData["totalData"]:
                    dataload["do_no"] = dataload.get("do_number")
                    dataload["quota"] = dataload.get("month")
                    # dataload["challan_gross_qty"] = round(dataload.get("challan_gross_qty"), 2) if dataload.get("challan_gross_qty") else 0
                    # dataload["challan_tare_qty"] = round(dataload.get("challan_tare_qty"), 2) if dataload.get("challan_tare_qty") else 0
                    # dataload["challan_net_qty"] = round(dataload.get("challan_net_qty"), 2) if dataload.get("challan_net_qty") else 0
                    # dataload["gwel_actual_gross_qty"] = round(dataload.get("gwel_actual_gross_qty"), 2) if dataload.get("gwel_actual_gross_qty") else 0
                    # dataload["gwel_actual_tare_qty"] = round(dataload.get("gwel_actual_tare_qty"), 2) if dataload.get("gwel_actual_tare_qty") else 0
                    # dataload["gwel_actual_net_qty"] = round(dataload.get("gwel_actual_net_qty"), 2) if dataload.get("gwel_actual_net_qty") else 0
                    # dataload["transist_loss"] = round(dataload.get("challan_net_qty") - dataload.get("gwel_actual_net_qty"), 2)
                    dataload["challan_gross_qty"] = round(dataload.get("challan_gross_qty"), 2) if dataload.get("challan_gross_qty") and isfinite(dataload.get("challan_gross_qty")) else 0
                    dataload["challan_tare_qty"] = round(dataload.get("challan_tare_qty"), 2) if dataload.get("challan_tare_qty") and isfinite(dataload.get("challan_tare_qty")) else 0
                    dataload["challan_net_qty"] = round(dataload.get("challan_net_qty"), 2) if dataload.get("challan_net_qty") and isfinite(dataload.get("challan_net_qty")) else 0
                    dataload["gwel_actual_gross_qty"] = round(dataload.get("gwel_actual_gross_qty"), 2) if dataload.get("gwel_actual_gross_qty") and isfinite(dataload.get("gwel_actual_gross_qty")) else 0
                    dataload["gwel_actual_tare_qty"] = round(dataload.get("gwel_actual_tare_qty"), 2) if dataload.get("gwel_actual_tare_qty") and isfinite(dataload.get("gwel_actual_tare_qty")) else 0
                    dataload["gwel_actual_net_qty"] = round(dataload.get("gwel_actual_net_qty"), 2) if dataload.get("gwel_actual_net_qty") and isfinite(dataload.get("gwel_actual_net_qty")) else 0
                    dataload["transist_loss"] = round(dataload.get("challan_net_qty", 0) - dataload.get("gwel_actual_net_qty", 0), 2) if isfinite(dataload.get("challan_net_qty", 0) - dataload.get("gwel_actual_net_qty", 0)) else 0
                    listData.append(dataload)
                
            for singlehistoricdata in gmrHistoricoutput:
                for historicDataload in singlehistoricdata["totalData"]:
                    # historicDataload["do_no"] = historicDataload.get("do_number")
                    # historicDataload["quota"] = historicDataload.get("month")
                    # historicDataload["challan_gross_qty"] = round(historicDataload.get("challan_gross_qty"), 2) if historicDataload.get("challan_gross_qty") else 0
                    # historicDataload["challan_tare_qty"] = round(historicDataload.get("challan_tare_qty"), 2) if historicDataload.get("challan_tare_qty") else 0
                    # historicDataload["challan_net_qty"] = round(historicDataload.get("challan_net_qty"), 2) if historicDataload.get("challan_net_qty") else 0
                    # historicDataload["gwel_actual_gross_qty"] = round(historicDataload.get("gwel_actual_gross_qty"), 2) if historicDataload.get("gwel_actual_gross_qty") else 0
                    # historicDataload["gwel_actual_tare_qty"] = round(historicDataload.get("gwel_actual_tare_qty"), 2) if historicDataload.get("gwel_actual_tare_qty") else 0
                    # historicDataload["gwel_actual_net_qty"] = round(historicDataload.get("gwel_actual_net_qty"), 2) if historicDataload.get("gwel_actual_net_qty") else 0
                    # historicDataload["transist_loss"] = round(historicDataload.get("challan_net_qty") - historicDataload.get("gwel_actual_net_qty"), 2)
                    
                    historicDataload["do_no"] = historicDataload.get("do_number")
                    historicDataload["quota"] = historicDataload.get("month")
                    historicDataload["challan_gross_qty"] = round(historicDataload.get("challan_gross_qty"), 2) if historicDataload.get("challan_gross_qty") and isfinite(historicDataload.get("challan_gross_qty")) else 0
                    historicDataload["challan_tare_qty"] = round(historicDataload.get("challan_tare_qty"), 2) if historicDataload.get("challan_tare_qty") and isfinite(historicDataload.get("challan_tare_qty")) else 0
                    historicDataload["challan_net_qty"] = round(historicDataload.get("challan_net_qty"), 2) if historicDataload.get("challan_net_qty") and isfinite(historicDataload.get("challan_net_qty")) else 0
                    historicDataload["gwel_actual_gross_qty"] = round(historicDataload.get("gwel_actual_gross_qty"), 2) if historicDataload.get("gwel_actual_gross_qty") and isfinite(historicDataload.get("gwel_actual_gross_qty")) else 0
                    historicDataload["gwel_actual_tare_qty"] = round(historicDataload.get("gwel_actual_tare_qty"), 2) if historicDataload.get("gwel_actual_tare_qty") and isfinite(historicDataload.get("gwel_actual_tare_qty")) else 0
                    historicDataload["gwel_actual_net_qty"] = round(historicDataload.get("gwel_actual_net_qty"), 2) if historicDataload.get("gwel_actual_net_qty") and isfinite(historicDataload.get("gwel_actual_net_qty")) else 0
                    historicDataload["transist_loss"] = round(historicDataload.get("challan_net_qty", 0) - historicDataload.get("gwel_actual_net_qty", 0), 2) if isfinite(historicDataload.get("challan_net_qty", 0) - historicDataload.get("gwel_actual_net_qty", 0)) else 0
                    
                    do_no_exists = any(item['do_no'] == historicDataload.get("do_number") for item in listData)
                    if not do_no_exists:
                        print("DO_No does not exist in final_data.")
                        listData.append(historicDataload)

            for singlesapdata in sapRecordsOutput:
                for sapDataload in singlesapdata["totalData"]:
                    sapDataload["do_no"] = sapDataload.get("do_number")
                    sapDataload["challan_gross_qty"] = 0
                    sapDataload["challan_tare_qty"] = 0
                    sapDataload["challan_net_qty"] = 0
                    sapDataload["gwel_actual_gross_qty"] = 0
                    sapDataload["gwel_actual_tare_qty"] = 0
                    sapDataload["gwel_actual_net_qty"] = 0
                    sapDataload["transist_loss"] = 0
                    sapDataload["source"] = sapDataload["source"]
                    sapDataload["quota"] = sapDataload["quota"]
                    sapDataload["line_item"] = sapDataload["line_item"]
                    sapDataload["consumer_type"] = sapDataload["consumer_type"]
                    sapDataload["start_date"] = sapDataload["start_date"]
                    sapDataload["end_date"] = sapDataload["end_date"]
                    sapDataload["transist_loss"] = 0
                    do_no_exists = any(item['do_no'] == sapDataload.get("do_number") for item in listData)
                    if not do_no_exists:
                        print("DO_No does not exist in final_data.")
                        listData.append(sapDataload)

            # total_count = len(listData)
            start_idx = (page_no - 1) * page_len
            end_idx = start_idx + page_len
            paginated_data = listData[start_idx:end_idx]

            # total_count = results[0]["totalCount"][0]["count"] + historicgmrresults[0]["totalCount"][0]["count"] + saprecordsResults[0]["totalCount"][0]["count"]
            total_count = results[0]["totalCount"][0]["count"] + historicgmrresults[0]["totalCount"][0]["count"] + saprecordsResults[0]["totalCount"][0]["count"]
            result["labels"] = ["do_no", "mine", "quota", "do_qty", "challan_gross_qty", "challan_tare_qty", "challan_net_qty", "gwel_actual_gross_qty", "gwel_actual_tare_qty", "gwel_actual_net_qty", "transist_loss", "line_item", "start_date", "end_date", "po_no", "po_date"]
            result["datasets"] = paginated_data
            result["total"] = total_count

            return result

        elif type and type == "download":
            del type

            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            data = Q()

            data &= Q(GWEL_Tare_Time__ne=None)
            # data &= Q(GWEL_Tare_Time__ne=None) & Q(vehicle_out_time__ne=None)

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(GWEL_Tare_Time__gte = start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M","Asia/Kolkata",False)
                data &= Q(GWEL_Tare_Time__lte = end_date)
            
            if search_text:
                if search_text.isdigit():
                    data &= Q(arv_cum_do_number__icontains = search_text) | Q(delivery_challan_number__icontains = search_text)
                else:
                    data &= Q(vehicle_number__icontains = search_text)
            
            # usecase_data = Gmrdata.objects(data).order_by("-GWEL_Tare_Time")
            # usecase_data = (
            #     Gmrdata.objects(
            #         Q(GWEL_Tare_Time__ne=None, GWEL_Tare_Time__gte=start_date, GWEL_Tare_Time__lte=end_date) | 
            #         Q(GWEL_Tare_Time=None, vehicle_in_time__gte=start_date, vehicle_in_time__lte=end_date)  # OR condition
            #     )
            #     # .order_by("-GWEL_Tare_Time")  # Order by GWEL_Tare_Time (if present)
            # )
            usecase_data = (
                Gmrdata.objects(
                    Q(GWEL_Tare_Time__ne=None, GWEL_Tare_Time__gte=start_date, GWEL_Tare_Time__lte=end_date)
                )
                # .order_by("-GWEL_Tare_Time")  # Order by GWEL_Tare_Time (if present)
            )
            count = len(usecase_data)
            path = None
            logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
            if usecase_data:
                try:
                    # Define the file path and filename
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "Road_Report_{}.xlsx".format(
                            datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                    filename = os.path.join(os.getcwd(), path)
                    
                    # Create the workbook and worksheet
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    worksheet = workbook.add_worksheet()
                    
                    # Define formats for the Excel file
                    cell_format2 = workbook.add_format()
                    cell_format2.set_bold()
                    cell_format2.set_font_size(10)
                    cell_format2.set_align("center")
                    cell_format2.set_align("vcenter")
                    cell_format2.set_text_wrap(True)
                    cell_format2.set_border(1)
                    
                    header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
                    date_format = workbook.add_format({'align': 'center', 'font_size': 12, "bold": True})
                    report_name_format = workbook.add_format({'align': 'center', 'font_size': 15, "bold": True})
                    
                    header_format.set_align("vcenter")
                    date_format.set_align("vcenter")
                    report_name_format.set_align("vcenter")

                    header_format.set_border(1)
                    date_format.set_border(1)
                    report_name_format.set_border(1)

                    # report_name_format.set_text_wrap(True)
                    # Set columns width and default row height
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)
                    
                    cell_format = workbook.add_format()
                    cell_format.set_font_size(10)
                    cell_format.set_align("center")
                    cell_format.set_align("vcenter")
                    cell_format.set_text_wrap(True)
                    cell_format.set_border(1)

                    worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                    
                    headers = filter_data
                    header_indexes = {header: index for index, header in enumerate(headers)}
                    headers = [header.title().replace("_", " ") for header in headers]
                    input_data = len(headers)
                    DataExecutionsHandler = DataExecutions()
                    excelTitle = DataExecutionsHandler.findExcelColumnTitleFromColumnNumber(n=input_data)
                    # Merge cells for the main header and place it in the center
                    main_header = "GMR Warora Energy Limited"  # Set your main header text here
                    worksheet.merge_range(f"A1:{excelTitle}1", main_header, header_format)  # Merge cells A1 to H1 for the header
                    
                    # Write the current date on the left side (A2)
                    worksheet.write("A2", f"Date: {end_date.strftime('%d-%m-%Y')}", date_format)
                    worksheet.merge_range(f"C2:{excelTitle}2", f"Road Coal Journey", report_name_format)
                    # Write headers starting from row 3 (which is Excel row C)
                    
                    weight_header = ["Challan_Gross_Wt(MT)",
                                    "Challan_Tare_Wt(MT)",
                                    "Challan_Net_Wt(MT)",
                                    "GWEL_Gross_Wt(MT)",
                                    "GWEL_Tare_Wt(MT)",
                                    "GWEL_Net_Wt(MT)",
                                    "Transit_Loss"]

                    for index, header in enumerate(headers):
                        worksheet.write(2, index, header, cell_format2)  # Writing headers in the 3rd row (row C)
                    
                    for row, query in enumerate(usecase_data, start=3):  # Starting from row 4 (index 3)
                        result = query.payload()
                        worksheet.write(row, 0, count, cell_format)
                        for header in filter_data:
                            if header in result:
                                value = result[header]
                                if header in weight_header:
                                    # if value is not None:
                                    #     worksheet.write(row, header_indexes[header], round(float(value), 2), cell_format)
                                    # else:
                                    #     worksheet.write(row, header_indexes[header], "N/A", cell_format)
                                    if isinstance(value, (float, int)):
                                        if math.isnan(value) or math.isinf(value):
                                            worksheet.write(row, header_indexes[header], "Error", cell_format)  # Write an error indicator or handle appropriately
                                        else:
                                            worksheet.write(row, header_indexes[header], round(float(value), 2), cell_format)
                                    else:
                                        worksheet.write(row, header_indexes[header], value, cell_format)
                                else:
                                    if value is not None:
                                        worksheet.write(row, header_indexes[header], str(value), cell_format)
                                    else:
                                        worksheet.write(row, header_indexes[header], "N/A", cell_format)
                        count -= 1
                    
                    # Close the workbook after writing the data
                    worksheet.autofit()
                    workbook.close()
                    
                    console_logger.debug("Successfully {} report generated".format(service_id))
                    console_logger.debug("sent data {}".format(path))
                    
                    return {
                        "Type": "gmr_road_journey_download_event",
                        "Datatype": "Report",
                        "File_Path": path,
                    }

                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))

            else:
                console_logger.error("No data found")
                return {
                    "Type": "gmr_road_journey_download_event",
                    "Datatype": "Report",
                    "File_Path": path,
                }

    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/save_dc_request_data", tags=["Road Map Request"])
async def store_dc_request_data(data:RequestData):
    try:
        challan_no = data.Delivery_Challan_Number
        record = Gmrdata.objects(delivery_challan_number = challan_no).order_by("-created_at").first()
        entry_exists = Gmrrequest.objects(delivery_challan_number=challan_no,vehicle_number__exists=True,expiry_validation=True).order_by("-created_at").first()
        
        if not record:    
            raise HTTPException(status_code=404, detail="Record not found")
        record.dc_request = True
        record.save()

        if not entry_exists:
            dc_data = Gmrrequest(delivery_challan_number = challan_no,
                                    vehicle_number = data.Vehicle_Truck_Registration_No.upper().strip() if data.Vehicle_Truck_Registration_No else data.Vehicle_Truck_Registration_No.strip(),
                                    arv_cum_do_number = data.ARV_Cum_DO_Number,
                                    mine = data.Mine_Name.upper(),
                                    net_qty = data.Net_Qty,
                                    delivery_challan_date = data.Delivery_Challan_Date,
                                    total_net_amount = data.Total_Net_Amount_of_Figures.replace(",",""),
                                    vehicle_chassis_number = data.Chassis_No,
                                    certificate_expiry = data.Certificate_will_expire_on,
                                    request = "DC_Expiry_Request",
                                    created_at = datetime.datetime.utcnow(),
                                    ID=Gmrrequest.objects.count() + 1)
            dc_data.save()
            record_num = Gmrrequest.objects(delivery_challan_number=challan_no,record_id__exists=True).order_by("-created_at").first()
            
            return {"message": "Successful"}
        return {"message": "Entry with this challan Number exist"}

    except NotUniqueError:
        new_record_id = uuid.uuid4().hex
        dc_data = Gmrrequest(
                            record_id=new_record_id,
                            delivery_challan_number = challan_no,
                            vehicle_number = data.Vehicle_Truck_Registration_No.upper().strip() if data.Vehicle_Truck_Registration_No else data.fitness.Vehicle_Truck_Registration_No.strip(),
                            arv_cum_do_number = data.ARV_Cum_DO_Number,
                            mine = data.Mine_Name.upper(),
                            net_qty = data.Net_Qty,
                            delivery_challan_date = data.Delivery_Challan_Date,
                            total_net_amount = data.Total_Net_Amount_of_Figures.replace(",",""),
                            vehicle_chassis_number = data.Chassis_No,
                            certificate_expiry = data.Certificate_will_expire_on,
                            request = "DC_Expiry_Request",                                
                            created_at = datetime.datetime.utcnow(),
                            ID=Gmrrequest.objects.count() + 1)
        dc_data.save()
        record_num = Gmrrequest.objects(delivery_challan_number=challan_no,record_id__exists=True).order_by("-created_at").first()
        
        return {"message": "Successful"}


@router.get("/road/fitness_validation_table", tags=["Road Map Request"])
def fitness_dc_validation(
    response: Response,
    currentPage: Optional[int] = None,
    perPage: Optional[int] = None,
    search_text: Optional[str] = None,
    start_timestamp: Optional[str] = None,
    end_timestamp: Optional[str] = None,
    search_type: Optional[str] = "All"
):
    try:
        result = {
            "labels": [],
            "datasets": [],
            "total": 0,
            "page_size": 15
        }

        page_no = 1
        page_len = result["page_size"]

        if currentPage:
            page_no = currentPage

        if perPage:
            page_len = perPage
            result["page_size"] = perPage

        data = Q(expiry_validation=True)

        if start_timestamp:
            start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M", "Asia/Kolkata", False)
            data &= Q(created_at__gte=start_date)

        if end_timestamp:
            end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M", "Asia/Kolkata", False)
            data &= Q(created_at__lte=end_date)

        if search_text:
            if search_text.isdigit():
                data &= Q(arv_cum_do_number__icontains=search_text) | Q(delivery_challan_number__icontains=search_text)
            else:
                data &= Q(vehicle_number__icontains=search_text)

        if search_type == "All":
            payload_method = "tare_payload"

        elif search_type == "fitness":
            data &= Q(request="Fitness_Expiry_Request")
            payload_method = "payload"

        elif search_type == "tare":
            data &= Q(request="Tare_Diff_Request")
            payload_method = "tare_payload"
            
        else:
            data &= Q(request="DC_Expiry_Request")
            payload_method = "payload"

        offset = (page_no - 1) * page_len

        logs = (
            Gmrrequest.objects(data)
            .order_by("-created_at")
            .skip(offset)
            .limit(page_len)
        )

        if logs:
            for log in logs:
                payload = getattr(log, payload_method)()
                result["labels"] = list(payload.keys())
                result["datasets"].append(payload)

        result["total"] = Gmrrequest.objects(data).count()
        return result

    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/road/record_table", tags=["Road Map Request"])
def fitness_dc_record(
    response: Response,
    currentPage: Optional[int] = None,
    perPage: Optional[int] = None,
    search_text: Optional[str] = None,
    start_timestamp: Optional[str] = None,
    end_timestamp: Optional[str] = None,
    type: Optional[str] = "display",
    search_type: Optional[str] = "All"
):
    try:
        result = {
            "labels": [],
            "datasets": [],
            "total": 0,
            "page_size": 15
        }

        if type == "display":
            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage

            data = Q(approved_at__ne=None)

            if search_type == "All":
                payload_method = "history_tare_payload"

            elif search_type == "fitness":
                data &= Q(request="Fitness_Expiry_Request")
                payload_method = "history_payload"

            elif search_type == "tare":
                data &= Q(request="Tare_Diff_Request")
                payload_method = "history_tare_payload"

            else:
                data &= Q(request="DC_Expiry_Request")
                payload_method = "history_payload"

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M", "Asia/Kolkata", False)
                data &= Q(approved_at__gte=start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M", "Asia/Kolkata", False)
                data &= Q(approved_at__lte=end_date)

            if search_text:
                if search_text.isdigit():
                    data &= Q(arv_cum_do_number__icontains=search_text) | Q(delivery_challan_number__icontains=search_text)
                else:
                    data &= Q(vehicle_number__icontains=search_text)

            offset = (page_no - 1) * page_len

            logs = (
                Gmrrequest.objects(data)
                .order_by("-approved_at")
                .skip(offset)
                .limit(page_len)
            )

            if logs:
                for log in logs:
                    payload = getattr(log, payload_method)()
                    result["labels"] = list(payload.keys())
                    result["datasets"].append(payload)

            result["total"] = Gmrrequest.objects(data).count()
            return result

        elif type == "download":
            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            data = Q(approved_at__ne=None)

            if search_type == "All":
                data &= Q(request__in=["Fitness_Expiry_Request", "Tare_Diff_Request", "DC_Expiry_Request"])
                payload_method = "history_tare_payload"
                headers = [
                    "Sr.No",
                    "Request Type",
                    "Mine",
                    "Vehicle Number",
                    "Delivery Challan No",
                    "DO No",
                    "Vehicle Chassis No",
                    "Fitness Expiry",
                    "DC Date",
                    "Challan Net Wt(MT)",
                    "Challan Tare Wt(MT)",
                    "GWEL Tare Wt(MT)",
                    "Total Net Amount",
                    "Remark",
                    "Request Time",
                    "Approval Time",
                    "TAT"
                ]

            elif search_type == "fitness":
                data &= Q(request="Fitness_Expiry_Request")
                payload_method = "history_payload"
                headers = [
                    "Sr.No",
                    "Request Type",
                    "Mine",
                    "Vehicle Number",
                    "Delivery Challan No",
                    "DO No",
                    "Vehicle Chassis No",
                    "Fitness Expiry",
                    "DC Date",
                    "Challan Net Wt(MT)",
                    "Total Net Amount",
                    "Remark",
                    "Request Time",
                    "Approval Time",
                    "TAT"
                ]
            elif search_type == "tare":
                data &= Q(request="Tare_Diff_Request")
                payload_method = "history_tare_payload"
                headers = [
                    "Sr.No",
                    "Request Type",
                    "Mine",
                    "Vehicle Number",
                    "Delivery Challan No",
                    "DO No",
                    "Vehicle Chassis No",
                    "Fitness Expiry",
                    "DC Date",
                    "Challan Net Wt(MT)",
                    "Challan Tare Wt(MT)",
                    "GWEL Tare Wt(MT)",
                    "Total Net Amount",
                    "Remark",
                    "Request Time",
                    "Approval Time",
                    "TAT"
                ]
            else:
                data &= Q(request="DC_Expiry_Request")
                payload_method = "history_payload"
                headers = [
                    "Sr.No",
                    "Request Type",
                    "Mine",
                    "Vehicle Number",
                    "Delivery Challan No",
                    "DO No",
                    "Vehicle Chassis No",
                    "Fitness Expiry",
                    "DC Date",
                    "Challan Net Wt(MT)",
                    "Total Net Amount",
                    "Remark",
                    "Request Time",
                    "Approval Time",
                    "TAT"
                ]

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M", "Asia/Kolkata", False)
                data &= Q(approved_at__gte=start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M", "Asia/Kolkata", False)
                data &= Q(approved_at__lte=end_date)

            if search_text:
                if search_text.isdigit():
                    data &= Q(arv_cum_do_number__icontains=search_text) | Q(delivery_challan_number__icontains=search_text)
                else:
                    data &= Q(vehicle_number__icontains=search_text)

            usecase_data = Gmrrequest.objects(data).order_by("-approved_at")
            count = len(usecase_data)
            path = None

            if usecase_data:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "Approval_Report_{}.xlsx".format(
                            datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format()
                    cell_format2.set_bold()
                    cell_format2.set_font_size(10)
                    cell_format2.set_align("center")
                    cell_format2.set_align("vcenter")

                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)
                    cell_format = workbook.add_format()
                    cell_format.set_font_size(10)
                    cell_format.set_align("center")
                    cell_format.set_align("vcenter")

                    for index, header in enumerate(headers):
                        worksheet.write(0, index, header, cell_format2)

                    for row, query in enumerate(usecase_data, start=1):
                        result = getattr(query, payload_method)()
                        worksheet.write(row, 0, row, cell_format)
                        worksheet.write(row, 1, str(result.get("Request_type", "")), cell_format)
                        worksheet.write(row, 2, str(result.get("Mine", "")), cell_format)
                        worksheet.write(row, 3, str(result.get("Vehicle_Number", "")), cell_format)
                        worksheet.write(row, 4, str(result.get("Delivery_Challan_No", "")), cell_format)
                        worksheet.write(row, 5, str(result.get("DO_No", "")), cell_format)
                        worksheet.write(row, 6, str(result.get("Vehicle_Chassis_No", "")), cell_format)
                        worksheet.write(row, 7, str(result.get("Fitness_Expiry", "")), cell_format)
                        worksheet.write(row, 8, str(result.get("DC_Date", "")), cell_format)
                        worksheet.write(row, 9, str(result.get("Challan_Net_Wt(MT)", "")), cell_format)
                        if search_text == "All":
                            worksheet.write(row, 10, str(result.get("Challan_Tare_Wt(MT)", "")), cell_format)
                            worksheet.write(row, 11, str(result.get("GWEL_Tare_Wt(MT)", "")), cell_format)
                            worksheet.write(row, 12, str(result.get("Total_net_amount", "")), cell_format)
                            worksheet.write(row, 13, str(result.get("Remark", "")), cell_format)
                            worksheet.write(row, 14, str(result.get("Request_Time", "")), cell_format)
                            worksheet.write(row, 15, str(result.get("Approval_Time", "")), cell_format)
                            worksheet.write(row, 16, str(result.get("TAT", "")), cell_format)
                        if search_type == "tare" or search_type == "All":
                            worksheet.write(row, 10, str(result.get("Challan_Tare_Wt(MT)", "")), cell_format)
                            worksheet.write(row, 11, str(result.get("GWEL_Tare_Wt(MT)", "")), cell_format)
                            worksheet.write(row, 12, str(result.get("Total_net_amount", "")), cell_format)
                            worksheet.write(row, 13, str(result.get("Remark", "")), cell_format)
                            worksheet.write(row, 14, str(result.get("Request_Time", "")), cell_format)
                            worksheet.write(row, 15, str(result.get("Approval_Time", "")), cell_format)
                            worksheet.write(row, 16, str(result.get("TAT", "")), cell_format)
                        else:
                            worksheet.write(row, 10, str(result.get("Total_net_amount", "")), cell_format)
                            worksheet.write(row, 11, str(result.get("Remark", "")), cell_format)
                            worksheet.write(row, 12, str(result.get("Request_Time", "")), cell_format)
                            worksheet.write(row, 13, str(result.get("Approval_Time", "")), cell_format)
                            worksheet.write(row, 14, str(result.get("TAT", "")), cell_format)
                    count -= 1

                    workbook.close()
                    console_logger.debug("Successfully {} report generated".format(service_id))
                    console_logger.debug("sent data {}".format(path))

                    return {
                        "Type": "Request_Approval_download_event",
                        "Datatype": "Report",
                        "File_Path": path,
                    }

                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))

            else:
                console_logger.error("No data found")
                return {
                    "Type": "Request_Approval_download_event",
                    "Datatype": "Report",
                    "File_Path": path,
                }


    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.put("/road/update_expiry_date", tags=["Road Map Request"])
async def update_fc_expiry_date(vehicle_number: str, remark: Optional[str] = None):
    try:
        record = Gmrdata.objects(vehicle_number=vehicle_number).order_by("-created_at").first()
        request_record = Gmrrequest.objects(vehicle_number=vehicle_number, expiry_validation=True).order_by("-created_at").first()

        if remark is None:
            remark = "Fitness Extended For 7 days"

        if request_record:
            request_record.expiry_validation = False
            request_record.approved_at = datetime.utcnow()
            request_record.remark = remark
            request_record.save()

        if not record:
            raise HTTPException(status_code=404, detail="Record not found")
        else:
            record.certificate_expiry = (datetime.datetime.now().date() + timedelta(days=7)).strftime("%d-%m-%Y")
        # record.fitness_verify = False
        record.save()

        return {"message": "Record updated successfully"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.put("/road/pass_expiry_date", tags=["Road Map Request"])
async def pass_fc_expiry_date(vehicle_number: str, remark: Optional[str] = None):
    try:
        record = Gmrdata.objects(vehicle_number = vehicle_number).order_by("-created_at").first()
        request_record = Gmrrequest.objects(vehicle_number = vehicle_number, expiry_validation=True).order_by("-created_at").first()

        if remark == None:
            remark = "Fitness Extension Declined"

        if request_record:
            request_record.expiry_validation = False
            request_record.approved_at =  datetime.datetime.utcnow()
            request_record.remark = remark
            request_record.save()

        if not record:
            raise HTTPException(status_code=404, detail="Record not found")
            
        return {"message": "Record updated successfully"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.put("/road/update_dc_expiry", tags=["Road Map Request"])
async def update_dc_expiry(challan_no: str, remark: Optional[str] = None):
    try:
        record = Gmrdata.objects(delivery_challan_number = challan_no).order_by("-created_at").first()
        request_record = Gmrrequest.objects(delivery_challan_number = challan_no, expiry_validation=True).order_by("-created_at").first()

        if remark == None:
            remark = "DC Approved"

        if request_record:
            request_record.expiry_validation = False
            request_record.approved_at =  datetime.datetime.utcnow()
            request_record.remark = remark
            request_record.save()

        if not record:
            raise HTTPException(status_code=404, detail="Record not found")
        
        record.dc_request_status = True
        record.save()

        return {"message": "Record updated successfully"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.put("/road/pass_dc_expiry", tags=["Road Map Request"])
async def pass_dc_expiry(challan_no: str, remark: Optional[str] = None):
    try:
        record = Gmrdata.objects(delivery_challan_number = challan_no).order_by("-created_at").first()
        request_record = Gmrrequest.objects(delivery_challan_number = challan_no, expiry_validation=True).order_by("-created_at").first()

        if remark == None:
            remark = "DC Declined"

        if request_record:
            request_record.expiry_validation = False
            request_record.approved_at =  datetime.datetime.utcnow()
            request_record.remark = remark
            request_record.save()

        if not record:
            raise HTTPException(status_code=404, detail="Record not found")

        record.dc_request_status = False
        record.dc_request = False
        record.save()
            
        return {"message": "Record updated successfully"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/delete_expiry", tags=["Road Map Request"])
def delete_expiry(delivery_challan_number: str):
    try:
        challan = Gmrrequest.objects.get(delivery_challan_number = delivery_challan_number)
        challan.delete()
        return {"message": "Fitness Expired Entry deleted successfully"}
    except DoesNotExist:
        raise HTTPException(status_code=404, detail="Entry not found")


@router.post("/save_tare_request_data", tags=["Road Map Request"])
async def store_tare_request_data(data:RequestData):
    try:
        challan_no = data.Delivery_Challan_Number
        record = Gmrdata.objects(delivery_challan_number = challan_no).order_by("-created_at").first()
        entry_exists = Gmrrequest.objects(delivery_challan_number=challan_no,vehicle_number__exists=True,expiry_validation=True).order_by("-created_at").first()
        
        if not record:    
            raise HTTPException(status_code=404, detail="Record not found")
        record.tare_request = True
        record.save()

        if not entry_exists:
            tare_data = Gmrrequest(
                                    delivery_challan_number = challan_no,
                                    vehicle_number = data.Vehicle_Truck_Registration_No.upper().strip() if data.Vehicle_Truck_Registration_No else data.Vehicle_Truck_Registration_No.strip(),
                                    arv_cum_do_number = data.ARV_Cum_DO_Number,
                                    mine = data.Mine_Name.upper(),
                                    net_qty = data.Net_Qty,
                                    tare_qty = data.Tare_Qty,
                                    actual_tare_qty = data.Actual_Tare_Qty,
                                    delivery_challan_date = data.Delivery_Challan_Date,
                                    total_net_amount = data.Total_Net_Amount_of_Figures.replace(",",""),
                                    vehicle_chassis_number = data.Chassis_No,
                                    certificate_expiry = data.Certificate_will_expire_on,
                                    request = "Tare_Diff_Request",
                                    created_at = datetime.datetime.utcnow(),
                                    ID=Gmrrequest.objects.count() + 1)
            tare_data.save()
            record_num = Gmrrequest.objects(delivery_challan_number=challan_no,record_id__exists=True).order_by("-created_at").first()
            console_logger.debug(record_num.record_id)
            return {"message": "Successful"}
        return {"message": "Entry with this challan Number exist"}

    except NotUniqueError:
        new_record_id = uuid.uuid4().hex
        tare_data = Gmrrequest(
                            record_id=new_record_id,
                            delivery_challan_number = challan_no,
                            vehicle_number = data.Vehicle_Truck_Registration_No.upper().strip() if data.Vehicle_Truck_Registration_No else data.Vehicle_Truck_Registration_No.strip(),
                            arv_cum_do_number = data.ARV_Cum_DO_Number,
                            mine = data.Mine_Name.upper(),
                            net_qty = data.Net_Qty,
                            tare_qty = data.Tare_Qty,
                            actual_tare_qty = data.Actual_Tare_Qty,
                            delivery_challan_date = data.Delivery_Challan_Date,
                            total_net_amount = data.Total_Net_Amount_of_Figures.replace(",",""),
                            vehicle_chassis_number = data.Chassis_No,
                            certificate_expiry = data.Certificate_will_expire_on,
                            request = "Tare_Diff_Request",                                
                            created_at = datetime.datetime.utcnow(),
                            ID=Gmrrequest.objects.count() + 1)
        tare_data.save()
        record_num = Gmrrequest.objects(delivery_challan_number=challan_no,record_id__exists=True).order_by("-created_at").first()
        
        return {"message": "Successful"}


@router.put("/road/approve_tare_req", tags=["Road Map Request"])
async def update_tare(challan_no: str, remark: Optional[str] = None):
    try:
        record = Gmrdata.objects(delivery_challan_number = challan_no).order_by("-created_at").first()
        request_record = Gmrrequest.objects(delivery_challan_number = challan_no, expiry_validation=True).order_by("-created_at").first()

        if remark == None:
            remark = "Tare Req Approved"

        if request_record:
            request_record.expiry_validation = False
            request_record.approved_at = datetime.datetime.utcnow()
            request_record.remark = remark
            request_record.save()

        if not record:
            raise HTTPException(status_code=404, detail="Record not found")

        record.tare_request_status = True
        record.save()

        return {"message": "Record updated successfully"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.put("/road/decline_tare_req", tags=["Road Map Request"])
async def decline_tare_req(challan_no: str, remark: Optional[str] = None):
    try:
        record = Gmrdata.objects(delivery_challan_number = challan_no).order_by("-created_at").first()
        request_record = Gmrrequest.objects(delivery_challan_number = challan_no, expiry_validation=True).order_by("-created_at").first()

        if remark == None:
            remark = "Tare Req Declined"

        if request_record:
            request_record.expiry_validation = False
            request_record.approved_at =  datetime.datetime.utcnow()
            request_record.remark = remark
            request_record.save()

        if not record:
            raise HTTPException(status_code=404, detail="Record not found")
            
        record.tare_request_status = False
        record.tare_request = False
        record.save()

        return {"message": "Record updated successfully"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/road/minewise_road_graph", tags=["Road Map"])
def minewise_road_analysis(response:Response,type: Optional[str] = "Daily",
                            Month: Optional[str] = None, Daily: Optional[str] = None, 
                            Year: Optional[str] = None):
    try:
        data={}
        timezone = pytz.timezone('Asia/Kolkata')

        basePipeline = [
            {
                "$match": {
                    "GWEL_Tare_Time": {
                        "$gte": None,
                    },
                },
            },
            {
                "$project": {
                    # "ts": {
                    #     "$hour": {"date": "$GWEL_Tare_Time", "timezone": timezone},
                    # },
                    "ts": None,
                    "mine": "$mine",
                    "actual_net_qty": "$actual_net_qty",
                    "_id": 0
                },
            },
            {
                "$group": {
                    "_id": {
                        "ts": "$ts",
                        "mine": "$mine"
                    },
                    "data": {
                        "$push": "$actual_net_qty"
                    }
                }
            },
        ]

        if type == "Daily":

            date = Daily
            end_date = f'{date} 23:59:59'
            start_date = f'{date} 00:00:00'
            format_data = "%Y-%m-%d %H:%M:%S"
            endd_date = convert_to_utc_format(end_date.__str__(), format_data)
            startd_date = convert_to_utc_format(start_date.__str__(), format_data)

            basePipeline[0]["$match"]["GWEL_Tare_Time"]["$lte"] = endd_date
            basePipeline[0]["$match"]["GWEL_Tare_Time"]["$gte"] = startd_date

            basePipeline[1]["$project"]["ts"] = {"$hour": {"date": "$GWEL_Tare_Time", "timezone": "Asia/Kolkata"}}
            

            result = {
                "data": {
                    "labels": [str(i) for i in range(1, 25)],
                    "datasets": [
                        {"label": "YEKONA", "data": [0 for i in range(1, 25)]},
                        {"label": "SASTI", "data": [0 for i in range(1, 25)]},
                        {"label": "PENGANGA", "data": [0 for i in range(1, 25)]},
                        {"label": "MUNGOLI", "data": [0 for i in range(1, 25)]},
                        {"label": "NEELJAY", "data": [0 for i in range(1, 25)]},             
                    ],
                }
            }

        elif type == "Week":
            start_date = (
                datetime.datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)
                - datetime.timedelta(days=7)
            )
            end_date = datetime.datetime.utcnow().replace(hour=23, minute=59, second=59, microsecond=0)
            endd_date = end_date-datetime.timedelta(days=1)

            basePipeline[0]["$match"]["GWEL_Tare_Time"]["$gte"] = convert_to_utc_format(start_date.__str__(), "%Y-%m-%d %H:%M:%S")
            basePipeline[1]["$project"]["ts"] = {"$dayOfMonth": {"date": "$GWEL_Tare_Time", "timezone": "Asia/Kolkata"}}
            result = {
                "data": {
                    "labels": [
                        (
                            basePipeline[0]["$match"]["GWEL_Tare_Time"]["$gte"]
                            + datetime.timedelta(days=i + 1)
                        ).strftime("%d")
                        for i in range(1, 8)
                    ],
                    "datasets": [
                        {"label": "YEKONA", "data": [0 for i in range(1, 8)]},
                        {"label": "SASTI", "data": [0 for i in range(1, 8)]},
                        {"label": "PENGANGA", "data": [0 for i in range(1, 8)]},
                        {"label": "MUNGOLI", "data": [0 for i in range(1, 8)]},
                        {"label": "NEELJAY", "data": [0 for i in range(1, 8)]},
                    ],
                }
            }

        elif type == "Month":
            date = Month
            format_data = "%Y - %m-%d"
            start_date = f'{date}-01'
            startd_date = timezone.localize(datetime.datetime.strptime(start_date, format_data))

            end_date = startd_date + relativedelta(day=31)
            end_label = end_date.strftime("%d")

            basePipeline[0]["$match"]["GWEL_Tare_Time"]["$lte"] = end_date
            basePipeline[0]["$match"]["GWEL_Tare_Time"]["$gte"] = startd_date
            # basePipeline[1]["$project"]["ts"] = {"$dayOfMonth": "$GWEL_Tare_Time"}
            basePipeline[1]["$project"]["ts"] = {"$dayOfMonth": {"date": "$GWEL_Tare_Time", "timezone": "Asia/Kolkata"}}

            result = {
                "data": {
                    "labels": [
                        (
                            basePipeline[0]["$match"]["GWEL_Tare_Time"]["$gte"]
                            + datetime.timedelta(days=i + 1)
                        ).strftime("%d")
                        for i in range(-1, (int(end_label))-1)
                    ],
                    "datasets": [
                        {"label": "YEKONA", "data": [0 for i in range(-1, (int(end_label))-1)]},
                        {"label": "SASTI", "data": [0 for i in range(-1, (int(end_label))-1)]},
                        {"label": "PENGANGA", "data": [0 for i in range(-1, (int(end_label))-1)]},
                        {"label": "MUNGOLI", "data": [0 for i in range(-1, (int(end_label))-1)]},
                        {"label": "NEELJAY", "data": [0 for i in range(-1, (int(end_label))-1)]},
                    ],
                }
            }

        elif type == "Year":

            date = Year
            end_date = f'{date}-12-31 23:59:59'
            start_date = f'{date}-01-01 00:00:00'
            format_data = "%Y-%m-%d %H:%M:%S"
            endd_date = timezone.localize(datetime.datetime.strptime(end_date, format_data))
            startd_date = timezone.localize(datetime.datetime.strptime(start_date, format_data))

            basePipeline[0]["$match"]["GWEL_Tare_Time"]["$lte"] = endd_date
            basePipeline[0]["$match"]["GWEL_Tare_Time"]["$gte"] = startd_date

            # basePipeline[1]["$project"]["ts"] = {"$month": "$GWEL_Tare_Time"}
            basePipeline[1]["$project"]["ts"] = {"$month": {"date": "$GWEL_Tare_Time", "timezone": "Asia/Kolkata"}}
            result = {
                "data": {
                    "labels": [
                        (
                            basePipeline[0]["$match"]["GWEL_Tare_Time"]["$gte"]
                            + relativedelta(months=i)
                        ).strftime("%m")
                        for i in range(0, 12)
                    ],
                    "datasets": [
                        {"label": "YEKONA", "data": [0 for i in range(0, 12)]},
                        {"label": "SASTI", "data": [0 for i in range(0, 12)]},
                        {"label": "PENGANGA", "data": [0 for i in range(0, 12)]},
                        {"label": "MUNGOLI", "data": [0 for i in range(0, 12)]},
                        {"label": "NEELJAY", "data": [0 for i in range(0, 12)]},
                    ],
                }
            }

        output = Gmrdata.objects().aggregate(basePipeline)
        outputDict = {}

        for data in output:
            if "_id" in data:
                ts = data["_id"]["ts"]
                mine = data["_id"]["mine"]
                # console_logger.debug(ts)
                data_list = data.get('data', [])
                sum_list = []
                for item in data_list:
                    if item is not None:
                        try:
                            sum_value = float(item)
                            sum_list.append(sum_value)
                        except ValueError:
                            pass
                    else:
                        sum_list.append(0)
                    
                if ts not in outputDict:
                    outputDict[ts] = {mine: sum_list}
                else:
                    if mine not in outputDict[ts]:
                        outputDict[ts][mine] = sum_list
                    else:
                        outputDict[ts][mine].append(sum_list)

        modified_labels = [i for i in range(len(result["data"]["labels"]))]

        for index, label in enumerate(result["data"]["labels"]):
            if type == "Week":
                modified_labels = [
                    (
                        basePipeline[0]["$match"]["GWEL_Tare_Time"]["$gte"]
                        + datetime.timedelta(days=i + 1)
                    ).strftime("%d-%m-%Y,%a")
                    for i in range(1, 8)
                ]
            
            elif type == "Month":
                modified_labels = [
                    (
                        basePipeline[0]["$match"]["GWEL_Tare_Time"]["$gte"]
                        + datetime.timedelta(days=i + 1)
                    ).strftime("%d/%m")
                    for i in range(-1, (int(end_label))-1)
                ]

            elif type == "Year":
                modified_labels = [
                    (
                        basePipeline[0]["$match"]["GWEL_Tare_Time"]["$gte"]
                        + relativedelta(months=i)
                    ).strftime("%b %y")
                    for i in range(0, 12)
                ]
            if int(label) in outputDict:
                for key, val in outputDict[int(label)].items():

                    total_sum = sum(val)

                    if key == "YEKONA":
                        result["data"]["datasets"][0]["data"][index] = total_sum

                    if key == "SASTI":
                        result["data"]["datasets"][1]["data"][index] = total_sum

                    if key == "PENGANGA":
                        result["data"]["datasets"][2]["data"][index] = total_sum

                    if key == "MUNGOLI":
                        result["data"]["datasets"][3]["data"][index] = total_sum

                    if key == "NEELJAY":
                        result["data"]["datasets"][4]["data"][index] = total_sum

        result["data"]["labels"] = copy.deepcopy(modified_labels)
        # console_logger.debug(f"-------- Road Minewise Graph Response -------- {result}")
        return result
    
    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e



@router.get("/minewise_road_table", tags=["Road Map"])
def gmr_table(response: Response, currentPage: Optional[int] = None,
                perPage: Optional[int] = None,
                date: Optional[str] = None,
                mine: Optional[str] = "All",
                start_timestamp: Optional[str] = None,
                end_timestamp: Optional[str] = None,
                type: Optional[str] = "display"):
    try:
        data = {}
        result = {        
                "labels": [],
                "datasets": [],
                "weight_total":[],
                "total" : 0,
                "page_size": 15
        }

        if type and type == "display":

            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage

            if date:
                end =f'{date} 23:59:59'
                start = f'{date} 00:00:00'
                
                data["created_at__gte"] = convert_to_utc_format(start, "%Y-%m-%d %H:%M:%S")
                data["created_at__lte"] = convert_to_utc_format(end, "%Y-%m-%d %H:%M:%S")

            if start_timestamp:
                data["created_at__gte"] = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")

            if end_timestamp:
                data["created_at__lte"] = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")

            if mine and mine != "All":
                data["mine__icontains"] = mine.upper()
            
            offset = (page_no - 1) * page_len

            logs = (
                Gmrdata.objects(**data)
                .order_by("mine", "arv_cum_do_number", "-created_at")
                .skip(offset)
                .limit(page_len)
            )

            overall_totals = {
                "Challan_Gross_Wt(MT)": 0,
                "Challan_Tare_Wt(MT)": 0,
                "Challan_Net_Wt(MT)": 0,
                "GWEL_Gross_Wt(MT)": 0,
                "GWEL_Tare_Wt(MT)": 0,
                "GWEL_Net_Wt(MT)": 0,
            }
            mine_grouped_data = {}

            if any(logs):
                for log in logs:
                    payload = log.payload()
                    result["labels"] = list(payload.keys())
                    mine_name = payload.get("Mines_Name")

                    if mine_name not in mine_grouped_data:
                        mine_grouped_data[mine_name] = []

                    mine_grouped_data[mine_name].append(payload)

                    for key in overall_totals:
                        value = payload.get(key)
                        if value is None:
                            value = 0.0
                        else:
                            try:
                                value = float(value)
                            except ValueError:
                                value = 0.0
                        
                        overall_totals[key] += value

                overall_totals = {key: str(overall_totals[key]) for key in overall_totals}

                for mine_name, records in mine_grouped_data.items():
                    result["datasets"].append({mine_name: records})
                result["weight_total"].append(overall_totals)

            result["total"] = Gmrdata.objects(**data).count()
            return result

        elif type and type == "download":
            del type

            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            if start_timestamp:
                data["created_at__gte"] = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")

            if end_timestamp:
                data["created_at__lte"] = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")

            if mine and mine != "All":
                data["mine__icontains"] = mine.upper()

            usecase_data = Gmrdata.objects(**data).order_by("-created_at")
            count = len(usecase_data)
            path = None
            logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
            if usecase_data:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "Minewise_Report_{}.xlsx".format(
                            datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format()
                    cell_format2.set_bold()
                    cell_format2.set_font_size(10)
                    cell_format2.set_align("center")
                    cell_format2.set_align("vcenter")
                    cell_format2.set_text_wrap(True)

                    header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
                    date_format = workbook.add_format({'align': 'left', 'font_size': 12, "bold": True})
                    report_name_format = workbook.add_format({'align': 'left', 'font_size': 12, "bold": True})


                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)
                    cell_format = workbook.add_format()
                    cell_format.set_font_size(10)
                    cell_format.set_align("center")
                    cell_format.set_align("vcenter")
                    cell_format.set_text_wrap(True)


                    worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                    
                    # Merge cells for the main header and place it in the center
                    main_header = "GMR Warora Energy Limited"  # Set your main header text here
                    worksheet.merge_range("A1:O1", main_header, header_format)  # Merge cells A1 to H1 for the header
                    
                    # Write the current date on the left side (A2)
                    worksheet.write("A2", f"Date: {datetime.datetime.now().strftime('%d-%m-%Y')}", date_format)
                    worksheet.merge_range("C2:H2", f"Report Name: Mine Wise Table", report_name_format)


                    headers = [
                        "Sr.No",
                        "PO No",
                        "DO No",
                        "Mines Name",
                        "Vehicle No.",
                        "Total Net Amount",
                        "Gross Wt. as per challan (MT)",
                        "Tare Wt. as per challan (MT)",
                        "Net Wt. as per challan (MT)",
                        "Gross Wt. as per actual (MT)",
                        "Tare Wt. as per actual (MT)",
                        "Net Wt. as per actual (MT)",
                        "Vehicle In Time",
                        "Transit Loss",
                        "LOT",
                        "Line Item"
                    ]

                    for index, header in enumerate(headers):
                        worksheet.write(2, index, header, cell_format2)
                    for row, query in enumerate(usecase_data,start=3):
                        result = query.payload()
                        worksheet.write(row, 0, count, cell_format)
                        worksheet.write(row, 1, str(result["PO_No"]), cell_format)
                        worksheet.write(row, 2, str(result["DO_No"]), cell_format)
                        worksheet.write(row, 3, str(result["Mines_Name"]), cell_format)
                        worksheet.write(row, 4, str(result["vehicle_number"]), cell_format)
                        worksheet.write(row, 5, str(result["Total_net_amount"]), cell_format)
                        worksheet.write(row, 6, str(result["Challan_Gross_Wt(MT)"]), cell_format)
                        worksheet.write(row, 7, str(result["Challan_Tare_Wt(MT)"]), cell_format)
                        worksheet.write(row, 8, str(result["Challan_Net_Wt(MT)"]), cell_format)
                        worksheet.write(row, 9, str(result["GWEL_Gross_Wt(MT)"]), cell_format)
                        worksheet.write(row, 10, str(result["GWEL_Tare_Wt(MT)"]), cell_format)
                        worksheet.write(row, 11, str(result["GWEL_Net_Wt(MT)"]), cell_format)
                        worksheet.write(row, 12, str(result["Vehicle_in_time"]), cell_format)
                        worksheet.write(row, 13, str(result["Transit_Loss"]), cell_format)
                        worksheet.write(row, 14, str(result["LOT"]), cell_format)
                        worksheet.write(row, 15, str(result["Line_Item"]), cell_format)
                        count-=1
                        
                    workbook.close()
                    console_logger.debug("Successfully {} report generated".format(service_id))
                    console_logger.debug("sent data {}".format(path))

                    return {
                            "Type": "Minewise_road_journey_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                            }
                
                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            else:
                console_logger.error("No data found")
                return {
                        "Type": "Minewise_road_journey_download_event",
                        "Datatype": "Report",
                        "File_Path": path,
                        }

    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/road/vehicle_scanned_count", tags=["Road Map"])
def daywise_vehicle_scanned_count(response:Response):
    try:
        current_time = datetime.datetime.now(IST)
        today = current_time.date()
        startdate = f'{today} 00:00:00'
        # from_ts = datetime.datetime.strptime(startdate,"%Y-%m-%d %H:%M:%S")
        from_ts = convert_to_utc_format(startdate, "%Y-%m-%d %H:%M:%S")

        vehicle_count = Gmrdata.objects(GWEL_Tare_Time__ne=None,
                                        GWEL_Tare_Time__gte=from_ts).count()

        return {"title": "Today's Mine Vehicle Scanned",
                "icon" : "vehicle",
                "data": vehicle_count,
                "last_updated": today}

    except Exception as e:
        console_logger.debug("----- Vehicle Scanned Count Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/road/vehicle_count", tags=["Road Map"])
def daywise_vehicle_count(response:Response):
    try:
        current_time = datetime.datetime.now(IST)
        today = current_time.date()
        startdate = f'{today} 00:00:00'
        # from_ts = datetime.datetime.strptime(startdate,"%Y-%m-%d %H:%M:%S")
        from_ts = convert_to_utc_format(startdate, "%Y-%m-%d %H:%M:%S")

        vehicle_in_count = Gmrdata.objects(GWEL_Tare_Time__ne=None,
                                           GWEL_Tare_Time__gte=from_ts).count()
        
        vehicle_out_count = Gmrdata.objects(GWEL_Tare_Time__ne=None,
                                            GWEL_Tare_Time__gte=from_ts).count()

        return {"title": "Today's Gate Vehicle",
                "icon" : "vehicle",
                "data":f"In: {vehicle_in_count} | Out: {vehicle_out_count}",
                "last_updated": today}

    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/road/grn_coal", tags=["Road Map"])
def daywise_grn_receive(response:Response):
    try:
        current_time = datetime.datetime.now(IST)
        today = current_time.date()
        startdate = f'{today} 00:00:00'
        # from_ts = datetime.datetime.strptime(startdate,"%Y-%m-%d %H:%M:%S")
        from_ts = convert_to_utc_format(startdate, "%Y-%m-%d %H:%M:%S")
        pipeline = [
                    {
                        "$match": {
                            "GWEL_Tare_Time": {"$gte": from_ts},
                            "net_qty": {"$ne": None}
                        }
                    },
                    {
                        "$group": {
                            "_id": None,
                            "total_net_qty": {
                                "$sum": {
                                    "$toDouble": "$net_qty"
                                }
                            }
                        }
                    }]
        
        result = Gmrdata.objects.aggregate(pipeline)

        total_coal = 0
        for doc in result:
            total_coal = doc["total_net_qty"]

        return {"title": "Today's Total GRN Coal(MT)",
                "icon" : "coal",
                "data": round(total_coal,2),
                "last_updated": today}

    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e
    

@router.get("/road/transit_loss_card", tags=["Road Map"])
def daywise_transit_loss(response:Response):
    try:
        current_time = datetime.datetime.now(IST)
        today = current_time.date()
        startdate = f'{today} 00:00:00'
        # from_ts = datetime.datetime.strptime(startdate,"%Y-%m-%d %H:%M:%S")
        from_ts = convert_to_utc_format(startdate, "%Y-%m-%d %H:%M:%S")
    
        pipeline = [
            {
                '$match': {
                    'GWEL_Tare_Time': {
                        '$gte': from_ts
                    }
                }
            }, {
                '$group': {
                    '_id': None, 
                    'net_qty': {
                        '$sum': {
                            '$toDouble': '$net_qty'
                        }
                    }, 
                    'actual_net_qty': {
                        '$sum': {
                            '$toDouble': '$actual_net_qty'
                        }
                    }
                }
            }, {
                '$project': {
                    'net_qty': 1, 
                    'actual_net_qty': 1, 
                    'transit_loss': {
                        '$subtract': [
                            '$actual_net_qty', '$net_qty'
                        ]
                    }
                }
            }
        ]
        
        result = Gmrdata.objects.aggregate(pipeline)

        transit_loss = 0
        for doc in result:
            transit_loss = doc["transit_loss"]

        return {"title": "Today's Total Transit Loss (MT)",
                "icon" : "coal",
                "data": round(transit_loss, 2),
                "last_updated": today}

    except Exception as e:
        console_logger.debug("----- Transit Loss Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/road/gwel_coal", tags=["Road Map"])
def daywise_gwel_receive(response:Response):
    try:
        current_time = datetime.datetime.now(IST)
        today = current_time.date()
        startdate = f'{today} 00:00:00'
        # from_ts = datetime.datetime.strptime(startdate,"%Y-%m-%d %H:%M:%S")
        from_ts = convert_to_utc_format(startdate, "%Y-%m-%d %H:%M:%S")


        pipeline = [
                    {
                        "$match": {
                            "GWEL_Tare_Time": {"$gte": from_ts},
                            "actual_net_qty": {"$ne": None}
                        }
                    },
                    {
                        "$group": {
                            "_id": None,
                            "total_actual_net_qty": {
                                "$sum": {
                                    "$toDouble": "$actual_net_qty"
                                }
                            }
                        }
                    }]
        
        result = Gmrdata.objects.aggregate(pipeline)

        total_coal = 0
        for doc in result:
            total_coal = doc["total_actual_net_qty"]

        return {"title": "Today's Total GWEL Coal(MT)",
                "icon" : "coal",
                "data": round(total_coal,2),
                "last_updated": today}

    except Exception as e:
        console_logger.debug("----- Total GWEL Coal Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/road/unit1_coal_generation", tags=["Road Map"])
def daywise_unit1_generation(response: Response):
    try:
        current_time = datetime.datetime.now(IST)
        today = current_time.date()
        startdate = datetime.datetime.strptime(f'{today} 00:00:00', "%Y-%m-%d %H:%M:%S")

        pipeline = [
            {
                "$match": {
                    "created_date": {"$gte": startdate},
                    "tagid": 2,
                    "sum": {"$ne": None}
                }
            },
            {
                "$sort": {
                    "created_date": -1
                }
            },
            {
                "$group": {
                    "_id": {
                        "tagid": "$tagid",
                        "created_date": {
                            "$dateToString": {
                                "format": "%Y-%m-%d %H:%M:%S",
                                "date": "$created_date"
                            }
                        }
                    },
                    "latest_sum": {
                        "$first": {
                            "$toDouble": "$sum"
                        }
                    },
                    "count": {"$first": 1}
                }
            },
            {
                "$group": {
                    "_id": "$_id.tagid",
                    "total_sum": { "$sum": "$latest_sum" },
                    "count": { "$sum": "$count" }
                }
            }
        ]

        result = Historian.objects.aggregate(pipeline)

        total_sum = 0
        count = 1
        for doc in result:
            count = doc["count"]
            total_sum = doc["total_sum"]

        result = total_sum / count if count > 0 else 0

        return {
            "title": "Today's Unit 1 Average Generation (MW)",
            "icon": "energy",
            "data": round(result, 2),
            "last_updated": today
        }

    except Exception as e:
        console_logger.debug(f"----- Unit 1 Generation Error ----- {e}")
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return {"error": str(e)}


@router.get("/road/unit2_coal_generation", tags=["Road Map"])
def daywise_unit2_generation(response: Response):
    try:
        current_time = datetime.datetime.now(IST)
        today = current_time.date()
        startdate = datetime.datetime.strptime(f'{today} 00:00:00',"%Y-%m-%d %H:%M:%S")

        # pipeline = [
        #     {
        #         "$match": {
        #             "created_date": {"$gte": startdate},
        #             "tagid": 3536,
        #             "sum": {"$ne": None}
        #         }
        #     },
        #     {
        #         "$group": {
        #             "_id": None,
        #             "total_sum": {
        #                 "$sum": {
        #                     "$toDouble": "$sum"
        #                 }
        #             },
        #             "count": {"$sum": 1}
        #         }
        #     }
        # ]

        pipeline = [
            {
                "$match": {
                    "created_date": {"$gte": startdate},
                    "tagid": 3536,
                    "sum": {"$ne": None}
                }
            },
            {
                "$sort": {
                    "created_date": -1
                }
            },
            {
                "$group": {
                    "_id": {
                        "tagid": "$tagid",
                        "created_date": {
                            "$dateToString": {
                                "format": "%Y-%m-%d %H:%M:%S",
                                "date": "$created_date"
                            }
                        }
                    },
                    "latest_sum": {
                        "$first": {
                            "$toDouble": "$sum"
                        }
                    },
                    "count": {"$first": 1}
                }
            },
            {
                "$group": {
                    "_id": "$_id.tagid",
                    "total_sum": { "$sum": "$latest_sum" },
                    "count": { "$sum": "$count" }
                }
            }
        ]

        result = Historian.objects.aggregate(pipeline)
        
        total_sum = 0
        count = 1
        for doc in result:
            count = doc["count"]
            total_sum = doc["total_sum"]
        
        result = total_sum / count

        return {
            "title": "Today's Unit 2 Average Generation(MW)",
            "icon" : "energy",
            "data": round(result, 2),
            "last_updated": today
        }

    except Exception as e:
        console_logger.debug(f"----- Unit 2 Generation Error -----{e}")
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return {"error": str(e)}


@router.get("/road/unit1_coal_consumption", tags=["Road Map"])
def daywise_unit1_consumption(response: Response):
    try:
        current_time = datetime.datetime.now(IST)
        today = current_time.date()
        startdate = datetime.datetime.strptime(f'{today} 00:00:00',"%Y-%m-%d %H:%M:%S")

        # pipeline = [
        #     {
        #         "$match": {
        #             "created_date": {"$gte": startdate},
        #             "tagid": 16,
        #             "sum": {"$ne": None}
        #         }
        #     },
        #     {
        #         "$group": {
        #             "_id": "$tagid",
        #             "total_sum": {
        #                 "$sum": {
        #                     "$toDouble": "$sum"
        #                 }
        #             },
        #             "count": {"$sum":1}
        #         }
        #     }
        # ]

        pipeline = [
            {
                "$match": {
                    "created_date": {"$gte": startdate},
                    "tagid": 16,
                    "sum": {"$ne": None}
                }
            },
            {
                "$sort": {
                    "created_date": -1
                }
            },
            {
                "$group": {
                    "_id": {
                        "tagid": "$tagid",
                        "created_date": {
                            "$dateToString": {
                                "format": "%Y-%m-%d %H:%M:%S",
                                "date": "$created_date"
                            }
                        }
                    },
                    "latest_sum": {
                        "$first": {
                            "$toDouble": "$sum"
                        }
                    },
                    "count": {"$first": 1}
                }
            },
            {
                "$group": {
                    "_id": "$_id.tagid",
                    "total_sum": { "$sum": "$latest_sum" },
                    "count": { "$sum": "$count" }
                }
            }
        ]

        result = Historian.objects.aggregate(pipeline)

        total_sum = 0
        count = 1
        for doc in result:
            count = doc["count"]
            total_sum = doc["total_sum"]
        
        result = total_sum / count

        return {
            "title": "Today's Unit 1 Coal Consumption(MT)",
            "icon" : "coal",
            "data": round(result, 2),
            "last_updated": today
        }

    except Exception as e:
        console_logger.debug(f"----- Unit 1 Consumption Error ----- {e}")
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return {"error": str(e)}


@router.get("/road/unit2_coal_consumption", tags=["Road Map"])
def daywise_unit2_consumption(response: Response):
    try:
        current_time = datetime.datetime.now(IST)
        today = current_time.date()
        startdate = datetime.datetime.strptime(f'{today} 00:00:00',"%Y-%m-%d %H:%M:%S")

        # pipeline = [
        #     {
        #         "$match": {
        #             "created_date": {"$gte": startdate},
        #             "tagid": 3538,
        #             "sum": {"$ne": None}
        #         }
        #     },
        #     {
        #         "$group": {
        #             "_id": "$tagid",
        #             "total_sum": {
        #                 "$sum": {
        #                     "$toDouble": "$sum"
        #                 }
        #             },
        #             "count": {"$sum":1}
        #         }
        #     }
        # ]

        pipeline = [
            {
                "$match": {
                    "created_date": {"$gte": startdate},
                    "tagid": 3538,
                    "sum": {"$ne": None}
                }
            },
            {
                "$sort": {
                    "created_date": -1
                }
            },
            {
                "$group": {
                    "_id": {
                        "tagid": "$tagid",
                        "created_date": {
                            "$dateToString": {
                                "format": "%Y-%m-%d %H:%M:%S",
                                "date": "$created_date"
                            }
                        }
                    },
                    "latest_sum": {
                        "$first": {
                            "$toDouble": "$sum"
                        }
                    },
                    "count": {"$first": 1}
                }
            },
            {
                "$group": {
                    "_id": "$_id.tagid",
                    "total_sum": { "$sum": "$latest_sum" },
                    "count": { "$sum": "$count" }
                }
            }
        ]       

        result = Historian.objects.aggregate(pipeline)

        total_sum = 0
        count=1
        for doc in result:
            count = doc["count"]
            total_sum = doc["total_sum"]
        
        result = total_sum / count

        return {
            "title": "Today's Unit 2 Coal Consumption(MT)",
            "icon" : "coal",
            "data": round(result, 2),
            "last_updated": today
        }

    except Exception as e:
        console_logger.debug(f"----- Unit 2 Consumption Error ----- {e}")
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return {"error": str(e)}


@router.get("/road/minewise_road_report", tags=["Road Map"])
def road_report(response:Response,start_timestamp: Optional[str] = None,
                end_timestamp: Optional[str] = None,
                type: Optional[str] = "display"):
    try:
        current_time = datetime.datetime.now(IST)
        today = current_time.date()
        startdate = f'{today} 00:00:00'
        # from_ts = datetime.datetime.strptime(startdate,"%Y-%m-%d %H:%M:%S")
        from_ts = convert_to_utc_format(startdate, "%Y-%m-%d %H:%M:%S")


        data = {"data": {}, "Total": {"mine_vehicle_scanned": 0, 
                                      "Gate_vehicle_in":0,
                                      "Gate_vehicle_out":0,
                                      "GRN_Coal(MT)": 0, 
                                      "GWEL_Coal(MT)": 0, 
                                      "Transit_Loss(MT)": 0}}

        pipeline = [
            {
                "$facet": {
                    "weight": [
                        {
                            "$match": {
                                "GWEL_Tare_Time": {"$gte": from_ts}
                            }
                        },
                        {
                            "$group": {
                                "_id": "$mine",
                                "net_qty": {
                                    "$sum": {
                                        "$toDouble": "$net_qty"
                                    }
                                },
                                "actual_net_qty": {
                                    "$sum": {
                                        "$toDouble": "$actual_net_qty"
                                    }
                                }
                            }
                        },
                        {
                            "$project": {
                                "net_qty": 1,
                                "actual_net_qty": 1,
                                "transit": {
                                    "$subtract": [
                                        "$actual_net_qty", "$net_qty"
                                    ]
                                }
                            }
                        }
                    ],
                    "scanned": [
                        {
                            "$match": {
                                "GWEL_Tare_Time": {"$gte": from_ts}
                            }
                        },
                        {
                            "$group": {
                                "_id": "$mine",
                                "vehicle_count": {
                                    "$sum": 1
                                }
                            }
                        },
                        {
                            "$project": {
                                "vehicle_count": "$vehicle_count"
                            }
                        }
                    ],
                    "vehicle_in": [
                        {
                            "$match": {
                                "GWEL_Tare_Time": {"$gte": from_ts}
                            }
                        },
                        {
                            "$group": {
                                "_id": "$mine",
                                "vehicle_count": {
                                    "$sum": 1
                                }
                            }
                        },
                        {
                            "$project": {
                                "mine": "$_id",
                                "vehicle_in_count": "$vehicle_count",
                                "_id": 0
                            }
                        }
                    ],
                    "vehicle_out": [
                        {
                            "$match": {
                                "GWEL_Tare_Time": {"$gte": from_ts}
                            }
                        },
                        {
                            "$group": {
                                "_id": "$mine",
                                "vehicle_count": {
                                    "$sum": 1
                                }
                            }
                        },
                        {
                            "$project": {
                                "mine": "$_id",
                                "vehicle_out_count": "$vehicle_count",
                                "_id": 0
                            }
                        }
                    ]
                }
            },
            {
                "$project": {
                    "weight": 1,
                    "scanned": 1,
                    "vehicle_in": 1,
                    "vehicle_out": 1
                }
            }
        ]

        if type == "display":
            if start_timestamp:
                end_date = f'{start_timestamp} 23:59:59'
                start_date = f'{start_timestamp} 00:00:00'
                format_data = "%Y-%m-%d %H:%M:%S"

                startd_date = convert_to_utc_format(start_date, format_data)
                endd_date = convert_to_utc_format(end_date, format_data)

                pipeline[0]["$facet"]["weight"][0]["$match"]["GWEL_Tare_Time"]["$lte"] = endd_date
                pipeline[0]["$facet"]["weight"][0]["$match"]["GWEL_Tare_Time"]["$gte"] = startd_date

                pipeline[0]["$facet"]["scanned"][0]["$match"]["GWEL_Tare_Time"]["$lte"] = endd_date
                pipeline[0]["$facet"]["scanned"][0]["$match"]["GWEL_Tare_Time"]["$gte"] = startd_date

                pipeline[0]["$facet"]["vehicle_in"][0]["$match"]["GWEL_Tare_Time"]["$lte"] = endd_date
                pipeline[0]["$facet"]["vehicle_in"][0]["$match"]["GWEL_Tare_Time"]["$gte"] = startd_date

                pipeline[0]["$facet"]["vehicle_out"][0]["$match"]["GWEL_Tare_Time"]["$lte"] = endd_date
                pipeline[0]["$facet"]["vehicle_out"][0]["$match"]["GWEL_Tare_Time"]["$gte"] = startd_date

            combined_pipeline_data = list(Gmrdata.objects.aggregate(pipeline))[0]
            # mine_data = list(Gmrdata.objects.aggregate(mine_pipeline))

            weight_data = combined_pipeline_data.get("weight", [])
            scanned_data = combined_pipeline_data.get("scanned", [])
            vehicle_in_data = combined_pipeline_data.get("vehicle_in", [])
            vehicle_out_data = combined_pipeline_data.get("vehicle_out", [])

            scanned_dict = {item["_id"]: item["vehicle_count"] for item in scanned_data}
            vehicle_in_dict = {item["mine"]: item["vehicle_in_count"] for item in vehicle_in_data}
            vehicle_out_dict = {item["mine"]: item["vehicle_out_count"] for item in vehicle_out_data}

            for mine in weight_data:
                mine_name = mine["_id"]
                net_qty = mine["net_qty"]
                actual_net_qty = mine["actual_net_qty"]
                transit_loss = mine["transit"]

                scanned_count = scanned_dict.get(mine_name, 0)
                vehicle_in_count = vehicle_in_dict.get(mine_name, 0)
                vehicle_out_count = vehicle_out_dict.get(mine_name, 0)

                data["data"][mine_name] = {
                    "mine_vehicle_scanned": scanned_count,
                    "Gate_vehicle_in": vehicle_in_count,
                    "Gate_vehicle_out": vehicle_out_count,
                    "GRN_Coal(MT)": round(net_qty, 2),
                    "GWEL_Coal(MT)": round(actual_net_qty, 2),
                    "Transit_Loss(MT)": round(transit_loss, 2)
                }

                data["Total"]["mine_vehicle_scanned"] += scanned_count
                data["Total"]["Gate_vehicle_in"] += vehicle_in_count
                data["Total"]["Gate_vehicle_out"] += vehicle_out_count
                data["Total"]["GRN_Coal(MT)"] += round(net_qty, 2)
                data["Total"]["GWEL_Coal(MT)"] += round(actual_net_qty, 2)
                data["Total"]["Transit_Loss(MT)"] += round(transit_loss, 2)


            return data

        elif type and type == "download":
            del type

            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "mine_count_Report_{}.xlsx".format(
                            datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )

            if start_timestamp and end_timestamp:
                end_date = f'{start_timestamp} 23:59:59'
                start_date = f'{start_timestamp} 00:00:00'
                format_data = "%Y-%m-%d %H:%M:%S"

                startd_date = convert_to_utc_format(start_date, format_data)
                endd_date = convert_to_utc_format(end_date, format_data)

                pipeline[0]["$facet"]["weight"][0]["$match"]["GWEL_Tare_Time"]["$lte"] = endd_date
                pipeline[0]["$facet"]["weight"][0]["$match"]["GWEL_Tare_Time"]["$gte"] = startd_date

                pipeline[0]["$facet"]["scanned"][0]["$match"]["GWEL_Tare_Time"]["$lte"] = endd_date
                pipeline[0]["$facet"]["scanned"][0]["$match"]["GWEL_Tare_Time"]["$gte"] = startd_date

                pipeline[0]["$facet"]["vehicle_in"][0]["$match"]["GWEL_Tare_Time"]["$lte"] = endd_date
                pipeline[0]["$facet"]["vehicle_in"][0]["$match"]["GWEL_Tare_Time"]["$gte"] = startd_date

                pipeline[0]["$facet"]["vehicle_out"][0]["$match"]["GWEL_Tare_Time"]["$lte"] = endd_date
                pipeline[0]["$facet"]["vehicle_out"][0]["$match"]["GWEL_Tare_Time"]["$gte"] = startd_date

            combined_pipeline_data = list(Gmrdata.objects.aggregate(pipeline))[0]
            # mine_data = list(Gmrdata.objects.aggregate(mine_pipeline))

            weight_data = combined_pipeline_data.get("weight", [])
            scanned_data = combined_pipeline_data.get("scanned", [])
            vehicle_in_data = combined_pipeline_data.get("vehicle_in", [])
            vehicle_out_data = combined_pipeline_data.get("vehicle_out", [])

            scanned_dict = {item["_id"]: item["vehicle_count"] for item in scanned_data}
            vehicle_in_dict = {item["mine"]: item["vehicle_in_count"] for item in vehicle_in_data}
            vehicle_out_dict = {item["mine"]: item["vehicle_out_count"] for item in vehicle_out_data}

            for mine in weight_data:
                mine_name = mine["_id"]
                net_qty = mine["net_qty"]
                actual_net_qty = mine["actual_net_qty"]
                transit_loss = mine["transit"]

                scanned_count = scanned_dict.get(mine_name, 0)
                vehicle_in_count = vehicle_in_dict.get(mine_name, 0)
                vehicle_out_count = vehicle_out_dict.get(mine_name, 0)

                data["data"][mine_name] = {
                    "mine_vehicle_scanned": scanned_count,
                    "Gate_vehicle_in": vehicle_in_count,
                    "Gate_vehicle_out": vehicle_out_count,
                    "GRN_Coal(MT)": round(net_qty, 2),
                    "GWEL_Coal(MT)": round(actual_net_qty, 2),
                    "Transit_Loss(MT)": round(transit_loss, 2)
                }

                data["Total"]["mine_vehicle_scanned"] += scanned_count
                data["Total"]["Gate_vehicle_in"] += vehicle_in_count
                data["Total"]["Gate_vehicle_out"] += vehicle_out_count
                data["Total"]["GRN_Coal(MT)"] += round(net_qty, 2)
                data["Total"]["GWEL_Coal(MT)"] += round(actual_net_qty, 2)
                data["Total"]["Transit_Loss(MT)"] += round(transit_loss, 2)

            df_data = pd.DataFrame.from_dict(data['data'], orient='index')

            total_df = pd.DataFrame.from_dict({'Total': data['Total']}).T
            df_data = pd.concat([df_data, total_df], axis=0)
            df_data.columns = df_data.columns.str.replace('_', ' ')

            df_data.to_excel(path, sheet_name='Report')

            console_logger.debug("Successfully {} report generated".format(service_id))
            console_logger.debug("sent data {}".format(path))

            return {
                    "Type": "Minewise_report_download_event",
                    "Datatype": "Report",
                    "File_Path": path,
                    }
        
        else:
            console_logger.error("No data found")
            return {
                    "Type": "Minewise_report_download_event",
                    "Datatype": "Report",
                    "File_Path": path,
                    }

    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


def daywise_in_vehicle_count_datewise(date):
    try:
        startdate = f'{date} 00:00:00'
        enddate = f'{date} 23:59:59'
        # from_ts = datetime.datetime.strptime(startdate,"%Y-%m-%d %H:%M:%S")
        # to_ts = datetime.datetime.strptime(enddate,"%Y-%m-%d %H:%M:%S")
        from_ts = convert_to_utc_format(startdate, "%Y-%m-%d %H:%M:%S")
        to_ts = convert_to_utc_format(enddate, "%Y-%m-%d %H:%M:%S")
        

        # vehicle_count = Gmrdata.objects(GWEL_Tare_Time__ne=None, 
        #                                 GWEL_Tare_Time__gte=from_ts, 
        #                                 GWEL_Tare_Time__lte=to_ts).count()
        
        vehicle_count = Gmrdata.objects(
                Q(GWEL_Tare_Time__ne=None, GWEL_Tare_Time__gte=from_ts, GWEL_Tare_Time__lte=to_ts)
                # Q(GWEL_Tare_Time=None, vehicle_in_time__gte=from_ts, vehicle_in_time__lte=to_ts)  # OR condition
            ).count()


        return {"title": "Vehicle in count",
                "data": vehicle_count}
    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


def daywise_in_vehicle_count_datewise_new(date):
    try:
        startdate = f'{date} 00:00:00'
        enddate = f'{date} 23:59:59'
        # from_ts = datetime.datetime.strptime(startdate,"%Y-%m-%d %H:%M:%S")
        # to_ts = datetime.datetime.strptime(enddate,"%Y-%m-%d %H:%M:%S")
        from_ts = convert_to_utc_format(startdate, "%Y-%m-%d %H:%M:%S")
        to_ts = convert_to_utc_format(enddate, "%Y-%m-%d %H:%M:%S")

        # vehicle_count = Gmrdata.objects(GWEL_Tare_Time__ne=None, 
        #                                 GWEL_Tare_Time__gte=from_ts, 
        #                                 GWEL_Tare_Time__lte=to_ts).count()
        
        vehicle_count = Gmrdatanew.objects(
                Q(GWEL_Tare_Time__ne=None, GWEL_Tare_Time__gte=from_ts, GWEL_Tare_Time__lte=to_ts)
                # Q(GWEL_Tare_Time=None, vehicle_in_time__gte=from_ts, vehicle_in_time__lte=to_ts)  # OR condition
            ).count()

        console_logger.debug(vehicle_count)

        return {"title": "Vehicle in count",
                "data": vehicle_count}
    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


def daywise_grn_receive_datewise(date):
    try:
        startdate = f'{date} 00:00:00'
        enddate = f'{date} 23:59:59'

        from_ts = convert_to_utc_format(startdate, "%Y-%m-%d %H:%M:%S")
        to_ts = convert_to_utc_format(enddate, "%Y-%m-%d %H:%M:%S")

        # console_logger.debug(from_ts)
        # console_logger.debug(to_ts)
        created_at_date = datetime.datetime(2024, 9, 23, 19, 50, 51, 572000)
        pipeline = [
                    {
                        '$match': {
                            'created_at': {
                                '$gt': created_at_date,
                            }
                        }
                    },
                    {
                        "$match": {
                            "GWEL_Tare_Time": {'$ne': None, "$gte": from_ts, "$lte": to_ts},
                                "net_qty": {"$ne": None}
                            }
                    },
                    {
                        "$group": {
                            "_id": None,
                            "total_net_qty": {
                                "$sum": {
                                    "$toDouble": "$net_qty"
                                }
                            }
                        }
                    }]
        HistoricPipeline = [
                    {
                        "$match": {
                            "GWEL_Tare_Time": {'$ne': None, "$gte": from_ts, "$lte": to_ts},
                                "net_qty": {"$ne": None}
                            }
                    },
                    {
                        "$group": {
                            "_id": None,
                            "total_net_qty": {
                                "$sum": {
                                    "$toDouble": "$net_qty"
                                }
                            }
                        }
                    }]
        # console_logger.debug(pipeline)

        # pipeline = [
        #     {
        #         "$match": {
        #             "$and": [
        #                 {
        #                     "$or": [
        #                         { "GWEL_Tare_Time": { "$gte": from_ts, "$lte": to_ts } },
        #                         # { "GWEL_Tare_Time": None, "vehicle_in_time": { "$gte": from_ts, "$lte": to_ts } }
        #                     ]
        #                 },
        #                 # { "net_qty": { "$ne": None } }
        #             ]
        #         }
        #     },
        #     {
        #         "$group": {
        #             "_id": None,
        #             "total_net_qty": {
        #                 "$sum": {
        #                     "$toDouble": "$net_qty"
        #                 }
        #             }
        #         }
        #     }
        # ]
        
        result = Gmrdata.objects.aggregate(pipeline)
        result1 = gmrdataHistoric.objects.aggregate(HistoricPipeline)

        total_coal = 0
        for doc in result:
            total_coal = doc.get("total_net_qty", 0)

        for doc in result1:
            total_coal += doc.get("total_net_qty", 0)

        return {"title": "Total GRN Coal(MT)",
                "data": round(total_coal,2)}

    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

# delete after matched value done by sachin bhai
def daywise_grn_receive_datewise_new(date):
    try:
        startdate = f'{date} 00:00:00'
        enddate = f'{date} 23:59:59'

        from_ts = convert_to_utc_format(startdate, "%Y-%m-%d %H:%M:%S")
        to_ts = convert_to_utc_format(enddate, "%Y-%m-%d %H:%M:%S")

        # console_logger.debug(from_ts)
        # console_logger.debug(to_ts)

        # pipeline = [
        #             {
        #                 "$match": {
        #                     "GWEL_Tare_Time": {"$gte": from_ts, "$lte": to_ts},
        #                         "net_qty": {"$ne": None}
        #                     }
        #             },
        #             {
        #                 "$group": {
        #                     "_id": None,
        #                     "total_net_qty": {
        #                         "$sum": {
        #                             "$toDouble": "$net_qty"
        #                         }
        #                     }
        #                 }
        #             }]

        pipeline = [
            {
                "$match": {
                    "$and": [
                        {
                            "$or": [
                                { "GWEL_Tare_Time": { "$gte": from_ts, "$lte": to_ts } },
                                # { "GWEL_Tare_Time": None, "vehicle_in_time": { "$gte": from_ts, "$lte": to_ts } }
                            ]
                        },
                        # { "net_qty": { "$ne": None } }
                    ]
                }
            },
            {
                "$group": {
                    "_id": None,
                    "total_net_qty": {
                        "$sum": {
                            "$toDouble": "$net_qty"
                        }
                    }
                }
            }
        ]

        
        result = Gmrdatanew.objects.aggregate(pipeline)

        total_coal = 0
        for doc in result:
            total_coal = doc["total_net_qty"]

        return {"title": "Total GRN Coal(MT)",
                "data": round(total_coal,2)}

    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


def daywise_gwel_receive_pdf_datewise(date):
    try:
        startdate = f'{date} 00:00:00'
        enddate = f'{date} 23:59:59'
        from_ts = convert_to_utc_format(startdate, "%Y-%m-%d %H:%M:%S")
        to_ts = convert_to_utc_format(enddate, "%Y-%m-%d %H:%M:%S")
        created_at_date = datetime.datetime(2024, 9, 23, 19, 50, 51, 572000)
        pipeline = [
            {
                '$match': {
                    'created_at': {
                        '$gt': created_at_date,
                    }
                }
            },
            {
                "$match": {
                    "GWEL_Tare_Time": {"$gte": from_ts, "$lte": to_ts},
                    "actual_net_qty": {"$ne": None}
                }
            },
            {
                "$group": {
                    "_id": None,
                    "total_actual_net_qty": {
                        "$sum": {
                            "$toDouble": "$actual_net_qty"
                        }
                    }
                }
            }
        ]

        historicpipeline = [
            {
                "$match": {
                    "GWEL_Tare_Time": {"$gte": from_ts, "$lte": to_ts},
                    "actual_net_qty": {"$ne": None}
                }
            },
            {
                "$group": {
                    "_id": None,
                    "total_actual_net_qty": {
                        "$sum": {
                            "$toDouble": "$actual_net_qty"
                        }
                    }
                }
            }
        ]
        # pipeline = [
        #     {
        #         "$match": {
        #             "GWEL_Tare_Time": {"$gte": from_ts, "$lte": to_ts},
        #             "actual_net_qty": {"$ne": None}
        #         }
        #     },
        #     {
        #         "$addFields": {
        #             "actual_net_qty": {
        #                 "$cond": {
        #                     "if": {"$isNumber": "$actual_net_qty"},
        #                     "then": "$actual_net_qty",
        #                     "else": 0
        #                 }
        #             }
        #         }
        #     },
        #     {
        #         "$group": {
        #             "_id": None,
        #             "total_actual_net_qty": {
        #                 "$sum": {
        #                     "$toDouble": {
        #                         "$ifNull": ["$actual_net_qty", 0]  # Handle NaN values
        #                     }
        #                 }
        #             }
        #         }
        #     }
        # ]
        
        result = Gmrdata.objects.aggregate(pipeline)
        result1 = gmrdataHistoric.objects.aggregate(historicpipeline)
        
        total_coal = 0
        for doc in result:
            total_coal += doc["total_actual_net_qty"]
        
        for doc in result1:
            total_coal += doc["total_actual_net_qty"]

        return {"title": "Total GWEL Coal(MT)",
                "data": round(total_coal, 2)}

    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return "Error occurred: {}".format(e)

# delete after matched value done by sachin bhai
def daywise_gwel_receive_pdf_datewise_new(date):
    try:
        startdate = f'{date} 00:00:00'
        enddate = f'{date} 23:59:59'
        from_ts = convert_to_utc_format(startdate, "%Y-%m-%d %H:%M:%S")
        to_ts = convert_to_utc_format(enddate, "%Y-%m-%d %H:%M:%S")
        pipeline = [
            {
                "$match": {
                    "GWEL_Tare_Time": {"$gte": from_ts, "$lte": to_ts},
                    "actual_net_qty": {"$ne": None}
                }
            },
            {
                "$group": {
                    "_id": None,
                    "total_actual_net_qty": {
                        "$sum": {
                            "$toDouble": "$actual_net_qty"
                        }
                    }
                }
            }
        ]
        # pipeline = [
        #     {
        #         "$match": {
        #             "GWEL_Tare_Time": {"$gte": from_ts, "$lte": to_ts},
        #             "actual_net_qty": {"$ne": None}
        #         }
        #     },
        #     {
        #         "$addFields": {
        #             "actual_net_qty": {
        #                 "$cond": {
        #                     "if": {"$isNumber": "$actual_net_qty"},
        #                     "then": "$actual_net_qty",
        #                     "else": 0
        #                 }
        #             }
        #         }
        #     },
        #     {
        #         "$group": {
        #             "_id": None,
        #             "total_actual_net_qty": {
        #                 "$sum": {
        #                     "$toDouble": {
        #                         "$ifNull": ["$actual_net_qty", 0]  # Handle NaN values
        #                     }
        #                 }
        #             }
        #         }
        #     }
        # ]
        
        result = Gmrdatanew.objects.aggregate(pipeline)

        total_coal = 0
        for doc in result:
            total_coal = doc["total_actual_net_qty"]

        return {"title": "Total GWEL Coal(MT)",
                "data": round(total_coal, 2)}

    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return "Error occurred: {}".format(e)


def daywise_out_vehicle_count_datewise(date):
    try:
        startdate = f'{date} 00:00:00'
        enddate = f'{date} 23:59:59'

        from_ts = convert_to_utc_format(startdate, "%Y-%m-%d %H:%M:%S")
        to_ts = convert_to_utc_format(enddate, "%Y-%m-%d %H:%M:%S")

        # vehicle_count = Gmrdata.objects(created_at__gte=from_ts, created_at__lte=to_ts, vehicle_out_time__ne=None).count()
        # vehicle_count = Gmrdata.objects(GWEL_Tare_Time__ne=None,
        #                                 GWEL_Tare_Time__gte=from_ts, 
        #                                 GWEL_Tare_Time__lte=to_ts).count()

        vehicle_count = Gmrdata.objects(
                Q(GWEL_Tare_Time__ne=None, GWEL_Tare_Time__gte=from_ts, GWEL_Tare_Time__lte=to_ts)
                # Q(GWEL_Tare_Time=None, vehicle_out_time__gte=from_ts, vehicle_out_time__lte=to_ts)  # OR condition
            ).count()
        

        return {"title": "Vehicle out count",
                "data": vehicle_count}

    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

# delete after matched value done by sachin bhai
def daywise_out_vehicle_count_datewise_new(date):
    try:
        startdate = f'{date} 00:00:00'
        enddate = f'{date} 23:59:59'

        from_ts = convert_to_utc_format(startdate, "%Y-%m-%d %H:%M:%S")
        to_ts = convert_to_utc_format(enddate, "%Y-%m-%d %H:%M:%S")

        # vehicle_count = Gmrdata.objects(created_at__gte=from_ts, created_at__lte=to_ts, vehicle_out_time__ne=None).count()
        # vehicle_count = Gmrdata.objects(GWEL_Tare_Time__ne=None,
        #                                 GWEL_Tare_Time__gte=from_ts, 
        #                                 GWEL_Tare_Time__lte=to_ts).count()

        vehicle_count = Gmrdatanew.objects(
                Q(GWEL_Tare_Time__ne=None, GWEL_Tare_Time__gte=from_ts, GWEL_Tare_Time__lte=to_ts) | 
                Q(GWEL_Tare_Time=None, vehicle_out_time__gte=from_ts, vehicle_out_time__lte=to_ts)  # OR condition
            ).count()
        

        return {"title": "Vehicle out count",
                "data": vehicle_count}

    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

@router.get("/testgraph")
def bar_graph_data(specified_date):
    try:
        if specified_date:
            
            specified_date = datetime.datetime.strptime(specified_date, "%Y-%m-%d")
            start_of_month = specified_date.replace(day=1)
            start_of_month = datetime.datetime.strftime(start_of_month, '%Y-%m-%d')
            end_of_month = datetime.datetime.strftime(specified_date, '%Y-%m-%d')

            # fetchCoalTesting = CoalTesting.objects(
            #     receive_date__gte= datetime.datetime.strptime(start_of_month, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), receive_date__lte= datetime.datetime.strptime(end_of_month, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M")
            # )

            fetchCoalTesting = RecieptCoalQualityAnalysis.objects(plant_analysis_date__gte = datetime.datetime.strptime(start_of_month, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), plant_analysis_date__lte = datetime.datetime.strptime(end_of_month, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), mode="Road")
            fetchCoalTestingTrain = RecieptCoalQualityAnalysis.objects(plant_analysis_date__gte = datetime.datetime.strptime(start_of_month, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), plant_analysis_date__lte = datetime.datetime.strptime(end_of_month, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), mode="Rail")
            
            # fetchCoalTestingTrain = CoalTestingTrain.objects(
            #     receive_date__gte = datetime.datetime.strptime(start_of_month, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), receive_date__lte= datetime.datetime.strptime(end_of_month, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M")
            # )
            # console_logger.debug()
            # fetchGmrData = Gmrdata.objects(created_at__gte=datetime.datetime.strptime(start_of_month, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), created_at__lte=datetime.datetime.strptime(end_of_month, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"))
            
            console_logger.debug(start_of_month)
            console_logger.debug(end_of_month)

            fetchGmrData = Gmrdata.objects(
                GWEL_Tare_Time__gte=f"{start_of_month}T00:00:00",
                GWEL_Tare_Time__lte=f"{end_of_month}T23:59:59"
            )
            fetchRailData = RailData.objects(
                created_at__gte=f"{start_of_month}T00:00:00",
                created_at__lte=f"{end_of_month}T23:59:59",
                avery_placement_date__ne=None,
                Total_gwel_net__ne=None,
            )
            fetchRcrData = RcrRoadData.objects(
                tar_wt_date__gte=f"{start_of_month}T00:00:00",
                tar_wt_date__lte=f"{end_of_month}T23:59:59",
            )

            fetchRcrRailData = RcrData.objects(
                created_at__gte=f"{start_of_month}T00:00:00",
                created_at__lte=f"{end_of_month}T23:59:59",
                avery_placement_date__ne=None,
                Total_gwel_net__ne=None,
            )
            
            rrNo_values = {}

            for single_coal_testing_train in fetchCoalTestingTrain:
                trainrrNo = single_coal_testing_train.sample_id
                location = single_coal_testing_train.mine
                calorific_value = single_coal_testing_train.plant_arb_gcv
                # for param in single_coal_testing_train.parameters:
                #     if param["parameter_Name"] == "Gross_Calorific_Value_(Arb)":
                #         if param["val1"] != None:
                #             calorific_value = float(param["val1"])
                #             break
                # else:
                #     continue

                if trainrrNo in rrNo_values:
                    rrNo_values[location] += calorific_value
                else:
                    rrNo_values[location] = calorific_value

            for single_coal_testing in fetchCoalTesting:
                roadrrNo = single_coal_testing.sample_id
                location = single_coal_testing.type_consumer if single_coal_testing.type_consumer is not None else "None"
                calorific_value = single_coal_testing.plant_arb_gcv
                # for param in single_coal_testing.parameters:
                #     if param["parameter_Name"] == "Gross_Calorific_Value_(Arb)":
                #         if param["val1"] != None and param["val1"] != "":
                #             calorific_value = float(param["val1"])
                #             break
                # else:
                #     continue

                if roadrrNo in rrNo_values:
                    rrNo_values[location] += calorific_value
                else:
                    rrNo_values[location] = calorific_value
            
            aopList = []
            fetchAopTarget = AopTarget.objects()
            if fetchAopTarget:
                for single_aop_target in fetchAopTarget:
                    aopList.append(single_aop_target.payload())

            net_qty_totals = {}
            actual_net_qty_totals = {}

            # Iterate over the retrieved data
            for single_gmr_data in fetchGmrData:
                mine_name = single_gmr_data.mine
                net_qty = single_gmr_data.net_qty
                actual_net_qty = single_gmr_data.actual_net_qty

                # net_qty_totals[mine_name] += float(net_qty)
                if mine_name in net_qty_totals:
                    net_qty_totals[mine_name] += float(net_qty)
                else:
                    net_qty_totals[mine_name] = float(net_qty)
                if actual_net_qty:
                    # actual_net_qty_totals[mine_name] += float(actual_net_qty)
                    if mine_name in actual_net_qty_totals:
                        actual_net_qty_totals[mine_name] += float(actual_net_qty)
                    else:
                        actual_net_qty_totals[mine_name] = float(actual_net_qty)



            for single_rail_data in fetchRailData:
                #skip
                if not single_rail_data.Total_gwel_net or single_rail_data.Total_gwel_net == "NaN":
                    continue
                if single_rail_data.Total_gwel_net:
                    rail_actual_net_qty = single_rail_data.Total_gwel_net

                    rail_mine_name = single_rail_data.source
                    if single_rail_data.source_type == "SECL Linkage(U1)" :
                        rail_net_qty = single_rail_data.total_secl_net_wt
                    else:
                        rail_net_qty = single_rail_data.total_rly_net_wt
                

                if rail_mine_name in net_qty_totals:
                    net_qty_totals[rail_mine_name] += float(rail_net_qty)
                else:
                    net_qty_totals[rail_mine_name] = float(rail_net_qty)
                    
                if rail_mine_name in actual_net_qty_totals:
                    actual_net_qty_totals[rail_mine_name] += float(rail_actual_net_qty)
                else:
                    actual_net_qty_totals[rail_mine_name] = float(rail_actual_net_qty)

            for single_rcr_data in fetchRcrData:
                rcr_mine_name = single_rcr_data.mine
                rcr_net_qty = single_rcr_data.dc_net_wt
                rcr_actual_net_qty = single_rcr_data.received_net_weight

                # net_qty_totals[mine_name] += float(net_qty)
                if rcr_mine_name in net_qty_totals:
                    net_qty_totals[rcr_mine_name] += float(rcr_net_qty)
                else:
                    net_qty_totals[rcr_mine_name] = float(rcr_net_qty)
                if rcr_actual_net_qty:
                    # actual_net_qty_totals[mine_name] += float(actual_net_qty)
                    if rcr_mine_name in actual_net_qty_totals:
                        actual_net_qty_totals[rcr_mine_name] += float(rcr_actual_net_qty)
                    else:
                        actual_net_qty_totals[rcr_mine_name] = float(rcr_actual_net_qty)


            for single_rcr_rail_data in fetchRcrRailData:
                #skip
                if not single_rcr_rail_data.Total_gwel_net or single_rcr_rail_data.Total_gwel_net == "NaN":
                    continue
                if single_rcr_rail_data.Total_gwel_net:
                    rail_rcr_actual_net_qty = single_rcr_rail_data.Total_gwel_net

                    rail_mine_name = single_rcr_rail_data.source
                    if single_rcr_rail_data.source_type == "SECL Linkage(U1)" :
                        rail_rcr_net_qty = single_rcr_rail_data.total_secl_net_wt
                    else:
                        rail_rcr_net_qty = single_rcr_rail_data.total_rly_net_wt
                

                if rail_mine_name in net_qty_totals:
                    net_qty_totals[rail_mine_name] += float(rail_rcr_net_qty)
                else:
                    net_qty_totals[rail_mine_name] = float(rail_rcr_net_qty)
                    
                if rail_mine_name in actual_net_qty_totals:
                    actual_net_qty_totals[rail_mine_name] += float(rail_rcr_actual_net_qty)
                else:
                    actual_net_qty_totals[rail_mine_name] = float(rail_rcr_actual_net_qty)

            clubbed_data = {
                mine: actual_net_qty_totals[mine] - net_qty_totals[mine]
                for mine in net_qty_totals
            }
            # console_logger.debug(clubbed_data)
            return rrNo_values, clubbed_data, aopList
    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

# delete after matched value done by sachin bhai
def bar_graph_data_new(specified_date):
    try:
        if specified_date:
            
            specified_date = datetime.datetime.strptime(specified_date, "%Y-%m-%d")
            start_of_month = specified_date.replace(day=1)
            start_of_month = datetime.datetime.strftime(start_of_month, '%Y-%m-%d')
            end_of_month = datetime.datetime.strftime(specified_date, '%Y-%m-%d')

            fetchCoalTesting = CoalTesting.objects(
                receive_date__gte= datetime.datetime.strptime(start_of_month, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), receive_date__lte= datetime.datetime.strptime(end_of_month, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M")
            )
            
            fetchCoalTestingTrain = CoalTestingTrain.objects(
                receive_date__gte = datetime.datetime.strptime(start_of_month, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), receive_date__lte= datetime.datetime.strptime(end_of_month, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M")
            )

            # fetchGmrData = Gmrdata.objects(created_at__gte=datetime.datetime.strptime(start_of_month, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), created_at__lte=datetime.datetime.strptime(end_of_month, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"))
            fetchGmrData = Gmrdata.objects(
                GWEL_Tare_Time__gte=f"{start_of_month}T00:00:00",
                GWEL_Tare_Time__lte=f"{end_of_month}T23:59:59"
            )
            fetchRailData = RailData.objects(
                created_at__gte=f"{start_of_month}T00:00:00",
                created_at__lte=f"{end_of_month}T23:59:59"
            )
            rrNo_values = {}

            for single_coal_testing in fetchCoalTesting:
                rrNo = single_coal_testing.rrNo
                location = single_coal_testing.mine
                calorific_value = single_coal_testing.plant_arb_gcv
                # for param in single_coal_testing.parameters:
                #     if param["parameter_Name"] == "Gross_Calorific_Value_(Arb)":
                #         if param["val1"] != None and param["val1"] != "":
                #             calorific_value = float(param["val1"])
                #             break
                # else:
                #     continue

                if rrNo in rrNo_values:
                    rrNo_values[location] += calorific_value
                else:
                    rrNo_values[location] = calorific_value

            for single_coal_testing_train in fetchCoalTestingTrain:
                rrNo = single_coal_testing_train.rrNo
                location = single_coal_testing_train.mine
                calorific_value = single_coal_testing_train.plant_arb_gcv
                # for param in single_coal_testing_train.parameters:
                #     if param["parameter_Name"] == "Gross_Calorific_Value_(Arb)":
                #         if param["val1"] != None:
                #             calorific_value = float(param["val1"])
                #             break
                # else:
                #     continue

                if rrNo in rrNo_values:
                    rrNo_values[location] += calorific_value
                else:
                    rrNo_values[location] = calorific_value
            
            aopList = []
            fetchAopTarget = AopTarget.objects()
            if fetchAopTarget:
                for single_aop_target in fetchAopTarget:
                    aopList.append(single_aop_target.payload())

            net_qty_totals = {}
            actual_net_qty_totals = {}

            # Iterate over the retrieved data
            for single_gmr_data in fetchGmrData:
                mine_name = single_gmr_data.mine
                net_qty = single_gmr_data.net_qty
                actual_net_qty = single_gmr_data.actual_net_qty

                # net_qty_totals[mine_name] += float(net_qty)
                if mine_name in net_qty_totals:
                    net_qty_totals[mine_name] += float(net_qty)
                else:
                    net_qty_totals[mine_name] = float(net_qty)
                if actual_net_qty:
                    # actual_net_qty_totals[mine_name] += float(actual_net_qty)
                    if mine_name in actual_net_qty_totals:
                        actual_net_qty_totals[mine_name] += float(actual_net_qty)
                    else:
                        actual_net_qty_totals[mine_name] = float(actual_net_qty)

            for single_rail_data in fetchRailData:
                rail_mine_name = single_rail_data.source
                rail_net_qty = single_rail_data.total_secl_net_wt
                rail_actual_net_qty = single_rail_data.total_rly_net_wt

                if rail_mine_name in net_qty_totals:
                    net_qty_totals[rail_mine_name] += float(rail_net_qty)
                else:
                    net_qty_totals[rail_mine_name] = float(rail_net_qty)
                if rail_actual_net_qty:
                    # actual_net_qty_totals[mine_name] += float(actual_net_qty)
                    if rail_mine_name in actual_net_qty_totals:
                        actual_net_qty_totals[rail_mine_name] += float(rail_actual_net_qty)
                    else:
                        actual_net_qty_totals[rail_mine_name] = float(rail_actual_net_qty)

            clubbed_data = {
                mine: actual_net_qty_totals[mine] - net_qty_totals[mine]
                for mine in net_qty_totals
            }

            return rrNo_values, clubbed_data, aopList
    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


def get_financial_year(datestring):
    try:
        # date = datetime.datetime.strptime(datestring, "%Y-%m-%d").date()
        date = convert_to_utc_format(datestring, "%Y-%m-%d").date()
        # Initialize the current year
        year_of_date = date.year
        # Initialize the current financial year start date
        # financial_year_start_date = datetime.datetime.strptime(str(year_of_date) + "-04-01", "%Y-%m-%d").date()
        financial_year_start_date = convert_to_utc_format(str(year_of_date) + "-04-01", "%Y-%m-%d").date()
        if date < financial_year_start_date:
            return {"start_date": f"{financial_year_start_date.year}-04-01", "end_date": f"{financial_year_start_date.year+1}-03-31"}
        else:
            return {"start_date": f"{financial_year_start_date.year}-04-01", "end_date": f"{financial_year_start_date.year+1}-03-31"}
    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

# def transit_loss_gain_road_mode_month(specified_date):
#     try:
#         data = {}
#         result = {
#             "labels": [],
#             "datasets": [],
#             "weight_total": [],
#             "total": 0,
#             "page_size": 15,
#         }
        
#         financial_year = get_financial_year(datetime.date.today().strftime("%Y-%m-%d"))

#         # logs = (
#         #     Gmrdata.objects(created_at__gte=financial_year.get("start_date"), created_at__lte=specified_date)
#         # )
#         logs = (
#             Gmrdata.objects(GWEL_Tare_Time__gte=financial_year.get("start_date"), GWEL_Tare_Time__lte=specified_date)
#         )

#         if any(logs):
#             aggregated_data = defaultdict(
#                 lambda: defaultdict(
#                     lambda: {
#                         "net_qty": 0,
#                         "mine_name": "",
#                         "actual_net_qty": 0,
#                         "count": 0,
#                     }
#                 )
#             )

#             start_dates = {}

#             for log in logs:
#                 if log.GWEL_Tare_Time is not None:
#                     month = log.GWEL_Tare_Time.strftime("%Y-%m")
#                     payload = log.payload()
#                     result["labels"] = list(payload.keys())
#                     mine_name = payload.get("Mines_Name")
#                     do_no = payload.get("DO_No")

#                     if do_no not in start_dates:
#                         start_dates[do_no] = month
#                     elif month < start_dates[do_no]:
#                         start_dates[do_no] = month
#                     if payload.get("GWEL_Net_Wt(MT)") and payload.get("GWEL_Net_Wt(MT)") != "NaN":
#                         aggregated_data[month][do_no]["actual_net_qty"] += float(payload["GWEL_Net_Wt(MT)"])
#                     if payload.get("Challan_Net_Wt(MT)") and payload.get("Challan_Net_Wt(MT)") != "NaN":
#                         aggregated_data[month][do_no]["net_qty"] += float(payload.get("Challan_Net_Wt(MT)"))
#                     if payload.get("Mines_Name"):
#                         aggregated_data[month][do_no]["mine_name"] = payload["Mines_Name"]

#                     aggregated_data[month][do_no]["count"] += 1 
#             dataList = [
#                 {
#                     "month": month,
#                     "data": {
#                         do_no: {
#                             "final_net_qty": data["actual_net_qty"] - data["net_qty"],
#                             "mine_name": data["mine_name"],
#                             "month": month,
#                         }
#                         for do_no, data in aggregated_data[month].items()
#                     },
#                 }
#                 for month in aggregated_data
#             ]
#             console_logger.debug(dataList)
#             total_monthly_final_net_qty = {}
#             yearly_final_data = {}
#             for data in dataList:
#                 month = data["month"]
#                 total_monthly_final_net_qty[month] = sum(
#                     entry["final_net_qty"] for entry in data["data"].values()
#                 )

#             total_monthly_final_net = dict(sorted(total_monthly_final_net_qty.items()))

#             for key, single_count in total_monthly_final_net.items():
#                 year = datetime.datetime.strptime(key, "%Y-%m").year
#                 if year in yearly_final_data:
#                     yearly_final_data[year] += single_count
#                 else:
#                     yearly_final_data[year] = single_count

#             yearly_final_data_sort = dict(sorted(yearly_final_data.items()))
#         console_logger.debug(total_monthly_final_net)
#         return total_monthly_final_net

#     except Exception as e:
#         console_logger.debug("----- Gate Vehicle Count Error -----", e)
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#         console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#         return e


# @router.get("/test_graph", tags=["Vipin Extra"])
def transit_loss_gain_road_mode_month(date_object):
    try:
        get_date = datetime.datetime.strptime(date_object, '%Y-%m-%d').date()
        specified_date = get_date.year
        dictData = {}
        timezone = pytz.timezone('Asia/Kolkata')
        created_at_date = datetime.datetime(2024, 9, 23, 19, 50, 51, 572000)
        basePipeline = [
            {
                '$match': {
                    'created_at': {
                        '$gt': created_at_date,
                    }
                }
            },
            {
                "$match": {
                        "GWEL_Tare_Time": {
                            "$gte": None,
                        },
                },
            },
            {
                '$project': {
                    'ts': None,
                    'actual_net_qty': {
                        '$toDouble': '$actual_net_qty'
                    }, 
                    'net_qty': {
                        '$toDouble': '$net_qty'
                    }, 
                    'label': {
                        '$cond': {
                            'if': {
                                '$ne': [
                                    '$vehicle_number', None
                                ]
                            }, 
                            'then': 'Road', 
                            'else': 'Rail'
                        }
                    }, 
                    '_id': 0
                }
            }, {
                '$group': {
                    '_id': {
                        'ts': '$ts', 
                        'label': '$label'
                    }, 
                    'actual_net_qty_sum': {
                        '$sum': '$actual_net_qty'
                    }, 
                    'net_qty_sum': {
                        '$sum': '$net_qty'
                    }
                }
            }, {
                '$project': {
                    '_id': 0, 
                    'ts': '$_id.ts', 
                    'label': '$_id.label', 
                    'data': {
                        '$subtract': [
                            '$actual_net_qty_sum', '$net_qty_sum'
                        ]
                    }
                }
            }
        ]

        historicBasePipeline = [
            {
                "$match": {
                        "GWEL_Tare_Time": {
                            "$gte": None,
                        },
                },
            },
            {
                '$project': {
                    'ts': None,
                    'actual_net_qty': {
                        '$toDouble': '$actual_net_qty'
                    }, 
                    'net_qty': {
                        '$toDouble': '$net_qty'
                    }, 
                    'label': {
                        '$cond': {
                            'if': {
                                '$ne': [
                                    '$vehicle_number', None
                                ]
                            }, 
                            'then': 'Road', 
                            'else': 'Rail'
                        }
                    }, 
                    '_id': 0
                }
            }, {
                '$group': {
                    '_id': {
                        'ts': '$ts', 
                        'label': '$label'
                    }, 
                    'actual_net_qty_sum': {
                        '$sum': '$actual_net_qty'
                    }, 
                    'net_qty_sum': {
                        '$sum': '$net_qty'
                    }
                }
            }, {
                '$project': {
                    '_id': 0, 
                    'ts': '$_id.ts', 
                    'label': '$_id.label', 
                    'data': {
                        '$subtract': [
                            '$actual_net_qty_sum', '$net_qty_sum'
                        ]
                    }
                }
            }
        ]

        date = specified_date
        end_date = f'{date}-12-31 23:59:59'
        start_date = f'{date}-01-01 00:00:00'
        format_data = "%Y-%m-%d %H:%M:%S"

        endd_date = timezone.localize(datetime.datetime.strptime(end_date, format_data))
        startd_date = timezone.localize(datetime.datetime.strptime(start_date, format_data))

        basePipeline[1]["$match"]["GWEL_Tare_Time"]["$lte"] = endd_date
        basePipeline[1]["$match"]["GWEL_Tare_Time"]["$gte"] = startd_date

        # basePipeline[1]["$project"]["ts"] = {"$month": "$GWEL_Tare_Time"}
        basePipeline[2]["$project"]["ts"] = {"$month": {"date": "$GWEL_Tare_Time", "timezone": "Asia/Kolkata"}}

        historicBasePipeline[0]["$match"]["GWEL_Tare_Time"]["$lte"] = endd_date
        historicBasePipeline[0]["$match"]["GWEL_Tare_Time"]["$gte"] = startd_date

        # basePipeline[1]["$project"]["ts"] = {"$month": "$GWEL_Tare_Time"}
        historicBasePipeline[1]["$project"]["ts"] = {"$month": {"date": "$GWEL_Tare_Time", "timezone": "Asia/Kolkata"}}

        labels = [(startd_date + relativedelta(months=i)).strftime("%b %y")
                    for i in range(12)]
        output = Gmrdata.objects().aggregate(basePipeline)
        historicoutput = gmrdataHistoric.objects().aggregate(historicBasePipeline)
        outputDict = {}
        for data in output:
            ts = data["ts"]
            label = data["label"]
            sum_value = data["data"]
            if ts not in outputDict:
                outputDict[ts] = {label: sum_value}
            else:
                if label not in outputDict[ts]:
                    outputDict[ts][label] = sum_value
                else:
                    outputDict[ts][label] += sum_value

        for histdata in historicoutput:
            ts = histdata["ts"]
            label = histdata["label"]
            sum_value = histdata["data"]
            if ts not in outputDict:
                outputDict[ts] = {label: sum_value}
            else:
                if label not in outputDict[ts]:
                    outputDict[ts][label] = sum_value
                else:
                    outputDict[ts][label] += sum_value
                    
        # console_logger.debug(outputDict)
        for index, label in enumerate(labels):
            # console_logger.debug(index)
            # console_logger.debug(label)
            if index in outputDict:
                for key, val in outputDict[index].items():
                    # console_logger.debug(key)
                    if key == "Road":
                        # console_logger.debug(label)
                        # result["data"]["datasets"][0]["data"][index-1] = val
                        dictData[f"{specified_date}-{index:02d}"] = val
                        # console_logger.debug(index)
                        # console_logger.debug(val)
                    elif key == "Rail":
                        # result["data"]["datasets"][1]["data"][index-1] = val
                        console_logger.debug(index)
                        console_logger.debug(val)

        # console_logger.debug(dictData)
        return dictData
    
    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

# delete after matched value done by sachin bhai
def transit_loss_gain_road_mode_month_new(date_object):
    try:
        get_date = datetime.datetime.strptime(date_object, '%Y-%m-%d').date()
        specified_date = get_date.year
        dictData = {}
        timezone = pytz.timezone('Asia/Kolkata')
        basePipeline = [
            {
                "$match": {
                        "GWEL_Tare_Time": {
                            "$gte": None,
                        },
                },
            },
            {
                '$project': {
                    'ts': None,
                    'actual_net_qty': {
                        '$toDouble': '$actual_net_qty'
                    }, 
                    'net_qty': {
                        '$toDouble': '$net_qty'
                    }, 
                    'label': {
                        '$cond': {
                            'if': {
                                '$ne': [
                                    '$vehicle_number', None
                                ]
                            }, 
                            'then': 'Road', 
                            'else': 'Rail'
                        }
                    }, 
                    '_id': 0
                }
            }, {
                '$group': {
                    '_id': {
                        'ts': '$ts', 
                        'label': '$label'
                    }, 
                    'actual_net_qty_sum': {
                        '$sum': '$actual_net_qty'
                    }, 
                    'net_qty_sum': {
                        '$sum': '$net_qty'
                    }
                }
            }, {
                '$project': {
                    '_id': 0, 
                    'ts': '$_id.ts', 
                    'label': '$_id.label', 
                    'data': {
                        '$subtract': [
                            '$actual_net_qty_sum', '$net_qty_sum'
                        ]
                    }
                }
            }
        ]

        date = specified_date
        end_date = f'{date}-12-31 23:59:59'
        start_date = f'{date}-01-01 00:00:00'
        format_data = "%Y-%m-%d %H:%M:%S"

        endd_date = timezone.localize(datetime.datetime.strptime(end_date, format_data))
        startd_date = timezone.localize(datetime.datetime.strptime(start_date, format_data))

        basePipeline[0]["$match"]["GWEL_Tare_Time"]["$lte"] = endd_date
        basePipeline[0]["$match"]["GWEL_Tare_Time"]["$gte"] = startd_date

        # basePipeline[1]["$project"]["ts"] = {"$month": "$GWEL_Tare_Time"}
        basePipeline[1]["$project"]["ts"] = {"$month": {"date": "$GWEL_Tare_Time", "timezone": "Asia/Kolkata"}}

        labels = [(startd_date + relativedelta(months=i)).strftime("%b %y")
                    for i in range(12)]
        
        output = Gmrdatanew.objects().aggregate(basePipeline)
        outputDict = {}
        for data in output:
            ts = data["ts"]
            label = data["label"]
            sum_value = data["data"]
            if ts not in outputDict:
                outputDict[ts] = {label: sum_value}
            else:
                if label not in outputDict[ts]:
                    outputDict[ts][label] = sum_value
                else:
                    outputDict[ts][label] += sum_value
        # console_logger.debug(outputDict)
        for index, label in enumerate(labels):
            # console_logger.debug(index)
            # console_logger.debug(label)
            if index in outputDict:
                for key, val in outputDict[index].items():
                    # console_logger.debug(key)
                    if key == "Road":
                        # console_logger.debug(label)
                        # result["data"]["datasets"][0]["data"][index-1] = val
                        dictData[f"{specified_date}-{index:02d}"] = val
                        # console_logger.debug(index)
                        # console_logger.debug(val)
                    elif key == "Rail":
                        # result["data"]["datasets"][1]["data"][index-1] = val
                        console_logger.debug(index)
                        console_logger.debug(val)
        return dictData
    
    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


    
def transit_loss_gain_road_mode():
    try:
        data = {}
        result = {
            "labels": [],
            "datasets": [],
            "weight_total": [],
            "total": 0,
            "page_size": 15,
        }
        
        financial_year = get_financial_year(datetime.date.today().strftime("%Y-%m-%d"))

        created_at_date = datetime.datetime(2024, 9, 23, 19, 50, 51, 572000)
        logs = (
            Gmrdata.objects(GWEL_Tare_Time__gte=financial_year.get("start_date"), GWEL_Tare_Time__lte=financial_year.get("end_date"), created_at__gt=created_at_date)
        )

        historiclogs = (
            gmrdataHistoric.objects(GWEL_Tare_Time__gte=financial_year.get("start_date"), GWEL_Tare_Time__lte=financial_year.get("end_date"))
        )

        if any(logs) or any(historiclogs):
            aggregated_data = defaultdict(
                lambda: defaultdict(
                    lambda: {
                        "net_qty": 0,
                        "mine_name": "",
                        "actual_net_qty": 0,
                        "count": 0,
                    }
                )
            )

            start_dates = {}
            for log in logs:
                if log.GWEL_Tare_Time is not None:
                    month = log.GWEL_Tare_Time.strftime("%Y-%m")
                    payload = log.payload()
                    result["labels"] = list(payload.keys())
                    mine_name = payload.get("Mines_Name")
                    do_no = payload.get("DO_No")

                    if do_no not in start_dates:
                        start_dates[do_no] = month
                    elif month < start_dates[do_no]:
                        start_dates[do_no] = month

                    if payload.get("GWEL_Net_Wt(MT)") and payload.get("GWEL_Net_Wt(MT)") != "NaN":
                        aggregated_data[month][do_no]["actual_net_qty"] += float(payload["GWEL_Net_Wt(MT)"])
                    else:
                        aggregated_data[month][do_no]["actual_net_qty"] = 0
                    if payload.get("Challan_Net_Wt(MT)") and payload.get("Challan_Net_Wt(MT)") != "NaN":
                        aggregated_data[month][do_no]["net_qty"] += float(payload.get("Challan_Net_Wt(MT)"))
                    else:
                        aggregated_data[month][do_no]["net_qty"] = 0
                    if payload.get("Mines_Name"):
                        aggregated_data[month][do_no]["mine_name"] = payload["Mines_Name"]
                    else:
                        aggregated_data[month][do_no]["mine_name"] = "-"
                    aggregated_data[month][do_no]["count"] += 1 

            for log in historiclogs:
                if log.GWEL_Tare_Time is not None:
                    month = log.GWEL_Tare_Time.strftime("%Y-%m")
                    payload = log.payload()
                    result["labels"] = list(payload.keys())
                    mine_name = payload.get("Mines_Name")
                    do_no = payload.get("DO_No")

                    if do_no not in start_dates:
                        start_dates[do_no] = month
                    elif month < start_dates[do_no]:
                        start_dates[do_no] = month

                    # if payload.get("GWEL_Net_Wt(MT)") and payload.get("GWEL_Net_Wt(MT)") != "NaN":
                    #     aggregated_data[month][do_no]["actual_net_qty"] += float(payload["GWEL_Net_Wt(MT)"])
                    # else:
                    #     aggregated_data[month][do_no]["actual_net_qty"] = 0
                    # if payload.get("Challan_Net_Wt(MT)") and payload.get("Challan_Net_Wt(MT)") != "NaN":
                    #     aggregated_data[month][do_no]["net_qty"] += float(payload.get("Challan_Net_Wt(MT)"))
                    # else:
                    #     aggregated_data[month][do_no]["net_qty"] = 0
                    # if payload.get("Mines_Name"):
                    #     aggregated_data[month][do_no]["mine_name"] = payload["Mines_Name"]
                    # else:
                    #     aggregated_data[month][do_no]["mine_name"] = "-"
                    # aggregated_data[month][do_no]["count"] += 1 

                    if do_no in aggregated_data[month]:
                        if payload.get("GWEL_Net_Wt(MT)") and payload.get("GWEL_Net_Wt(MT)") != "NaN":
                            aggregated_data[month][do_no]["actual_net_qty"] += float(payload["GWEL_Net_Wt(MT)"])
                        else:
                            aggregated_data[month][do_no]["actual_net_qty"] = 0

                        if payload.get("Challan_Net_Wt(MT)") and payload.get("Challan_Net_Wt(MT)") != "NaN":
                            aggregated_data[month][do_no]["net_qty"] += float(payload.get("Challan_Net_Wt(MT)"))
                        else:
                            aggregated_data[month][do_no]["net_qty"] = 0

                        if payload.get("Mines_Name"):
                            aggregated_data[month][do_no]["mine_name"] = payload["Mines_Name"]
                        else:
                            aggregated_data[month][do_no]["mine_name"] = "-"
                        
                        aggregated_data[month][do_no]["count"] += 1
                    else:
                        aggregated_data[month][do_no] = {
                            "net_qty": 0,
                            "mine_name": payload.get("Mines_Name", "-"),
                            "actual_net_qty": 0,
                            "count": 1,
                        }
                        if payload.get("GWEL_Net_Wt(MT)") and payload.get("GWEL_Net_Wt(MT)") != "NaN":
                            aggregated_data[month][do_no]["actual_net_qty"] += float(payload["GWEL_Net_Wt(MT)"])
                        if payload.get("Challan_Net_Wt(MT)") and payload.get("Challan_Net_Wt(MT)") != "NaN":
                            aggregated_data[month][do_no]["net_qty"] += float(payload.get("Challan_Net_Wt(MT)"))

            dataList = [
                {
                    "month": month,
                    "data": {
                        do_no: {
                            "final_net_qty": data["actual_net_qty"] - data["net_qty"],
                            "mine_name": data["mine_name"],
                            "month": month,
                        }
                        for do_no, data in aggregated_data[month].items()
                    },
                }
                for month in aggregated_data
            ]

            total_monthly_final_net_qty = {}
            yearly_final_data = {}
            for data in dataList:
                month = data["month"]
                total_monthly_final_net_qty[month] = sum(
                    entry["final_net_qty"] for entry in data["data"].values()
                )

            total_monthly_final_net = dict(sorted(total_monthly_final_net_qty.items()))

            for key, single_count in total_monthly_final_net.items():
                year = datetime.datetime.strptime(key, "%Y-%m").year
                if year in yearly_final_data:
                    # yearly_final_data[year] += single_count
                    yearly_final_data[year] += single_count
                else:
                    # yearly_final_data[year] = single_count
                    yearly_final_data[year] = single_count

            yearly_final_data_sort = dict(sorted(yearly_final_data.items()))
        return yearly_final_data_sort

    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


# delete after matched value done by sachin bhai
def transit_loss_gain_road_mode_new():
    try:
        data = {}
        result = {
            "labels": [],
            "datasets": [],
            "weight_total": [],
            "total": 0,
            "page_size": 15,
        }
        
        financial_year = get_financial_year(datetime.date.today().strftime("%Y-%m-%d"))

        # logs = (
        #     Gmrdata.objects(created_at__gte=financial_year.get("start_date"), created_at__lte=financial_year.get("end_date"))
        # )
        logs = (
            Gmrdatanew.objects(GWEL_Tare_Time__gte=financial_year.get("start_date"), GWEL_Tare_Time__lte=financial_year.get("end_date"))
        )

        if any(logs):
            aggregated_data = defaultdict(
                lambda: defaultdict(
                    lambda: {
                        "net_qty": 0,
                        "mine_name": "",
                        "actual_net_qty": 0,
                        "count": 0,
                    }
                )
            )

            start_dates = {}
            for log in logs:
                if log.GWEL_Tare_Time is not None:
                    month = log.GWEL_Tare_Time.strftime("%Y-%m")
                    payload = log.payload()
                    result["labels"] = list(payload.keys())
                    mine_name = payload.get("Mines_Name")
                    do_no = payload.get("DO_No")

                    if do_no not in start_dates:
                        start_dates[do_no] = month
                    elif month < start_dates[do_no]:
                        start_dates[do_no] = month

                    if payload.get("GWEL_Net_Wt(MT)") and payload.get("GWEL_Net_Wt(MT)") != "NaN":
                        aggregated_data[month][do_no]["actual_net_qty"] += float(payload["GWEL_Net_Wt(MT)"])
                    else:
                        aggregated_data[month][do_no]["actual_net_qty"] = 0
                    if payload.get("Challan_Net_Wt(MT)") and payload.get("Challan_Net_Wt(MT)") != "NaN":
                        aggregated_data[month][do_no]["net_qty"] += float(payload.get("Challan_Net_Wt(MT)"))
                    else:
                        aggregated_data[month][do_no]["net_qty"] = 0
                    if payload.get("Mines_Name"):
                        aggregated_data[month][do_no]["mine_name"] = payload["Mines_Name"]
                    else:
                        aggregated_data[month][do_no]["mine_name"] = "-"
                    aggregated_data[month][do_no]["count"] += 1 

            dataList = [
                {
                    "month": month,
                    "data": {
                        do_no: {
                            "final_net_qty": data["actual_net_qty"] - data["net_qty"],
                            "mine_name": data["mine_name"],
                            "month": month,
                        }
                        for do_no, data in aggregated_data[month].items()
                    },
                }
                for month in aggregated_data
            ]

            total_monthly_final_net_qty = {}
            yearly_final_data = {}
            for data in dataList:
                month = data["month"]
                total_monthly_final_net_qty[month] = sum(
                    entry["final_net_qty"] for entry in data["data"].values()
                )

            total_monthly_final_net = dict(sorted(total_monthly_final_net_qty.items()))

            for key, single_count in total_monthly_final_net.items():
                year = datetime.datetime.strptime(key, "%Y-%m").year
                if year in yearly_final_data:
                    # yearly_final_data[year] += single_count
                    yearly_final_data["road_mode"] += single_count
                else:
                    # yearly_final_data[year] = single_count
                    yearly_final_data["road_mode"] = single_count

            yearly_final_data_sort = dict(sorted(yearly_final_data.items()))
        return yearly_final_data_sort

    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


def transit_loss_gain_rail_mode():
    try:
        data = {}
        result = {
            "labels": [],
            "datasets": [],
            "weight_total": [],
            "total": 0,
            "page_size": 15,
        }
        
        financial_year = get_financial_year(datetime.date.today().strftime("%Y-%m-%d"))

        logs = (
            RailData.objects(avery_placement_date__gte=financial_year.get("start_date"), avery_placement_date__lte=financial_year.get("end_date"))
        )

        if any(logs):
            aggregated_data = defaultdict(
                lambda: defaultdict(
                    lambda: {
                        "net_qty": 0,
                        "mine_name": "",
                        "actual_net_qty": 0,
                        "count": 0,
                    }
                )
            )

            start_dates = {}
            for log in logs:
                if log.created_at is not None:
                    month = log.created_at.strftime("%Y-%m")
                    payload = log.payload()
                    result["labels"] = list(payload.keys())
                    mine_name = payload.get("source")
                    rr_no = payload.get("rr_no")

                    # Skip record if Total_gwel_net is empty or 'NaN'
                    if not payload.get("Total_gwel_net") or payload.get("Total_gwel_net") == "NaN":
                        continue

                    if rr_no not in start_dates:
                        start_dates[rr_no] = month
                    elif month < start_dates[rr_no]:
                        start_dates[rr_no] = month

                    if payload.get("total_secl_net_wt") and payload.get("total_secl_net_wt") != "NaN":
                        aggregated_data[month][rr_no]["actual_net_qty"] += float(payload["total_secl_net_wt"])
                    else:
                        aggregated_data[month][rr_no]["actual_net_qty"] = 0
                    if payload.get("Total_gwel_net") and payload.get("Total_gwel_net") != "NaN":
                        aggregated_data[month][rr_no]["net_qty"] += float(payload.get("Total_gwel_net"))
                    else:
                        aggregated_data[month][rr_no]["net_qty"] = 0
                    if payload.get("Mines_Name"):
                        aggregated_data[month][rr_no]["mine_name"] = payload["Mines_Name"]
                    else:
                        aggregated_data[month][rr_no]["mine_name"] = "-"
                    aggregated_data[month][rr_no]["count"] += 1 

            dataList = [
                {
                    "month": month,
                    "data": {
                        rr_no: {
                            "final_net_qty": data["actual_net_qty"] - data["net_qty"],
                            "mine_name": data["mine_name"],
                            "month": month,
                        }
                        for rr_no, data in aggregated_data[month].items()
                    },
                }
                for month in aggregated_data
            ]

            total_monthly_final_net_qty = {}
            yearly_final_data = {}
            for data in dataList:
                month = data["month"]
                total_monthly_final_net_qty[month] = sum(
                    entry["final_net_qty"] for entry in data["data"].values()
                )

            total_monthly_final_net = dict(sorted(total_monthly_final_net_qty.items()))

            for key, single_count in total_monthly_final_net.items():
                year = datetime.datetime.strptime(key, "%Y-%m").year
                if year in yearly_final_data:
                    # yearly_final_data[year] += single_count
                    yearly_final_data[year] += single_count
                else:
                    # yearly_final_data[year] = single_count
                    yearly_final_data[year] = single_count

            yearly_final_data_sort = dict(sorted(yearly_final_data.items()))

        return yearly_final_data_sort

    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


def gmr_main_graph():
    try:
        net_qty_all_totals = {}
        actual_net_qty_all_totals = {}
        fetchGmrDataMain = Gmrdata.objects()

        for single_gmr_data in fetchGmrDataMain:
            mine_name = single_gmr_data.mine
            net_qty = single_gmr_data.net_qty
            actual_net_qty = single_gmr_data.actual_net_qty
            if net_qty and net_qty != "undefined":
                if mine_name in actual_net_qty_all_totals:
                    net_qty_all_totals[mine_name] += float(net_qty)
                else:
                    net_qty_all_totals[mine_name] = float(net_qty)
            if actual_net_qty:
                if mine_name in actual_net_qty_all_totals:
                    actual_net_qty_all_totals[mine_name] += float(actual_net_qty)
                else:
                    actual_net_qty_all_totals[mine_name] = float(actual_net_qty)

        clubbed_data_final = {}
        
        for mine in net_qty_all_totals:
            clubbed_data_final[mine] = actual_net_qty_all_totals.get(mine, 0) - net_qty_all_totals[mine]

        return clubbed_data_final
    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

# delete after matched value done by sachin bhai
def gmr_main_graph_new():
    try:
        net_qty_all_totals = {}
        actual_net_qty_all_totals = {}
        fetchGmrDataMain = Gmrdata.objects()

        for single_gmr_data in fetchGmrDataMain:
            mine_name = single_gmr_data.mine
            net_qty = single_gmr_data.net_qty
            actual_net_qty = single_gmr_data.actual_net_qty
        
            if mine_name in actual_net_qty_all_totals:
                net_qty_all_totals[mine_name] += float(net_qty)
            else:
                net_qty_all_totals[mine_name] = float(net_qty)
            if actual_net_qty:
                if mine_name in actual_net_qty_all_totals:
                    actual_net_qty_all_totals[mine_name] += float(actual_net_qty)
                else:
                    actual_net_qty_all_totals[mine_name] = float(actual_net_qty)

        clubbed_data_final = {}
        
        for mine in net_qty_all_totals:
            clubbed_data_final[mine] = actual_net_qty_all_totals.get(mine, 0) - net_qty_all_totals[mine]

        return clubbed_data_final
    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


def rail_pdf(specified_date):
    try:
        if specified_date:
            data = {}

            specified_change_date = datetime.datetime.strptime(specified_date, "%Y-%m-%d")

            start_of_month = specified_change_date.replace(day=1)

            start_date = datetime.datetime.strftime(start_of_month, '%Y-%m-%d')
            end_date = datetime.datetime.strftime(specified_change_date, '%Y-%m-%d')

            logs = (RailData.objects().order_by("source", "rr_no", "-created_at"))

            coal_testing_train = CoalTestingTrain.objects(receive_date__gte=start_date, receive_date__lte=end_date).order_by("-ID")
            if any(logs):
                aggregated_data = defaultdict(
                    lambda: defaultdict(
                        lambda: {
                            "DO_Qty": 0,
                            "challan_lr_qty": 0,
                            "mine_name": "",
                            "balance_qty": 0,
                            "percent_of_supply": 0,
                            "actual_net_qty": 0,
                            "Gross_Calorific_Value_(Adb)": 0,
                            "count": 0,
                            "coal_count": 0,
                        }
                    )
                )

                aggregated_coal_data = defaultdict(
                    lambda: defaultdict(
                        lambda: {
                            "Gross_Calorific_Value_(Adb)": 0,
                            "coal_count": 0,
                        }
                    )
                )

                for single_log in coal_testing_train:
                    coal_date = single_log.receive_date.strftime("%Y-%m")
                    coal_payload = single_log.gradepayload()
                    mine = coal_payload["Mine"]
                    rr_no = coal_payload["rrNo"]
                    if coal_payload.get("Gross_Calorific_Value_(Adb)"):
                        aggregated_coal_data[coal_date][rr_no]["Gross_Calorific_Value_(Adb)"] += float(coal_payload["Gross_Calorific_Value_(Adb)"])
                        aggregated_coal_data[coal_date][rr_no]["coal_count"] += 1

                start_dates = {}
                grade = 0
                for log in logs:
                    if log.created_at!=None:
                        month = log.created_at.strftime("%Y-%m")
                        date = log.created_at.strftime("%Y-%m-%d")
                        payload = log.payload()
                        # result["labels"] = list(payload.keys())
                        mine_name = payload.get("source")
                        rr_no = payload.get("rr_no")
                        # if payload.get("Grade") is not None:
                        #     if '-' in payload.get("Grade"):
                        #         grade = payload.get("Grade").split("-")[0]
                        #     else:
                        #         grade = payload.get("Grade")
                        if rr_no not in start_dates:
                            start_dates[rr_no] = date
                        elif date < start_dates[rr_no]:
                            start_dates[rr_no] = date
                        if payload.get("rr_qty"):
                            aggregated_data[date][rr_no]["rr_qty"] = float(
                                payload["rr_qty"]
                            )
                        else:
                            aggregated_data[date][rr_no]["rr_qty"] = 0
                        if payload.get("total_secl_net_wt"):
                            aggregated_data[date][rr_no]["challan_lr_qty"] += float(
                                payload.get("total_secl_net_wt")
                            )
                        else:
                            aggregated_data[date][rr_no]["challan_lr_qty"] = 0
                        if payload.get("source"):
                            aggregated_data[date][rr_no]["source"] = payload[
                                "source"
                            ]
                        else:
                            aggregated_data[date][rr_no]["source"] = "-"
                        aggregated_data[date][rr_no]["count"] += 1 

                dataList = [
                    {
                        "date": date,
                        "data": {
                            rr_no: {
                                "rr_qty": data["rr_qty"],
                                "challan_lr_qty": data["challan_lr_qty"],
                                "mine_name": data["source"],
                                "date": date,
                            }
                            for rr_no, data in aggregated_data[date].items()
                        },
                    }
                    for date in aggregated_data
                ]
                coalDataList = [
                    {"date": coal_date, "data": {
                        rr_no: {
                            "average_Gross_Calorific_Value_(Adb)": data["Gross_Calorific_Value_(Adb)"] / data["coal_count"],
                        } for rr_no, data in aggregated_coal_data[coal_date].items()
                    }} for coal_date in aggregated_coal_data
                ]

                coal_grades = CoalGrades.objects()

                for month_data in coalDataList:
                    for key, mine_data in month_data["data"].items():
                        if mine_data["average_Gross_Calorific_Value_(Adb)"] is not None:
                            for single_coal_grades in coal_grades:
                                if single_coal_grades["end_value"] != "":
                                    if (int(single_coal_grades["start_value"]) <= int(float(mine_data.get("average_Gross_Calorific_Value_(Adb)"))) <= int(single_coal_grades["end_value"]) and single_coal_grades["start_value"] != "" and single_coal_grades["end_value"] != ""):
                                        mine_data["average_GCV_Grade"] = single_coal_grades["grade"]
                                    elif int(mine_data["average_Gross_Calorific_Value_(Adb)"]) > 7001:
                                        mine_data["average_GCV_Grade"] = "G-1"
                                        break
                
                final_data = []
                if specified_date:
                    filtered_data = [
                        entry for entry in dataList if entry["date"] == specified_date
                    ]
                    if filtered_data:
                        data = filtered_data[0]["data"]
                        # dictData["month"] = filtered_data[0]["month"]
                        for data_dom, values in data.items():
                            dictData = {}
                            dictData["rr_no"] = data_dom
                            dictData["mine_name"] = values["mine_name"]
                            dictData["rr_qty"] = round(values["rr_qty"], 2)
                            dictData["challan_lr_qty"] = round(values["challan_lr_qty"], 2)
                            dictData["date"] = values["date"]
                            dictData["cumulative_challan_lr_qty"] = 0
                            dictData["balance_qty"] = 0
                            dictData["percent_supply"] = 0
                            dictData["asking_rate"] = 0
                            # dictData['average_GCV_Grade'] = values["grade"]
                            if data_dom in start_dates:
                                dictData["start_date"] = start_dates[data_dom]
                                # a total of 45 days data is needed, so date + 44 days
                                endDataVariable = datetime.datetime.strptime(start_dates[data_dom], "%Y-%m-%d") + timedelta(days=44)
                                # dictData["balance_days"] = dictData["end_date"] - datetime.date.today()
                                balance_days = endDataVariable.date() - datetime.date.today()
                                dictData["end_date"] = endDataVariable.strftime("%Y-%m-%d")
                                dictData["balance_days"] = balance_days.days
                            else:
                                dictData["start_date"] = None
                                dictData["end_date"] = None
                                dictData["balance_days"] = None

                            # Look for data_dom match in coalDataList and add average_GCV_Grade
                            for coal_data in coalDataList:
                                # if coal_data['date'] == specified_date and data_dom in coal_data['data']:
                                if data_dom in coal_data['data']:
                                    dictData['average_GCV_Grade'] = coal_data['data'][data_dom]['average_GCV_Grade']
                                    break
                            else:
                                dictData['average_GCV_Grade'] = "-"
                
                            final_data.append(dictData)
                    
                    if final_data:
                        # Find the index of the month data in dataList
                        index_of_month = next((index for index, item in enumerate(dataList) if item['date'] == specified_date), None)

                        # If the month is not found, exit or handle the case
                        if index_of_month is None:
                            print("Month data not found.")
                            exit()

                        # Iterate over final_data
                        for entry in final_data:
                            rr_no = entry["rr_no"]
                            cumulative_lr_qty = 0
                            
                            # Iterate over dataList from the first month to the current month
                            for i in range(index_of_month + 1):
                                month_data = dataList[i]
                                data = month_data["data"].get(rr_no)
                                
                                # If data is found for the rr_no in the current month, update cumulative_lr_qty
                                if data:
                                    cumulative_lr_qty += data['challan_lr_qty']
                            
                            # Update cumulative_challan_lr_qty in final_data
                            entry['cumulative_challan_lr_qty'] = round(cumulative_lr_qty, 2)
                            if data["rr_qty"] != 0 and entry["cumulative_challan_lr_qty"] != 0:
                                entry["percent_supply"] = round((entry["cumulative_challan_lr_qty"] / data["rr_qty"]) * 100, 2)
                            else:
                                entry["percent_supply"] = 0

                            if entry["cumulative_challan_lr_qty"] != 0 and data["rr_qty"] != 0:
                                entry["balance_qty"] = round((data["rr_qty"] - entry["cumulative_challan_lr_qty"]), 2)
                            else:
                                entry["balance_qty"] = 0
                            
                            if entry["balance_qty"] and entry["balance_qty"] != 0:
                                if entry["balance_days"]:
                                    entry["asking_rate"] = round(entry["balance_qty"] / entry["balance_days"], 2)
                    return final_data
                
    except Exception as e:
        console_logger.debug(e)


@router.get("/pdf_minewise_road_old", tags=["PDF Report"])
def generate_gmr_report_old_python(
    response: Response,
    specified_date: Optional[str]=None,
    mine: Optional[str] = "All",
):
    try:
        # if specified_date:
        data = {}
        result = {
            "labels": [],
            "datasets": [],
            "weight_total": [],
            "total": 0,
            "page_size": 15,
        }

        if mine and mine != "All":
            data["mine__icontains"] = mine.upper()

        # if specified_date:
        from_ts_main = convert_to_utc_format(f'{specified_date} 00:00:00', "%Y-%m-%d %H:%M:%S")
        to_ts_main = convert_to_utc_format(f'{specified_date} 23:59:59', "%Y-%m-%d %H:%M:%S")

        # console_logger.debug(from_ts_main)
        # console_logger.debug(to_ts_main)

        # logs = (
        #     Gmrdata.objects(
        #         Q(GWEL_Tare_Time__ne=None, GWEL_Tare_Time__gte=from_ts_main, GWEL_Tare_Time__lte=to_ts_main) | 
        #         Q(GWEL_Tare_Time=None, vehicle_in_time__gte=from_ts_main, vehicle_in_time__lte=to_ts_main)  # OR condition
        #     )
        #     # .order_by("-GWEL_Tare_Time")  # Order by GWEL_Tare_Time (if present)
        # )

        # logs = (
        #         Gmrdata.objects(
        #             Q(GWEL_Tare_Time__ne=None, GWEL_Tare_Time__gte=from_ts_main, GWEL_Tare_Time__lte=to_ts_main) |
        #             Q(GWEL_Tare_Time=None, vehicle_in_time__gte=from_ts_main, vehicle_in_time__lte=to_ts_main)
        #         )
        #     # Uncomment this if ordering is required
        #     .order_by("-GWEL_Tare_Time", "-vehicle_in_time")  # Order by GWEL_Tare_Time or vehicle_in_time
        # )

        # logs = (
        #         Gmrdata.objects(
        #             Q(GWEL_Tare_Time__ne=None, GWEL_Tare_Time__gte=from_ts_main, GWEL_Tare_Time__lte=to_ts_main)
        #         )
        #     # Uncomment this if ordering is required
        #     .order_by("-GWEL_Tare_Time")  # Order by GWEL_Tare_Time or vehicle_in_time
        # )

        logs = (
            # Gmrdata.objects(GWEL_Tare_Time__ne=None,GWEL_Tare_Time__lte=to_ts, actual_tare_qty__ne=None, gate_approved=True)
            Gmrdata.objects(GWEL_Tare_Time__ne=None, GWEL_Tare_Time__gte=from_ts_main, GWEL_Tare_Time__lte=to_ts_main)                                   # modified by Faisal
            .order_by("-GWEL_Tare_Time")
        )


        # console_logger.debug(logs)

        sap_records = SapRecords.objects(start_date__gte=from_ts_main, end_date__lte=to_ts_main)
        # console_logger.debug(sap_records)
        final_data = []
        if any(logs) or any(sap_records):
            aggregated_data = defaultdict(
                lambda: defaultdict(
                    lambda: {
                        "DO_Qty": 0,
                        "challan_lr_qty": 0,
                        "challan_lr_qty_full": 0,
                        "mine_name": "",
                        "balance_qty": 0,
                        "percent_of_supply": 0,
                        "actual_net_qty": 0,
                        "Gross_Calorific_Value_(Adb)": 0,
                        "count": 0,
                        "coal_count": 0,
                        "start_date": "",
                        "end_date": "",
                        "source_type": "",
                        "grn_status": "",
                    }
                )
            )

            start_dates = {}
            grade = 0
            for log in logs:
                if log.GWEL_Tare_Time is not None:
                    date = log.GWEL_Tare_Time.strftime("%Y-%m-%d")
                    month = log.GWEL_Tare_Time.strftime("%Y-%m")
                else:
                    date = log.vehicle_in_time.strftime("%Y-%m-%d")
                    month = log.vehicle_in_time.strftime("%Y- %m")
                payload = log.payload()
                result["labels"] = list(payload.keys())
                mine_name = payload.get("Mines_Name")
                # console_logger.debug(mine_name)
                do_no = payload.get("DO_No")
                # console_logger.debug(do_no)
                if payload.get("Grade") is not None:
                    if '-' in payload.get("Grade"):
                        grade = payload.get("Grade").split("-")[0]
                    elif " " in payload.get("Grade"):
                        grade = payload.get("Grade").split(" ")[0]
                    else:
                        grade = payload.get("Grade")
                # If start_date is None or the current vehicle_in_time is earlier than start_date, update start_date
                # if do_no not in start_dates:
                #     start_dates[do_no] = date
                # elif date < start_dates[do_no]:
                #     start_dates[do_no] = date
                # console_logger.debug(payload.get("start_date"))
                if payload.get("slno"):
                    aggregated_data[date][do_no]["slno"] = datetime.datetime.strptime(payload.get("slno"), '%Y%m').strftime('%B %Y')
                else:
                    aggregated_data[date][do_no]["slno"] = "-"
                if payload.get("start_date"):
                    aggregated_data[date][do_no]["start_date"] = payload.get("start_date")
                else:
                    aggregated_data[date][do_no]["start_date"] = "0"
                if payload.get("end_date"):
                    aggregated_data[date][do_no]["end_date"] = payload.get("end_date")
                else:
                    aggregated_data[date][do_no]["end_date"] = "0"

                if payload.get("Type_of_consumer"):
                    aggregated_data[date][do_no]["source_type"] = payload.get("Type_of_consumer")

                if payload.get("DO_Qty"):
                    aggregated_data[date][do_no]["DO_Qty"] = float(
                        payload["DO_Qty"]
                    )
                else:
                    aggregated_data[date][do_no]["DO_Qty"] = 0

                challan_net_wt = payload.get("Challan_Net_Wt(MT)")    

                if challan_net_wt:
                    aggregated_data[date][do_no]["challan_lr_qty"] += float(challan_net_wt)

                if payload.get("Mines_Name"):
                    aggregated_data[date][do_no]["mine_name"] = payload[
                        "Mines_Name"
                    ]
                else:
                    aggregated_data[date][do_no]["mine_name"] = "-"

                aggregated_data[date][do_no]["grn_status"] = payload[
                    "grn_status"
                ]

                aggregated_data[date][do_no]["count"] += 1 

            # console_logger.debug(aggregated_data)
            
            for record in sap_records:
                do_no = record.do_no
                console_logger.debug(do_no)
                if do_no not in aggregated_data[specified_date]:
                    aggregated_data[specified_date][do_no]["DO_Qty"] = float(record.do_qty) if record.do_qty else 0
                    aggregated_data[specified_date][do_no]["mine_name"] = record.mine_name if record.mine_name else "-"
                    aggregated_data[specified_date][do_no]["start_date"] = record.start_date if record.start_date else "0"
                    aggregated_data[specified_date][do_no]["end_date"] = record.end_date if record.end_date else "0"
                    aggregated_data[specified_date][do_no]["source_type"] = record.consumer_type if record.consumer_type else "Unknown"
                    # aggregated_data[specified_date][do_no]["grn_status"] = record.grn_status
                    try:
                        aggregated_data[specified_date][do_no]["slno"] = datetime.datetime.strptime(record.slno, "%Y%m").strftime("%B %Y") if record.slno else "-"
                    except ValueError as e:
                        aggregated_data[specified_date][do_no]["slno"] = record.slno if record.slno else "-"
                    aggregated_data[specified_date][do_no]["count"] = 1

            dataList = [
                {
                    "date": date,
                    "data": {
                        do_no: {
                            "DO_Qty": data["DO_Qty"],
                            "challan_lr_qty": data["challan_lr_qty"],
                            "mine_name": data["mine_name"],
                            "grade": grade,
                            "date": date,
                            "start_date": data["start_date"],
                            "end_date": data["end_date"],
                            "source_type": data["source_type"],
                            "slno": data["slno"],
                            "grn_status": data["grn_status"],
                        }
                        for do_no, data in aggregated_data[date].items()
                    },
                }
                for date in aggregated_data
            ]

            # final_data = []
            for entry in dataList:
                date = entry["date"]
                for data_dom, values in entry['data'].items():
                    dictData = {}
                    dictData["DO_No"] = data_dom
                    dictData["mine_name"] = values["mine_name"]
                    dictData["DO_Qty"] = values["DO_Qty"]
                    dictData["club_challan_lr_qty"] = values["challan_lr_qty"]
                    dictData["challan_lr_qty"] = 0
                    dictData["grn_status"] = values["grn_status"]
                    dictData["date"] = values["date"]
                    dictData["start_date"] = values["start_date"]
                    dictData["end_date"] = values["end_date"]
                    dictData["source_type"] = values["source_type"]
                    dictData["slno"] = values["slno"]
                    dictData["cumulative_challan_lr_qty"] = 0
                    dictData["balance_qty"] = 0
                    dictData["percent_supply"] = 0
                    dictData["asking_rate"] = 0
                    dictData['average_GCV_Grade'] = values["grade"]
                    
                    if dictData["start_date"] != "0" and dictData["end_date"] != "0":
                        # today_date = datetime.datetime.today().date()
                        # today_date = datetime.datetime.strptime(specified_date, "%Y-%m-%d")
                        # balance_days = datetime.datetime.strptime(dictData["end_date"], "%Y-%m-%d").date() - datetime.datetime.strptime(dictData["start_date"], "%Y-%m-%d").date()
                        tomorrow_date = datetime.datetime.strptime(dictData["end_date"], "%Y-%m-%d").date() + datetime.timedelta(days=1)
                        # balance_days = datetime.datetime.strptime(dictData["end_date"], "%Y-%m-%d").date() - datetime.datetime.today().date()
                        balance_days = tomorrow_date - datetime.datetime.strptime(specified_date, "%Y-%m-%d").date()
                        dictData["balance_days"] = balance_days.days
                    else:
                        dictData["balance_days"] = 0
                    
                    final_data.append(dictData)
            if final_data:
                startdate = f'{specified_date} 00:00:00'
                enddate = f'{specified_date} 23:59:59'
                # to_ts = datetime.datetime.strptime(enddate,"%Y-%m-%d %H:%M:%S")
                from_ts = convert_to_utc_format(startdate, "%Y-%m-%d %H:%M:%S")
                to_ts = convert_to_utc_format(enddate, "%Y-%m-%d %H:%M:%S")
                
                # console_logger.debug(from_ts)
                # console_logger.debug(to_ts)

                # pipeline = [
                #     {
                #         # "$match": {
                #         #     "$and": [
                #         #         {
                #         #             "$or": [
                #         #                 { "GWEL_Tare_Time": { "$gte": from_ts, "$lte": to_ts } },
                #         #                 { "GWEL_Tare_Time": None, "vehicle_in_time": { "$gte": from_ts, "$lte": to_ts } }
                #         #             ]
                #         #         },
                #         #         # { "net_qty": { "$ne": None } }
                #         #     ]
                #         # }
                #         "$match": {
                #             "$or": [
                #                 {
                #                     "GWEL_Tare_Time": {
                #                         "$ne": None,
                #                         "$gte": from_ts,
                #                         "$lte": to_ts
                #                     }
                #                 },
                #                 # {
                #                 #     "GWEL_Tare_Time": None,
                #                 #     "vehicle_in_time": {
                #                 #         "$gte": from_ts,
                #                 #         "$lte": to_ts
                #                 #     }
                #                 # }
                #             ]
                #         }
                #     },
                #     # {
                #     # '$group': {
                #     #     '_id': {
                #     #         'date': {
                #     #             '$dateToString': {
                #     #                 'format': '%Y-%m-%d', 
                #     #                 'date': {
                #     #                     '$ifNull': ['$GWEL_Tare_Time', '$vehicle_in_time']
                #     #                 },
                #     #                 # 'date': '$GWEL_Tare_Time',
                #     #             }
                #     #         }, 
                #     #         'do_no': '$arv_cum_do_number'
                #     #     }, 
                #     #     'total_net_qty': {
                #     #         '$sum': {
                #     #             '$toDouble': '$net_qty'
                #     #         }
                #     #     }
                #     # }
                #     {
                #     '$group': {
                #         "_id": "$arv_cum_do_number", 
                #         'total_net_qty': {
                #             '$sum': {
                #                 '$toDouble': '$net_qty'
                #             }
                #         }
                #     }
                # }]

                pipeline = [
                    {
                        "$match": {
                            "GWEL_Tare_Time": {"$gte": from_ts, "$lte": to_ts},
                                "net_qty": {"$ne": None}
                            }
                    },
                    {
                    '$group': {
                        '_id': {
                            'date': {
                                '$dateToString': {
                                    'format': '%Y-%m-%d', 
                                    'date': '$GWEL_Tare_Time'
                                }
                            }, 
                            'do_no': '$arv_cum_do_number'
                        }, 
                        'total_net_qty': {
                            '$sum': {
                                '$toDouble': '$net_qty'
                            }
                        }
                    }
                }]

                cclrpipeline = [
                    {
                        "$match": {
                            # "GWEL_Tare_Time": {"$lt": to_ts},
                            "net_qty": {"$ne": None}
                        }
                    },
                    {
                        "$group": {
                            "_id": "$arv_cum_do_number",
                            "cumulative_challan_lr_qty": {
                                "$sum": {
                                    "$toDouble": "$net_qty"
                                }
                            }
                        }
                    }
                ]
                
                filtered_data_new = Gmrdata.objects.aggregate(pipeline)
                aggregation_result_new = Gmrdata.objects.aggregate(cclrpipeline)

                cumulative_data_by_do = {item['_id']: item['cumulative_challan_lr_qty'] for item in aggregation_result_new}

                aggregated_totals = defaultdict(float)
                for single_data_entry in filtered_data_new:
                    do_no = single_data_entry['_id']['do_no']
                    total_net_qty = single_data_entry['total_net_qty']
                    aggregated_totals[do_no] += total_net_qty

                # console_logger.debug(aggregated_totals)
                    
                data_by_do = {}

                finaldataMain = [single_data_list for single_data_list in final_data if single_data_list.get("balance_days") >= 0]

                # Update data1 with values from data2
                for item in finaldataMain:
                    do_no = item['DO_No']
                    if do_no in cumulative_data_by_do:
                        item['cumulative_challan_lr_qty'] = cumulative_data_by_do[do_no]
                    else:
                        item['cumulative_challan_lr_qty'] = 0

                for entry in finaldataMain:
                    do_no = entry['DO_No']

                    data_by_do[do_no] = entry

                    if do_no in aggregated_totals:
                        data_by_do[do_no]['challan_lr_qty'] = round(aggregated_totals[do_no], 2)
                    else:
                        data_by_do[do_no]['challan_lr_qty'] = 0

                    if data_by_do[do_no]['DO_Qty'] != 0 and data_by_do[do_no]['cumulative_challan_lr_qty'] != 0:
                        data_by_do[do_no]['percent_supply'] = round((data_by_do[do_no]['cumulative_challan_lr_qty'] / data_by_do[do_no]['DO_Qty']) * 100, 2)
                    else:
                        data_by_do[do_no]['percent_supply'] = 0

                    # if data_by_do[do_no]['cumulative_challan_lr_qty'] != 0 and data_by_do[do_no]['DO_Qty'] != 0:
                    data_by_do[do_no]['balance_qty'] = round(data_by_do[do_no]['DO_Qty'] - data_by_do[do_no]['cumulative_challan_lr_qty'], 2)
                    # else:
                    #     data_by_do[do_no]['balance_qty'] = 0
                    
                    if data_by_do[do_no]['balance_days'] and data_by_do[do_no]['balance_qty'] != 0:
                        data_by_do[do_no]['asking_rate'] = round(data_by_do[do_no]['balance_qty'] / data_by_do[do_no]['balance_days'], 2)

                    del entry['club_challan_lr_qty']
                
                # final_data = list(data_by_do.values())

                sort_final_data = list(data_by_do.values())
                # Sort the data by 'balance_days', placing entries with 'balance_days' of 0 at the end
                final_data_check = sorted(sort_final_data, key=lambda x: (x['balance_days'] == 0, x['balance_days']))
                
                console_logger.debug(final_data_check)
                # current_date = datetime.datetime.now()

                # Step 1: Remove dictionaries where grn_status = True and percent_supply = 100
                # filtered_list = [
                #     d for d in final_data_check 
                #     if not (d['grn_status'] and d['percent_supply'] == 100)
                # ]

                # console_logger.debug(final_data_check)

                # filtered_list = [
                #     d for d in final_data_check
                #     if d['percent_supply'] <= 100
                # ]

                # console_logger.debug(final_data_check)

                # another_final_data = [d for d in final_data_check if d['end_date'] != '0']

                # Filter records where grn_status is True and end_date is greater than specified_date
                # final_data = [
                #     d for d in another_final_data
                #     if d['grn_status'] and datetime.datetime.strptime(d['end_date'], '%Y-%m-%d') > datetime.datetime.strptime(specified_date, "%Y-%m-%d")
                # ]

                final_data = [
                    d for d in final_data_check 
                    if d['start_date'] != '0' and datetime.datetime.strptime(d['start_date'], '%Y-%m-%d').date() <= datetime.datetime.now().date()
                ]
                # console_logger.debug(final_data)

                # final_data = [
                #     d for d in final_data_check
                #     if (datetime.datetime.strptime(d['end_date'], '%Y-%m-%d') + datetime.timedelta(days=2)) > datetime.datetime.strptime(specified_date, "%Y-%m-%d")
                # ]

                # final_data = [
                #     d for d in filtered_list 
                #     if not (d['grn_status']) and d['end_date'] != '0' and
                #     datetime.datetime.strptime(d['end_date'], '%Y-%m-%d').date() > datetime.datetime.now().date()
                # ]

        rrNo_values_old, clubbed_data, aopList_old = bar_graph_data(specified_date)
        rrNo_values, aopList = endpoint_to_fetch_report_name(type="Daily", Daily=specified_date)
        console_logger.debug(rrNo_values)
        console_logger.debug(aopList)
        clubbed_data_final = gmr_main_graph()
        total_monthly_final_net_qty = transit_loss_gain_road_mode_month(specified_date)
        yearly_final_data = transit_loss_gain_road_mode()
        yearly_rail_final_data = transit_loss_gain_rail_mode()

        dayWiseVehicleInCount = daywise_in_vehicle_count_datewise(specified_date)
        dayWiseGrnReceive = daywise_grn_receive_datewise(specified_date)
        dayWiseGwelReceive = daywise_gwel_receive_pdf_datewise(specified_date)
        dayWiseOutVehicelCount = daywise_out_vehicle_count_datewise(specified_date)

        fetchRailData = rail_pdf(specified_date)

        fetchRakeQuota = end_point_to_fetch_rake_quota_test(response)

        # console_logger.debug(fetchRakeQuota)

        seclLinkagegraph = endpoint_to_fetch_secl_linkage_matrialization(response, str(datetime.datetime.strptime(specified_date, "%Y-%m-%d").strftime("%Y")))
        wclLinkagegraph = endpoint_to_fetch_wcl_linkage_matrialization(response, str(datetime.datetime.strptime(specified_date, "%Y-%m-%d").strftime("%Y")))

        # console_logger.debug(seclLinkagegraph)
        # console_logger.debug(wclLinkagegraph)

        if specified_date:
            month_data = specified_date
            fetchData = generate_report(final_data, rrNo_values, month_data, clubbed_data, clubbed_data_final, dayWiseVehicleInCount, dayWiseGrnReceive, dayWiseGwelReceive, dayWiseOutVehicelCount, total_monthly_final_net_qty, yearly_final_data, aopList, fetchRailData, yearly_rail_final_data, fetchRakeQuota.get('datasets'), seclLinkagegraph, wclLinkagegraph)
            return fetchData
        else:
            fetchData = generate_report(final_data, rrNo_values, "", clubbed_data, clubbed_data_final, dayWiseVehicleInCount, dayWiseGrnReceive, dayWiseGwelReceive, dayWiseOutVehicelCount, total_monthly_final_net_qty, yearly_final_data, aopList, fetchRailData, yearly_rail_final_data, fetchRakeQuota.get('datasets'), seclLinkagegraph, wclLinkagegraph)
            return fetchData
            
        # else:
        #     return 400
    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug(
            "Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno)
        )
        return e

@router.get("/pdf_minewise_road_aggregation", tags=["PDF Report"])
def generate_gmr_report_aggregation(response: Response, specified_date: Optional[str]=None):
    try:
        if specified_date:
            from_ts = convert_to_utc_format(f'{specified_date} 00:00:00', "%Y-%m-%d %H:%M:%S")
            to_ts = convert_to_utc_format(f'{specified_date} 23:59:59', "%Y-%m-%d %H:%M:%S")

            # basePipeline = [
            #     {
            #         '$match': {
            #             'GWEL_Tare_Time': {
            #                 '$ne': None, 
            #                 '$gte': from_ts, 
            #                 '$lte': to_ts,
            #             }
            #         }
            #     }, {
            #         '$group': {
            #             '_id': '$arv_cum_do_number', 
            #             'challan_lr_qty': {
            #                 '$sum': {
            #                     '$toDouble': '$net_qty'
            #                 }
            #             }, 
            #             'Grade': {
            #                 '$first': '$grade'
            #             }, 
            #             'slno': {
            #                 '$first': '$slno'
            #             }, 
            #             'start_date': {
            #                 '$first': '$start_date'
            #             }, 
            #             'end_date': {
            #                 '$first': '$end_date'
            #             }, 
            #             'type_consumer': {
            #                 '$first': '$type_consumer'
            #             }, 
            #             'do_qty': {
            #                 '$first': '$do_qty'
            #             }, 
            #             'mine_name': {
            #                 '$first': '$mine'
            #             }, 
            #             'grn_status': {
            #                 '$first': '$grn_status'
            #             },
            #             'date': {
            #                 '$last': '$GWEL_Tare_Time'
            #             }
            #         }
            #     }
            # ]
        basePipeline = [
            {
                '$match': {
                    'GWEL_Tare_Time': {
                        '$ne': None, 
                        '$gte': from_ts, 
                        '$lte': to_ts,
                    }
                }
            }, {
                '$group': {
                    '_id': '$arv_cum_do_number', 
                    'challan_lr_qty': {
                        '$sum': {
                            '$toDouble': '$net_qty'
                        }
                    }, 
                    'Grade': {
                        '$first': '$grade'
                    }, 
                    'slno': {
                        '$first': '$slno'
                    }, 
                    'start_date': {
                        '$first': '$start_date'
                    }, 
                    'end_date': {
                        '$first': '$end_date'
                    }, 
                    'type_consumer': {
                        '$first': '$type_consumer'
                    }, 
                    'do_qty': {
                        '$first': '$po_qty'
                    }, 
                    'mine_name': {
                        '$first': '$mine'
                    }, 
                    'grn_status': {
                        '$first': '$grn_status'
                    },
                    'date': {
                        '$last': '$GWEL_Tare_Time'
                    }
                }
            }, {
                '$lookup': {
                    'from': 'gmrdata', 
                    'localField': '_id', 
                    'foreignField': 'arv_cum_do_number', 
                    'as': 'cumulative_data'
                }
            }, {
                '$addFields': {
                    'cumulative_challan_lr_qty': {
                        '$sum': {
                            '$map': {
                                'input': '$cumulative_data', 
                                'as': 'item', 
                                'in': {
                                    '$toDouble': '$$item.net_qty'
                                }
                            }
                        }
                    }
                }
            }
        ]
        fetchGmrData = Gmrdata.objects.aggregate(basePipeline)
        listData= []
        for singleData in fetchGmrData:
            dictData = {}
            dictData["DO_No"] = singleData.get("_id")
            dictData["mine_name"] = singleData.get("mine_name")
            dictData["DO_Qty"] = singleData.get("do_qty")
            dictData["challan_lr_qty"] = singleData.get("challan_lr_qty")
            dictData["cumulative_challan_lr_qty"] = singleData.get("cumulative_challan_lr_qty")
            dictData["grade"] = singleData.get("")
            dictData["grn_status"] = singleData.get("grn_status")
            dictData["date"] = singleData.get("date").strftime("%Y-%m-%d")
            dictData["start_date"] = singleData.get("start_date")
            dictData["end_date"] = singleData.get("end_date")
            dictData["source_type"] = singleData.get("type_consumer")
            dictData["slno"] = singleData.get("slno")
            if singleData.get("Grade") is not None:
                if '-' in singleData.get("Grade"):
                    dictData["grade"] = singleData.get("Grade").split("-")[0]
                elif " " in singleData.get("Grade"):
                    dictData["grade"] = singleData.get("Grade").split(" ")[0]
                else:
                    dictData["grade"] = singleData.get("Grade")
            if singleData.get("start_date") is not None and singleData.get("end_date") is not None:
                tomorrow_date = datetime.datetime.strptime(singleData.get("end_date"), "%Y-%m-%d").date() + datetime.timedelta(days=1)
                balance_days = tomorrow_date - datetime.datetime.today().date()
                dictData["balance_days"] = balance_days.days
            else:
                dictData["balance_days"] = 0
            if singleData.get("do_qty") is not None and singleData.get("cumulative_challan_lr_qty") != 0:
                dictData['percent_supply'] = round((singleData.get('cumulative_challan_lr_qty') / int(singleData.get('do_qty'))) * 100, 2)
            else:
                dictData["percent_supply"] = 0
            if singleData.get("do_qty") is not None and singleData.get("cumulative_challan_lr_qty") != 0:
                dictData["balance_qty"] = round(int(singleData.get("do_qty")) - singleData.get("cumulative_challan_lr_qty"), 2)
            else:
                dictData["balance_qty"] = 0
                
            if dictData['balance_days'] and dictData['balance_qty'] != 0:
                dictData['asking_rate'] = round(dictData['balance_qty'] / dictData['balance_days'], 2)
            else:
                dictData["asking_rate"] = 0
            listData.append(dictData)
        return listData

    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(
            "Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno)
        )
        return e


@router.get("/pdf_minewise_road", tags=["PDF Report"])
def generate_gmr_report(response: Response, specified_date: Optional[str]=None, mine: Optional[str] = "All"):
    try:
        if specified_date:
            from_ts = convert_to_utc_format(f'{specified_date} 00:00:00', "%Y-%m-%d %H:%M:%S")
            to_ts = convert_to_utc_format(f'{specified_date} 23:59:59', "%Y-%m-%d %H:%M:%S")
        # staticdate "2024-09-23T19:50:51.572Z"

        # console_logger.debug(from_ts)
        # console_logger.debug(to_ts)
        
        created_at_date = datetime.datetime(2024, 9, 23, 19, 50, 51, 572000)
        basePipeline = [
            {
                '$match': {
                    'created_at': {
                        '$gt': created_at_date,
                    }
                }
            },
            {
                '$match': {
                    'GWEL_Tare_Time': {
                        '$ne': None, 
                        # '$gte': from_ts, 
                        '$lte': to_ts,
                    }
                }
            }, {
                '$group': {
                    '_id': '$arv_cum_do_number', 
                    'challan_lr_qty': {
                        '$sum': {
                            '$toDouble': '$net_qty'
                        }
                    }, 
                    'Grade': {
                        '$first': '$grade'
                    }, 
                    'slno': {
                        '$first': '$slno'
                    }, 
                    'start_date': {
                        '$first': '$start_date'
                    }, 
                    'end_date': {
                        '$first': '$end_date'
                    }, 
                    'type_consumer': {
                        '$first': '$type_consumer'
                    }, 
                    'do_qty': {
                        '$first': '$po_qty'
                    }, 
                    'mine_name': {
                        '$first': '$mine'
                    }, 
                    'grn_status': {
                        '$first': '$grn_status'
                    },
                    'date': {
                        '$last': '$GWEL_Tare_Time'
                    }
                }
            }, {
                '$lookup': {
                    'from': 'gmrdata', 
                    'localField': '_id', 
                    'foreignField': 'arv_cum_do_number', 
                    'as': 'cumulative_data'
                }
            }, 
            # {
            #     '$addFields': {
            #         'cumulative_challan_lr_qty': {
            #             '$sum': {
            #                 '$map': {
            #                     'input': '$cumulative_data', 
            #                     'as': 'item', 
            #                     'in': {
            #                         '$toDouble': '$$item.net_qty'
            #                     }
            #                 }
            #             }
            #         }
            #     }
            # }
            {
                '$addFields': {
                    'cumulative_challan_lr_qty': {
                        '$sum': {
                            '$map': {
                                'input': '$cumulative_data', 
                                'as': 'item', 
                                'in': {
                                    '$convert': {
                                        'input': '$$item.net_qty', 
                                        'to': 'double', 
                                        'onError': 0, 
                                        'onNull': 0
                                    }
                                }
                            }
                        }
                    }
                }
            }
        ]

        challanlrqtybasePipeline = [
            {
                '$match': {
                    'created_at': {
                        '$gt': created_at_date,
                    }
                }
            },
            {
                '$match': {
                    'GWEL_Tare_Time': {
                        '$ne': None, 
                        '$gte': from_ts, 
                        '$lte': to_ts,
                    }
                }
            }, {
                '$group': {
                    '_id': '$arv_cum_do_number', 
                    'challan_lr_qty': {
                        '$sum': {
                            '$toDouble': '$net_qty'
                        }
                    }, 
                    'Grade': {
                        '$first': '$grade'
                    }, 
                    'slno': {
                        '$first': '$slno'
                    }, 
                    'start_date': {
                        '$first': '$start_date'
                    }, 
                    'end_date': {
                        '$first': '$end_date'
                    }, 
                    'type_consumer': {
                        '$first': '$type_consumer'
                    }, 
                    'do_qty': {
                        '$first': '$po_qty'
                    }, 
                    'mine_name': {
                        '$first': '$mine'
                    }, 
                    'grn_status': {
                        '$first': '$grn_status'
                    },
                    'date': {
                        '$last': '$GWEL_Tare_Time'
                    }
                }
            }, {
                '$lookup': {
                    'from': 'gmrdata', 
                    'localField': '_id', 
                    'foreignField': 'arv_cum_do_number', 
                    'as': 'cumulative_data'
                }
            }, 
            # {
            #     '$addFields': {
            #         'cumulative_challan_lr_qty': {
            #             '$sum': {
            #                 '$map': {
            #                     'input': '$cumulative_data', 
            #                     'as': 'item', 
            #                     'in': {
            #                         '$toDouble': '$$item.net_qty'
            #                     }
            #                 }
            #             }
            #         }
            #     }
            # }
            {
                '$addFields': {
                    'cumulative_challan_lr_qty': {
                        '$sum': {
                            '$map': {
                                'input': '$cumulative_data', 
                                'as': 'item', 
                                'in': {
                                    '$convert': {
                                        'input': '$$item.net_qty', 
                                        'to': 'double', 
                                        'onError': 0, 
                                        'onNull': 0
                                    }
                                }
                            }
                        }
                    }
                }
            }
        ]
        basePipelineHistoric = [
            {
                '$match': {
                    'GWEL_Tare_Time': {
                        '$ne': None, 
                        # '$gte': from_ts, 
                        '$lte': to_ts,
                    }
                }
            }, {
                '$group': {
                    '_id': '$arv_cum_do_number', 
                    'challan_lr_qty': {
                        '$sum': {
                            '$toDouble': '$net_qty'
                        }
                    }, 
                    'Grade': {
                        '$first': '$grade'
                    }, 
                    'slno': {
                        '$first': '$slno'
                    }, 
                    'start_date': {
                        '$first': '$start_date'
                    }, 
                    'end_date': {
                        '$first': '$end_date'
                    }, 
                    'type_consumer': {
                        '$first': '$type_consumer'
                    }, 
                    'do_qty': {
                        '$first': '$po_qty'
                    }, 
                    'mine_name': {
                        '$first': '$mine'
                    }, 
                    'grn_status': {
                        '$first': '$grn_status'
                    },
                    'date': {
                        '$last': '$GWEL_Tare_Time'
                    }
                }
            }, {
                '$lookup': {
                    'from': 'gmrdata', 
                    'localField': '_id', 
                    'foreignField': 'arv_cum_do_number', 
                    'as': 'cumulative_data'
                }
            }, 
            # {
            #     '$addFields': {
            #         'cumulative_challan_lr_qty': {
            #             '$sum': {
            #                 '$map': {
            #                     'input': '$cumulative_data', 
            #                     'as': 'item', 
            #                     'in': {
            #                         '$toDouble': '$$item.net_qty'
            #                     }
            #                 }
            #             }
            #         }
            #     }
            # }
            {
                '$addFields': {
                    'cumulative_challan_lr_qty': {
                        '$sum': {
                            '$map': {
                                'input': '$cumulative_data', 
                                'as': 'item', 
                                'in': {
                                    '$convert': {
                                        'input': '$$item.net_qty', 
                                        'to': 'double', 
                                        'onError': 0, 
                                        'onNull': 0
                                    }
                                }
                            }
                        }
                    }
                }
            }
        ]

        basepipelineHistoricChallanLrQty = [
            {
                '$match': {
                    'GWEL_Tare_Time': {
                        '$ne': None, 
                        '$gte': from_ts, 
                        '$lte': to_ts,
                    }
                }
            }, {
                '$group': {
                    '_id': '$arv_cum_do_number', 
                    'challan_lr_qty': {
                        '$sum': {
                            '$toDouble': '$net_qty'
                        }
                    }, 
                    'Grade': {
                        '$first': '$grade'
                    }, 
                    'slno': {
                        '$first': '$slno'
                    }, 
                    'start_date': {
                        '$first': '$start_date'
                    }, 
                    'end_date': {
                        '$first': '$end_date'
                    }, 
                    'type_consumer': {
                        '$first': '$type_consumer'
                    }, 
                    'do_qty': {
                        '$first': '$po_qty'
                    }, 
                    'mine_name': {
                        '$first': '$mine'
                    }, 
                    'grn_status': {
                        '$first': '$grn_status'
                    },
                    'date': {
                        '$last': '$GWEL_Tare_Time'
                    }
                }
            }, {
                '$lookup': {
                    'from': 'gmrdata', 
                    'localField': '_id', 
                    'foreignField': 'arv_cum_do_number', 
                    'as': 'cumulative_data'
                }
            }, 
            # {
            #     '$addFields': {
            #         'cumulative_challan_lr_qty': {
            #             '$sum': {
            #                 '$map': {
            #                     'input': '$cumulative_data', 
            #                     'as': 'item', 
            #                     'in': {
            #                         '$toDouble': '$$item.net_qty'
            #                     }
            #                 }
            #             }
            #         }
            #     }
            # }
            {
                '$addFields': {
                    'cumulative_challan_lr_qty': {
                        '$sum': {
                            '$map': {
                                'input': '$cumulative_data', 
                                'as': 'item', 
                                'in': {
                                    '$convert': {
                                        'input': '$$item.net_qty', 
                                        'to': 'double', 
                                        'onError': 0, 
                                        'onNull': 0
                                    }
                                }
                            }
                        }
                    }
                }
            }
        ]


        # roadrcr start

        basePipelineRcrRoad = [
            {
                '$match': {
                    'tar_wt_date': {
                        '$ne': None, 
                        # '$gte': from_ts, 
                        '$lte': to_ts,
                    }
                }
            }, {
                '$group': {
                    '_id': '$do_number', 
                    'challan_lr_qty': {
                        '$sum': {
                            '$toDouble': '$dc_net_wt'
                        }
                    }, 
                    'Grade': {
                        '$first': '$grade'
                    }, 
                    'slno': {
                        '$first': '$slno'
                    }, 
                    'start_date': {
                        '$first': '$start_date'
                    }, 
                    'end_date': {
                        '$first': '$end_date'
                    }, 
                    'type_consumer': {
                        '$first': '$type_consumer'
                    }, 
                    'do_qty': {
                        '$first': '$po_qty'
                    }, 
                    'mine_name': {
                        '$first': '$mine'
                    }, 
                    # 'grn_status': {
                    #     '$first': '$grn_status'
                    # },
                    'date': {
                        '$last': '$tar_wt_date'
                    }
                }
            }, {
                '$lookup': {
                    'from': 'RcrRoadData', 
                    'localField': '_id', 
                    'foreignField': 'do_number', 
                    'as': 'cumulative_data'
                }
            }, 
            # {
            #     '$addFields': {
            #         'cumulative_challan_lr_qty': {
            #             '$sum': {
            #                 '$map': {
            #                     'input': '$cumulative_data', 
            #                     'as': 'item', 
            #                     'in': {
            #                         '$toDouble': '$$item.dc_net_wt'
            #                     }
            #                 }
            #             }
            #         }
            #     }
            # }
            {
                '$addFields': {
                    'cumulative_challan_lr_qty': {
                        '$sum': {
                            '$map': {
                                'input': '$cumulative_data', 
                                'as': 'item', 
                                'in': {
                                    '$convert': {
                                        'input': '$$item.dc_net_wt', 
                                        'to': 'double', 
                                        'onError': 0, 
                                        'onNull': 0
                                    }
                                }
                            }
                        }
                    }
                }
            }
        ]


        basepipelineRcrRoadChallanLrQty = [
            {
                '$match': {
                    'tar_wt_date': {
                        '$ne': None, 
                        '$gte': from_ts, 
                        '$lte': to_ts,
                    }
                }
            }, {
                '$group': {
                    '_id': '$do_number', 
                    'challan_lr_qty': {
                        '$sum': {
                            '$toDouble': '$dc_net_wt'
                        }
                    }, 
                    'Grade': {
                        '$first': '$grade'
                    }, 
                    'slno': {
                        '$first': '$slno'
                    }, 
                    'start_date': {
                        '$first': '$start_date'
                    }, 
                    'end_date': {
                        '$first': '$end_date'
                    }, 
                    'type_consumer': {
                        '$first': '$type_consumer'
                    }, 
                    'do_qty': {
                        '$first': '$po_qty'
                    }, 
                    'mine_name': {
                        '$first': '$mine'
                    }, 
                    # 'grn_status': {
                    #     '$first': '$grn_status'
                    # },
                    'date': {
                        '$last': '$tar_wt_date'
                    }
                }
            }, {
                '$lookup': {
                    'from': 'RcrRoadData', 
                    'localField': '_id', 
                    'foreignField': 'do_number', 
                    'as': 'cumulative_data'
                }
            }, 
            # {
            #     '$addFields': {
            #         'cumulative_challan_lr_qty': {
            #             '$sum': {
            #                 '$map': {
            #                     'input': '$cumulative_data', 
            #                     'as': 'item', 
            #                     'in': {
            #                         '$toDouble': '$$item.dc_net_wt'
            #                     }
            #                 }
            #             }
            #         }
            #     }
            # }
            {
                '$addFields': {
                    'cumulative_challan_lr_qty': {
                        '$sum': {
                            '$map': {
                                'input': '$cumulative_data', 
                                'as': 'item', 
                                'in': {
                                    '$convert': {
                                        'input': '$$item.dc_net_wt', 
                                        'to': 'double', 
                                        'onError': 0, 
                                        'onNull': 0
                                    }
                                }
                            }
                        }
                    }
                }
            }
        ]
        
        # roadrcr end 

        # saprecordsPipeline = [
        #     {
        #         '$match': {
        #             '$expr': {
        #                 '$and': [
        #                     {
        #                         '$gte': [
        #                             {
        #                                 '$dateFromString': {
        #                                     'dateString': '$start_date'
        #                                 }
        #                             }, specified_date
        #                         ]
        #                     }, {
        #                         '$lte': [
        #                             {
        #                                 '$dateFromString': {
        #                                     'dateString': '$end_date'
        #                                 }
        #                             }, specified_date
        #                         ]
        #                     }
        #                 ]
        #             }
        #         }
        #     }, {
        #         '$group': {
        #             '_id': '$do_no', 
        #             'mine_name': {
        #                 '$first': '$mine_name'
        #             }, 
        #             'do_qty': {
        #                 '$sum': {
        #                     '$toDouble': '$do_qty'
        #                 }
        #             }, 
        #             'start_date': {
        #                 '$first': '$start_date'
        #             }, 
        #             'end_date': {
        #                 '$first': '$end_date'
        #             }, 
        #             'source_type': {
        #                 '$first': '$source'
        #             }, 
        #             'slno': {
        #                 '$first': '$slno'
        #             }
        #         }
        #     }, {
        #         '$project': {
        #             '_id': 1, 
        #             'mine_name': 1, 
        #             'do_qty': 1, 
        #             'start_date': 1, 
        #             'end_date': 1, 
        #             'source_type': 1, 
        #             'slno': 1
        #         }
        #     }
        # ]
        saprecordsPipeline = [
            {
                '$match': {
                    '$expr': {
                        '$and': [
                            {
                                '$lte': [
                                    { '$dateFromString': { 'dateString': '$start_date' } }, 
                                    datetime.datetime.strptime(specified_date, "%Y-%m-%d")
                                ]
                            }, 
                            {
                                '$gte': [
                                    { '$dateFromString': { 'dateString': '$end_date' } }, 
                                    datetime.datetime.strptime(specified_date, "%Y-%m-%d")
                                ]
                            }
                        ]
                    }
                }
            }, 
            {
                '$group': {
                    '_id': '$do_no',  # Grouping by do_no
                    'mine_name': { '$first': '$mine_name' },  # Getting the first mine_name in the group
                    'do_qty': { '$sum': { '$toDouble': '$do_qty' } },  # Summing up the do_qty as double
                    'start_date': { '$first': '$start_date' },  # Getting the first start_date
                    'end_date': { '$first': '$end_date' },  # Getting the first end_date
                    'source_type': { '$first': '$consumer_type' },  # Getting the first source_type
                    'slno': { '$first': '$slno' },  # Getting the first slno
                    'Grade': {'$first': '$grade'}
                }
            }, 
            {
                '$project': {
                    '_id': 1, 
                    'mine_name': 1, 
                    'do_qty': 1, 
                    'start_date': 1, 
                    'end_date': 1, 
                    'source_type': 1, 
                    'slno': 1,
                    'Grade': 1,
                }
            }
        ]


        saprecordsRcrRoadPipeline = [
            {
                '$match': {
                    '$expr': {
                        '$and': [
                            {
                                '$lte': [
                                    { '$dateFromString': { 'dateString': '$start_date' } }, 
                                    datetime.datetime.strptime(specified_date, "%Y-%m-%d")
                                ]
                            }, 
                            {
                                '$gte': [
                                    { '$dateFromString': { 'dateString': '$end_date' } }, 
                                    datetime.datetime.strptime(specified_date, "%Y-%m-%d")
                                ]
                            }
                        ]
                    }
                }
            }, 
            {
                '$group': {
                    '_id': '$do_no',  # Grouping by do_no
                    'mine_name': { '$first': '$mine_name' },  # Getting the first mine_name in the group
                    'do_qty': { '$sum': { '$toDouble': '$do_qty' } },  # Summing up the do_qty as double
                    'start_date': { '$first': '$start_date' },  # Getting the first start_date
                    'end_date': { '$first': '$end_date' },  # Getting the first end_date
                    'source_type': { '$first': '$consumer_type' },  # Getting the first source_type
                    'slno': { '$first': '$slno' },  # Getting the first slno
                    'Grade': {'$first': '$grade'}
                }
            }, 
            {
                '$project': {
                    '_id': 1, 
                    'mine_name': 1, 
                    'do_qty': 1, 
                    'start_date': 1, 
                    'end_date': 1, 
                    'source_type': 1, 
                    'slno': 1,
                    'Grade': 1,
                }
            }
        ]


        fetchGmrData = Gmrdata.objects.aggregate(basePipeline)
        fetchGmrDatachallanltqty = Gmrdata.objects.aggregate(challanlrqtybasePipeline)
        
        fetchGmrHistoricData = gmrdataHistoric.objects.aggregate(basePipelineHistoric)
        fetchGmrHistoricDataChallanLrQty = gmrdataHistoric.objects.aggregate(basepipelineHistoricChallanLrQty)
        
        fetchRcrRoadData = RcrRoadData.objects.aggregate(basePipelineRcrRoad)
        fetchRcrRoadchallanlrqty = RcrRoadData.objects.aggregate(basepipelineRcrRoadChallanLrQty)
        
        fetchSapRecordsData = SapRecords.objects.aggregate(saprecordsPipeline)

        fetchSapRecordsRcrData = SapRecordsRcrRoad.objects.aggregate(saprecordsRcrRoadPipeline)
        listData= []
        for singleData in fetchGmrData:
            dictData = {}
            dictData["DO_No"] = singleData.get("_id")
            dictData["mine_name"] = singleData.get("mine_name")
            if singleData.get("do_qty"):
                dictData["DO_Qty"] = int(singleData.get("do_qty"))
            else:
                dictData["DO_Qty"] = 0
            dictData["cumulative_challan_lr_qty"] = round(singleData.get("cumulative_challan_lr_qty"), 2)
            dictData["grn_status"] = singleData.get("grn_status")
            dictData["date"] = singleData.get("date").strftime("%Y-%m-%d")
            dictData["start_date"] = singleData.get("start_date")
            dictData["end_date"] = singleData.get("end_date")
            dictData["source_type"] = singleData.get("type_consumer")
            dictData["slno"] = datetime.datetime.strptime(singleData.get("slno"), "%Y%m").strftime("%B %Y") if singleData.get("slno") else "-"
            if singleData.get("Grade") is not None:
                if '-' in singleData.get("Grade"):
                    dictData["average_GCV_Grade"] = singleData.get("Grade").split("-")[0]
                elif " " in singleData.get("Grade"):
                    dictData["average_GCV_Grade"] = singleData.get("Grade").split(" ")[0]
                else:
                    dictData["average_GCV_Grade"] = singleData.get("Grade")
            if singleData.get("start_date") is not None and singleData.get("end_date") is not None:
                tomorrow_date = datetime.datetime.strptime(singleData.get("end_date"), "%Y-%m-%d").date() + datetime.timedelta(days=1)
                balance_days = tomorrow_date - datetime.datetime.strptime(specified_date, "%Y-%m-%d").date()
                dictData["balance_days"] = balance_days.days
            else:
                dictData["balance_days"] = 0
            if singleData.get("do_qty") is not None:
                single_do_qty = singleData.get("do_qty")
            else:
                single_do_qty = 0
            if single_do_qty != 0:
                dictData['percent_supply'] = round((singleData.get('cumulative_challan_lr_qty') / int(single_do_qty)) * 100, 2)
            else:
                dictData['percent_supply'] = 0
                
            dictData["balance_qty"] = round(int(single_do_qty) - singleData.get("cumulative_challan_lr_qty"), 2)
                
            if dictData['balance_days'] and dictData['balance_qty'] != 0:
                dictData['asking_rate'] = round(dictData['balance_qty'] / dictData['balance_days'], 2)
            else:
                dictData["asking_rate"] = 0
            listData.append(dictData)


        # console_logger.debug(listData)
        
        for singleDataHistoric in fetchGmrHistoricData:
            dictDataHIstoric = {}
            dictDataHIstoric["DO_No"] = singleDataHistoric.get("_id")
            dictDataHIstoric["mine_name"] = singleDataHistoric.get("mine_name")
            if singleDataHistoric.get("do_qty"):
                dictDataHIstoric["DO_Qty"] = int(float(singleDataHistoric.get("do_qty")))
            else:
                dictDataHIstoric["DO_Qty"] = 0
            dictDataHIstoric["cumulative_challan_lr_qty"] = round(singleDataHistoric.get("cumulative_challan_lr_qty"), 2)
            dictDataHIstoric["grn_status"] = singleDataHistoric.get("grn_status")
            dictDataHIstoric["date"] = singleDataHistoric.get("date").strftime("%Y-%m-%d")
            dictDataHIstoric["start_date"] = singleDataHistoric.get("start_date")
            dictDataHIstoric["end_date"] = singleDataHistoric.get("end_date")
            dictDataHIstoric["source_type"] = singleDataHistoric.get("type_consumer")
            dictDataHIstoric["slno"] = datetime.datetime.strptime(singleDataHistoric.get("slno"), "%Y%m").strftime("%B %Y") if singleDataHistoric.get("slno") else "-"
            if singleDataHistoric.get("Grade") is not None:
                if '-' in singleDataHistoric.get("Grade"):
                    dictDataHIstoric["average_GCV_Grade"] = singleDataHistoric.get("Grade").split("-")[0]
                elif " " in singleDataHistoric.get("Grade"):
                    dictDataHIstoric["average_GCV_Grade"] = singleDataHistoric.get("Grade").split(" ")[0]
                else:
                    dictDataHIstoric["average_GCV_Grade"] = singleDataHistoric.get("Grade")
            if singleDataHistoric.get("start_date") is not None and singleDataHistoric.get("end_date") is not None:
                tomorrow_date = datetime.datetime.strptime(singleDataHistoric.get("end_date"), "%Y-%m-%d").date() + datetime.timedelta(days=1)
                balance_days = tomorrow_date - datetime.datetime.strptime(specified_date, "%Y-%m-%d").date()
                dictDataHIstoric["balance_days"] = balance_days.days
            else:
                dictDataHIstoric["balance_days"] = 0
            
            if singleDataHistoric.get("do_qty") is not None:
                historic_do_qty = singleDataHistoric.get("do_qty")
            else:
                historic_do_qty = 0
            if historic_do_qty != 0:
                dictDataHIstoric['percent_supply'] = round((singleDataHistoric.get('cumulative_challan_lr_qty') / int(float(historic_do_qty))) * 100, 2)
            else:
                dictDataHIstoric['percent_supply'] = 0
            
                
            dictDataHIstoric["balance_qty"] = round(int(float(historic_do_qty)) - singleDataHistoric.get("cumulative_challan_lr_qty"), 2)
                
            if dictDataHIstoric['balance_days'] and dictDataHIstoric['balance_qty'] != 0:
                dictDataHIstoric['asking_rate'] = round(dictDataHIstoric['balance_qty'] / dictDataHIstoric['balance_days'], 2)
            else:
                dictDataHIstoric["asking_rate"] = 0
            
            do_no_exists = any(item['DO_No'] == dictDataHIstoric["DO_No"] for item in listData)

            if not do_no_exists:
                console_logger.debug("DO_No does not exist in final_data.")
                listData.append(dictDataHIstoric)


        for singleDataRcrData in fetchRcrRoadData:
            dictDataRcr = {}
            dictDataRcr["DO_No"] = singleDataRcrData.get("_id")
            dictDataRcr["mine_name"] = singleDataRcrData.get("mine_name")
            if singleDataRcrData.get("do_qty"):
                dictDataRcr["DO_Qty"] = int(singleDataRcrData.get("do_qty"))
            else:
                dictDataRcr["DO_Qty"] = 0
            dictDataRcr["cumulative_challan_lr_qty"] = round(singleDataRcrData.get("cumulative_challan_lr_qty"), 2)
            dictDataRcr["grn_status"] = singleDataRcrData.get("grn_status")
            dictDataRcr["date"] = singleDataRcrData.get("date").strftime("%Y-%m-%d")
            dictDataRcr["start_date"] = singleDataRcrData.get("start_date")
            dictDataRcr["end_date"] = singleDataRcrData.get("end_date")
            dictDataRcr["source_type"] = singleDataRcrData.get("type_consumer")
            dictDataRcr["slno"] = datetime.datetime.strptime(singleDataRcrData.get("slno"), "%Y%m").strftime("%B %Y") if singleDataRcrData.get("slno") else "-"
            if singleDataRcrData.get("Grade") is not None:
                if '-' in singleDataRcrData.get("Grade"):
                    dictDataRcr["average_GCV_Grade"] = singleDataRcrData.get("Grade").split("-")[0]
                elif " " in singleDataRcrData.get("Grade"):
                    dictDataRcr["average_GCV_Grade"] = singleDataRcrData.get("Grade").split(" ")[0]
                else:
                    dictDataRcr["average_GCV_Grade"] = singleDataRcrData.get("Grade")
            if singleDataRcrData.get("start_date") is not None and singleDataRcrData.get("end_date") is not None:
                tomorrow_date = datetime.datetime.strptime(singleDataRcrData.get("end_date"), "%Y-%m-%d").date() + datetime.timedelta(days=1)
                balance_days = tomorrow_date - datetime.datetime.strptime(specified_date, "%Y-%m-%d").date()
                dictDataRcr["balance_days"] = balance_days.days
            else:
                dictDataRcr["balance_days"] = 0
            if singleDataRcrData.get("do_qty") is not None:
                single_do_qty = singleDataRcrData.get("do_qty")
            else:
                single_do_qty = 0
            if single_do_qty != 0:
                dictDataRcr['percent_supply'] = round((singleDataRcrData.get('cumulative_challan_lr_qty') / int(single_do_qty)) * 100, 2)
            else:
                dictDataRcr['percent_supply'] = 0
                
            dictDataRcr["balance_qty"] = round(int(single_do_qty) - singleDataRcrData.get("cumulative_challan_lr_qty"), 2)
                
            if dictDataRcr['balance_days'] and dictDataRcr['balance_qty'] != 0:
                dictDataRcr['asking_rate'] = round(dictDataRcr['balance_qty'] / dictDataRcr['balance_days'], 2)
            else:
                dictDataRcr["asking_rate"] = 0
            listData.append(dictDataRcr)
        

        for saprecordsSingle in fetchSapRecordsData:
            sapdict = {}
            sapdict["DO_No"] = saprecordsSingle.get("_id")
            sapdict["mine_name"] = saprecordsSingle.get("mine_name")
            sapdict["DO_Qty"] = int(saprecordsSingle.get("do_qty"))
            sapdict["start_date"] = saprecordsSingle.get("start_date")
            sapdict["end_date"] = saprecordsSingle.get("end_date")
            sapdict["source_type"] = saprecordsSingle.get("source_type")
            sapdict["challan_lr_qty"] = 0
            sapdict["cumulative_challan_lr_qty"] = 0
            sapdict["slno"] = datetime.datetime.strptime(saprecordsSingle.get("slno"), "%Y%m").strftime("%B %Y") if saprecordsSingle.get("slno") else "-"
            if saprecordsSingle.get("Grade") is not None:
                if '-' in saprecordsSingle.get("Grade"):
                    sapdict["average_GCV_Grade"] = saprecordsSingle.get("Grade").split("-")[0]
                elif " " in saprecordsSingle.get("Grade"):
                    sapdict["average_GCV_Grade"] = saprecordsSingle.get("Grade").split(" ")[0]
                else:
                    sapdict["average_GCV_Grade"] = saprecordsSingle.get("Grade")
            if saprecordsSingle.get("start_date") is not None and saprecordsSingle.get("end_date") is not None:
                tomorrow_date = datetime.datetime.strptime(saprecordsSingle.get("end_date"), "%Y-%m-%d").date() + datetime.timedelta(days=1)
                balance_days = tomorrow_date - datetime.datetime.strptime(specified_date, "%Y-%m-%d").date()
                sapdict["balance_days"] = balance_days.days
            else:
                sapdict["balance_days"] = 0
            if saprecordsSingle.get("do_qty") is not None:
                do_qty_val = saprecordsSingle.get("do_qty")
            else:
                do_qty_val = 0
            if do_qty_val != 0:
                sapdict['percent_supply'] = round((sapdict["cumulative_challan_lr_qty"] / int(do_qty_val)) * 100, 2)
            else:
                sapdict['percent_supply'] = 0
            sapdict["balance_qty"] = round(int(do_qty_val) - sapdict["cumulative_challan_lr_qty"], 2)
            if sapdict['balance_days'] and sapdict['balance_qty'] != 0:
                sapdict['asking_rate'] = round(sapdict['balance_qty'] / sapdict['balance_days'], 2)
            else:
                sapdict["asking_rate"] = 0

            sap_do_no_exists = any(item['DO_No'] == sapdict.get("DO_No") for item in listData)
            
            if not sap_do_no_exists:
                console_logger.debug("DO_No does not exist in final_data for sap_records")
                listData.append(sapdict)
        

        for saprecordsRcrSingle in fetchSapRecordsRcrData:
            sapdictRcr = {}
            sapdictRcr["DO_No"] = saprecordsRcrSingle.get("_id")
            sapdictRcr["mine_name"] = saprecordsRcrSingle.get("mine_name")
            sapdictRcr["DO_Qty"] = int(saprecordsRcrSingle.get("do_qty"))
            sapdictRcr["start_date"] = saprecordsRcrSingle.get("start_date")
            sapdictRcr["end_date"] = saprecordsRcrSingle.get("end_date")
            sapdictRcr["source_type"] = saprecordsRcrSingle.get("source_type")
            sapdictRcr["challan_lr_qty"] = 0
            sapdictRcr["cumulative_challan_lr_qty"] = 0
            sapdictRcr["slno"] = datetime.datetime.strptime(saprecordsRcrSingle.get("slno"), "%Y%m").strftime("%B %Y") if saprecordsRcrSingle.get("slno") else "-"
            if saprecordsRcrSingle.get("Grade") is not None:
                if '-' in saprecordsRcrSingle.get("Grade"):
                    sapdictRcr["average_GCV_Grade"] = saprecordsRcrSingle.get("Grade").split("-")[0]
                elif " " in saprecordsRcrSingle.get("Grade"):
                    sapdictRcr["average_GCV_Grade"] = saprecordsRcrSingle.get("Grade").split(" ")[0]
                else:
                    sapdictRcr["average_GCV_Grade"] = saprecordsRcrSingle.get("Grade")
            if saprecordsRcrSingle.get("start_date") is not None and saprecordsRcrSingle.get("end_date") is not None:
                tomorrow_date = datetime.datetime.strptime(saprecordsRcrSingle.get("end_date"), "%Y-%m-%d").date() + datetime.timedelta(days=1)
                balance_days = tomorrow_date - datetime.datetime.strptime(specified_date, "%Y-%m-%d").date()
                sapdictRcr["balance_days"] = balance_days.days
            else:
                sapdictRcr["balance_days"] = 0
            if saprecordsRcrSingle.get("do_qty") is not None:
                do_qty_val = saprecordsRcrSingle.get("do_qty")
            else:
                do_qty_val = 0
            if do_qty_val != 0:
                sapdictRcr['percent_supply'] = round((sapdictRcr["cumulative_challan_lr_qty"] / int(do_qty_val)) * 100, 2)
            else:
                sapdictRcr['percent_supply'] = 0
            sapdictRcr["balance_qty"] = round(int(do_qty_val) - sapdictRcr["cumulative_challan_lr_qty"], 2)
            if sapdictRcr['balance_days'] and sapdictRcr['balance_qty'] != 0:
                sapdictRcr['asking_rate'] = round(sapdictRcr['balance_qty'] / sapdictRcr['balance_days'], 2)
            else:
                sapdictRcr["asking_rate"] = 0

            sap_do_no_exists = any(item['DO_No'] == sapdictRcr.get("DO_No") for item in listData)
            
            if not sap_do_no_exists:
                console_logger.debug("DO_No does not exist in final_data for sap_records")
                listData.append(sapdictRcr)


        for singlelrqtyData in fetchGmrDatachallanltqty:
            dictDatalrQty = {}
            dictDatalrQty["DO_No"] = singlelrqtyData.get("_id")
            dictDatalrQty["mine_name"] = singlelrqtyData.get("mine_name")
            if singlelrqtyData.get("do_qty"):
                dictDatalrQty["DO_Qty"] = int(float(singlelrqtyData.get("do_qty")))
            else:
                dictDatalrQty["DO_Qty"] = 0
            dictDatalrQty["challan_lr_qty"] = round(singlelrqtyData.get("challan_lr_qty"), 2)
            
            # Check if there is an item with the same DO_No in listData
            do_no_exists_data = next((item for item in listData if item["DO_No"] == dictDatalrQty["DO_No"]), None)
            
            # If it exists, update the "challan_lr_qty" in listData
            if do_no_exists_data:
                do_no_exists_data["challan_lr_qty"] = dictDatalrQty["challan_lr_qty"]

        for singlehistoriclrqty in fetchGmrHistoricDataChallanLrQty:
            dictDatahistoriclrQty = {}
            dictDatahistoriclrQty["DO_No"] = singlehistoriclrqty.get("_id")
            dictDatahistoriclrQty["mine_name"] = singlehistoriclrqty.get("mine_name")
            if singlehistoriclrqty.get("do_qty"):
                dictDatahistoriclrQty["DO_Qty"] = int(float(singlehistoriclrqty.get("do_qty")))
            else:
                dictDatahistoriclrQty["DO_Qty"] = 0
            dictDatahistoriclrQty["challan_lr_qty"] = round(singlehistoriclrqty.get("challan_lr_qty"), 2)
            
            # Check if there is an item with the same DO_No in listData
            do_no_exists_historic = next((item for item in listData if item["DO_No"] == dictDatahistoriclrQty["DO_No"]), None)
            
            # If it exists, update the "challan_lr_qty" in listData
            if do_no_exists_historic:
                do_no_exists_historic["challan_lr_qty"] = dictDatahistoriclrQty["challan_lr_qty"]


        
        for singlercrroadlrqty in fetchRcrRoadchallanlrqty:
            dictDataRcrRoadlrQty = {}
            dictDataRcrRoadlrQty["DO_No"] = singlercrroadlrqty.get("_id")
            dictDataRcrRoadlrQty["mine_name"] = singlercrroadlrqty.get("mine_name")
            if singlercrroadlrqty.get("do_qty"):
                dictDataRcrRoadlrQty["DO_Qty"] = int(float(singlercrroadlrqty.get("do_qty")))
            else:
                dictDataRcrRoadlrQty["DO_Qty"] = 0
            dictDataRcrRoadlrQty["challan_lr_qty"] = round(singlercrroadlrqty.get("challan_lr_qty"), 2)
            
            # Check if there is an item with the same DO_No in listData
            do_no_exists_rcr_road = next((item for item in listData if item["DO_No"] == dictDataRcrRoadlrQty["DO_No"]), None)
            
            # If it exists, update the "challan_lr_qty" in listData
            if do_no_exists_rcr_road:
                do_no_exists_rcr_road["challan_lr_qty"] = dictDataRcrRoadlrQty["challan_lr_qty"]
            
        
        # final_data = [
        #     d for d in listData 
        #     if d['start_date'] is not None and datetime.datetime.strptime(d['start_date'], '%Y-%m-%d').date() <= datetime.datetime.now().date()
        # ]

        final_data_check = [
            d for d in listData
            if d['end_date'] is not None and 
            (datetime.datetime.strptime(d['end_date'], '%Y-%m-%d') + datetime.timedelta(days=2)) > datetime.datetime.strptime(specified_date, "%Y-%m-%d")
        ]

        # console_logger.debug(final_data_check)

        filtered_data = []
        for single_data_percent in final_data_check:
            percent_supply = single_data_percent.get('percent_supply')

            # Check for percent_supply greater than or equal to 100.0
            if percent_supply >= 100.0:
                # Query Gmrdata with the DO_No
                fetchGmrData = Gmrdata.objects(arv_cum_do_number=single_data_percent.get("DO_No")).first()
                
                # Check if data exists and GWEL_Tare_Time is not None
                if fetchGmrData is not None and fetchGmrData.GWEL_Tare_Time:
                    # Compare GWEL_Tare_Time + 2 days with today's date
                    if (fetchGmrData.GWEL_Tare_Time + datetime.timedelta(days=2)) < datetime.datetime.now():
                        # console_logger.debug("Data removed due to GWEL_Tare_Time being older than today's date.")
                        continue  # Skip entry
                
            # If not removed, append to filtered_data
            filtered_data.append(single_data_percent)
            
        # Sort the data by 'balance_days', placing entries with 'balance_days' of 0 at the end
        final_data_check = sorted(filtered_data, key=lambda x: (x['balance_days'] == 0, x['balance_days']))

        final_data = final_data_check

        # rrNo_values, clubbed_data, aopList = bar_graph_data(specified_date)
        rrNo_values_old, clubbed_data, aopList_old = bar_graph_data(specified_date)
        rrNo_values, aopList = endpoint_to_fetch_report_name_pdf(Daily=specified_date)
        clubbed_data_final = gmr_main_graph()
        total_monthly_final_net_qty = transit_loss_gain_road_mode_month(specified_date)
        yearly_final_data = transit_loss_gain_road_mode()
        yearly_rail_final_data = transit_loss_gain_rail_mode()

        dayWiseVehicleInCount = daywise_in_vehicle_count_datewise(specified_date)
        dayWiseGrnReceive = daywise_grn_receive_datewise(specified_date)
        dayWiseGwelReceive = daywise_gwel_receive_pdf_datewise(specified_date)
        dayWiseOutVehicelCount = daywise_out_vehicle_count_datewise(specified_date)

        fetchRailData = rail_pdf(specified_date)

        fetchRakeQuota = end_point_to_fetch_rake_quota_test(response)

        # console_logger.debug(fetchRakeQuota)

        fetchRcrRakeQuota = end_point_to_fetch_rcr_rake_quota_pdf(response)

        # console_logger.debug(fetchRakeQuota)

        seclLinkagegraph = endpoint_to_fetch_secl_linkage_matrialization(response, str(datetime.datetime.strptime(specified_date, "%Y-%m-%d").strftime("%Y")))
        # seclLinkagegraph = endpoint_to_fetch_secl_linkage_matrialization_for_pdf(response, str(datetime.datetime.strptime(specified_date, "%Y-%m-%d").strftime("%Y")))
        wclLinkagegraph = endpoint_to_fetch_wcl_linkage_matrialization(response, str(datetime.datetime.strptime(specified_date, "%Y-%m-%d").strftime("%Y")))

        fetchConsumerType = roadjourneyconsumertype.objects.first()
        consumerList = fetchConsumerType.consumer_type

        if specified_date:
            month_data = specified_date
            fetchData = generate_report(final_data, rrNo_values, month_data, clubbed_data, clubbed_data_final, dayWiseVehicleInCount, dayWiseGrnReceive, dayWiseGwelReceive, dayWiseOutVehicelCount, total_monthly_final_net_qty, yearly_final_data, aopList, fetchRailData, yearly_rail_final_data, fetchRakeQuota.get('datasets'), fetchRcrRakeQuota.get("datasets"), seclLinkagegraph, wclLinkagegraph, consumerList)
            return fetchData
        else:
            fetchData = generate_report(final_data, rrNo_values, "", clubbed_data, clubbed_data_final, dayWiseVehicleInCount, dayWiseGrnReceive, dayWiseGwelReceive, dayWiseOutVehicelCount, total_monthly_final_net_qty, yearly_final_data, aopList, fetchRailData, yearly_rail_final_data, fetchRakeQuota.get('datasets'), fetchRcrRakeQuota.get("datasets"), seclLinkagegraph, wclLinkagegraph, consumerList)
            return fetchData

        # return listData

    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(
            "Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno)
        )
        return e



# delete after matched value done by sachin bhai
@router.get("/pdf_minewise_road_new", tags=["PDF Report"])
def generate_gmr_report_new(
    response: Response,
    specified_date: Optional[str]=None,
    mine: Optional[str] = "All",
):
    try:
        # if specified_date:
        data = {}
        result = {
            "labels": [],
            "datasets": [],
            "weight_total": [],
            "total": 0,
            "page_size": 15,
        }

        if mine and mine != "All":
            data["mine__icontains"] = mine.upper()

        # if specified_date:
        from_ts = convert_to_utc_format(f'{specified_date} 00:00:00', "%Y-%m-%d %H:%M:%S")
        to_ts = convert_to_utc_format(f'{specified_date} 23:59:59', "%Y-%m-%d %H:%M:%S")

        logs = (
            Gmrdatanew.objects(
                Q(GWEL_Tare_Time__ne=None, GWEL_Tare_Time__gte=from_ts, GWEL_Tare_Time__lte=to_ts) | 
                Q(GWEL_Tare_Time=None, vehicle_in_time__gte=from_ts, vehicle_in_time__lte=to_ts)  # OR condition
            )
            # .order_by("-GWEL_Tare_Time")  # Order by GWEL_Tare_Time (if present)
        )

        sap_records = SapRecords.objects.all()
        
        if any(logs) or any(sap_records):
            aggregated_data = defaultdict(
                lambda: defaultdict(
                    lambda: {
                        "DO_Qty": 0,
                        "challan_lr_qty": 0,
                        "challan_lr_qty_full": 0,
                        "mine_name": "",
                        "balance_qty": 0,
                        "percent_of_supply": 0,
                        "actual_net_qty": 0,
                        "Gross_Calorific_Value_(Adb)": 0,
                        "count": 0,
                        "coal_count": 0,
                        "start_date": "",
                        "end_date": "",
                        "source_type": "",
                        "grn_status": "",
                    }
                )
            )

            start_dates = {}
            grade = 0
            for log in logs:
                if log.GWEL_Tare_Time is not None:
                    date = log.GWEL_Tare_Time.strftime("%Y-%m-%d")
                    month = log.GWEL_Tare_Time.strftime("%Y-%m")
                else:
                    date = log.vehicle_in_time.strftime("%Y-%m-%d")
                    month = log.vehicle_in_time.strftime("%Y-%m")
                
                payload = log.payload()
                result["labels"] = list(payload.keys())
                mine_name = payload.get("Mines_Name")
                # console_logger.debug(mine_name)
                do_no = payload.get("DO_No")
                # console_logger.debug(do_no)
                if payload.get("Grade") is not None:
                    if '-' in payload.get("Grade"):
                        grade = payload.get("Grade").split("-")[0]
                    elif " " in payload.get("Grade"):
                        grade = payload.get("Grade").split(" ")[0]
                    else:
                        grade = payload.get("Grade")
                # If start_date is None or the current vehicle_in_time is earlier than start_date, update start_date
                # if do_no not in start_dates:
                #     start_dates[do_no] = date
                # elif date < start_dates[do_no]:
                #     start_dates[do_no] = date
                # console_logger.debug(payload.get("start_date"))
                if payload.get("slno"):
                    aggregated_data[date][do_no]["slno"] = datetime.datetime.strptime(payload.get("slno"), '%Y%m').strftime('%B %Y')
                else:
                    aggregated_data[date][do_no]["slno"] = "-"
                if payload.get("start_date"):
                    aggregated_data[date][do_no]["start_date"] = payload.get("start_date")
                else:
                    aggregated_data[date][do_no]["start_date"] = "0"
                if payload.get("end_date"):
                    aggregated_data[date][do_no]["end_date"] = payload.get("end_date")
                else:
                    aggregated_data[date][do_no]["end_date"] = "0"

                if payload.get("Type_of_consumer"):
                    aggregated_data[date][do_no]["source_type"] = payload.get("Type_of_consumer")

                if payload.get("DO_Qty"):
                    aggregated_data[date][do_no]["DO_Qty"] = float(
                        payload["DO_Qty"]
                    )
                else:
                    aggregated_data[date][do_no]["DO_Qty"] = 0

                challan_net_wt = payload.get("Challan_Net_Wt(MT)")    

                if challan_net_wt:
                    aggregated_data[date][do_no]["challan_lr_qty"] += float(challan_net_wt)

                if payload.get("Mines_Name"):
                    aggregated_data[date][do_no]["mine_name"] = payload[
                        "Mines_Name"
                    ]
                else:
                    aggregated_data[date][do_no]["mine_name"] = "-"

                aggregated_data[date][do_no]["grn_status"] = payload[
                    "grn_status"
                ]

                aggregated_data[date][do_no]["count"] += 1 

            for record in sap_records:
                do_no = record.do_no
                if do_no not in aggregated_data[specified_date]:
                    aggregated_data[specified_date][do_no]["DO_Qty"] = float(record.do_qty) if record.do_qty else 0
                    aggregated_data[specified_date][do_no]["mine_name"] = record.mine_name if record.mine_name else "-"
                    aggregated_data[specified_date][do_no]["start_date"] = record.start_date if record.start_date else "0"
                    aggregated_data[specified_date][do_no]["end_date"] = record.end_date if record.end_date else "0"
                    aggregated_data[specified_date][do_no]["source_type"] = record.consumer_type if record.consumer_type else "Unknown"
                    # aggregated_data[specified_date][do_no]["grn_status"] = record.grn_status
                    try:
                        aggregated_data[specified_date][do_no]["slno"] = datetime.datetime.strptime(record.slno, "%Y%m").strftime("%B %Y") if record.slno else "-"
                    except ValueError as e:
                        aggregated_data[specified_date][do_no]["slno"] = record.slno if record.slno else "-"
                    aggregated_data[specified_date][do_no]["count"] = 1

            dataList = [
                {
                    "date": date,
                    "data": {
                        do_no: {
                            "DO_Qty": data["DO_Qty"],
                            "challan_lr_qty": data["challan_lr_qty"],
                            "mine_name": data["mine_name"],
                            "grade": grade,
                            "date": date,
                            "start_date": data["start_date"],
                            "end_date": data["end_date"],
                            "source_type": data["source_type"],
                            "slno": data["slno"],
                            "grn_status": data["grn_status"],
                        }
                        for do_no, data in aggregated_data[date].items()
                    },
                }
                for date in aggregated_data
            ]

            final_data = []
            for entry in dataList:
                date = entry["date"]
                for data_dom, values in entry['data'].items():
                    dictData = {}
                    dictData["DO_No"] = data_dom
                    dictData["mine_name"] = values["mine_name"]
                    dictData["DO_Qty"] = values["DO_Qty"]
                    dictData["club_challan_lr_qty"] = values["challan_lr_qty"]
                    dictData["challan_lr_qty"] = 0
                    dictData["grn_status"] = values["grn_status"]
                    dictData["date"] = values["date"]
                    dictData["start_date"] = values["start_date"]
                    dictData["end_date"] = values["end_date"]
                    dictData["source_type"] = values["source_type"]
                    dictData["slno"] = values["slno"]
                    dictData["cumulative_challan_lr_qty"] = 0
                    dictData["balance_qty"] = 0
                    dictData["percent_supply"] = 0
                    dictData["asking_rate"] = 0
                    dictData['average_GCV_Grade'] = values["grade"]
                    
                    if dictData["start_date"] != "0" and dictData["end_date"] != "0":
                        today_date = datetime.datetime.today().date()
                        # balance_days = datetime.datetime.strptime(dictData["end_date"], "%Y-%m-%d").date() - datetime.datetime.strptime(dictData["start_date"], "%Y-%m-%d").date()
                        tomorrow_date = datetime.datetime.strptime(dictData["end_date"], "%Y-%m-%d").date() + datetime.timedelta(days=1)
                        # balance_days = datetime.datetime.strptime(dictData["end_date"], "%Y-%m-%d").date() - datetime.datetime.today().date()
                        balance_days = tomorrow_date - datetime.datetime.today().date()
                        dictData["balance_days"] = balance_days.days
                    else:
                        dictData["balance_days"] = 0
                    
                    final_data.append(dictData)
            if final_data:
                startdate = f'{specified_date} 00:00:00'
                enddate = f'{specified_date} 23:59:59'
                # to_ts = datetime.datetime.strptime(enddate,"%Y-%m-%d %H:%M:%S")
                from_ts = convert_to_utc_format(startdate, "%Y-%m-%d %H:%M:%S")
                to_ts = convert_to_utc_format(enddate, "%Y-%m-%d %H:%M:%S")
                

                pipeline = [
                    {
                        "$match": {
                            "$and": [
                                {
                                    "$or": [
                                        { "GWEL_Tare_Time": { "$gte": from_ts, "$lte": to_ts } },
                                        { "GWEL_Tare_Time": None, "vehicle_in_time": { "$gte": from_ts, "$lte": to_ts } }
                                    ]
                                },
                                # { "net_qty": { "$ne": None } }
                            ]
                        }
                    },
                    {
                    '$group': {
                        '_id': {
                            'date': {
                                '$dateToString': {
                                    'format': '%Y-%m-%d', 
                                    'date': {
                                        '$ifNull': ['$GWEL_Tare_Time', '$vehicle_in_time']
                                    },
                                    # 'date': '$GWEL_Tare_Time',
                                }
                            }, 
                            'do_no': '$arv_cum_do_number'
                        }, 
                        'total_net_qty': {
                            '$sum': {
                                '$toDouble': '$net_qty'
                            }
                        }
                    }
                }]

                cclrpipeline = [
                    {
                        "$match": {
                            # "GWEL_Tare_Time": {"$lt": to_ts},
                            "net_qty": {"$ne": None}
                        }
                    },
                    {
                        "$group": {
                            "_id": "$arv_cum_do_number",
                            "cumulative_challan_lr_qty": {
                                "$sum": {
                                    "$toDouble": "$net_qty"
                                }
                            }
                        }
                    }
                ]
                
                filtered_data_new = Gmrdatanew.objects.aggregate(pipeline)
                aggregation_result_new = Gmrdatanew.objects.aggregate(cclrpipeline)

                cumulative_data_by_do = {item['_id']: item['cumulative_challan_lr_qty'] for item in aggregation_result_new}

                aggregated_totals = defaultdict(float)
                for single_data_entry in filtered_data_new:
                    do_no = single_data_entry['_id']['do_no']
                    total_net_qty = single_data_entry['total_net_qty']
                    aggregated_totals[do_no] += total_net_qty
                    
                data_by_do = {}

                finaldataMain = [single_data_list for single_data_list in final_data if single_data_list.get("balance_days") >= 0]

                # Update data1 with values from data2
                for item in finaldataMain:
                    do_no = item['DO_No']
                    if do_no in cumulative_data_by_do:
                        item['cumulative_challan_lr_qty'] = cumulative_data_by_do[do_no]
                    else:
                        item['cumulative_challan_lr_qty'] = 0

                # console_logger.debug(finaldataMain)
                for entry in finaldataMain:
                    do_no = entry['DO_No']

                    data_by_do[do_no] = entry

                    if do_no in aggregated_totals:
                        
                        data_by_do[do_no]['challan_lr_qty'] = round(aggregated_totals[do_no], 2)
                    else:
                        data_by_do[do_no]['challan_lr_qty'] = 0

                        

                    if data_by_do[do_no]['DO_Qty'] != 0 and data_by_do[do_no]['cumulative_challan_lr_qty'] != 0:
                        data_by_do[do_no]['percent_supply'] = round((data_by_do[do_no]['cumulative_challan_lr_qty'] / data_by_do[do_no]['DO_Qty']) * 100, 2)
                    else:
                        data_by_do[do_no]['percent_supply'] = 0

                    # if data_by_do[do_no]['cumulative_challan_lr_qty'] != 0 and data_by_do[do_no]['DO_Qty'] != 0:
                    data_by_do[do_no]['balance_qty'] = round(data_by_do[do_no]['DO_Qty'] - data_by_do[do_no]['cumulative_challan_lr_qty'], 2)
                    # else:
                    #     data_by_do[do_no]['balance_qty'] = 0
                    
                    if data_by_do[do_no]['balance_days'] and data_by_do[do_no]['balance_qty'] != 0:
                        data_by_do[do_no]['asking_rate'] = round(data_by_do[do_no]['balance_qty'] / data_by_do[do_no]['balance_days'], 2)

                    del entry['club_challan_lr_qty']
                
                # final_data = list(data_by_do.values())

                sort_final_data = list(data_by_do.values())
                # Sort the data by 'balance_days', placing entries with 'balance_days' of 0 at the end
                final_data_check = sorted(sort_final_data, key=lambda x: (x['balance_days'] == 0, x['balance_days']))
                
                # current_date = datetime.datetime.now()

                # Step 1: Remove dictionaries where grn_status = True and percent_supply = 100
                filtered_list = [
                    d for d in final_data_check 
                    if not (d['grn_status'] and d['percent_supply'] == 100)
                ]

                # Step 2: Remove dictionaries where current date exceeds end_date, but skip '0'
                final_data = [
                    d for d in filtered_list 
                    if not (d['grn_status']) and d['end_date'] != '0' and
                    datetime.datetime.strptime(d['end_date'], '%Y-%m-%d') > datetime.datetime.strptime(specified_date, "%Y-%m-%d")
                ]

                rrNo_values, clubbed_data, aopList = bar_graph_data_new(specified_date)
                clubbed_data_final = gmr_main_graph_new()
                total_monthly_final_net_qty = transit_loss_gain_road_mode_month_new(specified_date)
                yearly_final_data = transit_loss_gain_road_mode_new()
                yearly_rail_final_data = transit_loss_gain_rail_mode()

                dayWiseVehicleInCount = daywise_in_vehicle_count_datewise_new(specified_date)
                dayWiseGrnReceive = daywise_grn_receive_datewise_new(specified_date)
                dayWiseGwelReceive = daywise_gwel_receive_pdf_datewise_new(specified_date)
                dayWiseOutVehicelCount = daywise_out_vehicle_count_datewise_new(specified_date)

                fetchRailData = rail_pdf(specified_date)

                fetchRakeQuota = end_point_to_fetch_rake_quota_test(response)

                # console_logger.debug(fetchRakeQuota)

                seclLinkagegraph = endpoint_to_fetch_secl_linkage_matrialization(response, str(datetime.datetime.strptime(specified_date, "%Y-%m-%d").strftime("%Y")))
                wclLinkagegraph = endpoint_to_fetch_wcl_linkage_matrialization_new(response, str(datetime.datetime.strptime(specified_date, "%Y-%m-%d").strftime("%Y")))

                # console_logger.debug(seclLinkagegraph)
                # console_logger.debug(wclLinkagegraph)

                if specified_date:
                    month_data = specified_date
                    fetchData = generate_report(final_data, rrNo_values, month_data, clubbed_data, clubbed_data_final, dayWiseVehicleInCount, dayWiseGrnReceive, dayWiseGwelReceive, dayWiseOutVehicelCount, total_monthly_final_net_qty, yearly_final_data, aopList, fetchRailData, yearly_rail_final_data, fetchRakeQuota.get('datasets'), seclLinkagegraph, wclLinkagegraph)
                    return fetchData
                else:
                    fetchData = generate_report(final_data, rrNo_values, "", clubbed_data, clubbed_data_final, dayWiseVehicleInCount, dayWiseGrnReceive, dayWiseGwelReceive, dayWiseOutVehicelCount, total_monthly_final_net_qty, yearly_final_data, aopList, fetchRailData, yearly_rail_final_data, fetchRakeQuota.get('datasets'), seclLinkagegraph, wclLinkagegraph)
                    return fetchData
            
        else:
            return 400
    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug(
            "Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno)
        )
        return e



@router.post("/add/scheduler", tags=["PDF Report"])
def endpoint_to_add_scheduler(response: Response, payload: MisReportData):
    try:
        dataName = payload.dict()
        try:
            reportScheduler = ReportScheduler.objects.get(report_name=dataName.get("report_name"))
            reportScheduler.recipient_list = dataName.get("recipient_list")
            reportScheduler.cc_list = dataName.get("cc_list")
            reportScheduler.bcc_list = dataName.get("bcc_list")
            reportScheduler.filter = dataName.get("filter")
            reportScheduler.schedule = dataName.get("schedule")
            reportScheduler.shift_schedule = dataName.get("shift_schedule")
            # time_fetch_data = datetime.datetime.strptime(dataName.get("time"), "%H:%M") + timedelta(hours=5, minutes=30)
            # reportScheduler.time = time_fetch_data.strftime("%H:%M")
            reportScheduler.time = dataName.get("time")
            reportScheduler.save()

        except DoesNotExist as e:
            # time_fetch_data = datetime.datetime.strptime(dataName.get("time"), "%H:%M") + timedelta(hours=5, minutes=30)
            reportScheduler = ReportScheduler(report_name=dataName.get("report_name"), recipient_list=dataName.get("recipient_list"), cc_list=dataName.get("cc_list"), bcc_list=dataName.get("bcc_list"), filter = dataName.get("filter"), schedule = dataName.get("schedule"), shift_schedule = dataName.get("shift_schedule"), time=dataName.get("time"))
            reportScheduler.save()

        # hh, mm = reportScheduler.time.split(":")

        if reportScheduler.time != "":
            time_format = "%H:%M"
            given_time = datetime.datetime.strptime(reportScheduler.time, time_format)

            time_to_subtract = datetime.timedelta(hours=5, minutes=30)

            new_time = given_time - time_to_subtract
            new_time_str = new_time.strftime(time_format)
            hh, mm = new_time_str.split(":")

        if len(reportScheduler) > 0:
            if reportScheduler.filter == "daily":
                backgroundTaskHandler.run_job(task_name=reportScheduler.report_name, func=send_report_generate, trigger="cron", **{"day": "*", "hour": hh, "minute": mm}, func_kwargs={"report_name":payload.report_name}, max_instances=1)
                # backgroundTaskHandler.run_job(task_name=reportScheduler.report_name, func=send_report_generate, trigger="cron", **{"day": "*", "second": 2})
            elif reportScheduler.filter == "weekly":
                # backgroundTaskHandler.run_job(task_name=reportScheduler.report_name, func=send_report_generate, trigger="cron", **{"week": reportScheduler.schedule}) # week (int|str) - ISO week (1-53)
                backgroundTaskHandler.run_job(task_name=reportScheduler.report_name, func=send_report_generate, trigger="cron", **{"day_of_week": reportScheduler.schedule, "hour": hh, "minute": mm}, func_kwargs={"report_name":payload.report_name}, max_instances=1)
            elif reportScheduler.filter == "monthly":
                # backgroundTaskHandler.run_job(task_name=reportScheduler.report_name, func=send_report_generate, trigger="cron", **{"month": reportScheduler.schedule}) # month (int|str) - month (1-12)
                backgroundTaskHandler.run_job(task_name=reportScheduler.report_name, func=send_report_generate, trigger="cron", **{"day": reportScheduler.schedule, "hour": hh, "minute": mm}, func_kwargs={"report_name":payload.report_name}, max_instances=1)
            elif reportScheduler.filter == "shift_schedule":
                shift_schedule = reportScheduler.shift_schedule
                for single_shift in shift_schedule:
                    shift_time = datetime.datetime.strptime(single_shift.get("time"), time_format)
                    shift_time_ist = shift_time - time_to_subtract
                    shift_hh, shift_mm = shift_time_ist.strftime(time_format).split(":")
                    backgroundTaskHandler.run_job(
                        task_name=f"{reportScheduler.report_name}_{single_shift.get('shift_wise')}", 
                        func=send_shift_report_generate, 
                        trigger="cron", **{"day": "*", "hour": shift_hh, "minute": shift_mm}, 
                        func_kwargs={"report_name":f"{reportScheduler.report_name}", "shift_name": single_shift.get('shift_wise'), "shift_time": single_shift.get("time")},
                        max_instances=1)
        try:
            fetchEmailNotifications = emailNotifications.objects(notification_name=dataName.get("report_name"))
            for singleEmailData in fetchEmailNotifications:
                singleEmailData.delete()
        except DoesNotExist as e:
            console_logger.debug("No report name found in emailnotifications db")
        return {"detail": "success"}
    except Exception as e:
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug("Error on line {}".format(sys.exc_info()[-1].tb_lineno))
        return {"detail": "failed"}


@router.get("/fetch/singlescheduler", tags=["PDF Report"])
def endpoint_to_fetch_scheduler_id(response: Response, name: str):
    try:
        fetchScheduler = ReportScheduler.objects.get(report_name=name)
        return fetchScheduler.payload()
    except Exception as e:
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug("Error on line {}".format(sys.exc_info()[-1].tb_lineno))
        return {"detail": "failed"}


@router.delete("/delete/scheduler", tags=["PDF Report"])
def endpoint_to_delete_scheduler(response: Response, id: str):
    try:
        fetchScheduler = ReportScheduler.objects.get(id=id)
        fetchScheduler.delete()
        return {"detail": "success"}
    except Exception as e:
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug("Error on line {}".format(sys.exc_info()[-1].tb_lineno))
        return {"detail": "failed"}


@router.post("/smtp", tags=["Mail"])
async def add_smtp_settings(response: Response):
    try:
        headers = {}
        data = {}
        # url_data = f"http://192.168.1.57/api/v1/base/smtp/unprotected"
        url_data = f"http://{host}/api/v1/base/smtp/unprotected"
        
        response = requests.request("GET", url=url_data, headers=headers, data=data, proxies=proxies)
        data = json.loads(response.text)
        if response.status_code == 200:
            smtp_settings = SmtpSettings(**data)
            smtp_settings.save()
        return {"detail": "success"}
    except Exception as e:
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug("Error on line {}".format(sys.exc_info()[-1].tb_lineno))
        return {"detail": "failed"} 


@router.get("/smtp", tags=["Mail"])
async def endpoint_to_get_smtp_settings(response: Response):
    try:
        fetchSmtpSettings = SmtpSettings.objects.get()
        return fetchSmtpSettings.payload()
    except Exception as e:
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug("Error on line {}".format(sys.exc_info()[-1].tb_lineno))
        return {"detail": "failed"}
    

@router.post("/insert/aoptarget", tags=["PDF Report"])
def endpoint_to_insert_aoptarget(response: Response, payload: AopTargetData):
    try:
        dataName = payload.dict()
        try:
            aopTargetData = AopTarget.objects.get(source_name=dataName.get("source_name"))
            aopTargetData.aop_target = dataName.get("aop_target")
            aopTargetData.save()

        except DoesNotExist as e:
            aopTargetData = AopTarget(source_name=dataName.get("source_name"), aop_target=dataName.get("aop_target"))
            aopTargetData.save()
        return {"detail": "success"}
    except Exception as e:
        console_logger.debug(e)
        console_logger.debug("Error on line {}".format(sys.exc_info()[-1].tb_lineno))
        return {"detail": "failed"}


@router.get("/fetch/coallocation", tags=["PDF Report"])
def endpoint_to_fetch_coal_location(response: Response):
    try:
        fetchCoalTestingLocation = CoalTesting.objects()
        fetchCoalTestingTrainLocation = CoalTestingTrain.objects()

        coalTestingData = [singlecoalTesting["location"] for singlecoalTesting in fetchCoalTestingLocation]
        coalTestingTrainData = [singlecoalTestingTrain["location"] for singlecoalTestingTrain in fetchCoalTestingTrainLocation]

        return list(set(coalTestingData)) + list(set(coalTestingTrainData))

    except Exception as e:
        console_logger.debug(e)
        console_logger.debug("Error on line {}".format(sys.exc_info()[-1].tb_lineno))
        return {"detail": "failed"}


@router.get("/fetch/aoptarget", tags=["PDF Report"])
def endpoint_to_fetch_aoptarget(response: Response, target_name: str=None):
    try:
        dataList = []
        data = {"source_name": "", "aop_target": ""}
        if target_name:
            try:
                fetchAopTargetData = AopTarget.objects.get(source_name=target_name)
                # dataList.append(fetchAopTargetData.payload())
                # if fetchAopTargetData:
                dataList.append(fetchAopTargetData.reportpayload())
            except DoesNotExist as e:
                data["source_name"] = target_name
                dataList.append(data)
        else:
            fetchAopTargetData = AopTarget.objects()
            for singleAopTarget in fetchAopTargetData:
                dataList.append(singleAopTarget.reportpayload())
        return dataList
    except Exception as e:
        console_logger.debug(e)
        console_logger.debug("Error on line {}".format(sys.exc_info()[-1].tb_lineno))
        return {"detail": "failed"}

    

@router.get("/fetch/minecoaljourneysource", tags=["PDF Report"])
def endpoint_to_fetch_aoptarget(response: Response, mine_name: str=None):
    try:
        mine_data = short_mine_collection.find_one({"mine_name":mine_name.upper()})
        return {
            "id": str(mine_data.get("_id")),
            "mine_name": mine_data.get("mine_name"),
            "short_code": mine_data.get("short_code"),
            "coal_journey": mine_data.get("coal_journey"),
            "source_type": mine_data.get("source_type"),
        }
    except Exception as e:
        console_logger.debug(e)
        console_logger.debug("Error on line {}".format(sys.exc_info()[-1].tb_lineno))
        return {"detail": "failed"}


@router.get("/mine/shortcode/sourcetype", tags=["PDF Report"])
def get_mine_shortcodes_sourcetype(response:Response):
    try:
        sourcetype = []
        result = short_mine_collection.find({},{"_id":0})
        for single_data in result:
            if single_data["source_type"] is not None:
                sourcetype.append(single_data["source_type"])
        return sourcetype
    except Exception as e:
        console_logger.debug(e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(f"{exc_type} in {fname} on line {exc_tb.tb_lineno}")
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return {"detail": str(e)}


@router.post("/update/minecoaljourneysource", tags=["PDF Report"])
def endpoint_to_update_minename_data(response: Response, data: mineNameUpdate):
    try:
        final_data = data.dict()
        existing_entry = short_mine_collection.find_one({"_id": ObjectId(final_data.get("id"))})
        short_mine_collection.update_one(
            {"_id": ObjectId(final_data.get("id"))},
            {"$set": {"short_code": final_data.get("mine_code"), "coal_journey": final_data.get("mine_mode"), "source_type": final_data.get("source_type")}})
        return {"detail": "success"}
    except Exception as e:
        console_logger.debug(e)
        console_logger.debug("Error on line {}".format(sys.exc_info()[-1].tb_lineno))
        return {"detail": "failed"}


@router.get("/fetch/similarminelocation", tags=["PDF Report"])
def endpoint_to_fetch_aoptarget(response: Response, mine_name: str):
    try:    
        # fetchCoalTestingLocation = CoalTesting.objects.filter(location__contains = mine_name)
        # fetchCoalTestingTrainLocation = CoalTestingTrain.objects.filter(location__contains = mine_name)

        fetchCoalTestingLocation = RecieptCoalQualityAnalysis.objects.filter(mine__icontains = mine_name, mode="Road")
        fetchCoalTestingTrainLocation = RecieptCoalQualityAnalysis.objects.filter(mine__icontains = mine_name, mode="Rail")

        coalTestingData = [singlecoalTesting["mine"] for singlecoalTesting in fetchCoalTestingLocation]
        coalTestingTrainData = [singlecoalTestingTrain["mine"] for singlecoalTestingTrain in fetchCoalTestingTrainLocation]

        return list(set(coalTestingData)) + list(set(coalTestingTrainData))
    except Exception as e:
        console_logger.debug(e)
        console_logger.debug("Error on line {}".format(sys.exc_info()[-1].tb_lineno))
        return {"detail": "failed"}
    

@router.delete("/delete/aoptarget", tags=["PDF Report"])
def endpoint_to_delete_aoptarget(response: Response, id: str):
    try:
        fetchAopTarget = AopTarget.objects.get(id=id)
        fetchAopTarget.delete()
        return {"detail": "success"}
    except Exception as e:
        console_logger.debug(e)
        console_logger.debug("Error on line {}".format(sys.exc_info()[-1].tb_lineno))
        return {"detail": "failed"}


@router.get("/fetch/scheduler", tags=["PDF Report"])
def endpoint_to_fetch_report_scheduler(response: Response):
    try:
        dataList = []
        fetchReportScheduler = ReportScheduler.objects()
        for single_report in fetchReportScheduler:
            dataList.append(single_report.payload())
        return dataList
    except Exception as e:
        console_logger.debug(e)
        console_logger.debug("Error on line {}".format(sys.exc_info()[-1].tb_lineno))
        return {"detail": "failed"}


def fetch_email_data():
    try:
        headers = {}
        data = {}
        url_data = f"http://{ip}/api/v1/base/smtp/unprotected"
        response = requests.request("GET", url=url_data, headers=headers, data=data, proxies=proxies)
        data = json.loads(response.text)
        console_logger.debug(data)
        return response.status_code, data
    except Exception as e:
        console_logger.debug(e)

def check_existing_notification(notification_name):
    current_time = datetime.datetime.now()
    start_of_day = current_time.replace(hour=0, minute=0, second=0, microsecond=0)
    end_of_day = start_of_day + datetime.timedelta(days=1)
    console_logger.debug(start_of_day)
    console_logger.debug(end_of_day)
    console_logger.debug(emailNotifications.objects(
        Q(notification_name=notification_name) & 
        Q(created_at__gte=start_of_day) & 
        Q(created_at__lt=end_of_day)
    ).count() > 0)
    return emailNotifications.objects(
        Q(notification_name=notification_name) & 
        Q(created_at__gte=start_of_day) & 
        Q(created_at__lt=end_of_day)
    ).count() > 0 # getting either true or false

def send_shift_report_generate(**kwargs):
    console_logger.debug(("scheduler report generate",kwargs))
    reportSchedule = ReportScheduler.objects.get(report_name=kwargs.get("report_name"))
    if reportSchedule.active == False:
        console_logger.debug("scheduler is off")
        return
    elif reportSchedule.active == True:
        if not check_existing_notification(f"coal_bunkering_schedule_{kwargs.get('shift_name')}"):
            emailNotifications(notification_name=f"coal_bunkering_schedule_{kwargs.get('shift_name')}").save()
            console_logger.debug("inside Coal Bunkering Schedule")
            fetchShiftScheduler = shiftScheduler.objects.get(report_name=kwargs.get("report_name"), shift_name=kwargs.get('shift_name'))
            if fetchShiftScheduler:
                console_logger.debug(fetchShiftScheduler.start_shift_time)
                console_logger.debug(fetchShiftScheduler.end_shift_time)
                shift_start_time = datetime.datetime.strptime(fetchShiftScheduler.start_shift_time, '%H:%M').time()
                shift_end_time = datetime.datetime.strptime(fetchShiftScheduler.end_shift_time, '%H:%M').time()

                # Format the time to output only in 'HH:MM' format
                formatted_start_time = shift_start_time.strftime('%H:%M')
                formatted_end_time = shift_end_time.strftime('%H:%M')
                # fetchBunkerAnalysis = bunkerAnalysis.objects.filter(Q(created_at__gte=fetchShiftScheduler.start_shift_time) & Q(created_at__lte=fetchShiftScheduler.end_shift_time))
                fetchBunkerAnalysis = bunkerAnalysis.objects.filter(Q(start_date=formatted_start_time) & Q(created_date=formatted_end_time))
                html_per = ""
                if fetchBunkerAnalysis:
                    html_per += "<table border='1'><tr><th>ID</th><th>Shift Name</th><th>Units</th><th>Bunkering</th><th>MGCV</th><th>HGCV</th><th>Ratio</th><th>Date</th></tr>"
                    for single_bunker in fetchBunkerAnalysis:
                        html_per += "<tr>"
                        html_per +=f"<td>{single_bunker.ID}</td>"
                        if single_bunker.shift_name:
                            html_per += f"<td>{single_bunker.shift_name}</td>"
                        else:
                            html_per += "<td>-</td>"
                        html_per +=f"<td>{single_bunker.units}</td>"
                        html_per +=f"<td>{single_bunker.bunkering}</td>"
                        if single_bunker.mgcv:
                            html_per += f"<td>{single_bunker.mgcv}</td>"
                        else:
                            html_per += "<td>-</td>"
                        if single_bunker.hgcv:
                            html_per += f"<td>{single_bunker.hgcv}</td>"
                        else:
                            html_per += "<td>-</td>"
                        if single_bunker.ratio:
                            html_per += f"<td>{single_bunker.ratio}</td>"
                        else:
                            html_per += "<td>-</td>"
                        html_per += f"<td>{single_bunker.created_date.strftime('%d %b %Y')}</td>"
                        html_per += "</tr>"
                    html_per += "</table>"
                else:
                    html_per += "<b>No data found</b>"
                response_code, fetch_email = fetch_email_data()
                if response_code == 200:
                    subject = f"Bunkering Report for {kwargs.get('shift_name')} Date: {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                    body = f"""
                        <b>Bunkering Report for {kwargs.get('shift_name')} Date: {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}</b>
                        <br>
                        <br>
                        <!doctype html>
                        <html>
                        <head>
                            <meta charset="utf-8">
                            <title>Bunkering Report for {kwargs.get('shift_name')} Report</title>
                        </head>
                        <body>
                            {html_per}
                        </body>
                        </html>"""
                    checkEmailDevelopment = EmailDevelopmentCheck.objects()
                    if checkEmailDevelopment[0].development == "local":
                        console_logger.debug("inside 192")
                        send_email(fetch_email.get("Smtp_user"), subject, fetch_email.get("Smtp_password"), fetch_email.get("Smtp_host"), fetch_email.get("Smtp_port"), reportSchedule.recipient_list, body, "", reportSchedule.cc_list, reportSchedule.bcc_list)
                    elif checkEmailDevelopment[0].development == "prod":
                        console_logger.debug("outside 192")
                        send_data = {
                            "sender_email": fetch_email.get("Smtp_user"),
                            "subject": subject,
                            "password": fetch_email.get("Smtp_password"),
                            "smtp_host": fetch_email.get("Smtp_host"),
                            "smtp_port": fetch_email.get("Smtp_port"),
                            "receiver_email": reportSchedule.recipient_list,
                            "body": body,
                            "file_path": "",
                            "cc_list": reportSchedule.cc_list,
                            "bcc_list": reportSchedule.bcc_list
                        }
                        console_logger.debug(send_data)
                        generate_email(Response, email=send_data)

        else:
            return
    # except Exception as e:
    #     console_logger.debug(e)

def send_report_generate(**kwargs):
    try:
        console_logger.debug(("scheduler report generate",kwargs))
        reportSchedule = ReportScheduler.objects()
        if kwargs["report_name"] == "daily_coal_logistic_report":
            if reportSchedule[0].active == False:
                console_logger.debug("scheduler is off")
                return
            elif reportSchedule[0].active == True:
                if not check_existing_notification("daily_coal_logistic_report"):
                    emailNotifications(notification_name="daily_coal_logistic_report").save()
                    yesterday = datetime.datetime.now() - datetime.timedelta(days=1)
                    generateReportData = generate_gmr_report(Response, yesterday.strftime("%Y-%m-%d"), "All")
                    # generateReportData = generate_gmr_report(Response, "2024-08-01", "All")
                    
                    response_code, fetch_email = fetch_email_data()
                    if response_code == 200:
                        console_logger.debug(reportSchedule[0].recipient_list)
                        subject = f"GMR Daily Coal Logistic Report {datetime.datetime.strptime(yesterday.strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                        body = f"Daily Coal Logistic Report for Date: {datetime.datetime.strptime(yesterday.strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                        # send_email(smtpData.Smtp_user, subject, smtpData.Smtp_password, smtpData.Smtp_host, smtpData.Smtp_port, receiver_email, body, f"{os.path.join(os.getcwd())}{generateReportData}")
                        checkEmailDevelopment = EmailDevelopmentCheck.objects()
                        if checkEmailDevelopment[0].development == "local":
                            send_email(fetch_email.get("Smtp_user"), subject, fetch_email.get("Smtp_password"), fetch_email.get("Smtp_host"), fetch_email.get("Smtp_port"), reportSchedule[0].recipient_list, body, f"{os.path.join(os.getcwd())}/{generateReportData}", reportSchedule[0].cc_list, reportSchedule[0].bcc_list)
                            # another_test_email(fetch_email.get("Smtp_user"), subject, fetch_email.get("Smtp_password"), fetch_email.get("Smtp_host"), fetch_email.get("Smtp_port"), reportSchedule[0].recipient_list, body, f"{os.path.join(os.getcwd())}/{generateReportData}", reportSchedule[0].cc_list, reportSchedule[0].bcc_list)
                            # another_test_email()                        
                        elif checkEmailDevelopment[0].development == "prod":
                            send_data = {
                                "sender_email": fetch_email.get("Smtp_user"),
                                "subject": subject,
                                "password": fetch_email.get("Smtp_password"),
                                "smtp_host": fetch_email.get("Smtp_host"),
                                "smtp_port": fetch_email.get("Smtp_port"),
                                "receiver_email": reportSchedule[0].recipient_list,
                                "body": body,
                                "file_path": f"{os.path.join(os.getcwd())}/{generateReportData}",
                                "cc_list": reportSchedule[0].cc_list,
                                "bcc_list": reportSchedule[0].bcc_list
                            }
                            generate_email(Response, email=send_data)
                else:
                    return
        elif kwargs["report_name"] == "expiring_fitness_certificate":
            if reportSchedule[1].active == False:
                return
            elif reportSchedule[1].active == True:
                if not check_existing_notification("expiring_fitness_certificate"):
                    emailNotifications(notification_name="expiring_fitness_certificate").save()
                    generateExpiryData = endpoint_to_fetch_going_to_expiry_vehicle(Response)
                    tabledata = ""
                    for single_data in generateExpiryData["datasets"]:
                        tabledata +="<tr>"
                        tabledata +=f"<td>{single_data['vehicle_number']}</td>"
                        tabledata +=f"<td>{single_data['vehicle_chassis_number']}</td>"
                        tabledata +=f"<td>{single_data['expiry_date']}</td>"
                        tabledata +=f"<td>{single_data['days_to_go']}</td>"
                        tabledata +="</tr>"
                    response_code, fetch_email = fetch_email_data()
                    if response_code == 200:
                        # for receiver_email in reportSchedule[1].recipient_list:
                        subject = f"Expiring Fitness Certificate for Date: {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                        body = f"""
                        <b>Expiring Fitness Certificate for Date: {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}</b>
                        <br>
                        <br>
                        <!doctype html>
                        <html>
                        <head>
                            <meta charset="utf-8">
                            <title>Expiry Certificate</title>
                        </head>
                        <body>
                            <table border='1'>
                                <tr>
                                    <th>Vehicle Number</th>
                                    <th>Vehicle Chassis Number</th>
                                    <th>Certificate Expiry</th>
                                    <th>Days To Go</th>
                                </tr>
                                {tabledata}
                            </table>
                        </body>
                        </html>"""
                        checkEmailDevelopment = EmailDevelopmentCheck.objects()
                        if checkEmailDevelopment[0].development == "local":
                            send_email(fetch_email.get("Smtp_user"), subject, fetch_email.get("Smtp_password"), fetch_email.get("Smtp_host"), fetch_email.get("Smtp_port"), reportSchedule[1].recipient_list, body, "", reportSchedule[1].cc_list, reportSchedule[1].bcc_list)
                        elif checkEmailDevelopment[0].development == "prod":
                            send_data = {
                                "sender_email": fetch_email.get("Smtp_user"),
                                "subject": subject,
                                "password": fetch_email.get("Smtp_password"),
                                "smtp_host": fetch_email.get("Smtp_host"),
                                "smtp_port": fetch_email.get("Smtp_port"),
                                "receiver_email": reportSchedule[1].recipient_list,
                                "body": body,
                                "file_path": "",
                                "cc_list": reportSchedule[1].cc_list,
                                "bcc_list": reportSchedule[1].bcc_list
                            }
                            generate_email(Response, email=send_data)
                else:
                    return

        elif kwargs["report_name"] == "gwel_receipt_quality_analysis":
            if reportSchedule[2].active == False:
                console_logger.debug("scheduler is off")
                return
            elif reportSchedule[2].active == True:
                if not check_existing_notification("gwel_receipt_quality_analysis"):
                    emailNotifications(notification_name="gwel_receipt_quality_analysis").save()
                    console_logger.debug("inside gwel receipt quality analysis")
                    start_date = datetime.datetime.today().strftime('%Y-%m-%d')
                    end_date = datetime.datetime.today().strftime('%Y-%m-%d')
                    # start_date = (datetime.datetime.today() - datetime.timedelta(days=1)).strftime('%Y-%m-%d')
                    # end_date = (datetime.datetime.today() - datetime.timedelta(days=1)).strftime('%Y-%m-%d')
                    
                    # console_logger.debug(start_date)
                    # console_logger.debug(end_date)

                    # start_date = "2024-07-29"
                    # end_date = "2024-07-29"
                    filter_type = "gwel"
                    generateGwelReportData = fetch_excel_data_rail(Response, f"{start_date}T00:00", f"{end_date}T23:59", filter_type)
                    response_code, fetch_email = fetch_email_data()
                    if response_code == 200:
                        if generateGwelReportData.get("road") is None and generateGwelReportData.get("rail") is None:
                            console_logger.debug(reportSchedule[2].recipient_list)
                            subject = f"GWEL Received Coal Analysis {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                            body = f"""<b>No data found for GWEL Received Coal Analysis for Date: {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}</b>"""
                            checkEmailDevelopment = EmailDevelopmentCheck.objects()
                            if checkEmailDevelopment[0].development == "local":
                                send_email(fetch_email.get("Smtp_user"), subject, fetch_email.get("Smtp_password"), fetch_email.get("Smtp_host"), fetch_email.get("Smtp_port"), reportSchedule[2].recipient_list, body, "", reportSchedule[2].cc_list, reportSchedule[2].bcc_list)
                            elif checkEmailDevelopment[0].development == "prod":
                                send_data = {
                                    "sender_email": fetch_email.get("Smtp_user"),
                                    "subject": subject,
                                    "password": fetch_email.get("Smtp_password"),
                                    "smtp_host": fetch_email.get("Smtp_host"),
                                    "smtp_port": fetch_email.get("Smtp_port"),
                                    "receiver_email": reportSchedule[2].recipient_list,
                                    "body": body,
                                    "file_path": "",
                                    "cc_list": reportSchedule[2].cc_list,
                                    "bcc_list": reportSchedule[2].bcc_list
                                }
                                # console_logger.debug(send_data)
                                generate_email(Response, email=send_data)
                        else:
                            console_logger.debug(reportSchedule[2].recipient_list)
                            subject = f"GWEL Received Coal Analysis {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                            body = f"GWEL Received Coal Analysis for Date: {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                            # send_email(smtpData.Smtp_user, subject, smtpData.Smtp_password, smtpData.Smtp_host, smtpData.Smtp_port, receiver_email, body, f"{os.path.join(os.getcwd())}{generateReportData}")
                            checkEmailDevelopment = EmailDevelopmentCheck.objects()
                            if checkEmailDevelopment[0].development == "local":
                                send_email(fetch_email.get("Smtp_user"), subject, fetch_email.get("Smtp_password"), fetch_email.get("Smtp_host"), fetch_email.get("Smtp_port"), reportSchedule[2].recipient_list, body, generateGwelReportData, reportSchedule[2].cc_list, reportSchedule[2].bcc_list)
                            elif checkEmailDevelopment[0].development == "prod":
                                send_data = {
                                    "sender_email": fetch_email.get("Smtp_user"),
                                    "subject": subject,
                                    "password": fetch_email.get("Smtp_password"),
                                    "smtp_host": fetch_email.get("Smtp_host"),
                                    "smtp_port": fetch_email.get("Smtp_port"),
                                    "receiver_email": reportSchedule[2].recipient_list,
                                    "body": body,
                                    "file_path": f"{generateGwelReportData}",
                                    "cc_list": reportSchedule[2].cc_list,
                                    "bcc_list": reportSchedule[2].bcc_list
                                }
                                # console_logger.debug(send_data)
                                generate_email(Response, email=send_data)
                else:
                    return
        elif kwargs["report_name"] == "thirdparty_receipt_quality_analysis":
            if reportSchedule[3].active == False:
                console_logger.debug("scheduler is off")
                return
            elif reportSchedule[3].active == True:
                if not check_existing_notification("thirdparty_receipt_quality_analysis"):
                    emailNotifications(notification_name="thirdparty_receipt_quality_analysis").save()
                    console_logger.debug("inside thirdparty receipt quality analysis")
                    start_date = datetime.datetime.today().strftime('%Y-%m-%d')
                    end_date = datetime.datetime.today().strftime('%Y-%m-%d')
                    # start_date = (datetime.datetime.today() - datetime.timedelta(days=1)).strftime('%Y-%m-%d')
                    # end_date = (datetime.datetime.today() - datetime.timedelta(days=1)).strftime('%Y-%m-%d')

                    # console_logger.debug(start_date)
                    # console_logger.debug(start_date)

                    # start_date = "2024-07-29"
                    # end_date = "2024-07-29"
                    # filter_type = "third_party"
                    filter_type = "all"
                    generateThirdPartyReportData = fetch_excel_data_rail(Response, f"{start_date}T00:00", f"{end_date}T23:59", filter_type)
                    # console_logger.debug(f"{os.path.join(os.getcwd())}/{generateThirdPartyReportData}")
                    response_code, fetch_email = fetch_email_data()
                    if response_code == 200:
                        if generateThirdPartyReportData.get("road") is None and generateThirdPartyReportData.get("rail") is None:
                            console_logger.debug(reportSchedule[3].recipient_list)
                            subject = f"Third-Party Coal Analysis(Mahabal) {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                            body = f"""<b>No data found for Third-Party Coal Analysis(Mahabal) for Date: {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}</b>"""
                            checkEmailDevelopment = EmailDevelopmentCheck.objects()
                            if checkEmailDevelopment[0].development == "local":
                                send_email(fetch_email.get("Smtp_user"), subject, fetch_email.get("Smtp_password"), fetch_email.get("Smtp_host"), fetch_email.get("Smtp_port"), reportSchedule[3].recipient_list, body, "", reportSchedule[3].cc_list, reportSchedule[3].bcc_list)
                            elif checkEmailDevelopment[0].development == "prod":
                                send_data = {
                                    "sender_email": fetch_email.get("Smtp_user"),
                                    "subject": subject,
                                    "password": fetch_email.get("Smtp_password"),
                                    "smtp_host": fetch_email.get("Smtp_host"),
                                    "smtp_port": fetch_email.get("Smtp_port"),
                                    "receiver_email": reportSchedule[3].recipient_list,
                                    "body": body,
                                    "file_path": "",
                                    "cc_list": reportSchedule[3].cc_list,
                                    "bcc_list": reportSchedule[3].bcc_list
                                }
                                # console_logger.debug(send_data)
                                generate_email(Response, email=send_data)
                        else:
                            console_logger.debug(reportSchedule[3].recipient_list)
                            subject = f"Third-Party Coal Analysis(Mahabal) {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                            body = f"Third-Party Coal Analysis(Mahabal) Report for Date: {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                            # send_email(smtpData.Smtp_user, subject, smtpData.Smtp_password, smtpData.Smtp_host, smtpData.Smtp_port, receiver_email, body, f"{os.path.join(os.getcwd())}{generateReportData}")
                            checkEmailDevelopment = EmailDevelopmentCheck.objects()
                            if checkEmailDevelopment[0].development == "local":
                                send_email(fetch_email.get("Smtp_user"), subject, fetch_email.get("Smtp_password"), fetch_email.get("Smtp_host"), fetch_email.get("Smtp_port"), reportSchedule[3].recipient_list, body, generateThirdPartyReportData, reportSchedule[3].cc_list, reportSchedule[3].bcc_list)
                            elif checkEmailDevelopment[0].development == "prod":
                                send_data = {
                                    "sender_email": fetch_email.get("Smtp_user"),
                                    "subject": subject,
                                    "password": fetch_email.get("Smtp_password"),
                                    "smtp_host": fetch_email.get("Smtp_host"),
                                    "smtp_port": fetch_email.get("Smtp_port"),
                                    "receiver_email": reportSchedule[3].recipient_list,
                                    "body": body,
                                    "file_path": f"{generateThirdPartyReportData}",
                                    "cc_list": reportSchedule[3].cc_list,
                                    "bcc_list": reportSchedule[3].bcc_list
                                }
                                # console_logger.debug(send_data)
                                generate_email(Response, email=send_data)
                else:
                    return
        elif kwargs["report_name"] == "coal_logistics_table":
            if reportSchedule[4].active == False:
                console_logger.debug("scheduler is off")
                return
            elif reportSchedule[4].active == True:
                if not check_existing_notification("coal_logistics_table"):
                    emailNotifications(notification_name="coal_logistics_table").save()
                    console_logger.debug("inside Coal Logistics Table")
                    specified_date = datetime.datetime.today().strftime('%Y-%m-%d')
                    # specified_date = "2024-07-02"
                    roadData = fetch_data_road_logistics(Response, specified_date)
                    railData = fetch_data_rail_logistics(Response, specified_date)

                    # console_logger.debug(roadData)
                    # console_logger.debug(railData)
                    response_code, fetch_email = fetch_email_data()
                    if response_code == 200:
                        htmlData = ""
                        if roadData != 404 and roadData is not None:
                            htmlData += f"<b>Daily Road Coal Logistic Report for {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}</b><br><br>"
                            htmlData += roadData
                            htmlData += "<br><br>"
                        else:
                            htmlData += f"<b>No data found for Daily Road Coal Logistic Report for {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}</b><br><br>"
                        if railData != 404 and railData is not None:
                            htmlData += f"<b>Daily Rail Coal Logistic Report for {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}</b><br><br>"
                            htmlData += railData
                            htmlData += "<br><br>"
                        else:
                            htmlData += f"<b>No data found for Daily Rail Coal Logistic Report for {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}</b><br><br>"

                        console_logger.debug(reportSchedule[4].recipient_list)
                        subject = f"Daily Coal Receipt by Road & Rail for Date: {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                        body = f"""
                        <h3>Daily Coal Receipt by Road & Rail for Date: {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}</h3>
                        <br>
                        <br>
                        <br>
                        <!doctype html>
                        <html>
                        <head>
                            <meta charset="utf-8">
                            <title>Daily Coal Receipt by Road & Rail</title>
                        </head>
                        <body>
                            {htmlData}
                        </body>
                        </html>"""
                        checkEmailDevelopment = EmailDevelopmentCheck.objects()
                        if checkEmailDevelopment[0].development == "local":
                            send_email(fetch_email.get("Smtp_user"), subject, fetch_email.get("Smtp_password"), fetch_email.get("Smtp_host"), fetch_email.get("Smtp_port"), reportSchedule[4].recipient_list, body, "", reportSchedule[4].cc_list, reportSchedule[4].bcc_list)
                        elif checkEmailDevelopment[0].development == "prod":
                            send_data = {
                                "sender_email": fetch_email.get("Smtp_user"),
                                "subject": subject,
                                "password": fetch_email.get("Smtp_password"),
                                "smtp_host": fetch_email.get("Smtp_host"),
                                "smtp_port": fetch_email.get("Smtp_port"),
                                "receiver_email": reportSchedule[4].recipient_list,
                                "body": body,
                                "file_path": "",
                                "cc_list": reportSchedule[4].cc_list,
                                "bcc_list": reportSchedule[4].bcc_list
                            }
                            # console_logger.debug(send_data)
                            generate_email(Response, email=send_data)
                else:
                    return
        elif kwargs["report_name"] == "coal_bunkering_table":
            if reportSchedule[5].active == False:
                console_logger.debug("scheduler is off")
                return
            elif reportSchedule[5].active == True:
                if not check_existing_notification("coal_bunkering_table"):
                    emailNotifications(notification_name="coal_bunkering_table").save()
                    console_logger.debug("inside Coal Bunkering Table")
                    specified_date = datetime.datetime.today().strftime('%Y-%m-%d')
                    # specified_date = "2024-07-02"
                    bunkerData = coal_bunker_analysis(Response, specified_date)

                    response_code, fetch_email = fetch_email_data()
                    if response_code == 200:
                        htmlData = ""
                        if bunkerData != 404 and bunkerData is not None:
                            htmlData += f"<b>Daily Coal Bunkering Report for {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}</b><br><br>"
                            htmlData += bunkerData
                            htmlData += "<br><br>"
                        else:
                            htmlData += f"<b>No data found for Daily Coal Bunkering Report for {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}</b><br><br>"
                        
                        console_logger.debug(reportSchedule[5].recipient_list)
                        subject = f"Daily Coal Bunkering Report for Date: {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                        body = f"""
                        <h3>Daily Coal Bunkering Report for Date: {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}</h3>
                        <br>
                        <br>
                        <br>
                        <!doctype html>
                        <html>
                        <head>
                            <meta charset="utf-8">
                            <title>Daily Bunkering Report</title>
                        </head>
                        <body>
                            {htmlData}
                        </body>
                        </html>"""
                        checkEmailDevelopment = EmailDevelopmentCheck.objects()
                        if checkEmailDevelopment[0].development == "local":
                            send_email(fetch_email.get("Smtp_user"), subject, fetch_email.get("Smtp_password"), fetch_email.get("Smtp_host"), fetch_email.get("Smtp_port"), reportSchedule[5].recipient_list, body, "", reportSchedule[5].cc_list, reportSchedule[5].bcc_list)
                        elif checkEmailDevelopment[0].development == "prod":
                            send_data = {
                                "sender_email": fetch_email.get("Smtp_user"),
                                "subject": subject,
                                "password": fetch_email.get("Smtp_password"),
                                "smtp_host": fetch_email.get("Smtp_host"),
                                "smtp_port": fetch_email.get("Smtp_port"),
                                "receiver_email": reportSchedule[5].recipient_list,
                                "body": body,
                                "file_path": "",
                                "cc_list": reportSchedule[5].cc_list,
                                "bcc_list": reportSchedule[5].bcc_list
                            }
                            # console_logger.debug(send_data)
                            generate_email(Response, email=send_data)
                else:
                    return
        elif kwargs["report_name"] == "road_coal_journey_report":
            if reportSchedule[8].active == False:
                console_logger.debug("scheduler is off")
                return
            elif reportSchedule[8].active == True:
                if not check_existing_notification("road_coal_journey_report"):
                    emailNotifications(notification_name="road_coal_journey_report").save()
                    console_logger.debug("inside road coal journey report")
                    date_data = datetime.datetime.today().strftime('%Y-%m-%d')
                    # date_data = "2024-09-17"
                    headers = {
                        'accept': 'application/json',
                    }

                    params = {
                        'filter_data': ["Sr.No.", "Mines_Name", "Type_of_consumer", "PO_No", "Line_Item", "Delivery_Challan_No", "DO_No", "LOT", "vehicle_number", "Challan_Gross_Wt(MT)", "Challan_Tare_Wt(MT)", "Challan_Net_Wt(MT)", "GWEL_Gross_Wt(MT)", "GWEL_Tare_Wt(MT)", "GWEL_Net_Wt(MT)", "GWEL_Gross_Time", "GWEL_Tare_Time", "Transit_Loss", "Transporter_LR_No", "Transporter_LR_Date", "Total_net_amount", "Vehicle_in_time", "Vehicle_out_time", "TAT_difference", "PO_Date", "DO_Qty", "Weightment_Date", "Weightment_Time", "Vehicle_Chassis_No", "Fitness_Expiry", "Driver_Name", "Gate_Pass_No", "Total_net_amount", "Eway_bill_No" ],
                        'start_timestamp': f'{date_data}T00:00',
                        'end_timestamp': f'{date_data}T23:59',
                        'type': 'download',
                    }
                    response = requests.get(f'http://{ip}:7704/road_journey_table_new_filter', params=params, headers=headers)
                    if response.status_code == 200:
                        finalData = json.loads(response.text)
                        console_logger.debug(f"{os.path.join(os.getcwd())}/{finalData.get('File_Path')}")
                        response_code, fetch_email = fetch_email_data()
                        if response_code == 200:
                            console_logger.debug(reportSchedule[8].recipient_list)
                            subject = f"GMR Road Coal Journey Report {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                            body = f"Daily GMR Road Coal Journey Report for Date: {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                            checkEmailDevelopment = EmailDevelopmentCheck.objects()
                            if checkEmailDevelopment[0].development == "local":
                                console_logger.debug("inside local")
                                send_email(fetch_email.get("Smtp_user"), subject, fetch_email.get("Smtp_password"), fetch_email.get("Smtp_host"), fetch_email.get("Smtp_port"), reportSchedule[8].recipient_list, body, f"{os.path.join(os.getcwd())}/{finalData.get('File_Path')}", reportSchedule[8].cc_list, reportSchedule[8].bcc_list)
                            elif checkEmailDevelopment[0].development == "prod":
                                console_logger.debug("inside prod")
                                send_data = {
                                    "sender_email": fetch_email.get("Smtp_user"),
                                    "subject": subject,
                                    "password": fetch_email.get("Smtp_password"),
                                    "smtp_host": fetch_email.get("Smtp_host"),
                                    "smtp_port": fetch_email.get("Smtp_port"),
                                    "receiver_email": reportSchedule[8].recipient_list,
                                    "body": body,
                                    "file_path": f"{os.path.join(os.getcwd())}/{finalData.get('File_Path')}",
                                    "cc_list": reportSchedule[8].cc_list,
                                    "bcc_list": reportSchedule[8].bcc_list
                                }
                                # console_logger.debug(send_data)
                                generate_email(Response, email=send_data)
                else:
                    return
        elif kwargs["report_name"] == "specific_coal_consumption_report":
            if reportSchedule[9].active == False:
                console_logger.debug("scheduler is off")
                return
            elif reportSchedule[9].active == True:
                if not check_existing_notification("specific_coal_consumption_report"):
                    emailNotifications(notification_name="specific_coal_consumption_report").save()
                    generateReportData = endpoint_to_generate_coal_consumption_report(Response, datetime.date.today().strftime("%Y-%m-%d"))
                    response_code, fetch_email = fetch_email_data()
                    if response_code == 200:
                        console_logger.debug(reportSchedule[9].recipient_list)
                        subject = f"GMR Specific Coal Consumption Report {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                        body = f"Specific Coal Consumption Report for Date: {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                        # send_email(smtpData.Smtp_user, subject, smtpData.Smtp_password, smtpData.Smtp_host, smtpData.Smtp_port, receiver_email, body, f"{os.path.join(os.getcwd())}{generateReportData}")
                        checkEmailDevelopment = EmailDevelopmentCheck.objects()
                        if checkEmailDevelopment[0].development == "local":
                            send_email(fetch_email.get("Smtp_user"), subject, fetch_email.get("Smtp_password"), fetch_email.get("Smtp_host"), fetch_email.get("Smtp_port"), reportSchedule[9].recipient_list, body, f"{os.path.join(os.getcwd())}/{generateReportData}", reportSchedule[9].cc_list, reportSchedule[9].bcc_list)
                        elif checkEmailDevelopment[0].development == "prod":
                            send_data = {
                                "sender_email": fetch_email.get("Smtp_user"),
                                "subject": subject,
                                "password": fetch_email.get("Smtp_password"),
                                "smtp_host": fetch_email.get("Smtp_host"),
                                "smtp_port": fetch_email.get("Smtp_port"),
                                "receiver_email": reportSchedule[9].recipient_list,
                                "body": body,
                                "file_path": f"{os.path.join(os.getcwd())}/{generateReportData}",
                                "cc_list": reportSchedule[9].cc_list,
                                "bcc_list": reportSchedule[9].bcc_list
                            }
                            generate_email(Response, email=send_data)
                else:
                    return   
        elif kwargs["report_name"] == "bunker_quality_analysis":
            if reportSchedule[11].active == False:
                console_logger.debug("scheduler is off")
                return
            elif reportSchedule[11].active == True:
                if not check_existing_notification("bunker_quality_analysis"):
                    emailNotifications(notification_name="bunker_quality_analysis").save()
                    # todays_date = f"{datetime.date.today().strftime("%Y-%m-%d")} 00"
                    start_todays_date = f"{datetime.date.today().strftime('%Y-%m-%d')}T00:00"
                    end_todays_date = f"{datetime.date.today().strftime('%Y-%m-%d')}T23:59"

                    # start_todays_date = "2024-08-23T00:00"
                    # end_todays_date = "2024-08-23T23:59"
                    fetchBunkerData = fetch_bunker_data(Response, start_timestamp=start_todays_date, end_timestamp=end_todays_date, type="download")
                    console_logger.debug(fetchBunkerData.get("File_Path"))
                    response_code, fetch_email = fetch_email_data()
                    if response_code == 200:
                        console_logger.debug(reportSchedule[11].recipient_list)
                        if fetchBunkerData.get("File_Path") is None:
                            console_logger.debug(reportSchedule[11].recipient_list)
                            subject = f"GMR Bunker Analysis Report {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                            body = f"""<b>No data found for Bunker Analysis Report for Date: {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}</b>"""
                            checkEmailDevelopment = EmailDevelopmentCheck.objects()
                            if checkEmailDevelopment[0].development == "local":
                                send_email(fetch_email.get("Smtp_user"), subject, fetch_email.get("Smtp_password"), fetch_email.get("Smtp_host"), fetch_email.get("Smtp_port"), reportSchedule[11].recipient_list, body, "", reportSchedule[11].cc_list, reportSchedule[11].bcc_list)
                            elif checkEmailDevelopment[0].development == "prod":
                                send_data = {
                                    "sender_email": fetch_email.get("Smtp_user"),
                                    "subject": subject,
                                    "password": fetch_email.get("Smtp_password"),
                                    "smtp_host": fetch_email.get("Smtp_host"),
                                    "smtp_port": fetch_email.get("Smtp_port"),
                                    "receiver_email": reportSchedule[11].recipient_list,
                                    "body": body,
                                    "file_path": "",
                                    "cc_list": reportSchedule[11].cc_list,
                                    "bcc_list": reportSchedule[11].bcc_list
                                }
                                # console_logger.debug(send_data)
                                generate_email(Response, email=send_data)
                        else:
                            subject = f"GMR Bunker Analysis Report {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                            body = f"Bunker Analysis Report for Date: {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                            # send_email(smtpData.Smtp_user, subject, smtpData.Smtp_password, smtpData.Smtp_host, smtpData.Smtp_port, receiver_email, body, f"{os.path.join(os.getcwd())}{generateReportData}")
                            checkEmailDevelopment = EmailDevelopmentCheck.objects()
                            if checkEmailDevelopment[0].development == "local":
                                send_email(fetch_email.get("Smtp_user"), subject, fetch_email.get("Smtp_password"), fetch_email.get("Smtp_host"), fetch_email.get("Smtp_port"), reportSchedule[11].recipient_list, body, f"{os.path.join(os.getcwd())}/{fetchBunkerData.get('File_Path')}", reportSchedule[11].cc_list, reportSchedule[11].bcc_list)
                            elif checkEmailDevelopment[0].development == "prod":
                                console_logger.debug("inside prod")
                                send_data = {
                                    "sender_email": fetch_email.get("Smtp_user"),
                                    "subject": subject,
                                    "password": fetch_email.get("Smtp_password"),
                                    "smtp_host": fetch_email.get("Smtp_host"),
                                    "smtp_port": fetch_email.get("Smtp_port"),
                                    "receiver_email": reportSchedule[11].recipient_list,
                                    "body": body,
                                    "file_path": f"{os.path.join(os.getcwd())}/{fetchBunkerData.get('File_Path')}",
                                    "cc_list": reportSchedule[11].cc_list,
                                    "bcc_list": reportSchedule[11].bcc_list
                                }
                                generate_email(Response, email=send_data)
                else:
                    return  
        elif kwargs["report_name"] == "coal_analysis":
            if reportSchedule[12].active == False:
                console_logger.debug("scheduler is off")
                return
            elif reportSchedule[12].active == True:
                if not check_existing_notification("coal_analysis"):
                    emailNotifications(notification_name="coal_analysis").save()
                    input_date = date.today()
                    DataExecutionsHandler = DataExecutions()
                    start_date, end_date = DataExecutionsHandler.get_financial_year_dates_from_todays(input_date=input_date)
                    start_todays_date = f"{start_date}T00:00"
                    end_todays_date = f"{end_date}T23:59"
                    fetchOverAllAnalysisData = get_overall_analysis(Response, start_timestamp=start_todays_date, end_timestamp=end_todays_date, type="download")
                    console_logger.debug(fetchOverAllAnalysisData.get("File_Path"))
                    response_code, fetch_email = fetch_email_data()
                    if response_code == 200:
                        console_logger.debug(reportSchedule[12].recipient_list)
                        if fetchOverAllAnalysisData.get("File_Path") is None:
                            console_logger.debug(reportSchedule[12].recipient_list)
                            subject = f"GMR Coal Analysis Report {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                            body = f"""<b>No data found for Coal Analysis Report for Date: {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}</b>"""
                            checkEmailDevelopment = EmailDevelopmentCheck.objects()
                            if checkEmailDevelopment[0].development == "local":
                                send_email(fetch_email.get("Smtp_user"), subject, fetch_email.get("Smtp_password"), fetch_email.get("Smtp_host"), fetch_email.get("Smtp_port"), reportSchedule[12].recipient_list, body, "", reportSchedule[12].cc_list, reportSchedule[12].bcc_list)
                            elif checkEmailDevelopment[0].development == "prod":
                                send_data = {
                                    "sender_email": fetch_email.get("Smtp_user"),
                                    "subject": subject,
                                    "password": fetch_email.get("Smtp_password"),
                                    "smtp_host": fetch_email.get("Smtp_host"),
                                    "smtp_port": fetch_email.get("Smtp_port"),
                                    "receiver_email": reportSchedule[12].recipient_list,
                                    "body": body,
                                    "file_path": "",
                                    "cc_list": reportSchedule[12].cc_list,
                                    "bcc_list": reportSchedule[12].bcc_list
                                }
                                # console_logger.debug(send_data)
                                generate_email(Response, email=send_data)
                        else:
                            subject = f"GMR Coal Analysis Report {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                            body = f"Coal Analysis Report for Date: {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                            # send_email(smtpData.Smtp_user, subject, smtpData.Smtp_password, smtpData.Smtp_host, smtpData.Smtp_port, receiver_email, body, f"{os.path.join(os.getcwd())}{generateReportData}")
                            checkEmailDevelopment = EmailDevelopmentCheck.objects()
                            if checkEmailDevelopment[0].development == "local":
                                send_email(fetch_email.get("Smtp_user"), subject, fetch_email.get("Smtp_password"), fetch_email.get("Smtp_host"), fetch_email.get("Smtp_port"), reportSchedule[12].recipient_list, body, f"{os.path.join(os.getcwd())}/{fetchOverAllAnalysisData.get('File_Path')}", reportSchedule[12].cc_list, reportSchedule[12].bcc_list)
                            elif checkEmailDevelopment[0].development == "prod":
                                console_logger.debug("inside prod")
                                send_data = {
                                    "sender_email": fetch_email.get("Smtp_user"),
                                    "subject": subject,
                                    "password": fetch_email.get("Smtp_password"),
                                    "smtp_host": fetch_email.get("Smtp_host"),
                                    "smtp_port": fetch_email.get("Smtp_port"),
                                    "receiver_email": reportSchedule[12].recipient_list,
                                    "body": body,
                                    "file_path": f"{os.path.join(os.getcwd())}/{fetchOverAllAnalysisData.get('File_Path')}",
                                    "cc_list": reportSchedule[12].cc_list,
                                    "bcc_list": reportSchedule[12].bcc_list
                                }
                                generate_email(Response, email=send_data)
                else:
                    return  
        elif kwargs["report_name"] == "coal_gcv_analysis":
            if reportSchedule[13].active == False:
                console_logger.debug("scheduler is off")
                return
            elif reportSchedule[13].active == True:
                if not check_existing_notification("coal_gcv_analysis"):
                    emailNotifications(notification_name="coal_gcv_analysis").save()
                    input_date = date.today()
                    DataExecutionsHandler = DataExecutions()
                    fetch_financial_year = DataExecutionsHandler.get_financial_year_final(input_date=input_date)
                    console_logger.debug(f"FY {fetch_financial_year}")
                    fetchGcvAnalysisData = endpoint_to_fetch_gcv_analysis(Response, search_text=f"FY {fetch_financial_year}", type="download")
                    console_logger.debug(fetchGcvAnalysisData.get("File_Path"))
                    response_code, fetch_email = fetch_email_data()
                    if response_code == 200:
                        console_logger.debug(reportSchedule[13].recipient_list)
                        if fetchGcvAnalysisData.get("File_Path") is None:
                            subject = f"GMR Coal GCV Analysis Report {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                            body = f"""<b>No data found for Coal GCV Analysis Report for Date: {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}</b>"""
                            checkEmailDevelopment = EmailDevelopmentCheck.objects()
                            if checkEmailDevelopment[0].development == "local":
                                send_email(fetch_email.get("Smtp_user"), subject, fetch_email.get("Smtp_password"), fetch_email.get("Smtp_host"), fetch_email.get("Smtp_port"), reportSchedule[13].recipient_list, body, "", reportSchedule[13].cc_list, reportSchedule[13].bcc_list)
                            elif checkEmailDevelopment[0].development == "prod":
                                send_data = {
                                    "sender_email": fetch_email.get("Smtp_user"),
                                    "subject": subject,
                                    "password": fetch_email.get("Smtp_password"),
                                    "smtp_host": fetch_email.get("Smtp_host"),
                                    "smtp_port": fetch_email.get("Smtp_port"),
                                    "receiver_email": reportSchedule[13].recipient_list,
                                    "body": body,
                                    "file_path": "",
                                    "cc_list": reportSchedule[13].cc_list,
                                    "bcc_list": reportSchedule[13].bcc_list
                                }
                                # console_logger.debug(send_data)
                                generate_email(Response, email=send_data)
                        else:
                            subject = f"GMR Coal GCV Analysis Report {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                            body = f"Coal GCV Analysis Report for Date: {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                            # send_email(smtpData.Smtp_user, subject, smtpData.Smtp_password, smtpData.Smtp_host, smtpData.Smtp_port, receiver_email, body, f"{os.path.join(os.getcwd())}{generateReportData}")
                            checkEmailDevelopment = EmailDevelopmentCheck.objects()
                            if checkEmailDevelopment[0].development == "local":
                                send_email(fetch_email.get("Smtp_user"), subject, fetch_email.get("Smtp_password"), fetch_email.get("Smtp_host"), fetch_email.get("Smtp_port"), reportSchedule[13].recipient_list, body, f"{os.path.join(os.getcwd())}/{fetchGcvAnalysisData.get('File_Path')}", reportSchedule[13].cc_list, reportSchedule[13].bcc_list)
                            elif checkEmailDevelopment[0].development == "prod":
                                console_logger.debug("inside prod")
                                send_data = {
                                    "sender_email": fetch_email.get("Smtp_user"),
                                    "subject": subject,
                                    "password": fetch_email.get("Smtp_password"),
                                    "smtp_host": fetch_email.get("Smtp_host"),
                                    "smtp_port": fetch_email.get("Smtp_port"),
                                    "receiver_email": reportSchedule[13].recipient_list,
                                    "body": body,
                                    "file_path": f"{os.path.join(os.getcwd())}/{fetchGcvAnalysisData.get('File_Path')}",
                                    "cc_list": reportSchedule[13].cc_list,
                                    "bcc_list": reportSchedule[13].bcc_list
                                }
                                generate_email(Response, email=send_data)
                else:
                    return  
        return "success"
    except Exception as e:
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug("Error on line {}".format(sys.exc_info()[-1].tb_lineno))
        return {"detail": "failed"}


@router.get("/mail_testing_data", tags=["test"])
def test_api(response: Response):
    try:
        # send_report_generate(**{'report_name': 'daily_coal_logistic_report'})
        # send_report_generate(**{'report_name': 'certificate_expiry_notifications'})
        # send_report_generate(**{'report_name': 'coal_bunkering_schedule'})
        send_report_generate(**{'report_name': 'road_coal_journey_report'})
        return {"detail": "success"}
    except Exception as e:
        console_logger.debug(e)


@router.get("/consumer_type", tags=["Road Map"])
def endpoint_to_fetch_consumer_type(response: Response):
    try:
        listData = []
        checkConsumerType = Gmrdata.objects.only("type_consumer")
        for singleConsumerType in checkConsumerType:
            type_consumer_data = singleConsumerType.payload()["Type_of_consumer"]
            # console_logger.debug(singleConsumerType.payload())
            if type_consumer_data:
                listData.append(type_consumer_data.strip())
        return list(set(listData))
    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(
            "Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno)
        )
        return e


@router.get("/coal_logistics_report", tags=["Road Map"])
def coal_logistics_report_test(
    response: Response,
    specified_date: str,
    search_text: Optional[str] = None,
    currentPage: Optional[int] = None,
    perPage: Optional[int] = None,
    mine: Optional[str] = "All",
    consumer_type: Optional[str] = "All",
    type: Optional[str] = "display"
):
    try:
        result = {"labels": [], "datasets": [], "total": 0, "page_size": 15}
        if type and type == "display":

            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage
            
            skip_value = (page_no - 1) * page_len

            # Apply filters based on specified conditions
            if mine and mine != "All":
                mine_filter = {'$match': {'mine': {'$regex': f'{mine.upper()}', '$options': 'i'}}}
            else:
                mine_filter = {}

            if consumer_type and consumer_type != "All":
                consumer_type_filter = {'$match': {'type_consumer': consumer_type}}
            else:
                consumer_type_filter = {}

            if search_text:
                if search_text.isdigit():
                    search_filter = {'$match': {'arv_cum_do_number': {'$regex': f'{search_text}', '$options': 'i'}}}
                else:
                    search_filter = {'$match': {'mine': {'$regex': f'{search_text}', '$options': 'i'}}}
            else:
                search_filter = {}

            # Date filter using specified_date
            if specified_date:
                from_ts = convert_to_utc_format(f'{specified_date} 00:00:00', "%Y-%m-%d %H:%M:%S")
                to_ts = convert_to_utc_format(f'{specified_date} 23:59:59', "%Y-%m-%d %H:%M:%S")
                date_filter = {
                    '$match': {
                        'GWEL_Tare_Time': {
                            '$ne': None,
                            # '$gte': from_ts,
                            '$lte': to_ts
                        }
                    }
                }
                challan_date_filter = {
                    '$match': {
                        'GWEL_Tare_Time': {
                            '$ne': None,
                            '$gte': from_ts,
                            '$lte': to_ts
                        }
                    }
                }
            else:
                date_filter = {}
                challan_date_filter = {}
            created_at_date = datetime.datetime(2024, 9, 23, 19, 50, 51, 572000)
            basePipeline = [
                {
                    '$match': {
                        'created_at': {
                            '$gt': created_at_date,
                        }
                    }
                },
                date_filter,  # Add date filter
                mine_filter,  # Add mine filter
                consumer_type_filter,  # Add consumer_type filter
                search_filter,  # Add search_text filter if any
                {
                    '$group': {
                        '_id': '$arv_cum_do_number',
                        'challan_lr_qty': {
                            '$sum': {
                                '$toDouble': '$net_qty'
                            }
                        },
                        'Grade': {
                            '$first': '$grade'
                        },
                        'slno': {
                            '$first': '$slno'
                        },
                        'start_date': {
                            '$first': '$start_date'
                        },
                        'end_date': {
                            '$first': '$end_date'
                        },
                        'type_consumer': {
                            '$first': '$type_consumer'
                        },
                        'do_qty': {
                            '$first': '$po_qty'
                        },
                        'mine_name': {
                            '$first': '$mine'
                        },
                        'grn_status': {
                            '$first': '$grn_status'
                        },
                        'date': {
                            '$last': '$GWEL_Tare_Time'
                        }
                    }
                },
                {
                    '$lookup': {
                        'from': 'gmrdata',
                        'localField': '_id',
                        'foreignField': 'arv_cum_do_number',
                        'as': 'cumulative_data'
                    }
                },
                {
                    '$addFields': {
                        'cumulative_challan_lr_qty': {
                            '$sum': {
                                '$map': {
                                    'input': '$cumulative_data',
                                    'as': 'item',
                                    'in': {
                                        '$toDouble': '$$item.net_qty'
                                    }
                                }
                            }
                        }
                    }
                },
                # {'$skip': skip_value},
                # {'$limit': page_len}
            ]
            # Remove empty filters from the pipeline
            basePipeline = [stage for stage in basePipeline if stage]

            challanlrqtybasePipeline = [
                {
                    '$match': {
                        'created_at': {
                            '$gt': created_at_date,
                        }
                    }
                },
                challan_date_filter,  # Add date filter
                mine_filter,  # Add mine filter
                consumer_type_filter,  # Add consumer_type filter
                search_filter,  # Add search_text filter if any
                {
                    '$group': {
                        '_id': '$arv_cum_do_number', 
                        'challan_lr_qty': {
                            '$sum': {
                                '$toDouble': '$net_qty'
                            }
                        }, 
                        'Grade': {
                            '$first': '$grade'
                        }, 
                        'slno': {
                            '$first': '$slno'
                        }, 
                        'start_date': {
                            '$first': '$start_date'
                        }, 
                        'end_date': {
                            '$first': '$end_date'
                        }, 
                        'type_consumer': {
                            '$first': '$type_consumer'
                        }, 
                        'do_qty': {
                            '$first': '$po_qty'
                        }, 
                        'mine_name': {
                            '$first': '$mine'
                        }, 
                        'grn_status': {
                            '$first': '$grn_status'
                        },
                        'date': {
                            '$last': '$GWEL_Tare_Time'
                        }
                    }
                }, {
                    '$lookup': {
                        'from': 'gmrdata', 
                        'localField': '_id', 
                        'foreignField': 'arv_cum_do_number', 
                        'as': 'cumulative_data'
                    }
                }, 
                # {
                #     '$addFields': {
                #         'cumulative_challan_lr_qty': {
                #             '$sum': {
                #                 '$map': {
                #                     'input': '$cumulative_data', 
                #                     'as': 'item', 
                #                     'in': {
                #                         '$toDouble': '$$item.net_qty'
                #                     }
                #                 }
                #             }
                #         }
                #     }
                # }
                {
                    '$addFields': {
                        'cumulative_challan_lr_qty': {
                            '$sum': {
                                '$map': {
                                    'input': '$cumulative_data', 
                                    'as': 'item', 
                                    'in': {
                                        '$convert': {
                                            'input': '$$item.net_qty', 
                                            'to': 'double', 
                                            'onError': 0, 
                                            'onNull': 0
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            ]


            challanlrqtybasePipeline = [stage for stage in challanlrqtybasePipeline if stage] 

            basePipelineHistoric = [
                date_filter,  # Add date filter
                mine_filter,  # Add mine filter
                consumer_type_filter,  # Add consumer_type filter
                search_filter,  # Add search_text filter if any
                {
                    '$group': {
                        '_id': '$arv_cum_do_number',
                        'challan_lr_qty': {
                            '$sum': {
                                '$toDouble': '$net_qty'
                            }
                        },
                        'Grade': {
                            '$first': '$grade'
                        },
                        'slno': {
                            '$first': '$slno'
                        },
                        'start_date': {
                            '$first': '$start_date'
                        },
                        'end_date': {
                            '$first': '$end_date'
                        },
                        'type_consumer': {
                            '$first': '$type_consumer'
                        },
                        'do_qty': {
                            '$first': '$po_qty'
                        },
                        'mine_name': {
                            '$first': '$mine'
                        },
                        'grn_status': {
                            '$first': '$grn_status'
                        },
                        'date': {
                            '$last': '$GWEL_Tare_Time'
                        }
                    }
                },
                {
                    '$lookup': {
                        'from': 'gmrdata',
                        'localField': '_id',
                        'foreignField': 'arv_cum_do_number',
                        'as': 'cumulative_data'
                    }
                },
                {
                    '$addFields': {
                        'cumulative_challan_lr_qty': {
                            '$sum': {
                                '$map': {
                                    'input': '$cumulative_data',
                                    'as': 'item',
                                    'in': {
                                        '$toDouble': '$$item.net_qty'
                                    }
                                }
                            }
                        }
                    }
                },
                # {'$skip': skip_value},
                # {'$limit': page_len}
            ]

            basePipelineHistoric = [stage for stage in basePipelineHistoric if stage]
            

            basepipelineHistoricChallanLrQty = [
                challan_date_filter,  # Add date filter
                mine_filter,  # Add mine filter
                consumer_type_filter,  # Add consumer_type filter
                search_filter,  # Add search_text filter if any
                {
                    '$group': {
                        '_id': '$arv_cum_do_number', 
                        'challan_lr_qty': {
                            '$sum': {
                                '$toDouble': '$net_qty'
                            }
                        }, 
                        'Grade': {
                            '$first': '$grade'
                        }, 
                        'slno': {
                            '$first': '$slno'
                        }, 
                        'start_date': {
                            '$first': '$start_date'
                        }, 
                        'end_date': {
                            '$first': '$end_date'
                        }, 
                        'type_consumer': {
                            '$first': '$type_consumer'
                        }, 
                        'do_qty': {
                            '$first': '$po_qty'
                        }, 
                        'mine_name': {
                            '$first': '$mine'
                        }, 
                        'grn_status': {
                            '$first': '$grn_status'
                        },
                        'date': {
                            '$last': '$GWEL_Tare_Time'
                        }
                    }
                }, {
                    '$lookup': {
                        'from': 'gmrdata', 
                        'localField': '_id', 
                        'foreignField': 'arv_cum_do_number', 
                        'as': 'cumulative_data'
                    }
                }, 
                # {
                #     '$addFields': {
                #         'cumulative_challan_lr_qty': {
                #             '$sum': {
                #                 '$map': {
                #                     'input': '$cumulative_data', 
                #                     'as': 'item', 
                #                     'in': {
                #                         '$toDouble': '$$item.net_qty'
                #                     }
                #                 }
                #             }
                #         }
                #     }
                # }
                {
                    '$addFields': {
                        'cumulative_challan_lr_qty': {
                            '$sum': {
                                '$map': {
                                    'input': '$cumulative_data', 
                                    'as': 'item', 
                                    'in': {
                                        '$convert': {
                                            'input': '$$item.net_qty', 
                                            'to': 'double', 
                                            'onError': 0, 
                                            'onNull': 0
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            ]

            # Remove empty filters from the pipeline
            basepipelineHistoricChallanLrQty = [stage for stage in basepipelineHistoricChallanLrQty if stage]

            basePipelineRcrRoad = [
                date_filter,  # Add date filter
                mine_filter,  # Add mine filter
                consumer_type_filter,  # Add consumer_type filter
                search_filter,  # Add search_text filter if any
                {
                    '$group': {
                        '_id': '$do_number', 
                        'challan_lr_qty': {
                            '$sum': {
                                '$toDouble': '$dc_net_wt'
                            }
                        }, 
                        'Grade': {
                            '$first': '$grade'
                        }, 
                        'slno': {
                            '$first': '$slno'
                        }, 
                        'start_date': {
                            '$first': '$start_date'
                        }, 
                        'end_date': {
                            '$first': '$end_date'
                        }, 
                        'type_consumer': {
                            '$first': '$type_consumer'
                        }, 
                        'do_qty': {
                            '$first': '$po_qty'
                        }, 
                        'mine_name': {
                            '$first': '$mine'
                        }, 
                        # 'grn_status': {
                        #     '$first': '$grn_status'
                        # },
                        'date': {
                            '$last': '$tar_wt_date'
                        }
                    }
                }, {
                    '$lookup': {
                        'from': 'RcrRoadData', 
                        'localField': '_id', 
                        'foreignField': 'do_number', 
                        'as': 'cumulative_data'
                    }
                }, 
                # {
                #     '$addFields': {
                #         'cumulative_challan_lr_qty': {
                #             '$sum': {
                #                 '$map': {
                #                     'input': '$cumulative_data', 
                #                     'as': 'item', 
                #                     'in': {
                #                         '$toDouble': '$$item.dc_net_wt'
                #                     }
                #                 }
                #             }
                #         }
                #     }
                # }
                {
                    '$addFields': {
                        'cumulative_challan_lr_qty': {
                            '$sum': {
                                '$map': {
                                    'input': '$cumulative_data', 
                                    'as': 'item', 
                                    'in': {
                                        '$convert': {
                                            'input': '$$item.net_qty', 
                                            'to': 'double', 
                                            'onError': 0, 
                                            'onNull': 0
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            ]

            basePipelineRcrRoad = [stage for stage in basePipelineRcrRoad if stage]

            basepipelineRcrRoadChallanLrQty = [
                challan_date_filter,  # Add date filter
                mine_filter,  # Add mine filter
                consumer_type_filter,  # Add consumer_type filter
                search_filter,  # Add search_text filter if any
                {
                    '$group': {
                        '_id': '$do_number', 
                        'challan_lr_qty': {
                            '$sum': {
                                '$toDouble': '$dc_net_wt'
                            }
                        }, 
                        'Grade': {
                            '$first': '$grade'
                        }, 
                        'slno': {
                            '$first': '$slno'
                        }, 
                        'start_date': {
                            '$first': '$start_date'
                        }, 
                        'end_date': {
                            '$first': '$end_date'
                        }, 
                        'type_consumer': {
                            '$first': '$type_consumer'
                        }, 
                        'do_qty': {
                            '$first': '$po_qty'
                        }, 
                        'mine_name': {
                            '$first': '$mine'
                        }, 
                        # 'grn_status': {
                        #     '$first': '$grn_status'
                        # },
                        'date': {
                            '$last': '$tar_wt_date'
                        }
                    }
                }, {
                    '$lookup': {
                        'from': 'RcrRoadData', 
                        'localField': '_id', 
                        'foreignField': 'do_number', 
                        'as': 'cumulative_data'
                    }
                }, 
                # {
                #     '$addFields': {
                #         'cumulative_challan_lr_qty': {
                #             '$sum': {
                #                 '$map': {
                #                     'input': '$cumulative_data', 
                #                     'as': 'item', 
                #                     'in': {
                #                         '$toDouble': '$$item.dc_net_wt'
                #                     }
                #                 }
                #             }
                #         }
                #     }
                # }
                {
                    '$addFields': {
                        'cumulative_challan_lr_qty': {
                            '$sum': {
                                '$map': {
                                    'input': '$cumulative_data', 
                                    'as': 'item', 
                                    'in': {
                                        '$convert': {
                                            'input': '$$item.net_qty', 
                                            'to': 'double', 
                                            'onError': 0, 
                                            'onNull': 0
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            ]

            basepipelineRcrRoadChallanLrQty = [stage for stage in basepipelineRcrRoadChallanLrQty if stage]

            saprecordsPipeline = [
                {
                    '$match': {
                        '$expr': {
                            '$and': [
                                {
                                    '$lte': [
                                        {
                                            '$dateFromString': {
                                                'dateString': '$start_date'
                                            }
                                        }, datetime.datetime.strptime(specified_date, "%Y-%m-%d")
                                    ]
                                },
                                {
                                    '$gte': [
                                        {
                                            '$dateFromString': {
                                                'dateString': '$end_date'
                                            }
                                        }, datetime.datetime.strptime(specified_date, "%Y-%m-%d")
                                    ]
                                }
                            ]
                        }
                    }
                },
                mine_filter,  # Add mine filter
                consumer_type_filter,  # Add consumer_type filter
                search_filter,  # Add search_text filter if any
                {
                    '$group': {
                        '_id': '$do_no',
                        'mine_name': {
                            '$first': '$mine_name'
                        },
                        'do_qty': {
                            '$sum': {
                                '$toDouble': '$do_qty'
                            }
                        },
                        'start_date': {
                            '$first': '$start_date'
                        },
                        'end_date': {
                            '$first': '$end_date'
                        },
                        'source_type': {
                            '$first': '$source'
                        },
                        'slno': {
                            '$first': '$slno'
                        }
                    }
                },
                {
                    '$project': {
                        '_id': 1,
                        'mine_name': 1,
                        'do_qty': 1,
                        'start_date': 1,
                        'end_date': 1,
                        'source_type': 1,
                        'slno': 1
                    }
                },
                # {'$skip': skip_value},
                # {'$limit': page_len}
            ]

            saprecordsPipeline = [stage for stage in saprecordsPipeline if stage]


            saprecordsRcrRoadPipeline = [
                {
                    '$match': {
                        '$expr': {
                            '$and': [
                                {
                                    '$lte': [
                                        { '$dateFromString': { 'dateString': '$start_date' } }, 
                                        datetime.datetime.strptime(specified_date, "%Y-%m-%d")
                                    ]
                                }, 
                                {
                                    '$gte': [
                                        { '$dateFromString': { 'dateString': '$end_date' } }, 
                                        datetime.datetime.strptime(specified_date, "%Y-%m-%d")
                                    ]
                                }
                            ]
                        }
                    }
                }, 
                mine_filter,  # Add mine filter
                consumer_type_filter,  # Add consumer_type filter
                search_filter,  # Add search_text filter if any
                {
                    '$group': {
                        '_id': '$do_no',  # Grouping by do_no
                        'mine_name': { '$first': '$mine_name' },  # Getting the first mine_name in the group
                        'do_qty': { '$sum': { '$toDouble': '$do_qty' } },  # Summing up the do_qty as double
                        'start_date': { '$first': '$start_date' },  # Getting the first start_date
                        'end_date': { '$first': '$end_date' },  # Getting the first end_date
                        'source_type': { '$first': '$consumer_type' },  # Getting the first source_type
                        'slno': { '$first': '$slno' },  # Getting the first slno
                        'Grade': {'$first': '$grade'}
                    }
                }, 
                {
                    '$project': {
                        '_id': 1, 
                        'mine_name': 1, 
                        'do_qty': 1, 
                        'start_date': 1, 
                        'end_date': 1, 
                        'source_type': 1, 
                        'slno': 1,
                        'Grade': 1,
                    }
                }
            ]

            saprecordsRcrRoadPipeline = [stage for stage in saprecordsRcrRoadPipeline if stage]


            # saprecordsPipeline = [stage for stage in saprecordsPipeline if stage]

            fetchGmrData = Gmrdata.objects.aggregate(basePipeline)
            fetchGmrDatachallanltqty = Gmrdata.objects.aggregate(challanlrqtybasePipeline)
            fetchGmrHistoricData = gmrdataHistoric.objects.aggregate(basePipelineHistoric)
            fetchGmrHistoricDataChallanLrQty = gmrdataHistoric.objects.aggregate(basepipelineHistoricChallanLrQty)

            fetchRcrRoadData = RcrRoadData.objects.aggregate(basePipelineRcrRoad)
            fetchRcrRoadchallanlrqty = RcrRoadData.objects.aggregate(basepipelineRcrRoadChallanLrQty)

            fetchSapRecordsData = SapRecords.objects.aggregate(saprecordsPipeline)

            fetchSapRecordsRcrData = SapRecordsRcrRoad.objects.aggregate(saprecordsRcrRoadPipeline)

            listData= []

            for singleData in fetchGmrData:
                dictData = {}
                dictData["DO_No"] = singleData.get("_id")
                dictData["mine_name"] = singleData.get("mine_name")
                if singleData.get("do_qty"):
                    dictData["DO_Qty"] = int(singleData.get("do_qty"))
                else:
                    dictData["DO_Qty"] = 0
                # dictData["challan_lr_/_qty"] = round(singleData.get("challan_lr_qty"), 2)
                dictData["cumulative_challan_lr_/_qty"] = round(singleData.get("cumulative_challan_lr_qty"), 2)
                # dictData["grade"] = singleData.get("Grade")
                dictData["grn_status"] = singleData.get("grn_status")
                dictData["date"] = singleData.get("date").strftime("%Y-%m-%d")
                if singleData.get("start_date"):
                    dictData["start_date"] = singleData.get("start_date")
                else:
                    dictData["start_date"] = "N/A"
                if singleData.get("end_date"):
                    dictData["end_date"] = singleData.get("end_date")
                else:
                    dictData["end_date"] = "N/A"
                dictData["source_type"] = singleData.get("type_consumer")
                # dictData["month"] = singleData.get("slno")
                dictData["month"] = datetime.datetime.strptime(singleData.get("slno"), "%Y%m").strftime("%B %Y") if singleData.get("slno") else "-"
                if singleData.get("Grade") is not None:
                    if '-' in singleData.get("Grade"):
                        dictData["average_GCV_Grade"] = singleData.get("Grade").split("-")[0]
                    elif " " in singleData.get("Grade"):
                        dictData["average_GCV_Grade"] = singleData.get("Grade").split(" ")[0]
                    else:
                        dictData["average_GCV_Grade"] = singleData.get("Grade")
                else:
                    dictData["average_GCV_Grade"] = "N/A"
                if singleData.get("start_date") is not None and singleData.get("end_date") is not None:
                    tomorrow_date = datetime.datetime.strptime(singleData.get("end_date"), "%Y-%m-%d").date() + datetime.timedelta(days=1)
                    balance_days = tomorrow_date - datetime.datetime.strptime(specified_date, "%Y-%m-%d").date()
                    dictData["balance_days"] = balance_days.days
                else:
                    dictData["balance_days"] = 0
                    
                if singleData.get("do_qty") is not None:
                    single_do_qty = singleData.get("do_qty")
                else:
                    single_do_qty = 0

                if single_do_qty != 0:
                    dictData['percent_supply'] = round((singleData.get('cumulative_challan_lr_qty') / int(single_do_qty)) * 100, 2)
                else:
                    dictData['percent_supply'] = 0
                
                dictData["balance_qty"] = round(int(single_do_qty) - singleData.get("cumulative_challan_lr_qty"), 2)
                    
                if dictData['balance_days'] and dictData['balance_qty'] != 0:
                    dictData['asking_rate'] = round(dictData['balance_qty'] / dictData['balance_days'], 2)
                else:
                    dictData["asking_rate"] = 0
                listData.append(dictData)
            
            # console_logger.debug(listData)
            
            for singleDataHistoric in fetchGmrHistoricData:
                dictDataHIstoric = {}
                dictDataHIstoric["DO_No"] = singleDataHistoric.get("_id")
                dictDataHIstoric["mine_name"] = singleDataHistoric.get("mine_name")
                if singleDataHistoric.get("do_qty"):
                    dictDataHIstoric["DO_Qty"] = float(singleDataHistoric.get("do_qty"))
                else:
                    dictDataHIstoric["DO_Qty"] = 0
                # dictDataHIstoric["DO_Qty"] = int(singleDataHistoric.get("do_qty"))
                # dictDataHIstoric["challan_lr_/_qty"] = round(singleDataHistoric.get("challan_lr_qty"), 2)
                dictDataHIstoric["cumulative_challan_lr_/_qty"] = round(singleDataHistoric.get("cumulative_challan_lr_qty"), 2)
                # dictDataHIstoric["grade"] = singleDataHistoric.get("Grade")
                dictDataHIstoric["grn_status"] = singleDataHistoric.get("grn_status")
                dictDataHIstoric["date"] = singleDataHistoric.get("date").strftime("%Y-%m-%d")
                if singleDataHistoric.get("start_date"):
                    dictDataHIstoric["start_date"] = singleDataHistoric.get("start_date")
                else:
                    dictDataHIstoric["start_date"] = "N/A"
                if singleDataHistoric.get("end_date"):
                    dictDataHIstoric["end_date"] = singleDataHistoric.get("end_date")
                else:
                    dictDataHIstoric["end_date"] = "N/A"
                dictDataHIstoric["source_type"] = singleDataHistoric.get("type_consumer")
                # dictDataHIstoric["month"] = singleDataHistoric.get("slno")
                dictDataHIstoric["month"] = datetime.datetime.strptime(singleDataHistoric.get("slno"), "%Y%m").strftime("%B %Y") if singleDataHistoric.get("slno") else "-"
                if singleDataHistoric.get("Grade") is not None:
                    if '-' in singleDataHistoric.get("Grade"):
                        dictDataHIstoric["average_GCV_Grade"] = singleDataHistoric.get("Grade").split("-")[0]
                    elif " " in singleDataHistoric.get("Grade"):
                        dictDataHIstoric["average_GCV_Grade"] = singleDataHistoric.get("Grade").split(" ")[0]
                    else:
                        dictDataHIstoric["average_GCV_Grade"] = singleDataHistoric.get("Grade")
                else:
                    dictDataHIstoric["average_GCV_Grade"] = "N/A"
                if singleDataHistoric.get("start_date") is not None and singleDataHistoric.get("end_date") is not None:
                    tomorrow_date = datetime.datetime.strptime(singleDataHistoric.get("end_date"), "%Y-%m-%d").date() + datetime.timedelta(days=1)
                    balance_days = tomorrow_date - datetime.datetime.strptime(specified_date, "%Y-%m-%d").date()
                    dictDataHIstoric["balance_days"] = balance_days.days
                else:
                    dictDataHIstoric["balance_days"] = 0

                if singleDataHistoric.get("do_qty") is not None:
                    single_do_qty = singleDataHistoric.get("do_qty")
                else:
                    single_do_qty = 0

                if single_do_qty != 0:
                    dictDataHIstoric['percent_supply'] = round((singleDataHistoric.get('cumulative_challan_lr_qty') / float(single_do_qty)) * 100, 2)
                else:
                    dictDataHIstoric['percent_supply'] = 0
                    
                dictDataHIstoric["balance_qty"] = round(float(single_do_qty) - singleDataHistoric.get("cumulative_challan_lr_qty"), 2)
                    
                if dictDataHIstoric['balance_days'] and dictDataHIstoric['balance_qty'] != 0:
                    dictDataHIstoric['asking_rate'] = round(dictDataHIstoric['balance_qty'] / dictDataHIstoric['balance_days'], 2)
                else:
                    dictDataHIstoric["asking_rate"] = 0
                
                # console_logger.debug(dictDataHIstoric)
                
                do_no_exists = any(item['DO_No'] == dictDataHIstoric["DO_No"] for item in listData)

                if not do_no_exists:
                    print("DO_No does not exist in final_data.")
                    listData.append(dictDataHIstoric)

            for singleDataRcrData in fetchRcrRoadData:
                dictDataRcr = {}
                dictDataRcr["DO_No"] = singleDataRcrData.get("_id")
                dictDataRcr["mine_name"] = singleDataRcrData.get("mine_name")
                if singleDataRcrData.get("do_qty"):
                    dictDataRcr["DO_Qty"] = int(singleDataRcrData.get("do_qty"))
                else:
                    dictDataRcr["DO_Qty"] = 0
                dictDataRcr["cumulative_challan_lr_/_qty"] = round(singleDataRcrData.get("cumulative_challan_lr_qty"), 2)
                dictDataRcr["grn_status"] = singleDataRcrData.get("grn_status")
                dictDataRcr["date"] = singleDataRcrData.get("date").strftime("%Y-%m-%d")
                dictDataRcr["start_date"] = singleDataRcrData.get("start_date")
                dictDataRcr["end_date"] = singleDataRcrData.get("end_date")
                dictDataRcr["source_type"] = singleDataRcrData.get("type_consumer")
                dictDataRcr["slno"] = datetime.datetime.strptime(singleDataRcrData.get("slno"), "%Y%m").strftime("%B %Y") if singleDataRcrData.get("slno") else "-"
                if singleDataRcrData.get("Grade") is not None:
                    if '-' in singleDataRcrData.get("Grade"):
                        dictDataRcr["average_GCV_Grade"] = singleDataRcrData.get("Grade").split("-")[0]
                    elif " " in singleDataRcrData.get("Grade"):
                        dictDataRcr["average_GCV_Grade"] = singleDataRcrData.get("Grade").split(" ")[0]
                    else:
                        dictDataRcr["average_GCV_Grade"] = singleDataRcrData.get("Grade")
                if singleDataRcrData.get("start_date") is not None and singleDataRcrData.get("end_date") is not None:
                    tomorrow_date = datetime.datetime.strptime(singleDataRcrData.get("end_date"), "%Y-%m-%d").date() + datetime.timedelta(days=1)
                    balance_days = tomorrow_date - datetime.datetime.strptime(specified_date, "%Y-%m-%d").date()
                    dictDataRcr["balance_days"] = balance_days.days
                else:
                    dictDataRcr["balance_days"] = 0
                if singleDataRcrData.get("do_qty") is not None:
                    single_do_qty = singleDataRcrData.get("do_qty")
                else:
                    single_do_qty = 0
                if single_do_qty != 0:
                    dictDataRcr['percent_supply'] = round((singleDataRcrData.get('cumulative_challan_lr_qty') / int(single_do_qty)) * 100, 2)
                else:
                    dictDataRcr['percent_supply'] = 0
                    
                dictDataRcr["balance_qty"] = round(int(single_do_qty) - singleDataRcrData.get("cumulative_challan_lr_qty"), 2)
                    
                if dictDataRcr['balance_days'] and dictDataRcr['balance_qty'] != 0:
                    dictDataRcr['asking_rate'] = round(dictDataRcr['balance_qty'] / dictDataRcr['balance_days'], 2)
                else:
                    dictDataRcr["asking_rate"] = 0
                # listData.append(dictDataRcr)

                do_no_existsRcr = any(item['DO_No'] == dictDataRcr["DO_No"] for item in listData)

                if not do_no_existsRcr:
                    print("DO_No does not exist in final_data.")
                    listData.append(dictDataRcr)
            
            for saprecordsSingle in fetchSapRecordsData:
                sapdict = {}
                sapdict["DO_No"] = saprecordsSingle.get("_id")
                sapdict["mine_name"] = saprecordsSingle.get("mine_name")
                sapdict["DO_Qty"] = int(saprecordsSingle.get("do_qty"))
                sapdict["start_date"] = saprecordsSingle.get("start_date")
                sapdict["end_date"] = saprecordsSingle.get("end_date")
                sapdict["source_type"] = saprecordsSingle.get("source_type")
                sapdict["challan_lr_/_qty"] = 0
                sapdict["cumulative_challan_lr_/_qty"] = 0
                sapdict["date"] = "N/A"
                sapdict["month"] = datetime.datetime.strptime(saprecordsSingle.get("slno"), "%Y%m").strftime("%B %Y") if saprecordsSingle.get("slno") else "-"
                if saprecordsSingle.get("Grade") is not None:
                    if '-' in saprecordsSingle.get("Grade"):
                        sapdict["average_GCV_Grade"] = saprecordsSingle.get("Grade").split("-")[0]
                    elif " " in saprecordsSingle.get("Grade"):
                        sapdict["average_GCV_Grade"] = saprecordsSingle.get("Grade").split(" ")[0]
                    else:
                        sapdict["average_GCV_Grade"] = saprecordsSingle.get("Grade")
                if saprecordsSingle.get("start_date") is not None and saprecordsSingle.get("end_date") is not None:
                    tomorrow_date = datetime.datetime.strptime(saprecordsSingle.get("end_date"), "%Y-%m-%d").date() + datetime.timedelta(days=1)
                    balance_days = tomorrow_date - datetime.datetime.strptime(specified_date, "%Y-%m-%d").date()
                    sapdict["balance_days"] = balance_days.days
                else:
                    sapdict["balance_days"] = 0
                if saprecordsSingle.get("do_qty") is not None and sapdict["cumulative_challan_lr_/_qty"] != 0:
                    sapdict['percent_supply'] = round((sapdict["cumulative_challan_lr_/_qty"] / int(saprecordsSingle.get('do_qty'))) * 100, 2)
                else:
                    sapdict["percent_supply"] = 0
                if saprecordsSingle.get("do_qty") is not None and sapdict["cumulative_challan_lr_/_qty"] != 0:
                    sapdict["balance_qty"] = round(int(saprecordsSingle.get("do_qty")) - sapdict["cumulative_challan_lr_/_qty"], 2)
                else:
                    sapdict["balance_qty"] = 0
                if sapdict['balance_days'] and sapdict['balance_qty'] != 0:
                    sapdict['asking_rate'] = round(sapdict['balance_qty'] / sapdict['balance_days'], 2)
                else:
                    sapdict["asking_rate"] = 0

                sap_do_no_exists = any(item['DO_No'] == sapdict["DO_No"] for item in listData)

                if not sap_do_no_exists:
                    print("DO_No does not exist in final_data.")
                    listData.append(sapdict)


            for saprecordsRcrSingle in fetchSapRecordsRcrData:
                sapdictRcr = {}
                sapdictRcr["DO_No"] = saprecordsRcrSingle.get("_id")
                sapdictRcr["mine_name"] = saprecordsRcrSingle.get("mine_name")
                sapdictRcr["DO_Qty"] = int(saprecordsRcrSingle.get("do_qty"))
                sapdictRcr["start_date"] = saprecordsRcrSingle.get("start_date")
                sapdictRcr["end_date"] = saprecordsRcrSingle.get("end_date")
                sapdictRcr["source_type"] = saprecordsRcrSingle.get("source_type")
                sapdictRcr["challan_lr_/_qty"] = 0
                sapdictRcr["date"] = "N/A"
                sapdictRcr["cumulative_challan_lr_/_qty"] = 0
                sapdictRcr["slno"] = datetime.datetime.strptime(saprecordsRcrSingle.get("slno"), "%Y%m").strftime("%B %Y") if saprecordsRcrSingle.get("slno") else "-"
                if saprecordsRcrSingle.get("Grade") is not None:
                    if '-' in saprecordsRcrSingle.get("Grade"):
                        sapdictRcr["average_GCV_Grade"] = saprecordsRcrSingle.get("Grade").split("-")[0]
                    elif " " in saprecordsRcrSingle.get("Grade"):
                        sapdictRcr["average_GCV_Grade"] = saprecordsRcrSingle.get("Grade").split(" ")[0]
                    else:
                        sapdictRcr["average_GCV_Grade"] = saprecordsRcrSingle.get("Grade")
                if saprecordsRcrSingle.get("start_date") is not None and saprecordsRcrSingle.get("end_date") is not None:
                    tomorrow_date = datetime.datetime.strptime(saprecordsRcrSingle.get("end_date"), "%Y-%m-%d").date() + datetime.timedelta(days=1)
                    balance_days = tomorrow_date - datetime.datetime.strptime(specified_date, "%Y-%m-%d").date()
                    sapdictRcr["balance_days"] = balance_days.days
                else:
                    sapdictRcr["balance_days"] = 0
                if saprecordsRcrSingle.get("do_qty") is not None:
                    do_qty_val = saprecordsRcrSingle.get("do_qty")
                else:
                    do_qty_val = 0
                if do_qty_val != 0:
                    sapdictRcr['percent_supply'] = round((sapdictRcr["cumulative_challan_lr_/_qty"] / int(do_qty_val)) * 100, 2)
                else:
                    sapdictRcr['percent_supply'] = 0
                sapdictRcr["balance_qty"] = round(int(do_qty_val) - sapdictRcr["cumulative_challan_lr_/_qty"], 2)
                if sapdictRcr['balance_days'] and sapdictRcr['balance_qty'] != 0:
                    sapdictRcr['asking_rate'] = round(sapdictRcr['balance_qty'] / sapdictRcr['balance_days'], 2)
                else:
                    sapdictRcr["asking_rate"] = 0

                sap_do_no_rcr_exists = any(item['DO_No'] == sapdictRcr.get("DO_No") for item in listData)
                
                if not sap_do_no_rcr_exists:
                    console_logger.debug("DO_No does not exist in final_data for sap_records")
                    listData.append(sapdictRcr)

            
            for singlelrqtyData in fetchGmrDatachallanltqty:
                dictDatalrQty = {}
                dictDatalrQty["DO_No"] = singlelrqtyData.get("_id")
                dictDatalrQty["mine_name"] = singlelrqtyData.get("mine_name")
                if singlelrqtyData.get("do_qty"):
                    dictDatalrQty["DO_Qty"] = int(float(singlelrqtyData.get("do_qty")))
                else:
                    dictDatalrQty["DO_Qty"] = 0
                if singlelrqtyData.get("challan_lr_qty"):
                    dictDatalrQty["challan_lr_/_qty"] = round(singlelrqtyData.get("challan_lr_qty"), 2)
                else:
                    dictDatalrQty["challan_lr_/_qty"] = 0
                
                # Check if there is an item with the same DO_No in listData
                do_no_exists_data = next((item for item in listData if item["DO_No"] == dictDatalrQty["DO_No"]), None)
                
                # If it exists, update the "challan_lr_qty" in listData
                if do_no_exists_data:
                    do_no_exists_data["challan_lr_/_qty"] = dictDatalrQty["challan_lr_/_qty"]

            for singlehistoriclrqty in fetchGmrHistoricDataChallanLrQty:
                dictDatahistoriclrQty = {}
                dictDatahistoriclrQty["DO_No"] = singlehistoriclrqty.get("_id")
                dictDatahistoriclrQty["mine_name"] = singlehistoriclrqty.get("mine_name")
                if singlehistoriclrqty.get("do_qty"):
                    dictDatahistoriclrQty["DO_Qty"] = int(float(singlehistoriclrqty.get("do_qty")))
                else:
                    dictDatahistoriclrQty["DO_Qty"] = 0
                if singlehistoriclrqty.get("challan_lr_qty"):
                    dictDatahistoriclrQty["challan_lr_/_qty"] = round(singlehistoriclrqty.get("challan_lr_qty"), 2)
                else:
                    dictDatahistoriclrQty["challan_lr_/_qty"] = 0
                
                # Check if there is an item with the same DO_No in listData
                do_no_exists_historic = next((item for item in listData if item["DO_No"] == dictDatahistoriclrQty["DO_No"]), None)
                
                # If it exists, update the "challan_lr_qty" in listData
                if do_no_exists_historic:
                    do_no_exists_historic["challan_lr_/_qty"] = dictDatahistoriclrQty["challan_lr_/_qty"]


            
            for singlercrroadlrqty in fetchRcrRoadchallanlrqty:
                dictDataRcrRoadlrQty = {}
                dictDataRcrRoadlrQty["DO_No"] = singlercrroadlrqty.get("_id")
                dictDataRcrRoadlrQty["mine_name"] = singlercrroadlrqty.get("mine_name")
                if singlercrroadlrqty.get("do_qty"):
                    dictDataRcrRoadlrQty["DO_Qty"] = int(float(singlercrroadlrqty.get("do_qty")))
                else:
                    dictDataRcrRoadlrQty["DO_Qty"] = 0
                if singlercrroadlrqty.get("challan_lr_qty"):
                    dictDataRcrRoadlrQty["challan_lr_/_qty"] = round(singlercrroadlrqty.get("challan_lr_qty"), 2)
                else:
                    dictDataRcrRoadlrQty["challan_lr_/_qty"] = 0
                
                # Check if there is an item with the same DO_No in listData
                do_no_exists_rcr_road = next((item for item in listData if item["DO_No"] == dictDataRcrRoadlrQty["DO_No"]), None)
                
                # If it exists, update the "challan_lr_qty" in listData
                if do_no_exists_rcr_road:
                    do_no_exists_historic["challan_lr_/_qty"] = dictDataRcrRoadlrQty["challan_lr_/_qty"]

            # console_logger.debug(listData)
            
            # final_data = [
            #     d for d in listData 
            #     if d['start_date'] is not None and datetime.datetime.strptime(d['start_date'], '%Y-%m-%d').date() <= datetime.datetime.now().date()
            # ]

            # final_data = listData

            result["labels"] = ["month", "DO_No", "mine_name", "DO_Qty", "date", "challan_lr_/_qty", "cumulative_challan_lr_/_qty","balance_qty", "percent_supply", "asking_rate", "average_GCV_Grade", "start_date", "end_date", "balance_days"]
            result["total"] = len(listData)

            start_idx = (page_no - 1) * page_len
            end_idx = start_idx + page_len
            paginated_data = listData[start_idx:end_idx]
            result["datasets"] = paginated_data
            return result
        elif type and type == "download":
            del type
            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)
            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage
            
            skip_value = (page_no - 1) * page_len

            # Apply filters based on specified conditions
            if mine and mine != "All":
                mine_filter = {'$match': {'mine': {'$regex': f'{mine.upper()}', '$options': 'i'}}}
            else:
                mine_filter = {}

            if consumer_type and consumer_type != "All":
                consumer_type_filter = {'$match': {'type_consumer': consumer_type}}
            else:
                consumer_type_filter = {}

            if search_text:
                if search_text.isdigit():
                    search_filter = {'$match': {'arv_cum_do_number': {'$regex': f'{search_text}', '$options': 'i'}}}
                else:
                    search_filter = {'$match': {'mine': {'$regex': f'{search_text}', '$options': 'i'}}}
            else:
                search_filter = {}

            # Date filter using specified_date
            if specified_date:
                from_ts = convert_to_utc_format(f'{specified_date} 00:00:00', "%Y-%m-%d %H:%M:%S")
                to_ts = convert_to_utc_format(f'{specified_date} 23:59:59', "%Y-%m-%d %H:%M:%S")
                date_filter = {
                    '$match': {
                        'GWEL_Tare_Time': {
                            '$ne': None,
                            # '$gte': from_ts,
                            '$lte': to_ts
                        }
                    }
                }
                challan_date_filter = {
                    '$match': {
                        'GWEL_Tare_Time': {
                            '$ne': None,
                            '$gte': from_ts,
                            '$lte': to_ts
                        }
                    }
                }
            else:
                date_filter = {}
                challan_date_filter = {}
            created_at_date = datetime.datetime(2024, 9, 23, 19, 50, 51, 572000)
            basePipeline = [
                {
                    '$match': {
                        'created_at': {
                            '$gt': created_at_date,
                        }
                    }
                },
                date_filter,  # Add date filter
                mine_filter,  # Add mine filter
                consumer_type_filter,  # Add consumer_type filter
                search_filter,  # Add search_text filter if any
                {
                    '$group': {
                        '_id': '$arv_cum_do_number',
                        'challan_lr_qty': {
                            '$sum': {
                                '$toDouble': '$net_qty'
                            }
                        },
                        'Grade': {
                            '$first': '$grade'
                        },
                        'slno': {
                            '$first': '$slno'
                        },
                        'start_date': {
                            '$first': '$start_date'
                        },
                        'end_date': {
                            '$first': '$end_date'
                        },
                        'type_consumer': {
                            '$first': '$type_consumer'
                        },
                        'do_qty': {
                            '$first': '$po_qty'
                        },
                        'mine_name': {
                            '$first': '$mine'
                        },
                        'grn_status': {
                            '$first': '$grn_status'
                        },
                        'date': {
                            '$last': '$GWEL_Tare_Time'
                        }
                    }
                },
                {
                    '$lookup': {
                        'from': 'gmrdata',
                        'localField': '_id',
                        'foreignField': 'arv_cum_do_number',
                        'as': 'cumulative_data'
                    }
                },
                {
                    '$addFields': {
                        'cumulative_challan_lr_qty': {
                            '$sum': {
                                '$map': {
                                    'input': '$cumulative_data',
                                    'as': 'item',
                                    'in': {
                                        '$toDouble': '$$item.net_qty'
                                    }
                                }
                            }
                        }
                    }
                },
                # {'$skip': skip_value},
                # {'$limit': page_len}
            ]

            # Remove empty filters from the pipeline
            basePipeline = [stage for stage in basePipeline if stage] 

            challanlrqtybasePipeline = [
                {
                    '$match': {
                        'created_at': {
                            '$gt': created_at_date,
                        }
                    }
                },
                challan_date_filter,  # Add date filter
                mine_filter,  # Add mine filter
                consumer_type_filter,  # Add consumer_type filter
                search_filter,  # Add search_text filter if any
                {
                    '$group': {
                        '_id': '$arv_cum_do_number', 
                        'challan_lr_qty': {
                            '$sum': {
                                '$toDouble': '$net_qty'
                            }
                        }, 
                        'Grade': {
                            '$first': '$grade'
                        }, 
                        'slno': {
                            '$first': '$slno'
                        }, 
                        'start_date': {
                            '$first': '$start_date'
                        }, 
                        'end_date': {
                            '$first': '$end_date'
                        }, 
                        'type_consumer': {
                            '$first': '$type_consumer'
                        }, 
                        'do_qty': {
                            '$first': '$po_qty'
                        }, 
                        'mine_name': {
                            '$first': '$mine'
                        }, 
                        'grn_status': {
                            '$first': '$grn_status'
                        },
                        'date': {
                            '$last': '$GWEL_Tare_Time'
                        }
                    }
                }, {
                    '$lookup': {
                        'from': 'gmrdata', 
                        'localField': '_id', 
                        'foreignField': 'arv_cum_do_number', 
                        'as': 'cumulative_data'
                    }
                }, 
                # {
                #     '$addFields': {
                #         'cumulative_challan_lr_qty': {
                #             '$sum': {
                #                 '$map': {
                #                     'input': '$cumulative_data', 
                #                     'as': 'item', 
                #                     'in': {
                #                         '$toDouble': '$$item.net_qty'
                #                     }
                #                 }
                #             }
                #         }
                #     }
                # }
                {
                    '$addFields': {
                        'cumulative_challan_lr_qty': {
                            '$sum': {
                                '$map': {
                                    'input': '$cumulative_data', 
                                    'as': 'item', 
                                    'in': {
                                        '$convert': {
                                            'input': '$$item.net_qty', 
                                            'to': 'double', 
                                            'onError': 0, 
                                            'onNull': 0
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            ]


            challanlrqtybasePipeline = [stage for stage in challanlrqtybasePipeline if stage]

            basePipelineHistoric = [
                date_filter,  # Add date filter
                mine_filter,  # Add mine filter
                consumer_type_filter,  # Add consumer_type filter
                search_filter,  # Add search_text filter if any
                {
                    '$group': {
                        '_id': '$arv_cum_do_number',
                        'challan_lr_qty': {
                            '$sum': {
                                '$toDouble': '$net_qty'
                            }
                        },
                        'Grade': {
                            '$first': '$grade'
                        },
                        'slno': {
                            '$first': '$slno'
                        },
                        'start_date': {
                            '$first': '$start_date'
                        },
                        'end_date': {
                            '$first': '$end_date'
                        },
                        'type_consumer': {
                            '$first': '$type_consumer'
                        },
                        'do_qty': {
                            '$first': '$po_qty'
                        },
                        'mine_name': {
                            '$first': '$mine'
                        },
                        'grn_status': {
                            '$first': '$grn_status'
                        },
                        'date': {
                            '$last': '$GWEL_Tare_Time'
                        }
                    }
                },
                {
                    '$lookup': {
                        'from': 'gmrdata',
                        'localField': '_id',
                        'foreignField': 'arv_cum_do_number',
                        'as': 'cumulative_data'
                    }
                },
                {
                    '$addFields': {
                        'cumulative_challan_lr_qty': {
                            '$sum': {
                                '$map': {
                                    'input': '$cumulative_data',
                                    'as': 'item',
                                    'in': {
                                        '$toDouble': '$$item.net_qty'
                                    }
                                }
                            }
                        }
                    }
                },
                {'$skip': skip_value},
                {'$limit': page_len}
            ]

            # Remove empty filters from the pipeline
            basePipelineHistoric = [stage for stage in basePipelineHistoric if stage]

            basepipelineHistoricChallanLrQty = [
                challan_date_filter,  # Add date filter
                mine_filter,  # Add mine filter
                consumer_type_filter,  # Add consumer_type filter
                search_filter,  # Add search_text filter if any
                {
                    '$group': {
                        '_id': '$arv_cum_do_number', 
                        'challan_lr_qty': {
                            '$sum': {
                                '$toDouble': '$net_qty'
                            }
                        }, 
                        'Grade': {
                            '$first': '$grade'
                        }, 
                        'slno': {
                            '$first': '$slno'
                        }, 
                        'start_date': {
                            '$first': '$start_date'
                        }, 
                        'end_date': {
                            '$first': '$end_date'
                        }, 
                        'type_consumer': {
                            '$first': '$type_consumer'
                        }, 
                        'do_qty': {
                            '$first': '$po_qty'
                        }, 
                        'mine_name': {
                            '$first': '$mine'
                        }, 
                        'grn_status': {
                            '$first': '$grn_status'
                        },
                        'date': {
                            '$last': '$GWEL_Tare_Time'
                        }
                    }
                }, {
                    '$lookup': {
                        'from': 'gmrdata', 
                        'localField': '_id', 
                        'foreignField': 'arv_cum_do_number', 
                        'as': 'cumulative_data'
                    }
                }, 
                # {
                #     '$addFields': {
                #         'cumulative_challan_lr_qty': {
                #             '$sum': {
                #                 '$map': {
                #                     'input': '$cumulative_data', 
                #                     'as': 'item', 
                #                     'in': {
                #                         '$toDouble': '$$item.net_qty'
                #                     }
                #                 }
                #             }
                #         }
                #     }
                # }
                {
                    '$addFields': {
                        'cumulative_challan_lr_qty': {
                            '$sum': {
                                '$map': {
                                    'input': '$cumulative_data', 
                                    'as': 'item', 
                                    'in': {
                                        '$convert': {
                                            'input': '$$item.net_qty', 
                                            'to': 'double', 
                                            'onError': 0, 
                                            'onNull': 0
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            ]

            # Remove empty filters from the pipeline
            basepipelineHistoricChallanLrQty = [stage for stage in basepipelineHistoricChallanLrQty if stage]


            basePipelineRcrRoad = [
                date_filter,  # Add date filter
                mine_filter,  # Add mine filter
                consumer_type_filter,  # Add consumer_type filter
                search_filter,  # Add search_text filter if any
                {
                    '$group': {
                        '_id': '$do_number', 
                        'challan_lr_qty': {
                            '$sum': {
                                '$toDouble': '$dc_net_wt'
                            }
                        }, 
                        'Grade': {
                            '$first': '$grade'
                        }, 
                        'slno': {
                            '$first': '$slno'
                        }, 
                        'start_date': {
                            '$first': '$start_date'
                        }, 
                        'end_date': {
                            '$first': '$end_date'
                        }, 
                        'type_consumer': {
                            '$first': '$type_consumer'
                        }, 
                        'do_qty': {
                            '$first': '$po_qty'
                        }, 
                        'mine_name': {
                            '$first': '$mine'
                        }, 
                        # 'grn_status': {
                        #     '$first': '$grn_status'
                        # },
                        'date': {
                            '$last': '$tar_wt_date'
                        }
                    }
                }, {
                    '$lookup': {
                        'from': 'RcrRoadData', 
                        'localField': '_id', 
                        'foreignField': 'do_number', 
                        'as': 'cumulative_data'
                    }
                }, 
                # {
                #     '$addFields': {
                #         'cumulative_challan_lr_qty': {
                #             '$sum': {
                #                 '$map': {
                #                     'input': '$cumulative_data', 
                #                     'as': 'item', 
                #                     'in': {
                #                         '$toDouble': '$$item.dc_net_wt'
                #                     }
                #                 }
                #             }
                #         }
                #     }
                # }
                {
                    '$addFields': {
                        'cumulative_challan_lr_qty': {
                            '$sum': {
                                '$map': {
                                    'input': '$cumulative_data', 
                                    'as': 'item', 
                                    'in': {
                                        '$convert': {
                                            'input': '$$item.net_qty', 
                                            'to': 'double', 
                                            'onError': 0, 
                                            'onNull': 0
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            ]

            basePipelineRcrRoad = [stage for stage in basePipelineRcrRoad if stage]

            basepipelineRcrRoadChallanLrQty = [
                challan_date_filter,  # Add date filter
                mine_filter,  # Add mine filter
                consumer_type_filter,  # Add consumer_type filter
                search_filter,  # Add search_text filter if any
                {
                    '$group': {
                        '_id': '$do_number', 
                        'challan_lr_qty': {
                            '$sum': {
                                '$toDouble': '$dc_net_wt'
                            }
                        }, 
                        'Grade': {
                            '$first': '$grade'
                        }, 
                        'slno': {
                            '$first': '$slno'
                        }, 
                        'start_date': {
                            '$first': '$start_date'
                        }, 
                        'end_date': {
                            '$first': '$end_date'
                        }, 
                        'type_consumer': {
                            '$first': '$type_consumer'
                        }, 
                        'do_qty': {
                            '$first': '$po_qty'
                        }, 
                        'mine_name': {
                            '$first': '$mine'
                        }, 
                        # 'grn_status': {
                        #     '$first': '$grn_status'
                        # },
                        'date': {
                            '$last': '$tar_wt_date'
                        }
                    }
                }, {
                    '$lookup': {
                        'from': 'RcrRoadData', 
                        'localField': '_id', 
                        'foreignField': 'do_number', 
                        'as': 'cumulative_data'
                    }
                }, 
                # {
                #     '$addFields': {
                #         'cumulative_challan_lr_qty': {
                #             '$sum': {
                #                 '$map': {
                #                     'input': '$cumulative_data', 
                #                     'as': 'item', 
                #                     'in': {
                #                         '$toDouble': '$$item.dc_net_wt'
                #                     }
                #                 }
                #             }
                #         }
                #     }
                # }
                {
                    '$addFields': {
                        'cumulative_challan_lr_qty': {
                            '$sum': {
                                '$map': {
                                    'input': '$cumulative_data', 
                                    'as': 'item', 
                                    'in': {
                                        '$convert': {
                                            'input': '$$item.net_qty', 
                                            'to': 'double', 
                                            'onError': 0, 
                                            'onNull': 0
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            ]

            basepipelineRcrRoadChallanLrQty = [stage for stage in basepipelineRcrRoadChallanLrQty if stage]


            saprecordsPipeline = [
                {
                    '$match': {
                        '$expr': {
                            '$and': [
                                {
                                    '$gte': [
                                        {
                                            '$dateFromString': {
                                                'dateString': '$start_date'
                                            }
                                        }, specified_date
                                    ]
                                },
                                {
                                    '$lte': [
                                        {
                                            '$dateFromString': {
                                                'dateString': '$end_date'
                                            }
                                        }, specified_date
                                    ]
                                }
                            ]
                        }
                    }
                },
                mine_filter,  # Add mine filter
                consumer_type_filter,  # Add consumer_type filter
                search_filter,  # Add search_text filter if any
                {
                    '$group': {
                        '_id': '$do_no',
                        'mine_name': {
                            '$first': '$mine_name'
                        },
                        'do_qty': {
                            '$sum': {
                                '$toDouble': '$do_qty'
                            }
                        },
                        'start_date': {
                            '$first': '$start_date'
                        },
                        'end_date': {
                            '$first': '$end_date'
                        },
                        'source_type': {
                            '$first': '$source'
                        },
                        'slno': {
                            '$first': '$slno'
                        }
                    }
                },
                {
                    '$project': {
                        '_id': 1,
                        'mine_name': 1,
                        'do_qty': 1,
                        'start_date': 1,
                        'end_date': 1,
                        'source_type': 1,
                        'slno': 1
                    }
                },
                # {'$skip': skip_value},
                # {'$limit': page_len}
            ]

            # Remove empty filters from the pipeline
            saprecordsPipeline = [stage for stage in saprecordsPipeline if stage]

            saprecordsRcrRoadPipeline = [
                {
                    '$match': {
                        '$expr': {
                            '$and': [
                                {
                                    '$lte': [
                                        { '$dateFromString': { 'dateString': '$start_date' } }, 
                                        datetime.datetime.strptime(specified_date, "%Y-%m-%d")
                                    ]
                                }, 
                                {
                                    '$gte': [
                                        { '$dateFromString': { 'dateString': '$end_date' } }, 
                                        datetime.datetime.strptime(specified_date, "%Y-%m-%d")
                                    ]
                                }
                            ]
                        }
                    }
                }, 
                mine_filter,  # Add mine filter
                consumer_type_filter,  # Add consumer_type filter
                search_filter,  # Add search_text filter if any
                {
                    '$group': {
                        '_id': '$do_no',  # Grouping by do_no
                        'mine_name': { '$first': '$mine_name' },  # Getting the first mine_name in the group
                        'do_qty': { '$sum': { '$toDouble': '$do_qty' } },  # Summing up the do_qty as double
                        'start_date': { '$first': '$start_date' },  # Getting the first start_date
                        'end_date': { '$first': '$end_date' },  # Getting the first end_date
                        'source_type': { '$first': '$consumer_type' },  # Getting the first source_type
                        'slno': { '$first': '$slno' },  # Getting the first slno
                        'Grade': {'$first': '$grade'}
                    }
                }, 
                {
                    '$project': {
                        '_id': 1, 
                        'mine_name': 1, 
                        'do_qty': 1, 
                        'start_date': 1, 
                        'end_date': 1, 
                        'source_type': 1, 
                        'slno': 1,
                        'Grade': 1,
                    }
                }
            ]

            saprecordsRcrRoadPipeline = [stage for stage in saprecordsRcrRoadPipeline if stage]

            fetchGmrData = Gmrdata.objects.aggregate(basePipeline)
            fetchGmrDatachallanltqty = Gmrdata.objects.aggregate(challanlrqtybasePipeline)
            fetchGmrHistoricData = gmrdataHistoric.objects.aggregate(basePipelineHistoric)
            fetchGmrHistoricDataChallanLrQty = gmrdataHistoric.objects.aggregate(basepipelineHistoricChallanLrQty)

            fetchRcrRoadData = RcrRoadData.objects.aggregate(basePipelineRcrRoad)
            fetchRcrRoadchallanlrqty = RcrRoadData.objects.aggregate(basepipelineRcrRoadChallanLrQty)

            fetchSapRecordsData = SapRecords.objects.aggregate(saprecordsPipeline)

            fetchSapRecordsRcrData = SapRecordsRcrRoad.objects.aggregate(saprecordsRcrRoadPipeline)

            listData= []

            for singleData in fetchGmrData:
                dictData = {}
                dictData["DO_No"] = singleData.get("_id")
                dictData["mine_name"] = singleData.get("mine_name")
                if singleData.get("do_qty"):
                    dictData["DO_Qty"] = int(singleData.get("do_qty"))
                else:
                    dictData["DO_Qty"] = 0
                # dictData["challan_lr_/_qty"] = round(singleData.get("challan_lr_qty"), 2)
                dictData["cumulative_challan_lr_/_qty"] = round(singleData.get("cumulative_challan_lr_qty"), 2)
                # dictData["grade"] = singleData.get("Grade")
                dictData["grn_status"] = singleData.get("grn_status")
                dictData["date"] = singleData.get("date").strftime("%Y-%m-%d")
                if singleData.get("start_date"):
                    dictData["start_date"] = singleData.get("start_date")
                else:
                    dictData["start_date"] = "N/A"
                if singleData.get("end_date"):
                    dictData["end_date"] = singleData.get("end_date")
                else:
                    dictData["end_date"] = "N/A"
                dictData["source_type"] = singleData.get("type_consumer")
                # dictData["month"] = singleData.get("slno")
                dictData["slno"] = datetime.datetime.strptime(singleData.get("slno"), "%Y%m").strftime("%B %Y") if singleData.get("slno") else "-"
                if singleData.get("Grade") is not None:
                    if '-' in singleData.get("Grade"):
                        dictData["average_GCV_Grade"] = singleData.get("Grade").split("-")[0]
                    elif " " in singleData.get("Grade"):
                        dictData["average_GCV_Grade"] = singleData.get("Grade").split(" ")[0]
                    else:
                        dictData["average_GCV_Grade"] = singleData.get("Grade")
                else:
                    dictData["average_GCV_Grade"] = "N/A"
                if singleData.get("start_date") is not None and singleData.get("end_date") is not None:
                    tomorrow_date = datetime.datetime.strptime(singleData.get("end_date"), "%Y-%m-%d").date() + datetime.timedelta(days=1)
                    balance_days = tomorrow_date - datetime.datetime.strptime(specified_date, "%Y-%m-%d").date()
                    dictData["balance_days"] = balance_days.days
                else:
                    dictData["balance_days"] = 0
                    
                if singleData.get("do_qty") is not None:
                    single_do_qty = singleData.get("do_qty")
                else:
                    single_do_qty = 0

                if single_do_qty != 0:
                    dictData['percent_supply'] = round((singleData.get('cumulative_challan_lr_qty') / int(single_do_qty)) * 100, 2)
                else:
                    dictData['percent_supply'] = 0
                
                dictData["balance_qty"] = round(int(single_do_qty) - singleData.get("cumulative_challan_lr_qty"), 2)
                    
                if dictData['balance_days'] and dictData['balance_qty'] != 0:
                    dictData['asking_rate'] = round(dictData['balance_qty'] / dictData['balance_days'], 2)
                else:
                    dictData["asking_rate"] = 0
                listData.append(dictData)
            
            # console_logger.debug(listData)
            
            for singleDataHistoric in fetchGmrHistoricData:
                dictDataHIstoric = {}
                dictDataHIstoric["DO_No"] = singleDataHistoric.get("_id")
                dictDataHIstoric["mine_name"] = singleDataHistoric.get("mine_name")
                if singleDataHistoric.get("do_qty"):
                    dictDataHIstoric["DO_Qty"] = float(singleDataHistoric.get("do_qty"))
                else:
                    dictDataHIstoric["DO_Qty"] = 0
                # dictDataHIstoric["DO_Qty"] = int(singleDataHistoric.get("do_qty"))
                # dictDataHIstoric["challan_lr_/_qty"] = round(singleDataHistoric.get("challan_lr_qty"), 2)
                dictDataHIstoric["cumulative_challan_lr_/_qty"] = round(singleDataHistoric.get("cumulative_challan_lr_qty"), 2)
                # dictDataHIstoric["grade"] = singleDataHistoric.get("Grade")
                dictDataHIstoric["grn_status"] = singleDataHistoric.get("grn_status")
                dictDataHIstoric["date"] = singleDataHistoric.get("date").strftime("%Y-%m-%d")
                if singleDataHistoric.get("start_date"):
                    dictDataHIstoric["start_date"] = singleDataHistoric.get("start_date")
                else:
                    dictDataHIstoric["start_date"] = "N/A"
                if singleDataHistoric.get("end_date"):
                    dictDataHIstoric["end_date"] = singleDataHistoric.get("end_date")
                else:
                    dictDataHIstoric["end_date"] = "N/A"
                dictDataHIstoric["source_type"] = singleDataHistoric.get("type_consumer")
                # dictDataHIstoric["month"] = singleDataHistoric.get("slno")
                dictDataHIstoric["slno"] = datetime.datetime.strptime(singleDataHistoric.get("slno"), "%Y%m").strftime("%B %Y") if singleDataHistoric.get("slno") else "-"
                if singleDataHistoric.get("Grade") is not None:
                    if '-' in singleDataHistoric.get("Grade"):
                        dictDataHIstoric["average_GCV_Grade"] = singleDataHistoric.get("Grade").split("-")[0]
                    elif " " in singleDataHistoric.get("Grade"):
                        dictDataHIstoric["average_GCV_Grade"] = singleDataHistoric.get("Grade").split(" ")[0]
                    else:
                        dictDataHIstoric["average_GCV_Grade"] = singleDataHistoric.get("Grade")
                else:
                    dictDataHIstoric["average_GCV_Grade"] = "N/A"
                if singleDataHistoric.get("start_date") is not None and singleDataHistoric.get("end_date") is not None:
                    tomorrow_date = datetime.datetime.strptime(singleDataHistoric.get("end_date"), "%Y-%m-%d").date() + datetime.timedelta(days=1)
                    balance_days = tomorrow_date - datetime.datetime.strptime(specified_date, "%Y-%m-%d").date()
                    dictDataHIstoric["balance_days"] = balance_days.days
                else:
                    dictDataHIstoric["balance_days"] = 0

                if singleDataHistoric.get("do_qty") is not None:
                    single_do_qty = singleDataHistoric.get("do_qty")
                else:
                    single_do_qty = 0

                if single_do_qty != 0:
                    dictDataHIstoric['percent_supply'] = round((singleDataHistoric.get('cumulative_challan_lr_qty') / float(single_do_qty)) * 100, 2)
                else:
                    dictDataHIstoric['percent_supply'] = 0
                    
                dictDataHIstoric["balance_qty"] = round(float(single_do_qty) - singleDataHistoric.get("cumulative_challan_lr_qty"), 2)
                    
                if dictDataHIstoric['balance_days'] and dictDataHIstoric['balance_qty'] != 0:
                    dictDataHIstoric['asking_rate'] = round(dictDataHIstoric['balance_qty'] / dictDataHIstoric['balance_days'], 2)
                else:
                    dictDataHIstoric["asking_rate"] = 0
                
                # console_logger.debug(dictDataHIstoric)
                
                do_no_exists = any(item['DO_No'] == dictDataHIstoric["DO_No"] for item in listData)
                
                # if do_no_exists:
                #     print("DO_No exists in final_data.")
                # else:
                #     print("DO_No does not exist in final_data.")
                #     listData.append(dictDataHIstoric)

                if not do_no_exists:
                    print("DO_No does not exist in final_data.")
                    listData.append(dictDataHIstoric)

            for singleDataRcrData in fetchRcrRoadData:
                dictDataRcr = {}
                dictDataRcr["DO_No"] = singleDataRcrData.get("_id")
                dictDataRcr["mine_name"] = singleDataRcrData.get("mine_name")
                if singleDataRcrData.get("do_qty"):
                    dictDataRcr["DO_Qty"] = int(singleDataRcrData.get("do_qty"))
                else:
                    dictDataRcr["DO_Qty"] = 0
                dictDataRcr["cumulative_challan_lr_/_qty"] = round(singleDataRcrData.get("cumulative_challan_lr_qty"), 2)
                dictDataRcr["grn_status"] = singleDataRcrData.get("grn_status")
                dictDataRcr["date"] = singleDataRcrData.get("date").strftime("%Y-%m-%d")
                dictDataRcr["start_date"] = singleDataRcrData.get("start_date")
                dictDataRcr["end_date"] = singleDataRcrData.get("end_date")
                dictDataRcr["source_type"] = singleDataRcrData.get("type_consumer")
                dictDataRcr["slno"] = datetime.datetime.strptime(singleDataRcrData.get("slno"), "%Y%m").strftime("%B %Y") if singleDataRcrData.get("slno") else "-"
                if singleDataRcrData.get("Grade") is not None:
                    if '-' in singleDataRcrData.get("Grade"):
                        dictDataRcr["average_GCV_Grade"] = singleDataRcrData.get("Grade").split("-")[0]
                    elif " " in singleDataRcrData.get("Grade"):
                        dictDataRcr["average_GCV_Grade"] = singleDataRcrData.get("Grade").split(" ")[0]
                    else:
                        dictDataRcr["average_GCV_Grade"] = singleDataRcrData.get("Grade")
                if singleDataRcrData.get("start_date") is not None and singleDataRcrData.get("end_date") is not None:
                    tomorrow_date = datetime.datetime.strptime(singleDataRcrData.get("end_date"), "%Y-%m-%d").date() + datetime.timedelta(days=1)
                    balance_days = tomorrow_date - datetime.datetime.strptime(specified_date, "%Y-%m-%d").date()
                    dictDataRcr["balance_days"] = balance_days.days
                else:
                    dictDataRcr["balance_days"] = 0
                if singleDataRcrData.get("do_qty") is not None:
                    single_do_qty = singleDataRcrData.get("do_qty")
                else:
                    single_do_qty = 0
                if single_do_qty != 0:
                    dictDataRcr['percent_supply'] = round((singleDataRcrData.get('cumulative_challan_lr_qty') / int(single_do_qty)) * 100, 2)
                else:
                    dictDataRcr['percent_supply'] = 0
                    
                dictDataRcr["balance_qty"] = round(int(single_do_qty) - singleDataRcrData.get("cumulative_challan_lr_qty"), 2)
                    
                if dictDataRcr['balance_days'] and dictDataRcr['balance_qty'] != 0:
                    dictDataRcr['asking_rate'] = round(dictDataRcr['balance_qty'] / dictDataRcr['balance_days'], 2)
                else:
                    dictDataRcr["asking_rate"] = 0
                # listData.append(dictDataRcr)

                do_no_existsRcr = any(item['DO_No'] == dictDataRcr["DO_No"] for item in listData)
                
                # if do_no_exists:
                #     print("DO_No exists in final_data.")
                # else:
                #     print("DO_No does not exist in final_data.")
                #     listData.append(dictDataHIstoric)

                if not do_no_existsRcr:
                    print("DO_No does not exist in final_data.")
                    listData.append(dictDataRcr)
            
            for saprecordsSingle in fetchSapRecordsData:
                sapdict = {}
                sapdict["DO_No"] = saprecordsSingle.get("_id")
                sapdict["mine_name"] = saprecordsSingle.get("mine_name")
                sapdict["DO_Qty"] = int(saprecordsSingle.get("do_qty"))
                sapdict["start_date"] = saprecordsSingle.get("start_date")
                sapdict["end_date"] = saprecordsSingle.get("end_date")
                sapdict["source_type"] = saprecordsSingle.get("source_type")
                sapdict["challan_lr_/_qty"] = 0
                sapdict["cumulative_challan_lr_/_qty"] = 0
                sapdict["date"] = "N/A"
                sapdict["slno"] = datetime.datetime.strptime(saprecordsSingle.get("slno"), "%Y%m").strftime("%B %Y") if saprecordsSingle.get("slno") else "-"
                if saprecordsSingle.get("Grade") is not None:
                    if '-' in saprecordsSingle.get("Grade"):
                        sapdict["average_GCV_Grade"] = saprecordsSingle.get("Grade").split("-")[0]
                    elif " " in saprecordsSingle.get("Grade"):
                        sapdict["average_GCV_Grade"] = saprecordsSingle.get("Grade").split(" ")[0]
                    else:
                        sapdict["average_GCV_Grade"] = saprecordsSingle.get("Grade")
                if saprecordsSingle.get("start_date") is not None and saprecordsSingle.get("end_date") is not None:
                    tomorrow_date = datetime.datetime.strptime(saprecordsSingle.get("end_date"), "%Y-%m-%d").date() + datetime.timedelta(days=1)
                    balance_days = tomorrow_date - datetime.datetime.strptime(specified_date, "%Y-%m-%d").date()
                    sapdict["balance_days"] = balance_days.days
                else:
                    sapdict["balance_days"] = 0
                if saprecordsSingle.get("do_qty") is not None and sapdict["cumulative_challan_lr_/_qty"] != 0:
                    sapdict['percent_supply'] = round((sapdict["cumulative_challan_lr_/_qty"] / int(saprecordsSingle.get('do_qty'))) * 100, 2)
                else:
                    sapdict["percent_supply"] = 0
                if saprecordsSingle.get("do_qty") is not None and sapdict["cumulative_challan_lr_/_qty"] != 0:
                    sapdict["balance_qty"] = round(int(saprecordsSingle.get("do_qty")) - sapdict["cumulative_challan_lr_/_qty"], 2)
                else:
                    sapdict["balance_qty"] = 0
                if sapdict['balance_days'] and sapdict['balance_qty'] != 0:
                    sapdict['asking_rate'] = round(sapdict['balance_qty'] / sapdict['balance_days'], 2)
                else:
                    sapdict["asking_rate"] = 0

                sap_do_no_exists = any(item['DO_No'] == sapdict["DO_No"] for item in listData)
                
                # if do_no_exists:
                #     print("DO_No exists in final_data.")
                # else:
                #     print("DO_No does not exist in final_data.")
                #     listData.append(dictDataHIstoric)

                if not sap_do_no_exists:
                    print("DO_No does not exist in final_data.")
                    listData.append(sapdict)


            for saprecordsRcrSingle in fetchSapRecordsRcrData:
                sapdictRcr = {}
                sapdictRcr["DO_No"] = saprecordsRcrSingle.get("_id")
                sapdictRcr["mine_name"] = saprecordsRcrSingle.get("mine_name")
                sapdictRcr["DO_Qty"] = int(saprecordsRcrSingle.get("do_qty"))
                sapdictRcr["start_date"] = saprecordsRcrSingle.get("start_date")
                sapdictRcr["end_date"] = saprecordsRcrSingle.get("end_date")
                sapdictRcr["source_type"] = saprecordsRcrSingle.get("source_type")
                sapdictRcr["challan_lr_/_qty"] = 0
                sapdictRcr["date"] = "N/A"
                sapdictRcr["cumulative_challan_lr_/_qty"] = 0
                sapdictRcr["slno"] = datetime.datetime.strptime(saprecordsRcrSingle.get("slno"), "%Y%m").strftime("%B %Y") if saprecordsRcrSingle.get("slno") else "-"
                if saprecordsRcrSingle.get("Grade") is not None:
                    if '-' in saprecordsRcrSingle.get("Grade"):
                        sapdictRcr["average_GCV_Grade"] = saprecordsRcrSingle.get("Grade").split("-")[0]
                    elif " " in saprecordsRcrSingle.get("Grade"):
                        sapdictRcr["average_GCV_Grade"] = saprecordsRcrSingle.get("Grade").split(" ")[0]
                    else:
                        sapdictRcr["average_GCV_Grade"] = saprecordsRcrSingle.get("Grade")
                if saprecordsRcrSingle.get("start_date") is not None and saprecordsRcrSingle.get("end_date") is not None:
                    tomorrow_date = datetime.datetime.strptime(saprecordsRcrSingle.get("end_date"), "%Y-%m-%d").date() + datetime.timedelta(days=1)
                    balance_days = tomorrow_date - datetime.datetime.strptime(specified_date, "%Y-%m-%d").date()
                    sapdictRcr["balance_days"] = balance_days.days
                else:
                    sapdictRcr["balance_days"] = 0
                if saprecordsRcrSingle.get("do_qty") is not None:
                    do_qty_val = saprecordsRcrSingle.get("do_qty")
                else:
                    do_qty_val = 0
                if do_qty_val != 0:
                    sapdictRcr['percent_supply'] = round((sapdictRcr["cumulative_challan_lr_/_qty"] / int(do_qty_val)) * 100, 2)
                else:
                    sapdictRcr['percent_supply'] = 0
                sapdictRcr["balance_qty"] = round(int(do_qty_val) - sapdictRcr["cumulative_challan_lr_/_qty"], 2)
                if sapdictRcr['balance_days'] and sapdictRcr['balance_qty'] != 0:
                    sapdictRcr['asking_rate'] = round(sapdictRcr['balance_qty'] / sapdictRcr['balance_days'], 2)
                else:
                    sapdictRcr["asking_rate"] = 0

                sap_do_no_rcr_exists = any(item['DO_No'] == sapdictRcr.get("DO_No") for item in listData)
                
                if not sap_do_no_rcr_exists:
                    console_logger.debug("DO_No does not exist in final_data for sap_records")
                    listData.append(sapdictRcr)

            
            for singlelrqtyData in fetchGmrDatachallanltqty:
                dictDatalrQty = {}
                dictDatalrQty["DO_No"] = singlelrqtyData.get("_id")
                dictDatalrQty["mine_name"] = singlelrqtyData.get("mine_name")
                if singlelrqtyData.get("do_qty"):
                    dictDatalrQty["DO_Qty"] = int(float(singlelrqtyData.get("do_qty")))
                else:
                    dictDatalrQty["DO_Qty"] = 0
                dictDatalrQty["challan_lr_/_qty"] = round(singlelrqtyData.get("challan_lr_qty"), 2)
                
                # Check if there is an item with the same DO_No in listData
                do_no_exists_data = next((item for item in listData if item["DO_No"] == dictDatalrQty["DO_No"]), None)
                
                # If it exists, update the "challan_lr_qty" in listData
                if do_no_exists_data:
                    do_no_exists_data["challan_lr_/_qty"] = dictDatalrQty["challan_lr_/_qty"]

            for singlehistoriclrqty in fetchGmrHistoricDataChallanLrQty:
                dictDatahistoriclrQty = {}
                dictDatahistoriclrQty["DO_No"] = singlehistoriclrqty.get("_id")
                dictDatahistoriclrQty["mine_name"] = singlehistoriclrqty.get("mine_name")
                if singlehistoriclrqty.get("do_qty"):
                    dictDatahistoriclrQty["DO_Qty"] = int(float(singlehistoriclrqty.get("do_qty")))
                else:
                    dictDatahistoriclrQty["DO_Qty"] = 0
                dictDatahistoriclrQty["challan_lr_/_qty"] = round(singlehistoriclrqty.get("challan_lr_qty"), 2)
                
                # Check if there is an item with the same DO_No in listData
                do_no_exists_historic = next((item for item in listData if item["DO_No"] == dictDatahistoriclrQty["DO_No"]), None)
                
                # If it exists, update the "challan_lr_qty" in listData
                if do_no_exists_historic:
                    do_no_exists_historic["challan_lr_/_qty"] = dictDatahistoriclrQty["challan_lr_/_qty"]


            
            for singlercrroadlrqty in fetchRcrRoadchallanlrqty:
                dictDataRcrRoadlrQty = {}
                dictDataRcrRoadlrQty["DO_No"] = singlercrroadlrqty.get("_id")
                dictDataRcrRoadlrQty["mine_name"] = singlercrroadlrqty.get("mine_name")
                if singlercrroadlrqty.get("do_qty"):
                    dictDataRcrRoadlrQty["DO_Qty"] = int(float(singlercrroadlrqty.get("do_qty")))
                else:
                    dictDataRcrRoadlrQty["DO_Qty"] = 0
                dictDataRcrRoadlrQty["challan_lr_/_qty"] = round(singlercrroadlrqty.get("challan_lr_qty"), 2)
                
                # Check if there is an item with the same DO_No in listData
                do_no_exists_rcr_road = next((item for item in listData if item["DO_No"] == dictDataRcrRoadlrQty["DO_No"]), None)
                
                # If it exists, update the "challan_lr_qty" in listData
                if do_no_exists_rcr_road:
                    do_no_exists_historic["challan_lr_/_qty"] = dictDataRcrRoadlrQty["challan_lr_/_qty"]

            logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
            if listData:
                path = os.path.join(
                    "static_server",
                    "gmr_ai",
                    file,
                    "Coal_Logistics_Report_{}.xlsx".format(
                        datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                    ),
                )

                filename = os.path.join(os.getcwd(), path)
                workbook = xlsxwriter.Workbook(filename)
                workbook.use_zip64()
                cell_format2 = workbook.add_format()
                cell_format2.set_bold()
                cell_format2.set_font_size(10)
                cell_format2.set_align("center")
                cell_format2.set_align("vcenter")

                cell_format2.set_text_wrap(True)
                cell_format2.set_border(1)

                header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
                date_format = workbook.add_format({'align': 'center', 'font_size': 12, "bold": True})
                report_name_format = workbook.add_format({'align': 'center', 'font_size': 15, "bold": True})
                # report_name_format.set_text_wrap(True)

                header_format.set_align("vcenter")
                date_format.set_align("vcenter")
                report_name_format.set_align("vcenter")
                header_format.set_border(1)
                date_format.set_border(1)
                report_name_format.set_border(1)

                worksheet = workbook.add_worksheet()
                worksheet.set_column("A:AZ", 20)
                worksheet.set_default_row(50)
                cell_format = workbook.add_format()
                cell_format.set_font_size(10)
                cell_format.set_align("center")
                cell_format.set_align("vcenter")
                cell_format.set_text_wrap(True)
                cell_format.set_border(1)

                worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                    
                # Merge cells for the main header and place it in the center
                main_header = "GMR Warora Energy Limited"  # Set your main header text here
                worksheet.merge_range("A1:M1", main_header, header_format)  # Merge cells A1 to H1 for the header
                
                # Write the current date on the left side (A2)
                # worksheet.write("A2", f"Date: {datetime.datetime.now().strftime('%d-%m-%Y')}", date_format)
                worksheet.merge_range("A2:B2", f"Date: {datetime.datetime.now().strftime('%d-%m-%Y')}", date_format)
                worksheet.merge_range("C2:M2", f"Road Coal Logistics Report", report_name_format)

                result["datasets"] = listData

                headers = ["Month", "Mine Name", "DO No", "Grade", "DO Qty", "Challan Lr / Qty", "Cumulative Challan Lr / Qty", "Balance Qty", "% of Supply", "Balance Days", "Asking Rate", "Do Start Date", "Do End Date"]
                
                for index, header in enumerate(headers):
                    worksheet.write(2, index, header, cell_format2)
                
                row = 3
                for single_data in result["datasets"]:
                    # worksheet.write(row, 0, count, cell_format)
                    worksheet.write(row, 0, single_data["slno"], cell_format)
                    worksheet.write(row, 1, single_data["mine_name"], cell_format)
                    worksheet.write(row, 2, single_data["DO_No"], cell_format)
                    worksheet.write(row, 3, single_data["average_GCV_Grade"], cell_format)
                    worksheet.write(row, 4, single_data["DO_Qty"], cell_format)
                    if single_data.get("challan_lr_/_qty"):
                        worksheet.write(row, 5, single_data["challan_lr_/_qty"], cell_format)
                    else:
                        worksheet.write(row, 5, 0, cell_format)
                    worksheet.write(row, 6, single_data["cumulative_challan_lr_/_qty"], cell_format)
                    worksheet.write(row, 7, single_data["balance_qty"], cell_format)
                    worksheet.write(row, 8, single_data["percent_supply"], cell_format)
                    worksheet.write(row, 9, single_data["balance_days"], cell_format)
                    worksheet.write(row, 10, single_data["asking_rate"], cell_format)
                    worksheet.write(row, 11, single_data["start_date"], cell_format)
                    worksheet.write(row, 12, single_data["end_date"], cell_format)

                    # count -= 1
                    row += 1
                workbook.close()

                return {
                        "Type": "daily_coal_report",
                        "Datatype": "Report",
                        "File_Path": path,
                    }
            else:
                console_logger.error("No data found")
                return {
                            "Type": "daily_coal_report",
                            "Datatype": "Report",
                            "File_Path": path,
                        }

    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug(
            "Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno)
        )
        return e

# @router.get("/coal_logistics_report", tags=["Road Map"])
# def coal_logistics_report_test(
#     response: Response,
#     specified_date: str,
#     search_text: Optional[str] = None,
#     currentPage: Optional[int] = None,
#     perPage: Optional[int] = None,
#     mine: Optional[str] = "All",
#     consumer_type: Optional[str] = "All",
#     type: Optional[str] = "display"
# ):
#     try:
#         result = {"labels": [], "datasets": [], "total": 0, "page_size": 15}
#         if type and type == "display":

#             data = Q()
#             sap_data = Q()

#             if mine and mine != "All":
#                 # data["mine__icontains"] = mine.upper()
#                 data &= Q(mine__icontains=mine.upper())
#                 sap_data &= Q(mine_name__icontains=mine.upper())

#             if consumer_type and consumer_type != "All":
#                 # data["type_consumer__icontains"] = consumer_type
#                 data &= Q(type_consumer__icontains=consumer_type)
#                 # sap_data &= Q(consumer_type__icontains=consumer_type)
#                 sap_data &= Q(consumer_type__iexact=consumer_type)

#             page_no = 1
#             page_len = result["page_size"]

#             if currentPage:
#                 page_no = currentPage

#             if perPage:
#                 page_len = perPage
#                 result["page_size"] = perPage

#             if specified_date:
#                 specified_change_date = convert_to_utc_format(specified_date, "%Y-%m-%d")
#                 to_ts = convert_to_utc_format(f'{specified_date} 23:59:59', "%Y-%m-%d %H:%M:%S")

#             if search_text:
#                 if search_text.isdigit():
#                     data &= Q(arv_cum_do_number__icontains=search_text)
#                     # sap_data &= Q(do_no__icontains=search_text)
#                 else:
#                     data &= Q(mine__icontains=search_text)
#                     # sap_data &= Q(mine_name__icontains=search_text)
#                 logs = (
#                     Gmrdata.objects(GWEL_Tare_Time__lte=to_ts, actual_tare_qty__ne=None, gate_approved=True, GWEL_Tare_Time__ne=None)
#                     .filter(data)
#                     .order_by("-GWEL_Tare_Time")
#                 )
#                 # console_logger.debug(sap_data)
#                 # sap_records = SapRecords.objects.filter(sap_data)
#                 if not logs:  # If no data found in gmrData, search in sapRecords
#                     if search_text.isdigit():
#                         sap_data &= Q(do_no__icontains=search_text)
#                     else:
#                         sap_data &= Q(mine_name__icontains=search_text)
#                     sap_records = SapRecords.objects.filter(sap_data) 
#                 else:
#                     sap_records = []
#             else:
#                 logs = (
#                     Gmrdata.objects(GWEL_Tare_Time__lte=to_ts, actual_tare_qty__ne=None, gate_approved=True, GWEL_Tare_Time__ne=None)
#                     .filter(data)
#                     .order_by("-GWEL_Tare_Time")
#                 )
#                 sap_records = SapRecords.objects.filter(sap_data)

#             # sap_records = SapRecords.objects.all()

#             if any(logs) or any(sap_records):
#                 aggregated_data = defaultdict(
#                     lambda: defaultdict(
#                         lambda: {
#                             "DO_Qty": 0,
#                             "challan_lr_qty": 0,
#                             "challan_lr_qty_full": 0,
#                             "mine_name": "",
#                             "balance_qty": 0,
#                             "percent_of_supply": 0,
#                             "actual_net_qty": 0,
#                             "Gross_Calorific_Value_(Adb)": 0,
#                             "count": 0,
#                             "coal_count": 0,
#                             "start_date": "",
#                             "end_date": "",
#                             "source_type": "",
#                         }
#                     )
#                 )

#                 start_dates = {}
#                 grade = 0
#                 for log in logs:
#                     if log.GWEL_Tare_Time!=None:
#                         month = log.GWEL_Tare_Time.strftime("%Y-%m")
#                         date = log.GWEL_Tare_Time.strftime("%Y-%m-%d")
#                         payload = log.payload()
#                         result["labels"] = list(payload.keys())
#                         mine_name = payload.get("Mines_Name")
#                         do_no = payload.get("DO_No")
#                         # console_logger.debug(payload.get("Grade"))
#                         if payload.get("Grade") is not None:
#                             if '-' in payload.get("Grade"):
#                                 grade = payload.get("Grade").split("-")[0]
#                             elif " " in payload.get("Grade"):
#                                 grade = payload.get("Grade").split(" ")[0]
#                             else:
#                                 grade = payload.get("Grade")
#                         # If start_date is None or the current vehicle_in_time is earlier than start_date, update start_date
#                         # if do_no not in start_dates:
#                         #     start_dates[do_no] = date
#                         # elif date < start_dates[do_no]:
#                         #     start_dates[do_no] = date
#                         if payload.get("slno"):
#                             aggregated_data[date][do_no]["slno"] = datetime.datetime.strptime(payload.get("slno"), '%Y%m').strftime('%B %Y')
#                         else:
#                             aggregated_data[date][do_no]["slno"] = "-"
#                         if payload.get("start_date"):
#                             aggregated_data[date][do_no]["start_date"] = payload.get("start_date")
#                         else:
#                             aggregated_data[date][do_no]["start_date"] = "0"
#                         if payload.get("end_date"):
#                             aggregated_data[date][do_no]["end_date"] = payload.get("end_date")
#                         else:
#                             aggregated_data[date][do_no]["end_date"] = "0"

#                         if payload.get("Type_of_consumer"):
#                             aggregated_data[date][do_no]["source_type"] = payload.get("Type_of_consumer")
                        
#                         if payload.get("DO_Qty"):
#                             aggregated_data[date][do_no]["DO_Qty"] = float(
#                                 payload["DO_Qty"]
#                             )
#                         else:
#                             aggregated_data[date][do_no]["DO_Qty"] = 0
                        
#                         challan_net_wt = payload.get("Challan_Net_Wt(MT)")    
                
#                         if challan_net_wt:
#                             aggregated_data[date][do_no]["challan_lr_qty"] += float(challan_net_wt)

#                         # if payload.get("Challan_Net_Wt(MT)"):
#                         #     aggregated_data[date][do_no]["challan_lr_qty"] += float(
#                         #         payload.get("Challan_Net_Wt(MT)")
#                         #     )
#                         # else:
#                         #     aggregated_data[date][do_no]["challan_lr_qty"] = 0
#                         if payload.get("Mines_Name"):
#                             aggregated_data[date][do_no]["mine_name"] = payload[
#                                 "Mines_Name"
#                             ]
#                         else:
#                             aggregated_data[date][do_no]["mine_name"] = "-"
#                         aggregated_data[date][do_no]["count"] += 1 

#                 for record in sap_records:
#                     do_no = record.do_no
#                     if do_no not in aggregated_data[specified_date]:
#                         aggregated_data[specified_date][do_no]["DO_Qty"] = float(record.do_qty) if record.do_qty else 0
#                         aggregated_data[specified_date][do_no]["mine_name"] = record.mine_name if record.mine_name else "-"
#                         aggregated_data[specified_date][do_no]["start_date"] = record.start_date if record.start_date else "0"
#                         aggregated_data[specified_date][do_no]["end_date"] = record.end_date if record.end_date else "0"
#                         aggregated_data[specified_date][do_no]["source_type"] = record.consumer_type if record.consumer_type else "Unknown"
#                         try:
#                             aggregated_data[specified_date][do_no]["slno"] = datetime.datetime.strptime(record.slno, "%Y%m").strftime("%B %Y") if record.slno else "-"
#                         except ValueError as e:
#                             aggregated_data[specified_date][do_no]["slno"] = record.slno if record.slno else "-"
#                         aggregated_data[specified_date][do_no]["count"] = 1
                
#                 dataList = [
#                     {
#                         "date": date,
#                         "data": {
#                             do_no: {
#                                 "DO_Qty": data["DO_Qty"],
#                                 "challan_lr_qty": data["challan_lr_qty"],
#                                 "mine_name": data["mine_name"],
#                                 "grade": grade,
#                                 "date": date,
#                                 "start_date": data["start_date"],
#                                 "end_date": data["end_date"],
#                                 "source_type": data["source_type"],
#                                 "slno": data["slno"],
#                             }
#                             for do_no, data in aggregated_data[date].items()
#                         },
#                     }
#                     for date in aggregated_data
#                 ]
                
#                 final_data = []
#                 for entry in dataList:
#                     date = entry["date"]
#                     for data_dom, values in entry['data'].items():
#                         dictData = {}
#                         dictData["DO_No"] = data_dom
#                         dictData["mine_name"] = values["mine_name"]
#                         dictData["DO_Qty"] = values["DO_Qty"]
#                         dictData["club_challan_lr_qty"] = values["challan_lr_qty"]
#                         dictData['challan_lr_/_qty'] = 0
#                         dictData["date"] = values["date"]
#                         dictData["start_date"] = values["start_date"]
#                         dictData["end_date"] = values["end_date"]
#                         dictData["source_type"] = values["source_type"]
#                         dictData["month"] = values["slno"]
#                         dictData["cumulative_challan_lr_qty"] = 0
#                         dictData["balance_qty"] = 0
#                         dictData["percent_supply"] = 0
#                         dictData["asking_rate"] = 0
#                         dictData['average_GCV_Grade'] = values["grade"]
                        
                        
#                         if dictData["start_date"] != "0" and dictData["end_date"] != "0":
#                             # today_date = datetime.datetime.today().date()
#                             # balance_days = datetime.datetime.strptime(dictData["end_date"], "%Y-%m-%d").date() - datetime.datetime.strptime(dictData["start_date"], "%Y-%m-%d").date()
#                             tomorrow_date = datetime.datetime.strptime(dictData["end_date"], "%Y-%m-%d").date() + datetime.timedelta(days=1)
#                             # balance_days = datetime.datetime.strptime(dictData["end_date"], "%Y-%m-%d").date() - datetime.datetime.today().date()
#                             balance_days = tomorrow_date - datetime.datetime.strptime(specified_date, "%Y-%m-%d").date()
#                             dictData["balance_days"] = balance_days.days
#                         else:
#                             dictData["balance_days"] = 0

#                         # if data_dom in start_dates:
#                         #     dictData["start_date"] = start_dates[data_dom]
#                         #     dictData["end_date"] = datetime.datetime.strptime(start_dates[data_dom], "%Y-%m-%d") + timedelta(days=44)
#                         #     balance_days = dictData["end_date"].date() - datetime.datetime.today().date()
#                         #     dictData["balance_days"] = balance_days.days
#                         # else:
#                         #     dictData["start_date"] = None
#                         #     dictData["end_date"] = None
#                         #     dictData["balance_days"] = None
                        
#                         final_data.append(dictData)

#                 if final_data:
#                     # filtered_data = [
#                     #     entry for entry in dataList if entry["date"] == specified_date
#                     # ]

#                     startdate = f'{specified_date} 00:00:00'
#                     enddate = f'{specified_date} 23:59:59'
#                     # to_ts = datetime.datetime.strptime(enddate,"%Y-%m-%d %H:%M:%S")
#                     from_ts = convert_to_utc_format(startdate, "%Y-%m-%d %H:%M:%S")
#                     to_ts = convert_to_utc_format(enddate, "%Y-%m-%d %H:%M:%S")
                    
#                     pipeline = [
#                         {
#                             "$match": {
#                                 "GWEL_Tare_Time": {"$gte": from_ts, "$lte": to_ts},
#                                     "net_qty": {"$ne": None}
#                                 }
#                         },
#                         {
#                         '$group': {
#                             '_id': {
#                                 'date': {
#                                     '$dateToString': {
#                                         'format': '%Y-%m-%d', 
#                                         'date': '$GWEL_Tare_Time'
#                                     }
#                                 }, 
#                                 'do_no': '$arv_cum_do_number'
#                             }, 
#                             'total_net_qty': {
#                                 '$sum': {
#                                     '$toDouble': '$net_qty'
#                                 }
#                             }
#                         }
#                     }]

#                     filtered_data_new = Gmrdata.objects.aggregate(pipeline)
#                     aggregated_totals = defaultdict(float)
#                     for single_data_entry in filtered_data_new:
#                         do_no = single_data_entry['_id']['do_no']
#                         total_net_qty = single_data_entry['total_net_qty']
#                         aggregated_totals[do_no] += total_net_qty

#                     data_by_do = {}
#                     finaldataMain = [single_data_list for single_data_list in final_data if single_data_list.get("balance_days") >= 0]
#                     for entry in finaldataMain:
#                         do_no = entry['DO_No']
                        
#                         try:
#                             club_challan_lr_qty = float(entry.get('club_challan_lr_qty', 0))
#                         except ValueError:
#                             club_challan_lr_qty = 0

#                         if do_no not in data_by_do:
#                             data_by_do[do_no] = entry.copy()  # Copy the entry to avoid modifying the original
#                             data_by_do[do_no]['cumulative_challan_lr_/_qty'] = round(club_challan_lr_qty, 2)
#                         else:
#                             data_by_do[do_no]['cumulative_challan_lr_/_qty'] = round(
#                                 data_by_do[do_no].get('cumulative_challan_lr_/_qty', 0) + club_challan_lr_qty, 2
#                             )
#                         if filtered_data_new:
#                             # data = filtered_data[0]["data"]
#                             # Update challan_lr_qty if the DO_No matches
#                             if do_no in aggregated_totals:
#                                 data_by_do[do_no]['challan_lr_/_qty'] = round(aggregated_totals[do_no], 2)
#                             else:
#                                 data_by_do[do_no]['challan_lr_/_qty'] = 0
                        
#                         # Update calculated fields
#                         if data_by_do[do_no]['DO_Qty'] != 0 and data_by_do[do_no]['cumulative_challan_lr_/_qty'] != 0:
#                             data_by_do[do_no]['percent_supply'] = round((data_by_do[do_no]['cumulative_challan_lr_/_qty'] / data_by_do[do_no]['DO_Qty']) * 100, 2)
#                         else:
#                             data_by_do[do_no]['percent_supply'] = 0

#                         # if data_by_do[do_no]['cumulative_challan_lr_qty'] != 0 and data_by_do[do_no]['DO_Qty'] != 0:
#                         data_by_do[do_no]['balance_qty'] = round(data_by_do[do_no]['DO_Qty'] - data_by_do[do_no]['cumulative_challan_lr_/_qty'], 2)
#                         # else:
#                         #     data_by_do[do_no]['balance_qty'] = 0
                        
#                         if data_by_do[do_no]['balance_days'] and data_by_do[do_no]['balance_qty'] != 0:
#                             data_by_do[do_no]['asking_rate'] = round(data_by_do[do_no]['balance_qty'] / data_by_do[do_no]['balance_days'], 2)

#                         del entry['club_challan_lr_qty']
                    
#                 sort_final_data = list(data_by_do.values())
#                 # Sort the data by 'balance_days', placing entries with 'balance_days' of 0 at the end
#                 final_data = sorted(sort_final_data, key=lambda x: (x['balance_days'] == 0, x['balance_days']))
#                 # console_logger.debug(final_data)

#                 if final_data:
#                     start_index = (page_no - 1) * page_len
#                     end_index = start_index + page_len
#                     paginated_data = final_data[start_index:end_index]

#                     # result["labels"] = list(final_data[0].keys())
#                     result["labels"] = ["month", "DO_No", "mine_name", "DO_Qty", "date", "challan_lr_/_qty", "cumulative_challan_lr_/_qty","balance_qty", "percent_supply", "asking_rate", "average_GCV_Grade", "start_date", "end_date", "balance_days"]
#                     result["datasets"] = paginated_data
#                     result["total"] = len(final_data)

#                 return result
#             else:
#                 return result
#         elif type and type == "download":
#             del type
#             file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
#             target_directory = f"static_server/gmr_ai/{file}"
#             os.umask(0)
#             os.makedirs(target_directory, exist_ok=True, mode=0o777)
#             if specified_date:
#                 specified_change_date = convert_to_utc_format(specified_date, "%Y-%m-%d")
#                 to_ts = convert_to_utc_format(f'{specified_date} 23:59:59', "%Y-%m-%d %H:%M:%S")
#             data = Q()
#             sap_data = Q()
#             if search_text:
#                 if search_text.isdigit():
#                     data &= Q(arv_cum_do_number__icontains=search_text)
#                     # sap_data &= Q(do_no__icontains=search_text)
#                 else:
#                     data &= Q(mine__icontains=search_text)
#                     # sap_data &= Q(mine_name__icontains=search_text)
#                 logs = (
#                     Gmrdata.objects(GWEL_Tare_Time__lte=to_ts, actual_tare_qty__ne=None, gate_approved=True, GWEL_Tare_Time__ne=None)
#                     .filter(data)
#                     .order_by("-GWEL_Tare_Time")
#                 )
#                 # console_logger.debug(sap_data)
#                 # sap_records = SapRecords.objects.filter(sap_data)
#                 if not logs:  # If no data found in gmrData, search in sapRecords
#                     if search_text.isdigit():
#                         sap_data &= Q(do_no__icontains=search_text)
#                     else:
#                         sap_data &= Q(mine_name__icontains=search_text)
#                     sap_records = SapRecords.objects.filter(sap_data) 
#                 else:
#                     sap_records = []
#             else:
#                 logs = (
#                     Gmrdata.objects(GWEL_Tare_Time__lte=to_ts, actual_tare_qty__ne=None, gate_approved=True, GWEL_Tare_Time__ne=None)
#                     .filter(data)
#                     .order_by("-GWEL_Tare_Time")
#                 )
#                 sap_records = SapRecords.objects.filter(sap_data)

#             count = len(logs)
#             path = None
#             if any(logs):
#                 aggregated_data = defaultdict(
#                     lambda: defaultdict(
#                         lambda: {
#                             "DO_Qty": 0,
#                             "challan_lr_qty": 0,
#                             "challan_lr_qty_full": 0,
#                             "mine_name": "",
#                             "balance_qty": 0,
#                             "percent_of_supply": 0,
#                             "actual_net_qty": 0,
#                             "Gross_Calorific_Value_(Adb)": 0,
#                             "count": 0,
#                             "coal_count": 0,
#                             "start_date": "",
#                             "end_date": "",
#                             "source_type": "",
#                         }
#                     )
#                 )

#                 start_dates = {}
#                 grade = 0
#                 for log in logs:
#                     if log.GWEL_Tare_Time!=None:
#                         month = log.GWEL_Tare_Time.strftime("%Y-%m")
#                         date = log.GWEL_Tare_Time.strftime("%Y-%m-%d")
#                         payload = log.payload()
#                         result["labels"] = list(payload.keys())
#                         mine_name = payload.get("Mines_Name")
#                         do_no = payload.get("DO_No")
#                         if payload.get("Grade") is not None:
#                             if '-' in payload.get("Grade"):
#                                 grade = payload.get("Grade").split("-")[0]
#                             else:
#                                 grade = payload.get("Grade")
#                         # If start_date is None or the current vehicle_in_time is earlier than start_date, update start_date
#                         # if do_no not in start_dates:
#                         #     start_dates[do_no] = date
#                         # elif date < start_dates[do_no]:
#                         #     start_dates[do_no] = date
                        
#                         if payload.get("slno"):
#                             aggregated_data[date][do_no]["slno"] = datetime.datetime.strptime(payload.get("slno"), '%Y%m').strftime('%B %Y')
#                         else:
#                             aggregated_data[date][do_no]["slno"] = "-"

#                         if payload.get("start_date"):
#                             aggregated_data[date][do_no]["start_date"] = payload.get("start_date")
#                         else:
#                             aggregated_data[date][do_no]["start_date"] = "0"
#                         if payload.get("end_date"):
#                             aggregated_data[date][do_no]["end_date"] = payload.get("end_date")
#                         else:
#                             aggregated_data[date][do_no]["end_date"] = "0"
#                         if payload.get("Type_of_consumer"):
#                             aggregated_data[date][do_no]["source_type"] = payload.get("Type_of_consumer")
#                         if payload.get("DO_Qty"):
#                             aggregated_data[date][do_no]["DO_Qty"] = float(
#                                 payload["DO_Qty"]
#                             )
#                         else:
#                             aggregated_data[date][do_no]["DO_Qty"] = 0

#                         challan_net_wt = payload.get("Challan_Net_Wt(MT)")    
                
#                         if challan_net_wt:
#                             aggregated_data[date][do_no]["challan_lr_qty"] += float(challan_net_wt)
#                         # if payload.get("Challan_Net_Wt(MT)"):
#                         #     aggregated_data[date][do_no]["challan_lr_qty"] += float(
#                         #         payload.get("Challan_Net_Wt(MT)")
#                         #     )
#                         # else:
#                         #     aggregated_data[date][do_no]["challan_lr_qty"] = 0
#                         if payload.get("Mines_Name"):
#                             aggregated_data[date][do_no]["mine_name"] = payload[
#                                 "Mines_Name"
#                             ]
#                         else:
#                             aggregated_data[date][do_no]["mine_name"] = "-"
#                         aggregated_data[date][do_no]["count"] += 1 
                
#                 for record in sap_records:
#                     do_no = record.do_no
#                     if do_no not in aggregated_data[specified_date]:
#                         aggregated_data[specified_date][do_no]["DO_Qty"] = float(record.do_qty) if record.do_qty else 0
#                         aggregated_data[specified_date][do_no]["mine_name"] = record.mine_name if record.mine_name else "-"
#                         aggregated_data[specified_date][do_no]["start_date"] = record.start_date if record.start_date else "0"
#                         aggregated_data[specified_date][do_no]["end_date"] = record.end_date if record.end_date else "0"
#                         aggregated_data[specified_date][do_no]["source_type"] = record.consumer_type if record.consumer_type else "Unknown"
#                         try:
#                             aggregated_data[specified_date][do_no]["slno"] = datetime.datetime.strptime(record.slno, "%Y%m").strftime("%B %Y") if record.slno else "-"
#                         except ValueError as e:
#                             aggregated_data[specified_date][do_no]["slno"] = record.slno if record.slno else "-"
#                         aggregated_data[specified_date][do_no]["count"] = 1

#                 dataList = [
#                     {
#                         "date": date,
#                         "data": {
#                             do_no: {
#                                 "DO_Qty": data["DO_Qty"],
#                                 "challan_lr_qty": data["challan_lr_qty"],
#                                 "mine_name": data["mine_name"],
#                                 "grade": grade,
#                                 "date": date,
#                                 "start_date": data["start_date"],
#                                 "end_date": data["end_date"],
#                                 "source_type": data["source_type"],
#                                 "slno": data["slno"],
#                             }
#                             for do_no, data in aggregated_data[date].items()
#                         },
#                     }
#                     for date in aggregated_data
#                 ]
                
#                 final_data = []
#                 for entry in dataList:
#                     date = entry["date"]
#                     for data_dom, values in entry['data'].items():
#                         dictData = {}
#                         dictData["DO_No"] = data_dom
#                         dictData["mine_name"] = values["mine_name"]
#                         dictData["DO_Qty"] = values["DO_Qty"]
#                         dictData["club_challan_lr_qty"] = values["challan_lr_qty"]
#                         dictData['challan_lr_qty'] = 0
#                         dictData["date"] = values["date"]
#                         dictData["start_date"] = values["start_date"]
#                         dictData["end_date"] = values["end_date"]
#                         dictData["source_type"] = values["source_type"]
#                         dictData["month"] = values["slno"]
#                         dictData["cumulative_challan_lr_qty"] = 0
#                         dictData["balance_qty"] = 0
#                         dictData["percent_supply"] = 0
#                         dictData["asking_rate"] = 0
#                         dictData['average_GCV_Grade'] = values["grade"]
                        
                        
#                         if dictData["start_date"] != "0" and dictData["end_date"] != "0":
#                             # balance_days = datetime.datetime.strptime(dictData["end_date"], "%Y-%m-%d").date() - datetime.datetime.strptime(dictData["start_date"], "%Y-%m-%d").date()
#                             balance_days = datetime.datetime.strptime(dictData["end_date"], "%Y-%m-%d").date() - datetime.datetime.today().date()
#                             dictData["balance_days"] = balance_days.days
#                         else:
#                             dictData["balance_days"] = 0

#                         # if data_dom in start_dates:
#                         #     dictData["start_date"] = start_dates[data_dom]
#                         #     dictData["end_date"] = datetime.datetime.strptime(start_dates[data_dom], "%Y-%m-%d") + timedelta(days=44)
#                         #     balance_days = dictData["end_date"].date() - datetime.datetime.today().date()
#                         #     dictData["balance_days"] = balance_days.days
#                         # else:
#                         #     dictData["start_date"] = None
#                         #     dictData["end_date"] = None
#                         #     dictData["balance_days"] = None
                        
#                         final_data.append(dictData)  
#                 if final_data:
#                     path = os.path.join(
#                         "static_server",
#                         "gmr_ai",
#                         file,
#                         "Coal_Logistics_Report_{}.xlsx".format(
#                             datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
#                         ),
#                     )

#                     filename = os.path.join(os.getcwd(), path)
#                     workbook = xlsxwriter.Workbook(filename)
#                     workbook.use_zip64()
#                     cell_format2 = workbook.add_format()
#                     cell_format2.set_bold()
#                     cell_format2.set_font_size(10)
#                     cell_format2.set_align("center")
#                     cell_format2.set_align("vjustify")

#                     worksheet = workbook.add_worksheet()
#                     worksheet.set_column("A:AZ", 20)
#                     worksheet.set_default_row(50)
#                     cell_format = workbook.add_format()
#                     cell_format.set_font_size(10)
#                     cell_format.set_align("center")
#                     cell_format.set_align("vcenter")

#                     startdate = f'{specified_date} 00:00:00'
#                     enddate = f'{specified_date} 23:59:59'
#                     from_ts = convert_to_utc_format(startdate, "%Y-%m-%d %H:%M:%S")
#                     to_ts = convert_to_utc_format(enddate, "%Y-%m-%d %H:%M:%S")
                    
#                     pipeline = [
#                         {
#                             "$match": {
#                                 "GWEL_Tare_Time": {"$gte": from_ts, "$lte": to_ts},
#                                     "net_qty": {"$ne": None}
#                                 }
#                         },
#                         {
#                         '$group': {
#                             '_id': {
#                                 'date': {
#                                     '$dateToString': {
#                                         'format': '%Y-%m-%d', 
#                                         'date': '$GWEL_Tare_Time'
#                                     }
#                                 }, 
#                                 'do_no': '$arv_cum_do_number'
#                             }, 
#                             'total_net_qty': {
#                                 '$sum': {
#                                     '$toDouble': '$net_qty'
#                                 }
#                             }
#                         }
#                     }]

#                     # filtered_data = [
#                     #     entry for entry in dataList if entry["date"] == specified_date
#                     # ]
                    
#                     filtered_data_new = Gmrdata.objects.aggregate(pipeline)
#                     # dictDaata = {}
#                     aggregated_totals = defaultdict(float)
#                     for single_data_entry in filtered_data_new:
#                         do_no = single_data_entry['_id']['do_no']
#                         total_net_qty = single_data_entry['total_net_qty']
#                         aggregated_totals[do_no] += total_net_qty

#                     # Create a dictionary to store the latest entries based on DO_No
#                     data_by_do = {}
#                     finaldataMain = [single_data_list for single_data_list in final_data if single_data_list.get("balance_days") >= 0]
#                     # Iterate over final_data   
#                     for entry in finaldataMain:
#                         do_no = entry['DO_No']
                
#                         # clubbing all challan_lr_qty to get cumulative_challan_lr_qty
#                         if do_no not in data_by_do:
#                             data_by_do[do_no] = entry
#                             data_by_do[do_no]['cumulative_challan_lr_qty'] = round(entry['club_challan_lr_qty'], 2)
#                         else:
#                             data_by_do[do_no]['cumulative_challan_lr_qty'] += round(entry['club_challan_lr_qty'], 2)
                        
#                         if filtered_data_new:
#                             # data = filtered_data[0]["data"]
#                             # Update challan_lr_qty if the DO_No matches
#                             if do_no in aggregated_totals:
#                                 data_by_do[do_no]['challan_lr_qty'] = round(aggregated_totals[do_no], 2)
#                             else:
#                                 data_by_do[do_no]['challan_lr_qty'] = 0

#                         # Update calculated fields
#                         if data_by_do[do_no]['DO_Qty'] != 0 and data_by_do[do_no]['cumulative_challan_lr_qty'] != 0:
#                             data_by_do[do_no]['percent_supply'] = round((data_by_do[do_no]['cumulative_challan_lr_qty'] / data_by_do[do_no]['DO_Qty']) * 100, 2)
#                         else:
#                             data_by_do[do_no]['percent_supply'] = 0

#                         # if data_by_do[do_no]['cumulative_challan_lr_qty'] != 0 and data_by_do[do_no]['DO_Qty'] != 0:
#                         data_by_do[do_no]['balance_qty'] = round(data_by_do[do_no]['DO_Qty'] - data_by_do[do_no]['cumulative_challan_lr_qty'], 2)
#                         # else:
#                         #     data_by_do[do_no]['balance_qty'] = 0
                        
#                         if data_by_do[do_no]['balance_days'] and data_by_do[do_no]['balance_qty'] != 0:
#                             data_by_do[do_no]['asking_rate'] = round(data_by_do[do_no]['balance_qty'] / data_by_do[do_no]['balance_days'], 2)

#                     # final_data = list(data_by_do.values())
#                     sort_final_data = list(data_by_do.values())
#                     # Sort the data by 'balance_days', placing entries with 'balance_days' of 0 at the end
#                     final_data = sorted(sort_final_data, key=lambda x: (x['balance_days'] == 0, x['balance_days']))
#                     result["datasets"] = final_data

#                     headers = ["Month", "Mine Name", "DO_No", "Grade", "DO Qty", "Challan Lr / Qty", "Cumulative Challan Lr / Qty", "Balance Qty", "% of Supply", "Balance Days", "Asking Rate", "Do Start Date", "Do End Date"]
                    
#                     for index, header in enumerate(headers):
#                         worksheet.write(0, index, header, cell_format2)
                    
#                     row = 1
#                     for single_data in result["datasets"]:
#                         # worksheet.write(row, 0, count, cell_format)
#                         worksheet.write(row, 0, single_data["month"])
#                         worksheet.write(row, 1, single_data["mine_name"])
#                         worksheet.write(row, 2, single_data["DO_No"])
#                         worksheet.write(row, 3, single_data["average_GCV_Grade"])
#                         worksheet.write(row, 4, single_data["DO_Qty"])
#                         worksheet.write(row, 5, single_data["challan_lr_qty"])
#                         worksheet.write(row, 6, single_data["cumulative_challan_lr_qty"])
#                         worksheet.write(row, 7, single_data["balance_qty"])
#                         worksheet.write(row, 8, single_data["percent_supply"])
#                         worksheet.write(row, 9, single_data["balance_days"])
#                         worksheet.write(row, 10, single_data["asking_rate"])
#                         worksheet.write(row, 11, single_data["start_date"])
#                         worksheet.write(row, 12, single_data["end_date"])

#                         count -= 1
#                         row += 1
#                     workbook.close()

#                     return {
#                             "Type": "daily_coal_report",
#                             "Datatype": "Report",
#                             "File_Path": path,
#                         }
#                 else:
#                     console_logger.error("No data found")
#                     return {
#                                 "Type": "daily_coal_report",
#                                 "Datatype": "Report",
#                                 "File_Path": path,
#                             }

#     except Exception as e:
#         response.status_code = 400
#         console_logger.debug(e)
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#         console_logger.debug(
#             "Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno)
#         )
#         return e




@router.get("/coal_logistics_report_train", tags=["Rail Map"])
def coal_logistics_report_train(
    response: Response,
    specified_date: str,
    search_text: Optional[str] = None,
    currentPage: Optional[int] = None,
    perPage: Optional[int] = None,
    mine: Optional[str] = "All",
    type: Optional[str] = "display"
):
    try:
        result = {"labels": [], "datasets": [], "total": 0, "page_size": 15}
        if type and type == "display":

            if specified_date:
                data = {}

                if mine and mine != "All":
                    data["mine__icontains"] = mine.upper()

                
                page_no = 1
                page_len = result["page_size"]

                if currentPage:
                    page_no = currentPage

                if perPage:
                    page_len = perPage
                    result["page_size"] = perPage

                # specified_change_date = convert_to_utc_format(specified_date, "%Y-%m-%d")
                specified_change_date = datetime.datetime.strptime(specified_date, "%Y-%m-%d")

                start_of_month = specified_change_date.replace(day=1)

                start_date = datetime.datetime.strftime(start_of_month, '%Y-%m-%d')
                end_date = datetime.datetime.strftime(specified_change_date, '%Y-%m-%d')

                if search_text:
                    data = Q()
                    if search_text.isdigit():
                        data &= (Q(arv_cum_do_number__icontains=search_text))
                    else:
                        data &= (Q(mine__icontains=search_text))
        
                    logs = (RailData.objects(data).order_by("source", "rr_no", "-created_at"))
                else:
                    logs = (RailData.objects().order_by("source", "rr_no", "-created_at"))
                coal_testing_train = CoalTestingTrain.objects(receive_date__gte=start_date, receive_date__lte=end_date).order_by("-ID")
                if any(logs):
                    aggregated_data = defaultdict(
                        lambda: defaultdict(
                            lambda: {
                                "DO_Qty": 0,
                                "challan_lr_qty": 0,
                                "mine_name": "",
                                "balance_qty": 0,
                                "percent_of_supply": 0,
                                "actual_net_qty": 0,
                                "Gross_Calorific_Value_(Adb)": 0,
                                "count": 0,
                                "coal_count": 0,
                            }
                        )
                    )

                    aggregated_coal_data = defaultdict(
                        lambda: defaultdict(
                            lambda: {
                                "Gross_Calorific_Value_(Adb)": 0,
                                "coal_count": 0,
                            }
                        )
                    )
                    for single_log in coal_testing_train:
                        coal_date = single_log.receive_date.strftime("%Y-%m")
                        coal_payload = single_log.gradepayload()
                        mine = coal_payload["Mine"]
                        rr_no = coal_payload["rrNo"]
                        if coal_payload.get("Gross_Calorific_Value_(Adb)"):
                            aggregated_coal_data[coal_date][rr_no]["Gross_Calorific_Value_(Adb)"] += float(coal_payload["Gross_Calorific_Value_(Adb)"])
                            aggregated_coal_data[coal_date][rr_no]["coal_count"] += 1

                    start_dates = {}
                    grade = 0
                    for log in logs:
                        if log.created_at!=None:
                            month = log.created_at.strftime("%Y-%m")
                            date = log.created_at.strftime("%Y-%m-%d")
                            payload = log.payload()
                            result["labels"] = list(payload.keys())
                            mine_name = payload.get("source")
                            rr_no = payload.get("rr_no")
                            # if payload.get("Grade") is not None:
                            #     if '-' in payload.get("Grade"):
                            #         grade = payload.get("Grade").split("-")[0]
                            #     else:
                            #         grade = payload.get("Grade")
                            # If start_date is None or the current vehicle_in_time is earlier than start_date, update start_date
                            if rr_no not in start_dates:
                                start_dates[rr_no] = date
                            elif date < start_dates[rr_no]:
                                start_dates[rr_no] = date
                            if payload.get("rr_qty"):
                                # aggregated_data[date][do_no]["DO_Qty"] += float(
                                #     payload["DO_Qty"]
                                # )
                                aggregated_data[date][rr_no]["rr_qty"] = float(
                                    payload["rr_qty"]
                                )
                            else:
                                aggregated_data[date][rr_no]["rr_qty"] = 0
                            if payload.get("total_secl_net_wt"):
                                total_secl_net_wt = payload.get("total_secl_net_wt")
                                try:
                                    quantity = float(int(total_secl_net_wt))
                                except (ValueError, TypeError):
                                    quantity = 0.0 

                                aggregated_data[date][rr_no]["challan_lr_qty"] += quantity
                            else:
                                aggregated_data[date][rr_no]["challan_lr_qty"] = 0
                            if payload.get("source"):
                                aggregated_data[date][rr_no]["source"] = payload[
                                    "source"
                                ]
                            else:
                                aggregated_data[date][rr_no]["source"] = "-"
                            aggregated_data[date][rr_no]["count"] += 1 
                    dataList = [
                        {
                            "date": date,
                            "data": {
                                rr_no: {
                                    "rr_qty": data["rr_qty"] if data["rr_qty"] else 0,
                                    "challan_lr_qty": data["challan_lr_qty"],
                                    "mine_name": data["source"],
                                    "date": date,
                                }
                                for rr_no, data in aggregated_data[date].items()
                            },
                        }
                        for date in aggregated_data
                    ]

                    coalDataList = [
                        {"date": coal_date, "data": {
                            rr_no: {
                                "average_Gross_Calorific_Value_(Adb)": data["Gross_Calorific_Value_(Adb)"] / data["coal_count"],
                            } for rr_no, data in aggregated_coal_data[coal_date].items()
                        }} for coal_date in aggregated_coal_data
                    ]

                    coal_grades = CoalGrades.objects()  # Fetch all coal grades from the database

                    # Iterate through each month's data
                    for month_data in coalDataList:
                        for key, mine_data in month_data["data"].items():
                            if mine_data["average_Gross_Calorific_Value_(Adb)"] is not None:
                                for single_coal_grades in coal_grades:
                                    if single_coal_grades["end_value"] != "":
                                        if (int(single_coal_grades["start_value"]) <= int(float(mine_data.get("average_Gross_Calorific_Value_(Adb)"))) <= int(single_coal_grades["end_value"]) and single_coal_grades["start_value"] != "" and single_coal_grades["end_value"] != ""):
                                            mine_data["average_GCV_Grade"] = single_coal_grades["grade"]
                                        elif int(mine_data["average_Gross_Calorific_Value_(Adb)"]) > 7001:
                                            mine_data["average_GCV_Grade"] = "G-1"
                                            break
                    
                    final_data = []
                    if specified_date:
                        filtered_data = [
                            entry for entry in dataList if entry["date"] == specified_date
                        ]
                        if filtered_data:
                            data = filtered_data[0]["data"]
                            # dictData["month"] = filtered_data[0]["month"]
                            for data_dom, values in data.items():
                                dictData = {}
                                dictData["rr_no"] = data_dom
                                dictData["mine_name"] = values["mine_name"]
                                dictData["rr_qty"] = round(values["rr_qty"], 2)
                                dictData["challan_lr_qty"] = round(values["challan_lr_qty"], 2)
                                dictData["date"] = values["date"]
                                dictData["cumulative_challan_lr_qty"] = 0
                                dictData["balance_qty"] = 0
                                dictData["percent_supply"] = 0
                                dictData["asking_rate"] = 0
                                # dictData['average_GCV_Grade'] = values["grade"]
                                if data_dom in start_dates:
                                    dictData["start_date"] = start_dates[data_dom]
                                    # a total of 45 days data is needed, so date + 44 days
                                    endDataVariable = datetime.datetime.strptime(start_dates[data_dom], "%Y-%m-%d") + timedelta(days=44)
                                    # dictData["balance_days"] = dictData["end_date"] - datetime.date.today()
                                    balance_days = endDataVariable.date() - datetime.date.today()
                                    dictData["end_date"] = endDataVariable.strftime("%Y-%m-%d")
                                    dictData["balance_days"] = balance_days.days
                                else:
                                    dictData["start_date"] = None
                                    dictData["end_date"] = None
                                    dictData["balance_days"] = None

                                # Look for data_dom match in coalDataList and add average_GCV_Grade
                                for coal_data in coalDataList:
                                    # if coal_data['date'] == specified_date and data_dom in coal_data['data']:
                                    if data_dom in coal_data['data']:
                                        dictData['average_GCV_Grade'] = coal_data['data'][data_dom]['average_GCV_Grade']
                                        break
                                else:
                                    dictData['average_GCV_Grade'] = "-"
                    
                                # append data
                                final_data.append(dictData)
                        
                        if final_data:
                            # Find the index of the month data in dataList
                            index_of_month = next((index for index, item in enumerate(dataList) if item['date'] == specified_date), None)

                            # If the month is not found, exit or handle the case
                            if index_of_month is None:
                                print("Month data not found.")
                                exit()

                            # Iterate over final_data
                            for entry in final_data:
                                rr_no = entry["rr_no"]
                                cumulative_lr_qty = 0
                                
                                # Iterate over dataList from the first month to the current month
                                for i in range(index_of_month + 1):
                                    month_data = dataList[i]
                                    data = month_data["data"].get(rr_no)
                                    
                                    # If data is found for the rr_no in the current month, update cumulative_lr_qty
                                    if data:
                                        cumulative_lr_qty += data['challan_lr_qty']
                                
                                # Update cumulative_challan_lr_qty in final_data
                                entry['cumulative_challan_lr_qty'] = round(cumulative_lr_qty, 2)
                                if data["rr_qty"] != 0 and entry["cumulative_challan_lr_qty"] != 0:
                                    entry["percent_supply"] = round((entry["cumulative_challan_lr_qty"] / data["rr_qty"]) * 100, 2)
                                else:
                                    entry["percent_supply"] = 0

                                if entry["cumulative_challan_lr_qty"] != 0 and data["rr_qty"] != 0:
                                    entry["balance_qty"] = round((data["rr_qty"] - entry["cumulative_challan_lr_qty"]), 2)
                                else:
                                    entry["balance_qty"] = 0
                                
                                if entry["balance_qty"] and entry["balance_qty"] != 0:
                                    if entry["balance_days"]:
                                        entry["asking_rate"] = round(entry["balance_qty"] / entry["balance_days"], 2)

                    if final_data:
                        start_index = (page_no - 1) * page_len
                        end_index = start_index + page_len
                        paginated_data = final_data[start_index:end_index]

                        result["labels"] = list(final_data[0].keys())
                        result["datasets"] = paginated_data
                        result["total"] = len(final_data)

                return result
            else:
                return 400
        elif type and type == "download":
            del type
            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            specified_change_date = datetime.datetime.strptime(specified_date, "%Y-%m-%d")

            start_of_month = specified_change_date.replace(day=1)

            start_date = datetime.datetime.strftime(start_of_month, '%Y-%m-%d')
            end_date = datetime.datetime.strftime(specified_change_date, '%Y-%m-%d')

            if search_text:
                data = Q()
                if search_text.isdigit():
                    data &= (Q(arv_cum_do_number__icontains=search_text))
                else:
                    data &= (Q(mine__icontains=search_text))

                logs = (RailData.objects(data).order_by("source", "rr_no", "-created_at"))
            else:
                logs = (RailData.objects().order_by("source", "rr_no", "-created_at"))

            coal_testing_train = CoalTestingTrain.objects(receive_date__gte=start_date, receive_date__lte=end_date).order_by("-ID")
            count = len(logs)
            path = None
            if any(logs):
                aggregated_data = defaultdict(
                    lambda: defaultdict(
                        lambda: {
                            "DO_Qty": 0,
                            "challan_lr_qty": 0,
                            "mine_name": "",
                            "balance_qty": 0,
                            "percent_of_supply": 0,
                            "actual_net_qty": 0,
                            "Gross_Calorific_Value_(Adb)": 0,
                            "count": 0,
                            "coal_count": 0,
                        }
                    )
                )

                aggregated_coal_data = defaultdict(
                    lambda: defaultdict(
                        lambda: {
                            "Gross_Calorific_Value_(Adb)": 0,
                            "coal_count": 0,
                        }
                    )
                )

                for single_log in coal_testing_train:
                    coal_date = single_log.receive_date.strftime("%Y-%m")
                    coal_payload = single_log.gradepayload()
                    mine = coal_payload["Mine"]
                    rr_no = coal_payload["rrNo"]
                    if coal_payload.get("Gross_Calorific_Value_(Adb)"):
                        aggregated_coal_data[coal_date][rr_no]["Gross_Calorific_Value_(Adb)"] += float(coal_payload["Gross_Calorific_Value_(Adb)"])
                        aggregated_coal_data[coal_date][rr_no]["coal_count"] += 1

                start_dates = {}
                for log in logs:
                    if log.created_at!=None:
                        month = log.created_at.strftime("%Y-%m")
                        date = log.created_at.strftime("%Y-%m-%d")
                        payload = log.payload()
                        result["labels"] = list(payload.keys())
                        mine_name = payload.get("source")
                        rr_no = payload.get("rr_no")
                        # if payload.get("Grade") is not None:
                        #     if '-' in payload.get("Grade"):
                        #         grade = payload.get("Grade").split("-")[0]
                        #     else:
                        #         grade = payload.get("Grade")
                        # If start_date is None or the current vehicle_in_time is earlier than start_date, update start_date
                        if rr_no not in start_dates:
                            start_dates[rr_no] = date
                        elif date < start_dates[rr_no]:
                            start_dates[rr_no] = date
                        if payload.get("rr_qty"):
                            aggregated_data[date][rr_no]["rr_qty"] = float(
                                payload["rr_qty"]
                            )
                        else:
                            aggregated_data[date][rr_no]["rr_qty"] = 0
                        if payload.get("total_secl_net_wt"):
                            total_secl_net_wt = payload.get("total_secl_net_wt")
                            try:
                                quantity = float(int(total_secl_net_wt))
                            except (ValueError, TypeError):
                                quantity = 0.0 
                            aggregated_data[date][rr_no]["challan_lr_qty"] += quantity
                        else:
                            aggregated_data[date][rr_no]["challan_lr_qty"] = 0
                        if payload.get("source"):
                            aggregated_data[date][rr_no]["source"] = payload[
                                "source"
                            ]
                        else:
                            aggregated_data[date][rr_no]["source"] = "-"
                        aggregated_data[date][rr_no]["count"] += 1

                dataList = [
                    {
                        "date": date,
                        "data": {
                            rr_no: {
                                "rr_qty": data["rr_qty"],
                                "challan_lr_qty": data["challan_lr_qty"],
                                "mine_name": data["source"],
                                "date": date,
                            }
                            for rr_no, data in aggregated_data[date].items()
                        },
                    }
                    for date in aggregated_data
                ]

                coalDataList = [
                    {"date": coal_date, "data": {
                        rr_no: {
                            "average_Gross_Calorific_Value_(Adb)": data["Gross_Calorific_Value_(Adb)"] / data["coal_count"],
                        } for rr_no, data in aggregated_coal_data[coal_date].items()
                    }} for coal_date in aggregated_coal_data
                ]

                coal_grades = CoalGrades.objects()  # Fetch all coal grades from the database

                # Iterate through each month's data
                for month_data in coalDataList:
                    for key, mine_data in month_data["data"].items():
                        if mine_data["average_Gross_Calorific_Value_(Adb)"] is not None:
                            for single_coal_grades in coal_grades:
                                if single_coal_grades["end_value"] != "":
                                    if (int(single_coal_grades["start_value"]) <= int(float(mine_data.get("average_Gross_Calorific_Value_(Adb)"))) <= int(single_coal_grades["end_value"]) and single_coal_grades["start_value"] != "" and single_coal_grades["end_value"] != ""):
                                        mine_data["average_GCV_Grade"] = single_coal_grades["grade"]
                                    elif int(mine_data["average_Gross_Calorific_Value_(Adb)"]) > 7001:
                                        mine_data["average_GCV_Grade"] = "G-1"
                                        break
                
                final_data = []
                if specified_date:
                    filtered_data = [
                        entry for entry in dataList if entry["date"] == specified_date
                    ]
                    if filtered_data:
                        data = filtered_data[0]["data"]
                        # dictData["month"] = filtered_data[0]["month"]
                        for data_dom, values in data.items():
                            dictData = {}
                            dictData["rr_no"] = data_dom
                            dictData["mine_name"] = values["mine_name"]
                            dictData["rr_qty"] = round(values["rr_qty"], 2)
                            dictData["challan_lr_qty"] = round(values["challan_lr_qty"], 2)
                            dictData["date"] = values["date"]
                            dictData["cumulative_challan_lr_qty"] = 0
                            dictData["balance_qty"] = 0
                            dictData["percent_supply"] = 0
                            dictData["asking_rate"] = 0
                            # dictData['average_GCV_Grade'] = values["grade"] 
                            if data_dom in start_dates:
                                dictData["start_date"] = start_dates[data_dom]
                                # a total of 45 days data is needed, so date + 44 days
                                dictData["end_date"] = datetime.datetime.strptime(start_dates[data_dom], "%Y-%m-%d") + timedelta(days=44)
                                # dictData["balance_days"] = dictData["end_date"] - datetime.date.today()
                                balance_days = dictData["end_date"].date() - datetime.date.today()
                                dictData["balance_days"] = balance_days.days
                            else:
                                dictData["start_date"] = None
                                dictData["end_date"] = None
                                dictData["balance_days"] = None

                            # Look for data_dom match in coalDataList and add average_GCV_Grade
                            for coal_data in coalDataList:
                                # if coal_data['date'] == specified_date and data_dom in coal_data['data']:
                                if data_dom in coal_data['data']:
                                    dictData['average_GCV_Grade'] = coal_data['data'][data_dom]['average_GCV_Grade']
                                    break
                            else:
                                dictData['average_GCV_Grade'] = "-"
                
                            # append data
                            final_data.append(dictData)
                    logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
                    if final_data:
                        path = os.path.join(
                            "static_server",
                            "gmr_ai",
                            file,
                            "Coal_Logistics_Rail_Report_{}.xlsx".format(
                                datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                            ),
                        )

                        filename = os.path.join(os.getcwd(), path)
                        workbook = xlsxwriter.Workbook(filename)
                        workbook.use_zip64()
                        cell_format2 = workbook.add_format()
                        cell_format2.set_bold()
                        cell_format2.set_font_size(10)
                        cell_format2.set_align("center")
                        cell_format2.set_align("vcenter")
                        cell_format2.set_text_wrap(True)
                        cell_format2.set_border(1)

                        header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
                        date_format = workbook.add_format({'align': 'center', 'font_size': 12, "bold": True})
                        report_name_format = workbook.add_format({'align': 'center', 'font_size': 15, "bold": True})
                        # report_name_format.set_text_wrap(True)

                        header_format.set_align("vcenter")
                        date_format.set_align("vcenter")
                        report_name_format.set_align("vcenter")
                        header_format.set_border(1)
                        date_format.set_border(1)
                        report_name_format.set_border(1)

                        worksheet = workbook.add_worksheet()
                        worksheet.set_column("A:AZ", 20)
                        worksheet.set_default_row(50)
                        cell_format = workbook.add_format()
                        cell_format.set_font_size(10)
                        cell_format.set_align("center")
                        cell_format.set_align("vcenter")
                        cell_format.set_text_wrap(True)
                        cell_format.set_border(1)

                        worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                        
                        # Merge cells for the main header and place it in the center
                        main_header = "GMR Warora Energy Limited"  # Set your main header text here
                        worksheet.merge_range("A1:M1", main_header, header_format)  # Merge cells A1 to H1 for the header
                        
                        # Write the current date on the left side (A2)
                        # worksheet.write("A2", f"Date: {datetime.datetime.now().strftime('%d-%m-%Y')}", date_format)
                        worksheet.merge_range("A2:B2", f"Date: {datetime.datetime.now().strftime('%d-%m-%Y')}", date_format)
                        worksheet.merge_range("C2:M2", f"Rail Coal Logistics Report", report_name_format)

                        # Find the index of the month data in dataList
                        index_of_month = next((index for index, item in enumerate(dataList) if item['date'] == specified_date), None)

                        # If the month is not found, exit or handle the case
                        if index_of_month is None:
                            print("Month data not found.")
                            exit()

                        # Iterate over final_data
                        for entry in final_data:
                            rr_no = entry["rr_no"]
                            cumulative_lr_qty = 0
                            
                            # Iterate over dataList from the first month to the current month
                            for i in range(index_of_month + 1):
                                month_data = dataList[i]
                                data = month_data["data"].get(rr_no)
                                
                                # If data is found for the DO_No in the current month, update cumulative_lr_qty
                                if data:
                                    cumulative_lr_qty += data['challan_lr_qty']
                            
                            # Update cumulative_challan_lr_qty in final_data
                            entry['cumulative_challan_lr_qty'] = cumulative_lr_qty
                            if data["rr_qty"] != 0 and entry["cumulative_challan_lr_qty"] != 0:
                                entry["percent_supply"] = round((entry["cumulative_challan_lr_qty"] / data["rr_qty"]) * 100, 2)
                            else:
                                entry["percent_supply"] = 0

                            if entry["cumulative_challan_lr_qty"] != 0 and data["rr_qty"] != 0:
                                entry["balance_qty"] = round((data["rr_qty"] - entry["cumulative_challan_lr_qty"]), 2)
                            else:
                                entry["balance_qty"] = 0
                            
                            if entry["balance_qty"] and entry["balance_qty"] != 0:
                                if entry["balance_days"]:
                                    entry["asking_rate"] = round(entry["balance_qty"] / entry["balance_days"], 2)

                        result["datasets"] = final_data

                        headers = [
                            "Sr.No", 
                            "Mine Name", 
                            "RR No", 
                            "Grade", 
                            "RR Qty", 
                            "Challan LR Qty", 
                            "Cumulative Challan Lr_Qty", 
                            "Balance Qty", 
                            "% of Supply", 
                            "Balance Days", 
                            "Asking Rate", 
                            "Do Start Date", 
                            "Do End Date"
                        ]
                        
                        for index, header in enumerate(headers):
                            worksheet.write(0, index, header, cell_format2)
                        
                        row = 1
                        for single_data in result["datasets"]:
                            worksheet.write(row, 0, count, cell_format)
                            worksheet.write(row, 1, single_data["mine_name"])
                            worksheet.write(row, 2, single_data["rr_no"])
                            worksheet.write(row, 3, single_data["average_GCV_Grade"])
                            worksheet.write(row, 4, single_data["rr_qty"])
                            worksheet.write(row, 5, single_data["challan_lr_qty"])
                            worksheet.write(row, 6, single_data["cumulative_challan_lr_qty"])
                            worksheet.write(row, 7, single_data["balance_qty"])
                            worksheet.write(row, 8, single_data["percent_supply"])
                            worksheet.write(row, 9, single_data["balance_days"])
                            worksheet.write(row, 10, single_data["asking_rate"])
                            worksheet.write(row, 11, single_data["start_date"])
                            worksheet.write(row, 12, single_data["end_date"].strftime("%Y-%m-%d"))

                            count -= 1
                            row += 1
                        workbook.close()

                        return {
                                "Type": "daily_rail_coal_report",
                                "Datatype": "Report",
                                "File_Path": path,
                            }
                    else:
                        console_logger.error("No data found")
                        return {
                                    "Type": "daily_rail_coal_report",
                                    "Datatype": "Report",
                                    "File_Path": path,
                                }

    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug(
            "Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno)
        )
        return e

        
# @router.get("/mine_wise_average_gcv", tags=["Road Map"])
# def mine_wise_average_gwel_gcv(
#     response: Response,
#     type: Optional[str] = "Daily",
#     Month: Optional[str] = None, 
#     Daily: Optional[str] = None, 
#     Year: Optional[str] = None
# ):
#     try:
#         if type == "Daily":
#             specified_date = datetime.datetime.strptime(Daily, "%Y-%m-%d")
#             start_of_month = specified_date.replace(day=1)
#             start_date = datetime.datetime.strftime(start_of_month, '%Y-%m-%d')
#             end_date = datetime.datetime.strftime(specified_date, '%Y-%m-%d')
#         elif type == "Week":
#             specified_date = datetime.datetime.now().date()
#             start_of_week = specified_date - datetime.timedelta(days=7)
#             start_date = datetime.datetime.strftime(start_of_week, '%Y-%m-%d')
#             end_date = datetime.datetime.strftime(specified_date, '%Y-%m-%d')
#         elif type == "Month":
#             date=Month
#             datestructure = date.replace(" ", "").split("-")
#             final_month = f"{datestructure[0]}-{str(datestructure[1]).zfill(2)}"
#             start_month = f"{final_month}-01"
#             startd_date = datetime.datetime.strptime(start_month, "%Y-%m-%d")
#             endd_date = startd_date + datetime.timedelta(days=31)
#             start_date = datetime.datetime.strftime(startd_date, '%Y-%m-%d')
#             end_date = datetime.datetime.strftime(endd_date, '%Y-%m-%d')
#         elif type == "Year":
#             date = Year
#             endd_date =f'{date}-12-31'
#             startd_date = f'{date}-01-01'
#             format_data = "%Y-%m-%d"
#             end_date=datetime.datetime.strftime(datetime.datetime.strptime(endd_date,format_data), format_data)
#             start_date=datetime.datetime.strftime(datetime.datetime.strptime(startd_date,format_data), format_data)

#         # Query for CoalTesting objects
#         fetchCoalTesting = CoalTesting.objects(
#             receive_date__gte= datetime.datetime.strptime(start_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), receive_date__lte= datetime.datetime.strptime(end_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M")
#         )
#         # Query for CoalTestingTrain objects
#         fetchCoalTestingTrain = CoalTestingTrain.objects(
#             receive_date__gte = datetime.datetime.strptime(start_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), receive_date__lte= datetime.datetime.strptime(end_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M")
#         )
#         # Query for GMRData objects
#         # fetchGmrData = Gmrdata.objects(created_at__gte=datetime.datetime.strptime(start_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), created_at__lte=datetime.datetime.strptime(end_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"))
        
#         rrNo_values = {}

#         # Iterate through fetched CoalTesting objects
#         for single_coal_testing in fetchCoalTesting:
#             rrNo = single_coal_testing.rrNo
#             location = single_coal_testing.location
#             for param in single_coal_testing.parameters:
#                 if param["parameter_Name"] == "Gross_Calorific_Value_(Arb)":
#                     if param["val1"] != None and param["val1"] != "":
#                         calorific_value = float(param["val1"])
#                         break
#             else:
#                 continue

#             # Aggregate values based on rrNo
#             if rrNo in rrNo_values:
#                 rrNo_values[location] += calorific_value
#             else:
#                 rrNo_values[location] = calorific_value

#         # Iterate through fetched CoalTestingTrain objects
#         for single_coal_testing_train in fetchCoalTestingTrain:
#             rrNo = single_coal_testing_train.rrNo
#             location = single_coal_testing_train.location
#             for param in single_coal_testing_train.parameters:
#                 if param["parameter_Name"] == "Gross_Calorific_Value_(Arb)":
#                     if param["val1"] != None:
#                         calorific_value = float(param["val1"])
#                         break
#             else:
#                 continue

#             # Aggregate values based on rrNo
#             if rrNo in rrNo_values:
#                 rrNo_values[location] += calorific_value
#             else:
#                 rrNo_values[location] = calorific_value
        
#         # fetch data from AopTarget
#         aopList = []
#         fetchAopTarget = AopTarget.objects()
#         if fetchAopTarget:
#             for single_aop_target in fetchAopTarget:
#                 aopList.append(single_aop_target.payload())

#         target_dict = {item['source_name']: int(item['aop_target']) for item in aopList}
        
#         aligned_target_data = [target_dict.get(label.strip(), 0) for label in rrNo_values.keys()]

#         result = {
#             "data": {
#                 "labels": list(rrNo_values.keys()),
#                 "datasets": [
#                     {"label": "Mine", "data": list(rrNo_values.values()), "order": 1, "type": "bar"},
#                     {"label": "Target", "data": aligned_target_data, "order": 0, "type": "line"},
#                 ],
#             }
#         }

#         return result
        
#     except Exception as e:
#         console_logger.debug("----- Gate Vehicle Count Error -----",e)
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#         console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#         return e

# new db structure
# @router.get("/mine_wise_average_gcv", tags=["Road Map"])
# def mine_wise_average_gwel_gcv(
#     response: Response,
#     type: Optional[str] = "Daily",
#     Month: Optional[str] = None, 
#     Daily: Optional[str] = None, 
#     Year: Optional[str] = None
# ):
#     try:
#         if type == "Daily":
#             specified_date = datetime.datetime.strptime(Daily, "%Y-%m-%d")
#             start_of_month = specified_date.replace(day=1)
#             start_date = datetime.datetime.strftime(start_of_month, '%Y-%m-%d')
#             end_date = datetime.datetime.strftime(specified_date, '%Y-%m-%d')
#         elif type == "Week":
#             specified_date = datetime.datetime.now().date()
#             start_of_week = specified_date - datetime.timedelta(days=7)
#             start_date = datetime.datetime.strftime(start_of_week, '%Y-%m-%d')
#             end_date = datetime.datetime.strftime(specified_date, '%Y-%m-%d')
#         elif type == "Month":
#             date=Month
#             datestructure = date.replace(" ", "").split("-")
#             final_month = f"{datestructure[0]}-{str(datestructure[1]).zfill(2)}"
#             start_month = f"{final_month}-01"
#             startd_date = datetime.datetime.strptime(start_month, "%Y-%m-%d")
#             endd_date = startd_date + datetime.timedelta(days=31)
#             start_date = datetime.datetime.strftime(startd_date, '%Y-%m-%d')
#             end_date = datetime.datetime.strftime(endd_date, '%Y-%m-%d')
#         elif type == "Year":
#             date = Year
#             endd_date =f'{date}-12-31'
#             startd_date = f'{date}-01-01'
#             format_data = "%Y-%m-%d"
#             end_date=datetime.datetime.strftime(datetime.datetime.strptime(endd_date,format_data), format_data)
#             start_date=datetime.datetime.strftime(datetime.datetime.strptime(startd_date,format_data), format_data)

#         # # Query for CoalTesting objects
#         # fetchCoalTesting = CoalTesting.objects(
#         #     receive_date__gte= datetime.datetime.strptime(start_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), receive_date__lte= datetime.datetime.strptime(end_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M")
#         # )
#         # # Query for CoalTestingTrain objects
#         # fetchCoalTestingTrain = CoalTestingTrain.objects(
#         #     receive_date__gte = datetime.datetime.strptime(start_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), receive_date__lte= datetime.datetime.strptime(end_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M")
#         # )

#         fetchCoalTesting = RecieptCoalQualityAnalysis.objects(plant_analysis_date__gte=datetime.datetime.strptime(start_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), plant_analysis_date__lte=datetime.datetime.strptime(end_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), mode="Road")
#         fetchCoalTestingTrain = RecieptCoalQualityAnalysis.objects(plant_analysis_date__gte=datetime.datetime.strptime(start_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), plant_analysis_date__lte=datetime.datetime.strptime(end_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), mode="Rail")

#         # Query for GMRData objects
#         # fetchGmrData = Gmrdata.objects(created_at__gte=datetime.datetime.strptime(start_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), created_at__lte=datetime.datetime.strptime(end_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"))
        
#         rrNo_values = {}

#         # Iterate through fetched CoalTesting objects
#         for single_coal_testing in fetchCoalTesting:
#             rrNo = single_coal_testing.sample_id
#             location = single_coal_testing.mine
#             calorific_value = single_coal_testing.plant_arb_gcv

#             # Aggregate values based on rrNo
#             if rrNo in rrNo_values:
#                 rrNo_values[location] += calorific_value
#             else:
#                 rrNo_values[location] = calorific_value

#         # Iterate through fetched CoalTestingTrain objects
#         for single_coal_testing_train in fetchCoalTestingTrain:
#             rrNo = single_coal_testing_train.sample_id
#             location = single_coal_testing_train.mine
#             calorific_value = single_coal_testing_train.plant_arb_gcv

#             # Aggregate values based on rrNo
#             if rrNo in rrNo_values:
#                 rrNo_values[location] += calorific_value
#             else:
#                 rrNo_values[location] = calorific_value
        
#         # fetch data from AopTarget
#         aopList = []
#         fetchAopTarget = AopTarget.objects()
#         if fetchAopTarget:
#             for single_aop_target in fetchAopTarget:
#                 aopList.append(single_aop_target.payload())

#         target_dict = {item['source_name']: int(item['aop_target']) for item in aopList}
        
#         aligned_target_data = [target_dict.get(label.strip(), 0) for label in rrNo_values.keys()]

#         result = {
#             "data": {
#                 "labels": list(rrNo_values.keys()),
#                 "datasets": [
#                     {"label": "Mine", "data": list(rrNo_values.values()), "order": 1, "type": "bar"},
#                     {"label": "Target", "data": aligned_target_data, "order": 0, "type": "line"},
#                 ],
#             }
#         }

#         return result
        
#     except Exception as e:
#         console_logger.debug("----- Gate Vehicle Count Error -----",e)
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#         console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#         return e


@router.get("/mine_wise_average_gcv", tags=["Road Map"])
def mine_wise_average_gwel_gcv(
    response: Response,
    type: Optional[str] = "Daily",
    Month: Optional[str] = None, 
    Daily: Optional[str] = None, 
    Year: Optional[str] = None
):
    try:
        if type == "Daily":
            specified_date = datetime.datetime.strptime(Daily, "%Y-%m-%d")
            start_of_month = specified_date.replace(day=1)
            start_date = datetime.datetime.strftime(start_of_month, '%Y-%m-%d')
            end_date = datetime.datetime.strftime(specified_date, '%Y-%m-%d')
            month_val = datetime.datetime.strftime(specified_date, '%m')
        elif type == "Week":
            specified_date = datetime.datetime.now().date()
            start_of_week = specified_date - datetime.timedelta(days=7)
            start_date = datetime.datetime.strftime(start_of_week, '%Y-%m-%d')
            end_date = datetime.datetime.strftime(specified_date, '%Y-%m-%d')
            month_val = datetime.datetime.strftime(specified_date, '%m')
        elif type == "Month":
            date=Month
            datestructure = date.replace(" ", "").split("-")
            final_month = f"{datestructure[0]}-{str(datestructure[1]).zfill(2)}"
            start_month = f"{final_month}-01"
            startd_date = datetime.datetime.strptime(start_month, "%Y-%m-%d")
            endd_date = startd_date + datetime.timedelta(days=31)
            start_date = datetime.datetime.strftime(startd_date, '%Y-%m-%d')
            end_date = datetime.datetime.strftime(endd_date, '%Y-%m-%d')
            month_val = datetime.datetime.strftime(endd_date, '%m')
        elif type == "Year":
            date = Year
            endd_date =f'{date}-12-31'
            startd_date = f'{date}-01-01'
            format_data = "%Y-%m-%d"
            end_date=datetime.datetime.strftime(datetime.datetime.strptime(endd_date,format_data), format_data)
            start_date=datetime.datetime.strftime(datetime.datetime.strptime(startd_date,format_data), format_data)

        console_logger.debug(start_date)
        console_logger.debug(end_date)

        if "RecieptCoalQualityAnalysis" not in collectionList:
            RCA = gmrDB.create_collection("RecieptCoalQualityAnalysis")
        else:
            RCA = gmrDB.get_collection("RecieptCoalQualityAnalysis")

        listData = []
        rr_no_values = {}
        fetchRCAQualityRoad = RecieptCoalQualityAnalysis.objects(plant_analysis_date__gte=datetime.datetime.strptime(start_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), plant_analysis_date__lte=datetime.datetime.strptime(end_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), mode="Road")
        fetchRCAQualityRail = RecieptCoalQualityAnalysis.objects(plant_analysis_date__gte=datetime.datetime.strptime(start_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), plant_analysis_date__lte=datetime.datetime.strptime(end_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), mode="Rail")
        
        for single_data_train in fetchRCAQualityRail:
            console_logger.debug(single_data_train.sample_id)
            console_logger.debug(single_data_train.mine)
            rrNo = single_data_train.sample_id
            filter = {
            'mine':single_data_train.mine,
            'plant_analysis_date': {
                '$gte': datetime.datetime.now(datetime.timezone.utc).replace(day=1,month=int(month_val),hour=0,minute=0,second=0,microsecond=0),
                # '$lte':end_day,
            }, }

            df = pl.DataFrame(list(RCA.find(filter=filter)))
            
            df = df.with_columns([
                pl.lit(0.0).alias("cum_wt"),
                pl.lit(0.0).alias("weighted_gcv"),
                pl.lit(0.0).alias("cum_weighted_gcv"),
                pl.lit(0.0).alias("gcv")
            ])

            if "sample_qty" in df.columns:
                if df["sample_qty"][0]:
                    df = df.with_columns([
                        pl.lit(float(df["sample_qty"][0])).alias('cum_wt')
                    ])
                    
                    # initialize data to previous values    
                    cum_wt_list = [float(df['sample_qty'][0])]
                    weighted_gcv_list = [float(df['sample_qty'][0]) * float(df['plant_arb_gcv'][0])]
                    cum_weighted_gcv_list = [weighted_gcv_list[0]]
                    gcv_list = [cum_weighted_gcv_list[0]/cum_wt_list[0]]
                    
                    for i in range(1, len(df)):
                        
                        cum_wt_total = float(df['sample_qty'][i]) + cum_wt_list[i-1]
                        weighted_gcv_total = float(df['sample_qty'][i]) * float(df['plant_arb_gcv'][i])
                        cum_weighted_gcv_total = float(weighted_gcv_total) + cum_weighted_gcv_list[i-1]
                        gcv_total = cum_weighted_gcv_total/cum_wt_total
                        
                        cum_wt_list.append(cum_wt_total)
                        weighted_gcv_list.append(weighted_gcv_total)
                        cum_weighted_gcv_list.append(cum_weighted_gcv_total)
                        gcv_list.append(gcv_total)
                    
                    df = df.with_columns(
                        pl.Series('cum_wt', cum_wt_list),
                        pl.Series('weighted_gcv', weighted_gcv_list),
                        pl.Series('cum_weighted_gcv',cum_weighted_gcv_list),
                        pl.Series('gcv', gcv_list)
                    )
                    rr_no_values[single_data_train.mine] = gcv_list[-1]
                    # df.write_excel("gcv_calculation.xlsx")

        for single_data_raod in fetchRCAQualityRoad:
            rrNo = single_data_raod.sample_id
            filter = {
            'type_consumer':single_data_raod.type_consumer,
            'plant_analysis_date': {
                '$gte': datetime.datetime.now(datetime.timezone.utc).replace(day=1,month=int(month_val),hour=0,minute=0,second=0,microsecond=0),
                # '$lte':end_day,
            }, }

            df = pl.DataFrame(list(RCA.find(filter=filter)))
            
            df = df.with_columns([
                pl.lit(0.0).alias("cum_wt"),
                pl.lit(0.0).alias("weighted_gcv"),
                pl.lit(0.0).alias("cum_weighted_gcv"),
                pl.lit(0.0).alias("gcv")
            ])
            if "sample_qty" in df.columns:
                if df["sample_qty"][0]:
                    df = df.with_columns([
                        pl.lit(float(df["sample_qty"][0])).alias('cum_wt')
                    ])
                    
                    # initialize data to previous values    
                    cum_wt_list = [float(df['sample_qty'][0])]
                    weighted_gcv_list = [float(df['sample_qty'][0]) * float(df['plant_arb_gcv'][0])]
                    cum_weighted_gcv_list = [weighted_gcv_list[0]]
                    gcv_list = [cum_weighted_gcv_list[0]/cum_wt_list[0]]
                    
                    for i in range(1, len(df)):
                        cum_wt_total = float(df['sample_qty'][i]) + cum_wt_list[i-1]
                        weighted_gcv_total = float(df['sample_qty'][i]) * float(df['plant_arb_gcv'][i])
                        cum_weighted_gcv_total = float(weighted_gcv_total) + cum_weighted_gcv_list[i-1]
                        gcv_total = cum_weighted_gcv_total/cum_wt_total

                        cum_wt_list.append(cum_wt_total)
                        weighted_gcv_list.append(weighted_gcv_total)
                        cum_weighted_gcv_list.append(cum_weighted_gcv_total)
                        gcv_list.append(gcv_total)
                    
                    df = df.with_columns(
                        pl.Series('cum_wt', cum_wt_list),
                        pl.Series('weighted_gcv', weighted_gcv_list),
                        pl.Series('cum_weighted_gcv',cum_weighted_gcv_list),
                        pl.Series('gcv', gcv_list)
                    )
                    rr_no_values[single_data_raod.type_consumer] = gcv_list[-1]
                    # df.write_excel("gcv_calculation.xlsx")
                    
        aopList = []
        fetchAopTarget = AopTarget.objects()
        if fetchAopTarget:
            for single_aop_target in fetchAopTarget:
                aopList.append(single_aop_target.payload())

        target_dict = {item['source_name']: int(item['aop_target']) for item in aopList}
        
        values_dictData = [x if x is not None else 'None' for x in rr_no_values.keys()]

        aligned_target_data = [target_dict.get(label.strip(), 0) for label in values_dictData]

        if None in rr_no_values.keys():
            # changing keys of dictionary
            rr_no_values['None'] = rr_no_values[None]
            del rr_no_values[None]

        result = {
                "data": {
                    "labels": list(rr_no_values.keys()),
                    "datasets": [
                        {"label": "Mine", "data": [data for data in list(rr_no_values.values())], "order": 1, "type": "bar"},
                        {"label": "Target", "data": aligned_target_data, "order": 0, "type": "line"},
                    ],
                }
            }

        return result
        
    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/source_wise_profit_loss", tags=["Road Map"])
def source_wise_transist_loss_gain(response: Response, type: Optional[str] = "Daily", Month: Optional[str] = None, Daily: Optional[str] = None, Year: Optional[str] = None):
    try:

        if type == "Daily":
            specified_date = datetime.datetime.strptime(Daily, "%Y-%m-%d")
            start_of_month = specified_date.replace(day=1)
            start_date = datetime.datetime.strftime(start_of_month, '%Y-%m-%d')
            end_date = datetime.datetime.strftime(specified_date, '%Y-%m-%d')
        elif type == "Week":
            specified_date = datetime.datetime.now().date()
            start_of_week = specified_date - datetime.timedelta(days=7)
            start_date = datetime.datetime.strftime(start_of_week, '%Y-%m-%d')
            end_date = datetime.datetime.strftime(specified_date, '%Y-%m-%d')
        elif type == "Month":
            date=Month
            datestructure = date.replace(" ", "").split("-")
            final_month = f"{datestructure[0]}-{str(datestructure[1]).zfill(2)}"
            start_month = f"{final_month}-01"
            startd_date = datetime.datetime.strptime(start_month, "%Y-%m-%d")
            endd_date = startd_date + datetime.timedelta(days=31)
            start_date = datetime.datetime.strftime(startd_date, '%Y-%m-%d')
            end_date = datetime.datetime.strftime(endd_date, '%Y-%m-%d')
        elif type == "Year":
            date=Year
            endd_date =f'{date}-12-31'
            startd_date = f'{date}-01-01'
            format_data = "%Y-%m-%d"
            end_date=datetime.datetime.strftime(datetime.datetime.strptime(endd_date,format_data), format_data)
            start_date=datetime.datetime.strftime(datetime.datetime.strptime(startd_date,format_data), format_data)

        net_qty_totals = {}
        actual_net_qty_totals = {}

        fetchGmrData = Gmrdata.objects(created_at__gte=datetime.datetime.strptime(start_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), created_at__lte=datetime.datetime.strptime(end_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"))

        # Iterate over the retrieved data
        for single_gmr_data in fetchGmrData:
            mine_name = single_gmr_data.mine
            net_qty = single_gmr_data.net_qty
            actual_net_qty = single_gmr_data.actual_net_qty
            
            # Update net_qty totals dictionary
            if mine_name in net_qty_totals:
                net_qty_totals[mine_name] += float(net_qty)
            else:
                net_qty_totals[mine_name] = float(net_qty)
            if actual_net_qty:
                # Update actual_net_qty totals dictionary
                if mine_name in actual_net_qty_totals:
                    actual_net_qty_totals[mine_name] += float(actual_net_qty)
                else:
                    actual_net_qty_totals[mine_name] = float(actual_net_qty)

        # Perform clubbing - subtract actual_net_qty from net_qty for each mine
        clubbed_data = {}
        
        for mine in net_qty_totals:
            clubbed_data[mine] = actual_net_qty_totals.get(mine, 0) - net_qty_totals[mine]
        
        result = {
            "data": {
                "labels": list(clubbed_data.keys()),
                "datasets": [
                    {"label": "Mine", "data": list(clubbed_data.values()), "order": 1, "type": "bar"},
                ],
            }
        }

        return result

    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e
    

@router.get("/month_wise_transit_loss", tags=["Road Map"])
def month_wise_transist_loss_gain_mode(response: Response):
    try:
        result = {
            "labels": [],
            "datasets": [],
            "weight_total": [],
            "total": 0,
            "page_size": 15,
        }

        financial_year = get_financial_year(datetime.date.today().strftime("%Y-%m-%d"))

        logs = (
            Gmrdata.objects(vehicle_in_time__gte=financial_year.get("start_date"), vehicle_in_time__lte=financial_year.get("end_date"))
            .order_by("mine", "arv_cum_do_number", "-created_at")
        )
        if any(logs):
            aggregated_data = defaultdict(
                lambda: defaultdict(
                    lambda: {
                        "net_qty": 0,
                        "mine_name": "",
                        "actual_net_qty": 0,
                        "count": 0,
                    }
                )
            )

            start_dates = {}
            for log in logs:
                if log.vehicle_in_time!=None:
                    month = log.vehicle_in_time.strftime("%Y-%m")
                    payload = log.payload()
                    result["labels"] = list(payload.keys())
                    mine_name = payload.get("Mines_Name")
                    do_no = payload.get("DO_No")
                    # If start_date is None or the current GWEL_Tare_Time is earlier than start_date, update start_date
                    if do_no not in start_dates:
                        start_dates[do_no] = month
                    elif month < start_dates[do_no]:
                        start_dates[do_no] = month
                    if payload.get("GWEL_Net_Wt(MT)") and payload.get("GWEL_Net_Wt(MT)") != "NaN":
                        aggregated_data[month][do_no]["actual_net_qty"] += float(
                            payload["GWEL_Net_Wt(MT)"]
                        )
                    else:
                        aggregated_data[month][do_no]["actual_net_qty"] = 0
                    if payload.get("Challan_Net_Wt(MT)") and payload.get("Challan_Net_Wt(MT)") != "NaN":
                        aggregated_data[month][do_no]["net_qty"] += float(
                            payload.get("Challan_Net_Wt(MT)")
                        )
                    else:
                        aggregated_data[month][do_no]["net_qty"] = 0
                    if payload.get("Mines_Name"):
                        aggregated_data[month][do_no]["mine_name"] = payload[
                            "Mines_Name"
                        ]
                    else:
                        aggregated_data[month][do_no]["mine_name"] = "-"
                    aggregated_data[month][do_no]["count"] += 1 
            dataList = [
                {
                    "month": month,
                    "data": {
                        do_no: {
                            "final_net_qty": data["actual_net_qty"]-data["net_qty"],
                            "mine_name": data["mine_name"],
                            "month": month,
                        }
                        for do_no, data in aggregated_data[month].items()
                    },
                }
                for month in aggregated_data
            ]
            total_monthly_final_net_qty = {}
            yearly_final_data = {}
            for data in dataList:
                month = data["month"]

                total_monthly_final_net_qty[month] = 0

                for entry in data["data"].values():
                    total_monthly_final_net_qty[month] += entry["final_net_qty"]

            total_monthly_final_net = dict(sorted(total_monthly_final_net_qty.items()))
            # for key, single_count in total_monthly_final_net_qty.items():
            #     if datetime.datetime.strptime(key, "%Y-%m").year in yearly_final_data:
            #         yearly_final_data[datetime.datetime.strptime(key, "%Y-%m").year] += single_count
            #     else:
            #         yearly_final_data[datetime.datetime.strptime(key, "%Y-%m").year] = single_count
                    
            # yearly_final_data_sort = dict(sorted(yearly_final_data.items()))

        result = {
            "data": {
                "labels": list(total_monthly_final_net.keys()),
                "datasets": [
                    {"label": "month", "data": list(total_monthly_final_net.values())}
                ]
            }
        }

        return result
    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/overall_transit_loss", tags=["Road Map"])
def year_wise_transist_loss_gain_mode(response: Response):
    try:
        result = {
            "labels": [],
            "datasets": [],
            "weight_total": [],
            "total": 0,
            "page_size": 15,
        }

        financial_year = get_financial_year(datetime.date.today().strftime("%Y-%m-%d"))

        logs = (
            Gmrdata.objects(vehicle_in_time__gte=financial_year.get("start_date"), vehicle_in_time__lte=financial_year.get("end_date"))
            .order_by("mine", "arv_cum_do_number", "-created_at")
        )
        if any(logs):
            aggregated_data = defaultdict(
                lambda: defaultdict(
                    lambda: {
                        "net_qty": 0,
                        "mine_name": "",
                        "actual_net_qty": 0,
                        "count": 0,
                    }
                )
            )

            start_dates = {}
            for log in logs:
                if log.vehicle_in_time!=None:
                    month = log.vehicle_in_time.strftime("%Y-%m")
                    payload = log.payload()
                    result["labels"] = list(payload.keys())
                    mine_name = payload.get("Mines_Name")
                    do_no = payload.get("DO_No")
                    # If start_date is None or the current GWEL_Tare_Time is earlier than start_date, update start_date
                    if do_no not in start_dates:
                        start_dates[do_no] = month
                    elif month < start_dates[do_no]:
                        start_dates[do_no] = month
                    if payload.get("GWEL_Net_Wt(MT)") and payload.get("GWEL_Net_Wt(MT)") != "NaN":
                        aggregated_data[month][do_no]["actual_net_qty"] += float(
                            payload["GWEL_Net_Wt(MT)"]
                        )
                    else:
                        aggregated_data[month][do_no]["actual_net_qty"] = 0
                    if payload.get("Challan_Net_Wt(MT)") and payload.get("Challan_Net_Wt(MT)") != "NaN":
                        aggregated_data[month][do_no]["net_qty"] += float(
                            payload.get("Challan_Net_Wt(MT)")
                        )
                    else:
                        aggregated_data[month][do_no]["net_qty"] = 0
                    if payload.get("Mines_Name"):
                        aggregated_data[month][do_no]["mine_name"] = payload[
                            "Mines_Name"
                        ]
                    else:
                        aggregated_data[month][do_no]["mine_name"] = 0
                    aggregated_data[month][do_no]["count"] += 1 

            dataList = [
                {
                    "month": month,
                    "data": {
                        do_no: {
                            "final_net_qty": data["actual_net_qty"]-data["net_qty"],
                            "mine_name": data["mine_name"],
                            "month": month,
                        }
                        for do_no, data in aggregated_data[month].items()
                    },
                }
                for month in aggregated_data
            ]

            total_monthly_final_net_qty = {}
            yearly_final_data = {}
            for data in dataList:
                month = data["month"]

                total_monthly_final_net_qty[month] = 0

                for entry in data["data"].values():
                    total_monthly_final_net_qty[month] += entry["final_net_qty"]

            # total_monthly_final_net = dict(sorted(total_monthly_final_net_qty.items()))

            for key, single_count in total_monthly_final_net_qty.items():
                if datetime.datetime.strptime(key, "%Y-%m").year in yearly_final_data:
                    yearly_final_data[datetime.datetime.strptime(key, "%Y-%m").year] += single_count
                else:
                    yearly_final_data[datetime.datetime.strptime(key, "%Y-%m").year] = single_count
                    
            yearly_final_data_sort = dict(sorted(yearly_final_data.items()))

            result = {
                "data": {
                    "labels": ["Road Mode"],
                    "datasets": [
                        {"label": "Road Mode", "data": list(yearly_final_data_sort.values())}
                    ]
                }
            }

            return result
    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

@router.get("/transit_loss", tags=["Road Map"])
def endpoint_to_fetch_transit_loss(response: Response, type: Optional[str] = "Daily", 
                                   Daily: Optional[str] = None, 
                                   Month: Optional[str] = None,
                                   Year: Optional[str] = None,
                                   Overall: Optional[str] = None):
    try:
        data = {}
        timezone = pytz.timezone('Asia/Kolkata')
        current_time = datetime.datetime.now(timezone)
        UTC_OFFSET_TIMEDELTA = datetime.datetime.utcnow() - datetime.datetime.now()

        basePipeline = [
            {
                "$match": {
                        "GWEL_Tare_Time": {
                            "$gte": None,
                        },
                },
            },
            {
                '$project': {
                    'ts': None,
                    'actual_net_qty': {
                        '$toDouble': '$actual_net_qty'
                    }, 
                    'net_qty': {
                        '$toDouble': '$net_qty'
                    }, 
                    'label': {
                        '$cond': {
                            'if': {
                                '$ne': [
                                    '$vehicle_number', None
                                ]
                            }, 
                            'then': 'Road', 
                            'else': 'Rail'
                        }
                    }, 
                    '_id': 0
                }
            }, {
                '$group': {
                    '_id': {
                        'ts': '$ts', 
                        'label': '$label'
                    }, 
                    'actual_net_qty_sum': {
                        '$sum': '$actual_net_qty'
                    }, 
                    'net_qty_sum': {
                        '$sum': '$net_qty'
                    }
                }
            }, {
                '$project': {
                    '_id': 0, 
                    'ts': '$_id.ts', 
                    'label': '$_id.label', 
                    'data': {
                        '$subtract': [
                            '$actual_net_qty_sum', '$net_qty_sum'
                        ]
                    }
                }
            }
        ]

        if type == "Daily":
            date = Daily
            end_date = f'{date} 23:59:59'
            start_date = f'{date} 00:00:00'
            format_data = "%Y-%m-%d %H:%M:%S"
            endd_date = convert_to_utc_format(end_date.__str__(), format_data)
            startd_date = convert_to_utc_format(start_date.__str__(), format_data)

            basePipeline[0]["$match"]["GWEL_Tare_Time"]["$lte"] = endd_date
            basePipeline[0]["$match"]["GWEL_Tare_Time"]["$gte"] = startd_date

            basePipeline[1]["$project"]["ts"] = {"$hour": {"date": "$GWEL_Tare_Time", "timezone": "Asia/Kolkata"}}

            result = {
                "data": {
                    "labels": [str(i) for i in range(1, 25)],
                    "datasets": [
                        {"label": "Road", "data": [0 for i in range(1, 25)]},
                        {"label": "Rail", "data": [0 for i in range(1, 25)]},
                    ],
                }
            }

        elif type == "Week":
            start_date = (
                datetime.datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)
                - datetime.timedelta(days=7)
            )
            end_date = datetime.datetime.utcnow().replace(hour=23, minute=59, second=59, microsecond=0)
            endd_date = end_date-datetime.timedelta(days=1)

            basePipeline[0]["$match"]["GWEL_Tare_Time"]["$gte"] = convert_to_utc_format(start_date.__str__(), "%Y-%m-%d %H:%M:%S")
            basePipeline[1]["$project"]["ts"] = {"$dayOfMonth": {"date": "$GWEL_Tare_Time", "timezone": "Asia/Kolkata"}}
            result = {
                "data": {
                    "labels": [
                        (
                         convert_to_utc_format(start_date.__str__(),"%Y-%m-%d %H:%M:%S") + datetime.timedelta(days=i+1)
                        ).strftime("%d")
                        for i in range(1, 8)
                    ],
                    "datasets": [
                        {"label": "Road", "data": [0 for i in range(7)]},
                        {"label": "Rail", "data": [0 for i in range(7)]},
                    ],
                }
            }
        elif type == "Month":
            date = Month
            format_data = "%Y - %m-%d"
            start_date = f'{date}-01'
            startd_date = timezone.localize(datetime.datetime.strptime(start_date, format_data))

            end_date = startd_date + relativedelta(day=31)
            end_label = end_date.strftime("%d")

            basePipeline[0]["$match"]["GWEL_Tare_Time"]["$lte"] = end_date
            basePipeline[0]["$match"]["GWEL_Tare_Time"]["$gte"] = startd_date
            basePipeline[1]["$project"]["ts"] = {"$dayOfMonth": {"date": "$GWEL_Tare_Time", "timezone": "Asia/Kolkata"}}
            result = {
                "data": {
                    "labels": [
                        (
                            startd_date + datetime.timedelta(days=i)
                        ).strftime("%d")
                        for i in range(int(end_label))
                    ],
                    "datasets": [
                        {"label": "Road", "data": [0 for i in range(int(end_label))]},
                        {"label": "Rail", "data": [0 for i in range(int(end_label))]},
                    ],
                }
            }

        elif type == "Year":
            date = Year
            end_date = f'{date}-12-31 23:59:59'
            start_date = f'{date}-01-01 00:00:00'
            format_data = "%Y-%m-%d %H:%M:%S"
            endd_date = timezone.localize(datetime.datetime.strptime(end_date, format_data))
            startd_date = timezone.localize(datetime.datetime.strptime(start_date, format_data))

            basePipeline[0]["$match"]["GWEL_Tare_Time"]["$lte"] = endd_date
            basePipeline[0]["$match"]["GWEL_Tare_Time"]["$gte"] = startd_date

            basePipeline[1]["$project"]["ts"] = {"$month": {"date": "$GWEL_Tare_Time", "timezone": "Asia/Kolkata"}}

            result = {
                "data": {
                    "labels": [
                        (
                            startd_date + relativedelta(months=i)
                        ).strftime("%b %y")
                        for i in range(12)
                    ],
                    "datasets": [
                        {"label": "Road", "data": [0 for i in range(12)]},
                        {"label": "Rail", "data": [0 for i in range(12)]},
                    ],
                }
            }
        elif type == "Overall":
            date = Overall
            end_date = f'{int(date) + 1}-03-31 23:59:59'
            start_date = f'{date}-04-01 00:00:00'
            format_data = "%Y-%m-%d %H:%M:%S"
            endd_date = timezone.localize(datetime.datetime.strptime(end_date, format_data))
            startd_date = timezone.localize(datetime.datetime.strptime(start_date, format_data))

            basePipeline[0]["$match"]["GWEL_Tare_Time"]["$lte"] = endd_date
            basePipeline[0]["$match"]["GWEL_Tare_Time"]["$gte"] = startd_date

            basePipeline[1]["$project"]["ts"] = {"$year": {"date": "$GWEL_Tare_Time", "timezone": "Asia/Kolkata"}}

            result = {
                "data": {
                    "labels": [f"{int(date)} - {int(date) + 1}"],
                    "datasets": [
                        {"label": "Road", "data": [0]},
                        {"label": "Rail", "data": [0]},
                    ],
                }
            }

        output = Gmrdata.objects().aggregate(basePipeline)
        outputDict = {}

        if type == "Overall":
            modified_labels = [f"{int(date)} - {int(date) + 1}"]
            for data in output:
                label = data["label"]
                total_loss = data["data"]
                outputDict[label] = total_loss

            for dataset in result["data"]["datasets"]:
                label = dataset["label"]
                if label in outputDict:
                    dataset["data"] = [outputDict[label]]
        else:
            for data in output:
                ts = data["ts"]
                label = data["label"]
                sum_value = data["data"]
                if ts not in outputDict:
                    outputDict[ts] = {label: sum_value}
                else:
                    if label not in outputDict[ts]:
                        outputDict[ts][label] = sum_value
                    else:
                        outputDict[ts][label] += sum_value

            for index, label in enumerate(result["data"]["labels"]):

                if type == "Daily":
                    modified_labels = [str(i) for i in range(1, 25)]

                elif type == "Week":
                    modified_labels = [
                        (
                            start_date + datetime.timedelta(days=i+1)
                        ).strftime("%d-%m-%Y,%a")
                        for i in range(7)
                    ]

                elif type == "Month":
                    modified_labels = [
                        (
                            startd_date + datetime.timedelta(days=i + 1)
                        ).strftime("%d-%b")
                        for i in range(-1, (int(end_label))-1)
                    ]

                elif type == "Year":
                    modified_labels = [
                        (
                            startd_date + relativedelta(months=i)
                        ).strftime("%b %y")
                        for i in range(12)
                    ]
                if type == "Year":
                    ts = index
                else:
                    ts = int(label)

                if ts in outputDict:
                    for key, val in outputDict[ts].items():
                        if type == "Year":
                            if key == "Road":
                                result["data"]["datasets"][0]["data"][index-1] = round(val, 2)
                            elif key == "Rail":
                                result["data"]["datasets"][1]["data"][index-1] = round(val, 2)
                        else:
                            if key == "Road":
                                result["data"]["datasets"][0]["data"][index] = round(val, 2)
                            elif key == "Rail":
                                result["data"]["datasets"][1]["data"][index] = round(val, 2)

        result["data"]["labels"] = copy.deepcopy(modified_labels)
        return result

    except Exception as e:
        console_logger.debug("----- Overall Transit Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

@router.post("/add/sap/excel", tags=["Coal Testing"])
async def endpoint_to_add_sap_excel_data(response: Response, file: UploadFile = File(...)):
    try:
        if file is None:
            return {"error": "No file Uploaded!"}
        
        contents = await file.read()
        if not contents:
            response.status_code = 400
            return {"error": "Uploaded file is empty!"}

        if file.filename.endswith(".xlsx"):
            # file saving start
            date = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{date}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)
            file_extension = file.filename.split(".")[-1]
            file_name = f'coallab_sap_manual_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.{file_extension}'
            full_path = os.path.join(os.getcwd(), target_directory, file_name)
            with open(full_path, "wb") as file_object:
                file_object.write(contents)
            # file saving end

            excel_data = pd.read_excel(BytesIO(contents))
            data_excel_fetch = json.loads(excel_data.to_json(orient="records"))
            for single_data in data_excel_fetch:
                # console_logger.debug(single_data)
                try:
                    fetchSapRecords = SapRecords.objects.get(do_no=str(single_data["DO No"]))
                    fetchSapRecords.sap_po = str(single_data["SAP PO"]) if single_data["SAP PO"] else None
                    fetchSapRecords.do_date = str(datetime.datetime.fromtimestamp(single_data["SAP PO Date"] / 1000).strftime("%Y-%m-%d")) if single_data["SAP PO Date"] else None
                    fetchSapRecords.line_item = str(single_data["Line Item"]) if single_data["Line Item"] else None
                    fetchSapRecords.save()
                except DoesNotExist as e:
                    add_data_excel = SapRecords(
                        sap_po=str(single_data["SAP PO"]) if single_data["SAP PO"] else None,
                        do_date=str(datetime.datetime.fromtimestamp(single_data["SAP PO Date"] / 1000).strftime("%Y-%m-%d")) if single_data["SAP PO Date"] else None,
                        line_item=str(single_data["Line Item"]) if single_data["Line Item"] else None,
                        do_no=str(single_data["DO No"]) if single_data["DO No"] else None,
                    )
                    add_data_excel.save()

                # # take it here
                fetchGmrData = Gmrdata.objects(arv_cum_do_number = str(single_data["DO No"]))
                for single_gmr_data in fetchGmrData:
                    single_gmr_data.po_no = str(single_data["SAP PO"]) if single_data["SAP PO"] else None
                    single_gmr_data.line_item = str(single_data["Line Item"]) if single_data["Line Item"] else None
                    single_gmr_data.po_date = str(datetime.datetime.fromtimestamp(single_data["SAP PO Date"] / 1000).strftime("%Y-%m-%d")) if single_data["SAP PO Date"] else None
                    single_gmr_data.save()

        return {"detail": "success"}
    except KeyError as e:
        raise HTTPException(status_code=404, detail="Key Error")
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- Sap Excel Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

@router.post("/add/saprcr/excel", tags=["Coal Testing"])
async def endpoint_to_add_sap_excel_data(response: Response, file: UploadFile = File(...)):
    try:
        if file is None:
            return {"error": "No file Uploaded!"}
        
        contents = await file.read()
        if not contents:
            response.status_code = 400
            return {"error": "Uploaded file is empty!"}

        if file.filename.endswith(".xlsx"):
            # file saving start
            date = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{date}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)
            file_extension = file.filename.split(".")[-1]
            file_name = f'rail_saprcrrail_manual_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.{file_extension}'
            full_path = os.path.join(os.getcwd(), target_directory, file_name)
            with open(full_path, "wb") as file_object:
                file_object.write(contents)
            # file saving end

            excel_data = pd.read_excel(BytesIO(contents))
            data_excel_fetch = json.loads(excel_data.to_json(orient="records"))
            for single_data in data_excel_fetch:
                try:
                    fetchSapRecords = sapRecordsRCR.objects.get(rr_no=str(single_data["RR No"]))
                    fetchSapRecords.sap_po = str(single_data["SAP PO"]) if single_data["SAP PO"] else None
                    fetchSapRecords.rr_date = str(datetime.datetime.fromtimestamp(single_data["SAP PO Date"] / 1000).strftime("%Y-%m-%d")) if single_data["SAP PO Date"] else None
                    fetchSapRecords.line_item = str(single_data["Line Item"]) if single_data["Line Item"] else None
                    fetchSapRecords.save()
                except DoesNotExist as e:
                    add_data_excel = sapRecordsRCR(
                        sap_po=str(single_data["SAP PO"]) if single_data["SAP PO"] else None,
                        rr_date=str(datetime.datetime.fromtimestamp(single_data["SAP PO Date"] / 1000).strftime("%Y-%m-%d")) if single_data["SAP PO Date"] else None,
                        line_item=str(single_data["Line Item"]) if single_data["Line Item"] else None,
                        rr_no=str(single_data["RR No"]) if single_data["RR No"] else None,
                    )
                    add_data_excel.save()
            
                fetchGmrData = RcrData.objects(rr_no = str(single_data["RR No"]))
                for single_gmr_data in fetchGmrData:
                    single_gmr_data.po_no = str(single_data["SAP PO"]) if single_data["SAP PO"] else None
                    single_gmr_data.line_item = str(single_data["Line Item"]) if single_data["Line Item"] else None
                    single_gmr_data.po_date = str(datetime.datetime.fromtimestamp(single_data["SAP PO Date"] / 1000).strftime("%Y-%m-%d")) if single_data["SAP PO Date"] else None
                    single_gmr_data.save()

        return {"detail": "success"}
    except KeyError as e:
        raise HTTPException(status_code=404, detail="Key Error")
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- Sap Excel Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/add/saproadrcr/excel", tags=["Coal Testing"])
async def endpoint_to_add_sap_excel_data(response: Response, file: UploadFile = File(...)):
    try:
        if file is None:
            return {"error": "No file Uploaded!"}
        
        contents = await file.read()
        if not contents:
            response.status_code = 400
            return {"error": "Uploaded file is empty!"}

        if file.filename.endswith(".xlsx"):
            # file saving start
            date = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{date}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)
            file_extension = file.filename.split(".")[-1]
            file_name = f'coallab_saproadrcr_manual_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.{file_extension}'
            full_path = os.path.join(os.getcwd(), target_directory, file_name)
            with open(full_path, "wb") as file_object:
                file_object.write(contents)
            # file saving end

            excel_data = pd.read_excel(BytesIO(contents))
            data_excel_fetch = json.loads(excel_data.to_json(orient="records"))
            for single_data in data_excel_fetch:
                console_logger.debug(single_data)
                try:
                    fetchSapRecords = SapRecordsRcrRoad.objects.get(do_no=str(single_data["DO No"]))
                except DoesNotExist as e:
                    add_data_excel = SapRecordsRcrRoad(
                        # slno=single_data["Slno"] if single_data["Slno"] else None,
                        # source=single_data["source"],
                        # mine_name=single_data["Mines Name"],
                        sap_po=str(single_data["SAP PO"]) if single_data["SAP PO"] else None,
                        do_date=str(single_data["SAP PO Date"]) if single_data["SAP PO Date"] else None,
                        line_item=str(single_data["Line Item"]) if single_data["Line Item"] else None,
                        do_no=str(single_data["DO No"]) if single_data["DO No"] else None,
                        # do_qty=str(single_data["DO QTY"]),
                        # rake_no=single_data["DO/RR Qty"],
                        # start_date=single_data["DO Start Date"],
                        # end_date=single_data["DO End Date"],
                        # grade=single_data["Grade"]
                    )
                    add_data_excel.save()

                # # take it here
                fetchGmrData = RcrRoadData.objects(do_number = str(single_data["DO No"]))
                for single_gmr_data in fetchGmrData:
                    single_gmr_data.sap_po = str(single_data["SAP PO"]) if single_data["SAP PO"] else None
                    single_gmr_data.line_item = str(single_data["Line Item"]) if single_data["Line Item"] else None
                    single_gmr_data.po_date = str(single_data["SAP PO Date"]) if single_data["SAP PO Date"] else None
                    # single_gmr_data.slno = str(single_data["Slno"]) if single_data["Slno"] else None
                    single_gmr_data.save()

        return {"detail": "success"}
    except KeyError as e:
        raise HTTPException(status_code=404, detail="Key Error")
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- Sap Excel Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/add/railsap/excel", tags=["Coal Testing"])
async def endpoint_to_add_sap_excel_data(response: Response, file: UploadFile = File(...)):
    try:
        if file is None:
            return {"error": "No file Uploaded!"}
        
        contents = await file.read()
        if not contents:
            return {"error": "Uploaded file is empty!"}

        if file.filename.endswith(".xlsx"):
            # file saving start
            date = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{date}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)
            file_extension = file.filename.split(".")[-1]
            file_name = f'rail_sap_manual_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.{file_extension}'
            full_path = os.path.join(os.getcwd(), target_directory, file_name)
            with open(full_path, "wb") as file_object:
                file_object.write(contents)
            # file saving end

            excel_data = pd.read_excel(BytesIO(contents))
            data_excel_fetch = json.loads(excel_data.to_json(orient="records"))
            for single_data in data_excel_fetch:
                try:
                    fetchRailSapRecords = sapRecordsRail.objects.get(rr_no=str(single_data["RR No"]))
                    fetchRailSapRecords.sap_po = str(single_data["SAP PO"]) if single_data["SAP PO"] else None
                    fetchRailSapRecords.do_date = str(datetime.datetime.fromtimestamp(single_data["SAP PO Date"] / 1000).strftime("%Y-%m-%d")) if single_data["SAP PO Date"] else None
                    fetchRailSapRecords.line_item = str(single_data["Line Item"]) if single_data["Line Item"] else None
                    fetchRailSapRecords.save()
                except DoesNotExist as e:
                    add_raildata_excel = sapRecordsRail(
                        sap_po=str(single_data["SAP PO"]) if single_data["SAP PO"] else None,
                        do_date=str(datetime.datetime.fromtimestamp(single_data["SAP PO Date"] / 1000).strftime("%Y-%m-%d")) if single_data["SAP PO Date"] else None,
                        line_item=str(single_data["Line Item"]) if single_data["Line Item"] else None,
                        rr_no=str(single_data["RR No"]) if single_data["RR No"] else None,
                    )
                    add_raildata_excel.save()

                fetchRailGmrData = RailData.objects(rr_no = str(single_data["RR No"]))
                for single_rail_gmr_data in fetchRailGmrData:
                    single_rail_gmr_data.po_no = str(single_data["SAP PO"]) if single_data["SAP PO"] else None
                    single_rail_gmr_data.line_item = str(single_data["Line Item"]) if single_data["Line Item"] else None
                    single_rail_gmr_data.po_date = str(datetime.datetime.fromtimestamp(single_data["SAP PO Date"] / 1000).strftime("%Y-%m-%d")) if single_data["SAP PO Date"] else None
                    single_rail_gmr_data.save()

        return {"detail": "success"}
    except KeyError as e:
        raise HTTPException(status_code=404, detail="Key Error")
    except Exception as e:
        console_logger.debug("----- Sap Excel Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


def create_geofence(route_coordinates, tolerance_meters):
    try:
        # Constants
        meters_per_degree = 111_000  # Approximate value for the distance of 1 degree of latitude in meters
        tolerance_degrees = tolerance_meters / meters_per_degree

        # # Load the route coordinates from the JSON file
        # with open(route_file, 'r') as f:
        #     route_coordinates = json.load(f)

        # Convert the route coordinates into a LineString object
        geofenced_path = LineString(route_coordinates)

        # Create a buffer around the LineString to form a Polygon
        buffered_geofence = geofenced_path.buffer(tolerance_degrees)

        # Extract the exterior coordinates of the buffered polygon
        buffered_coordinates = list(buffered_geofence.exterior.coords)

        # Save the buffered coordinates to a new JSON file
        # output_file = route_file.replace('.json', '_buffered.json')
        # with open(output_file, 'w') as out_f:
            # json.dump(buffered_coordinates, out_f, indent=2)
        # with open(f'{Mine_Name}_geofence_coordinates.json', 'w') as json_file:
        #     json.dump(buffered_coordinates, json_file, indent=2)

        # print(f"Buffered geofence coordinates saved to '{Mine_Name}_geofence_coordinates.json'")
        return buffered_coordinates
    except Exception as e:
        console_logger.debug("----- Add edit lat long Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

def fetch_geofencing_data(name, latlong):
    try:
        tolerance_meters = 100  # Set the desired tolerance in meters

        # Replace 'YOUR_API_KEY' with your actual Google Maps API key
        gmaps = googlemaps.Client(key=os.getenv("GOOGLE_MAPS_KEY"))

        Mine_Name = name        #Mine Name

        # Define the starting and ending points using the provided coordinates
        # origin = "20.6853644, 79.3062271"            #Yekona Mines
        dataLatlong = latlong.split(',')
        origin = f"{dataLatlong[0]}, {dataLatlong[1]}"            #Yekona Mines
        destination = "20.2796104, 78.9765083"       #GMR Warora Energy Limited

        # Request directions
        directions_result = gmaps.directions(origin, destination)

        # Extract the polyline points from the directions result
        if directions_result:
            steps = directions_result[0]['legs'][0]['steps']
            polyline_points = [step['polyline']['points'] for step in steps]

            # Decode the polyline points to get the latitude and longitude
            route_coordinates = []
            for polyline_point in polyline_points:
                route_coordinates.extend(polyline.decode(polyline_point))

            fetch_geofence = create_geofence(route_coordinates, tolerance_meters)
            res_geofence = [list(ele) for ele in fetch_geofence]
            return res_geofence
        else:
            console_logger.debug("No route found.")
            return {"detail": "No data found"}

        
    except Exception as e:
        console_logger.debug("----- Add edit lat long Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

@router.post("/addeditlatlong", tags=["Map Data"])
def endpoint_to_add_lat_long(response: Response, payload: LatLongPostIn, id: str = None):
    try:
        dataName = payload.dict()
        if id:
            updateSchedulerData = SelectedLocation.objects(
                id=ObjectId(id),
            ).update(name=dataName.get("name"), latlong=dataName.get("latlong"), type=dataName.get("type"), geofence=dataName.get("geofencing"))
        else:
            selectedLocationData = SelectedLocation(name=dataName.get("name"), latlong=dataName.get("latlong"), type=dataName.get("type"), geofence=dataName.get("geofencing"))
            selectedLocationData.save()
        return {"detail": "success"}
    except Exception as e:
        console_logger.debug("----- Add edit lat long Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/fetch/geofence", tags=["Map Data"])
def endpoint_to_fetch_geofence(response: Response, name: str, latlang: str):
    try:
        getGeofencing = fetch_geofencing_data(name, latlang)
        return getGeofencing
    except Exception as e:
        console_logger.debug("----- Add edit lat long Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

    

@router.get("/getlatlong", tags=["Map Data"])
def endpoint_to_fetch_lat_long(response: Response):
    try:
        listData = []
        selectedData = SelectedLocation.objects()
        for single_data in selectedData:
            listData.append(single_data.payload())
        return listData
    except Exception as e:
        console_logger.debug("----- Get Lat Long Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e
    
# @router.post("/insert/geofence/json", tags=["Map Data"])
# async def endpoint_to_update_geofence_json(response: Response, name:str, json_file: Optional[UploadFile] = File(None)):
#     try:
#         console_logger.debug(json_file)
#         if json_file is None:
#             return {"error": "No file uploaded"}
        
#         contents = await json_file.read()

#         if not contents:
#             return {"error": "Uploaded file is empty"}
        
#         if not json_file.filename.endswith('.json'):
#             return {"error": "Uploaded file is not a JSON"}

#         try:
#             # Load JSON data from the file contents
#             fileJsonData = json.loads(contents)

#             # Check if the loaded data is a list of lists with numeric values
#             if not isinstance(fileJsonData, list):
#                 raise ValueError("JSON data is not a valid list")

#             for sublist in fileJsonData:
#                 if not isinstance(sublist, list):
#                     raise ValueError("JSON data does not contain lists of lists")
#                 for value in sublist:
#                     if not isinstance(value, (int, float)):
#                         raise ValueError("JSON data contains non-numeric values")

#         except (json.JSONDecodeError, ValueError) as e:
#             return {"error": str(e)}

#         console_logger.debug(fileJsonData)

#         updateSchedulerData = SelectedLocation.objects(
#                 name=name,
#         ).update(geofence=fileJsonData)

#         return {"detail": "success"}

#     except Exception as e:
#         console_logger.debug("----- Get Lat Long Error -----",e)
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#         console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#         return e

@router.delete("/deletelatlong", tags=["Map Data"])
def endpoint_to_delete_lat_long(response: Response, id: str):
    try:
        selectedLocationData = SelectedLocation.objects.get(id=id)
        selectedLocationData.delete()
        return {"detail": "success"}
    except DoesNotExist as e:
        return {"detail": "No data found"}
    except Exception as e:
        console_logger.debug("----- Delete Lat Long Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/getsaprecords", tags=["Sap Data"])
def endpoint_to_fetch_sap_records(response: Response, do_no: str):
    try:
        fetchSapRecords = SapRecords.objects.get(do_no=do_no)
        return fetchSapRecords.payload()
    except DoesNotExist as e:
        return {"detail": "No data found"}
    except Exception as e:
        console_logger.debug("----- Fetch Sap Records Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.delete("/deletesaprecord", tags=["Sap Data"])
def endpoint_to_fetch_sap_records(response: Response, do_no: str):
    try:
        fetchSapRecords = SapRecords.objects.get(do_no=do_no)
        fetchSapRecords.delete()
        return {"detail": "success"}
    except Exception as e:
        console_logger.debug("----- delete Sap Records Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


def list_report_name_pdf():
    try:
        checkPdfReportName = PdfReportName.objects()
        if checkPdfReportName:
            checkPdfReportName.delete()
        report_data = [
            {
                "report_id": 1,
                "name": "daily_coal_logistic_report",
            },
        ]
        for single_report_name in report_data:
            addreportName = PdfReportName(name=single_report_name["name"])
            addreportName.save()
        return {"detail": "success"}
    except Exception as e:
        console_logger.debug("----- List Report Name Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/addreportname", tags=["PDF Report"])
def endpoint_to_add_reportname():
    try:
        list_report_name_pdf()
        return {"detail": "success"}
    except Exception as e:
        console_logger.debug("----- Add Report Name Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

@router.post("/insert/reportname", tags=["PDF Report"])
def endpoint_to_insert_reportname(response: Response, name: str):
    try:
        if name:
            addreportName = PdfReportName(name=name)
            addreportName.save()

            addOnlyReportNameinReportScheduler = ReportScheduler(report_name=name)
            addOnlyReportNameinReportScheduler.save()
        return {"detail": "success"}
    except Exception as e:
        console_logger.debug("----- Add Report Name Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/fetch/reportname", tags=["PDF Report"])
def endpoint_to_fetch_report_name_extra():
    try:
        listData = []
        fetchAllPdfReportName = PdfReportName.objects()
        for single_report_name in fetchAllPdfReportName:
            listData.append(single_report_name.payload())
        return listData
    except Exception as e:
        console_logger.debug("----- Fetch Report Name Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


# @router.get("/coallabtestanalysis", tags=["Coal Testing"])
# def endpoint_to_fetch_report_name(response: Response,
#     type: Optional[str] = "Daily",
#     Month: Optional[str] = None, 
#     Daily: Optional[str] = None, 
#     Year: Optional[str] = None
# ):
#     try:
#         if type == "Daily":
#             specified_date = datetime.datetime.strptime(Daily, "%Y-%m-%d")
#             start_of_month = specified_date.replace(day=1)
#             start_date = datetime.datetime.strftime(start_of_month, '%Y-%m-%d')
#             end_date = datetime.datetime.strftime(specified_date, '%Y-%m-%d')
#         elif type == "Week":
#             specified_date = datetime.datetime.now().date()
#             start_of_week = specified_date - datetime.timedelta(days=7)
#             start_date = datetime.datetime.strftime(start_of_week, '%Y-%m-%d')
#             end_date = datetime.datetime.strftime(specified_date, '%Y-%m-%d')
#         elif type == "Month":
#             date=Month
#             datestructure = date.replace(" ", "").split("-")
#             final_month = f"{datestructure[0]}-{str(datestructure[1]).zfill(2)}"
#             start_month = f"{final_month}-01"
#             startd_date = datetime.datetime.strptime(start_month, "%Y-%m-%d")
#             endd_date = startd_date + datetime.timedelta(days=31)
#             start_date = datetime.datetime.strftime(startd_date, '%Y-%m-%d')
#             end_date = datetime.datetime.strftime(endd_date, '%Y-%m-%d')
#         elif type == "Year":
#             date = Year
#             endd_date =f'{date}-12-31'
#             startd_date = f'{date}-01-01'
#             format_data = "%Y-%m-%d"
#             end_date=datetime.datetime.strftime(datetime.datetime.strptime(endd_date,format_data), format_data)
#             start_date=datetime.datetime.strftime(datetime.datetime.strptime(startd_date,format_data), format_data)

#         # Query for CoalTesting objects
#         fetchCoalTesting = CoalTesting.objects(
#             receive_date__gte= datetime.datetime.strptime(start_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), receive_date__lte= datetime.datetime.strptime(end_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M")
#         )
#         # Query for CoalTestingTrain objects
#         fetchCoalTestingTrain = CoalTestingTrain.objects(
#             receive_date__gte = datetime.datetime.strptime(start_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), receive_date__lte= datetime.datetime.strptime(end_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M")
#         )
#         # If no data is found from db, return empty result
#         if not fetchCoalTesting and not fetchCoalTestingTrain:
#             return {"data": {"labels": [], "datasets": []}}
#         else:
#             rrNo_values = {}
#             calorific_value = 0
#             third_party_calorific_value = 0
#             # Iterate through fetched CoalTesting objects
#             for single_coal_testing in fetchCoalTesting:
#                 rrNo = single_coal_testing.rrNo
#                 location = single_coal_testing.location
#                 for param in single_coal_testing.parameters:
#                     if param["parameter_Name"] == "Gross_Calorific_Value_(Arb)":
#                         if param["val1"] != None and param["val1"] != "":
#                             calorific_value = float(param["val1"])
#                             # break

#                     if param["parameter_Name"] == "Third_Party_Gross_Calorific_Value_(Arb)":
#                         if param["val1"] != None and param["val1"] != "":
#                             third_party_calorific_value = float(param["val1"])


#                 # Aggregate values based on rrNo
#                 if rrNo in rrNo_values:
#                     rrNo_values[location]["gwel"] += int(calorific_value)
#                     rrNo_values[location]["third_party"] += int(third_party_calorific_value)
#                 else:
#                     rrNo_values[location] = {
#                         "gwel": int(calorific_value),
#                         "third_party": int(third_party_calorific_value) if third_party_calorific_value else 0
#                     }

#             # Iterate through fetched CoalTestingTrain objects
#             for single_coal_testing_train in fetchCoalTestingTrain:
#                 rrNo = single_coal_testing_train.rrNo
#                 location = single_coal_testing_train.location
#                 for param in single_coal_testing_train.parameters:
#                     if param["parameter_Name"] == "Gross_Calorific_Value_(Arb)":
#                         if param["val1"] != None and param["val1"] != "":
#                             calorific_value = float(param["val1"])
#                             # break
#                     if param["parameter_Name"] == "Third_Party_Gross_Calorific_Value_(Arb)":
#                         if param["val1"] != None and param["val1"] != "":
#                             third_party_calorific_value = float(param["val1"])
                            

#                 # Aggregate values based on rrNo
#                 if rrNo in rrNo_values:
#                     rrNo_values[location]["gwel"] += int(calorific_value)
#                     rrNo_values[location]["third_party"] += int(third_party_calorific_value)
#                 else:
#                     rrNo_values[location] = {
#                         "gwel": int(calorific_value),
#                         "third_party": int(third_party_calorific_value) if third_party_calorific_value else 0
#                     }
            
#             # fetch data from AopTarget
#             aopList = []
#             fetchAopTarget = AopTarget.objects()
#             if fetchAopTarget:
#                 for single_aop_target in fetchAopTarget:
#                     aopList.append(single_aop_target.payload())


#             target_dict = {item['source_name']: int(item['aop_target']) for item in aopList}
            
#             aligned_target_data = [target_dict.get(label.strip(), 0) for label in rrNo_values.keys()]

#             result = {
#                 "data": {
#                     "labels": list(rrNo_values.keys()),
#                     "datasets": [
#                         {"label": "GWEL", "data": [data['gwel'] for data in rrNo_values.values()], "order": 1, "type": "bar"},
#                         {"label": "Third Party", "data": [data['third_party'] for data in rrNo_values.values()], "order": 1, "type": "bar"},
#                         {"label": "Target", "data": aligned_target_data, "order": 0, "type": "line"},
#                     ],
#                 }
#             }

#             return result
        
#     except Exception as e:
#         console_logger.debug("----- Gate Vehicle Count Error -----",e)
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#         console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#         return e


@router.get("/coallabtestanalysis", tags=["Coal Testing"])
def endpoint_to_fetch_report_name_data(response: Response,
    type: Optional[str] = "Daily",
    Month: Optional[str] = None, 
    Daily: Optional[str] = None, 
    Year: Optional[str] = None):
    try:
        if type == "Daily":
            specified_date = datetime.datetime.strptime(Daily, "%Y-%m-%d")
            start_of_month = specified_date.replace(day=1)
            start_date = datetime.datetime.strftime(start_of_month, '%Y-%m-%d')
            end_date = datetime.datetime.strftime(specified_date, '%Y-%m-%d')
            month_val = datetime.datetime.strftime(specified_date, '%m')
        elif type == "Week":
            specified_date = datetime.datetime.now().date()
            start_of_week = specified_date - datetime.timedelta(days=7)
            start_date = datetime.datetime.strftime(start_of_week, '%Y-%m-%d')
            end_date = datetime.datetime.strftime(specified_date, '%Y-%m-%d')
            month_val = datetime.datetime.strftime(specified_date, '%m')
        elif type == "Month":
            date=Month
            datestructure = date.replace(" ", "").split("-")
            final_month = f"{datestructure[0]}-{str(datestructure[1]).zfill(2)}"
            start_month = f"{final_month}-01"
            startd_date = datetime.datetime.strptime(start_month, "%Y-%m-%d")
            endd_date = startd_date + datetime.timedelta(days=31)
            start_date = datetime.datetime.strftime(startd_date, '%Y-%m-%d')
            end_date = datetime.datetime.strftime(endd_date, '%Y-%m-%d')
        elif type == "Year":
            date = Year
            endd_date =f'{date}-12-31'
            startd_date = f'{date}-01-01'
            format_data = "%Y-%m-%d"
            end_date=datetime.datetime.strftime(datetime.datetime.strptime(endd_date,format_data), format_data)
            start_date=datetime.datetime.strftime(datetime.datetime.strptime(startd_date,format_data), format_data)
            
        if "RecieptCoalQualityAnalysis" not in collectionList:
            RCA = gmrDB.create_collection("RecieptCoalQualityAnalysis")
        else:
            RCA = gmrDB.get_collection("RecieptCoalQualityAnalysis")

        listData = []
        rr_no_values = {}
        fetchRCAQualityRoad = RecieptCoalQualityAnalysis.objects(plant_analysis_date__gte=datetime.datetime.strptime(start_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), plant_analysis_date__lte=datetime.datetime.strptime(end_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), mode="Road")
        fetchRCAQualityRail = RecieptCoalQualityAnalysis.objects(plant_analysis_date__gte=datetime.datetime.strptime(start_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), plant_analysis_date__lte=datetime.datetime.strptime(end_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), mode="Rail")
        
        for single_data_train in fetchRCAQualityRail:
            rrNo = single_data_train.sample_id
            filter = {
            'mine':single_data_train.mine,
            'plant_analysis_date': {
                '$gte': datetime.datetime.now(datetime.timezone.utc).replace(day=1,month=11,hour=0,minute=0,second=0,microsecond=0),
                # '$lte':end_day,
            }, }

            df = pl.DataFrame(list(RCA.find(filter=filter)))
            
            df = df.with_columns([
                pl.lit(0.0).alias("cum_wt"),
                pl.lit(0.0).alias("weighted_gcv"),
                pl.lit(0.0).alias("cum_weighted_gcv"),
                pl.lit(0.0).alias("gcv")
            ])
            
            df = df.with_columns([
                pl.lit(float(df["sample_qty"][0])).alias('cum_wt')
            ])
            
            # initialize data to previous values    
            cum_wt_list = [float(df['sample_qty'][0])]
            weighted_gcv_list = [float(df['sample_qty'][0]) * float(df['plant_arb_gcv'][0])]
            cum_weighted_gcv_list = [weighted_gcv_list[0]]
            gcv_list = [cum_weighted_gcv_list[0]/cum_wt_list[0]]
            
            for i in range(1, len(df)):
                
                cum_wt_total = float(df['sample_qty'][i]) + cum_wt_list[i-1]
                weighted_gcv_total = float(df['sample_qty'][i]) * float(df['plant_arb_gcv'][i])
                cum_weighted_gcv_total = float(weighted_gcv_total) + cum_weighted_gcv_list[i-1]
                gcv_total = cum_weighted_gcv_total/cum_wt_total
                
                cum_wt_list.append(cum_wt_total)
                weighted_gcv_list.append(weighted_gcv_total)
                cum_weighted_gcv_list.append(cum_weighted_gcv_total)
                gcv_list.append(gcv_total)
            
            df = df.with_columns(
                pl.Series('cum_wt', cum_wt_list),
                pl.Series('weighted_gcv', weighted_gcv_list),
                pl.Series('cum_weighted_gcv',cum_weighted_gcv_list),
                pl.Series('gcv', gcv_list)
            )
            rr_no_values[single_data_train.mine] = gcv_list[-1]
            # df.write_excel("gcv_calculation.xlsx")

        for single_data_raod in fetchRCAQualityRoad:
            rrNo = single_data_raod.sample_id
            console_logger.debug(rrNo)
            filter = {
            'type_consumer':single_data_raod.type_consumer,
            'plant_analysis_date': {
                '$gte': datetime.datetime.now(datetime.timezone.utc).replace(day=1,month=11,hour=0,minute=0,second=0,microsecond=0),
                # '$lte':end_day,
            }, }

            df = pl.DataFrame(list(RCA.find(filter=filter)))
            
            df = df.with_columns([
                pl.lit(0.0).alias("cum_wt"),
                pl.lit(0.0).alias("weighted_gcv"),
                pl.lit(0.0).alias("cum_weighted_gcv"),
                pl.lit(0.0).alias("gcv")
            ])
            
            df = df.with_columns([
                pl.lit(float(df["sample_qty"][0])).alias('cum_wt')
            ])
            
            # initialize data to previous values    
            cum_wt_list = [float(df['sample_qty'][0])]
            weighted_gcv_list = [float(df['sample_qty'][0]) * float(df['plant_arb_gcv'][0])]
            cum_weighted_gcv_list = [weighted_gcv_list[0]]
            gcv_list = [cum_weighted_gcv_list[0]/cum_wt_list[0]]
            
            for i in range(1, len(df)):
                
                cum_wt_total = float(df['sample_qty'][i]) + cum_wt_list[i-1]
                weighted_gcv_total = float(df['sample_qty'][i]) * float(df['plant_arb_gcv'][i])
                cum_weighted_gcv_total = float(weighted_gcv_total) + cum_weighted_gcv_list[i-1]
                gcv_total = cum_weighted_gcv_total/cum_wt_total
                
                cum_wt_list.append(cum_wt_total)
                weighted_gcv_list.append(weighted_gcv_total)
                cum_weighted_gcv_list.append(cum_weighted_gcv_total)
                gcv_list.append(gcv_total)
            
            df = df.with_columns(
                pl.Series('cum_wt', cum_wt_list),
                pl.Series('weighted_gcv', weighted_gcv_list),
                pl.Series('cum_weighted_gcv',cum_weighted_gcv_list),
                pl.Series('gcv', gcv_list)
            )
            rr_no_values[single_data_raod.type_consumer] = gcv_list[-1]
            # df.write_excel("gcv_calculation.xlsx")
        # console_logger.debug(rr_no_values)
        aopList = []
        fetchAopTarget = AopTarget.objects()
        if fetchAopTarget:
            for single_aop_target in fetchAopTarget:
                aopList.append(single_aop_target.payload())


        target_dict = {item['source_name']: int(item['aop_target']) for item in aopList}
        
        values_dictData = [x if x is not None else 'None' for x in rr_no_values.keys()]

        aligned_target_data = [target_dict.get(label.strip(), 0) for label in values_dictData]

        # console_logger.debug(rr_no_values)
        result = {
                "data": {
                    "labels": list(rr_no_values.keys()),
                    "datasets": [
                        {"label": "GWEL", "data": [data for data in list(rr_no_values.values())], "order": 1, "type": "bar"},
                        {"label": "Target", "data": aligned_target_data, "order": 0, "type": "line"},
                    ],
                }
            }
        
        return result

    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

@router.get("/coallabtestanalysisold", tags=["Coal Testing"])
def endpoint_to_fetch_report_name(type: Optional[str] = "Daily",
    Month: Optional[str] = None, 
    Daily: Optional[str] = None, 
    Year: Optional[str] = None
):
    try:
        if type == "Daily":
            specified_date = datetime.datetime.strptime(Daily, "%Y-%m-%d")
            start_of_month = specified_date.replace(day=1)
            start_date = datetime.datetime.strftime(start_of_month, '%Y-%m-%d')
            end_date = datetime.datetime.strftime(specified_date, '%Y-%m-%d')
        elif type == "Week":
            specified_date = datetime.datetime.now().date()
            start_of_week = specified_date - datetime.timedelta(days=7)
            start_date = datetime.datetime.strftime(start_of_week, '%Y-%m-%d')
            end_date = datetime.datetime.strftime(specified_date, '%Y-%m-%d')
        elif type == "Month":
            date=Month
            datestructure = date.replace(" ", "").split("-")
            final_month = f"{datestructure[0]}-{str(datestructure[1]).zfill(2)}"
            start_month = f"{final_month}-01"
            startd_date = datetime.datetime.strptime(start_month, "%Y-%m-%d")
            endd_date = startd_date + datetime.timedelta(days=31)
            start_date = datetime.datetime.strftime(startd_date, '%Y-%m-%d')
            end_date = datetime.datetime.strftime(endd_date, '%Y-%m-%d')
        elif type == "Year":
            date = Year
            endd_date =f'{date}-12-31'
            startd_date = f'{date}-01-01'
            format_data = "%Y-%m-%d"
            end_date=datetime.datetime.strftime(datetime.datetime.strptime(endd_date,format_data), format_data)
            start_date=datetime.datetime.strftime(datetime.datetime.strptime(startd_date,format_data), format_data)

        # Query for CoalTesting objects
        # fetchCoalTesting = CoalTesting.objects(
        #     receive_date__gte= datetime.datetime.strptime(start_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), receive_date__lte= datetime.datetime.strptime(end_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M")
        # )
        # # Query for CoalTestingTrain objects
        # fetchCoalTestingTrain = CoalTestingTrain.objects(
        #     receive_date__gte = datetime.datetime.strptime(start_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), receive_date__lte= datetime.datetime.strptime(end_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M")
        # )

        fetchCoalTesting = RecieptCoalQualityAnalysis.objects(plant_analysis_date__gte=datetime.datetime.strptime(start_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), plant_analysis_date__lte=datetime.datetime.strptime(end_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), mode="Road")
        fetchCoalTestingTrain = RecieptCoalQualityAnalysis.objects(plant_analysis_date__gte=datetime.datetime.strptime(start_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), plant_analysis_date__lte=datetime.datetime.strptime(end_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), mode="Rail")

        
        # If no data is found from db, return empty result
        if not fetchCoalTesting and not fetchCoalTestingTrain:
            return {"data": {"labels": [], "datasets": []}}
        else:
            rrNo_values = {}
            calorific_value = 0
            third_party_calorific_value = 0

            for single_coal_testing_train in fetchCoalTestingTrain:
                rrNo = single_coal_testing_train.sample_id
                location = single_coal_testing_train.mine
                calorific_value = single_coal_testing_train.plant_arb_gcv
                third_party_calorific_value = single_coal_testing_train.thirdparty_arb_gcv
                            
                if rrNo in rrNo_values:
                    rrNo_values[location]["gwel"] += int(calorific_value)
                    rrNo_values[location]["third_party"] += int(third_party_calorific_value)
                else:
                    rrNo_values[location] = {
                        "gwel": int(calorific_value),
                        "third_party": float(third_party_calorific_value) if third_party_calorific_value else 0
                    }
# 
            listData = []
            for single_coal_testing in fetchCoalTesting:
                rrNo = single_coal_testing.sample_id
                # location = single_coal_testing.mine
                location = single_coal_testing.type_consumer if single_coal_testing.type_consumer is not None else "None"
                calorific_value = single_coal_testing.plant_arb_gcv
                third_party_calorific_value = single_coal_testing.thirdparty_arb_gcv

                listData.append(calorific_value)

                if rrNo in rrNo_values:
                    rrNo_values[location]["gwel"] += int(calorific_value)
                    rrNo_values[location]["third_party"] += int(third_party_calorific_value)
                else:
                    rrNo_values[location] = {
                        "gwel": int(calorific_value),
                        "third_party": int(third_party_calorific_value) if third_party_calorific_value else 0
                    }
            console_logger.debug(listData)
            console_logger.debug(rrNo_values)

            aopList = []
            fetchAopTarget = AopTarget.objects()
            if fetchAopTarget:
                for single_aop_target in fetchAopTarget:
                    aopList.append(single_aop_target.payload())


            target_dict = {item['source_name']: int(item['aop_target']) for item in aopList}
            
            aligned_target_data = [target_dict.get(label.strip(), 0) for label in rrNo_values.keys()]

            console_logger.debug(rrNo_values)

            result = {
                "data": {
                    "labels": list(rrNo_values.keys()),
                    "datasets": [
                        {"label": "GWEL", "data": [data['gwel'] for data in rrNo_values.values()], "order": 1, "type": "bar"},
                        {"label": "Third Party", "data": [data['third_party'] for data in rrNo_values.values()], "order": 1, "type": "bar"},
                        {"label": "Target", "data": aligned_target_data, "order": 0, "type": "line"},
                    ],
                }
            }

            return result
        
    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

# @router.get("/coallabtestanalysisold", tags=["Coal Testing"])
def endpoint_to_fetch_report_name_pdf(type: Optional[str] = "Daily",
    Month: Optional[str] = None, 
    Daily: Optional[str] = None, 
    Year: Optional[str] = None
):
    try:
        if type == "Daily":
            specified_date = datetime.datetime.strptime(Daily, "%Y-%m-%d")
            start_of_month = specified_date.replace(day=1)
            start_date = datetime.datetime.strftime(start_of_month, '%Y-%m-%d')
            end_date = datetime.datetime.strftime(specified_date, '%Y-%m-%d')
            month_val = datetime.datetime.strftime(specified_date, '%m')
        elif type == "Week":
            specified_date = datetime.datetime.now().date()
            start_of_week = specified_date - datetime.timedelta(days=7)
            start_date = datetime.datetime.strftime(start_of_week, '%Y-%m-%d')
            end_date = datetime.datetime.strftime(specified_date, '%Y-%m-%d')
            month_val = datetime.datetime.strftime(specified_date, '%m')
        elif type == "Month":
            date=Month
            datestructure = date.replace(" ", "").split("-")
            final_month = f"{datestructure[0]}-{str(datestructure[1]).zfill(2)}"
            start_month = f"{final_month}-01"
            startd_date = datetime.datetime.strptime(start_month, "%Y-%m-%d")
            endd_date = startd_date + datetime.timedelta(days=31)
            start_date = datetime.datetime.strftime(startd_date, '%Y-%m-%d')
            end_date = datetime.datetime.strftime(endd_date, '%Y-%m-%d')
        elif type == "Year":
            date = Year
            endd_date =f'{date}-12-31'
            startd_date = f'{date}-01-01'
            format_data = "%Y-%m-%d"
            end_date=datetime.datetime.strftime(datetime.datetime.strptime(endd_date,format_data), format_data)
            start_date=datetime.datetime.strftime(datetime.datetime.strptime(startd_date,format_data), format_data)
            
        if "RecieptCoalQualityAnalysis" not in collectionList:
            RCA = gmrDB.create_collection("RecieptCoalQualityAnalysis")
        else:
            RCA = gmrDB.get_collection("RecieptCoalQualityAnalysis")

        listData = []
        rr_no_values = {}
        fetchRCAQualityRoad = RecieptCoalQualityAnalysis.objects(plant_analysis_date__gte=datetime.datetime.strptime(start_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), plant_analysis_date__lte=datetime.datetime.strptime(end_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), mode="Road")
        fetchRCAQualityRail = RecieptCoalQualityAnalysis.objects(plant_analysis_date__gte=datetime.datetime.strptime(start_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), plant_analysis_date__lte=datetime.datetime.strptime(end_date, "%Y-%m-%d").strftime("%Y-%m-%dT%H:%M"), mode="Rail")
        
        for single_data_train in fetchRCAQualityRail:
            rrNo = single_data_train.sample_id
            filter = {
            'mine':single_data_train.mine,
            'plant_analysis_date': {
                '$gte': datetime.datetime.now(datetime.timezone.utc).replace(day=1,month=int(month_val),hour=0,minute=0,second=0,microsecond=0),
                # '$lte':end_day,
            }, }

            df = pl.DataFrame(list(RCA.find(filter=filter)))
            
            df = df.with_columns([
                pl.lit(0.0).alias("cum_wt"),
                pl.lit(0.0).alias("weighted_gcv"),
                pl.lit(0.0).alias("cum_weighted_gcv"),
                pl.lit(0.0).alias("gcv")
            ])
            
            df = df.with_columns([
                pl.lit(float(df["sample_qty"][0])).alias('cum_wt')
            ])
            
            # initialize data to previous values    
            cum_wt_list = [float(df['sample_qty'][0])]
            weighted_gcv_list = [float(df['sample_qty'][0]) * float(df['plant_arb_gcv'][0])]
            cum_weighted_gcv_list = [weighted_gcv_list[0]]
            gcv_list = [cum_weighted_gcv_list[0]/cum_wt_list[0]]
            
            for i in range(1, len(df)):
                
                cum_wt_total = float(df['sample_qty'][i]) + cum_wt_list[i-1]
                weighted_gcv_total = float(df['sample_qty'][i]) * float(df['plant_arb_gcv'][i])
                cum_weighted_gcv_total = float(weighted_gcv_total) + cum_weighted_gcv_list[i-1]
                gcv_total = cum_weighted_gcv_total/cum_wt_total
                
                cum_wt_list.append(cum_wt_total)
                weighted_gcv_list.append(weighted_gcv_total)
                cum_weighted_gcv_list.append(cum_weighted_gcv_total)
                gcv_list.append(gcv_total)
            
            df = df.with_columns(
                pl.Series('cum_wt', cum_wt_list),
                pl.Series('weighted_gcv', weighted_gcv_list),
                pl.Series('cum_weighted_gcv',cum_weighted_gcv_list),
                pl.Series('gcv', gcv_list)
            )
            rr_no_values[single_data_train.mine] = gcv_list[-1]
            # df.write_excel("gcv_calculation.xlsx")

        for single_data_raod in fetchRCAQualityRoad:
            rrNo = single_data_raod.sample_id
            filter = {
            'type_consumer':single_data_raod.type_consumer,
            'plant_analysis_date': {
                '$gte': datetime.datetime.now(datetime.timezone.utc).replace(day=1,month=int(month_val),hour=0,minute=0,second=0,microsecond=0),
                # '$lte':end_day,
            }, }

            df = pl.DataFrame(list(RCA.find(filter=filter)))
            
            df = df.with_columns([
                pl.lit(0.0).alias("cum_wt"),
                pl.lit(0.0).alias("weighted_gcv"),
                pl.lit(0.0).alias("cum_weighted_gcv"),
                pl.lit(0.0).alias("gcv")
            ])
            
            df = df.with_columns([
                pl.lit(float(df["sample_qty"][0])).alias('cum_wt')
            ])
            
            # initialize data to previous values    
            cum_wt_list = [float(df['sample_qty'][0])]
            weighted_gcv_list = [float(df['sample_qty'][0]) * float(df['plant_arb_gcv'][0])]
            cum_weighted_gcv_list = [weighted_gcv_list[0]]
            gcv_list = [cum_weighted_gcv_list[0]/cum_wt_list[0]]
            
            for i in range(1, len(df)):
                
                cum_wt_total = float(df['sample_qty'][i]) + cum_wt_list[i-1]
                weighted_gcv_total = float(df['sample_qty'][i]) * float(df['plant_arb_gcv'][i])
                cum_weighted_gcv_total = float(weighted_gcv_total) + cum_weighted_gcv_list[i-1]
                gcv_total = cum_weighted_gcv_total/cum_wt_total
                
                cum_wt_list.append(cum_wt_total)
                weighted_gcv_list.append(weighted_gcv_total)
                cum_weighted_gcv_list.append(cum_weighted_gcv_total)
                gcv_list.append(gcv_total)
            
            df = df.with_columns(
                pl.Series('cum_wt', cum_wt_list),
                pl.Series('weighted_gcv', weighted_gcv_list),
                pl.Series('cum_weighted_gcv',cum_weighted_gcv_list),
                pl.Series('gcv', gcv_list)
            )
            rr_no_values[single_data_raod.type_consumer] = gcv_list[-1]

        aopList = []
        fetchAopTarget = AopTarget.objects()
        if fetchAopTarget:
            for single_aop_target in fetchAopTarget:
                aopList.append(single_aop_target.payload())


        target_dict = {item['source_name']: int(item['aop_target']) for item in aopList}

        # values_dictData = [x if x is not None else 'None' for x in rr_no_values.keys()]
        
        # aligned_target_data = [target_dict.get(label.strip(), 0) for label in values_dictData]


        # console_logger.debug(rr_no_values)
        # result = {
        #         "data": {
        #             "labels": list(rr_no_values.keys()),
        #             "datasets": [
        #                 {"label": "GWEL", "data": [data for data in list(rr_no_values.values())], "order": 1, "type": "bar"},
        #                 {"label": "Target", "data": aligned_target_data, "order": 0, "type": "line"},
        #             ],
        #         }
        #     }
        
        return rr_no_values, aopList

    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e
        
    except Exception as e:
        console_logger.debug("----- Gate Vehicle Count Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/report/inventory", tags=["Road Map"])
def endpoint_to_fetch_inventory(response: Response):
    try:
        results = {
            "title": "Inventory",
            "icon": "inventory",
            "data": ""
            }
        fetchGmrData = Gmrdata.objects()
        overall_gwel = 0
        overall_historian = 0
        for single_gmr_data in fetchGmrData:
            if single_gmr_data.payload()["GWEL_Net_Wt(MT)"] != None:
                overall_gwel += float(single_gmr_data.payload()["GWEL_Net_Wt(MT)"])
        
        fetchHistorianData = Historian.objects(tagid__in=[16, 3536])

        for single_historian in fetchHistorianData:
            overall_historian += float(single_historian.payload()["sum"])

        results["data"] = round(overall_gwel - overall_historian/1000, 2)

        return results

    except Exception as e:
        console_logger.debug("----- Fetch Report Name Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e
    

@router.get("/fetch/expiry/vehicle", tags=["Reports"])
def endpoint_to_fetch_going_to_expiry_vehicle(response: Response, page_no:int=None, page_size:int=None, search_text: Optional[str]=None, type: Optional[str] = "display"):
    try:
        if type and type == "display":
            if page_no and page_size:
                skip = page_size * (page_no - 1)
                limit = page_size
            query = {}
            if search_text:
                query["$or"] = [
                    {"vehicle_number": {"$regex": f"{search_text}", "$options": "i"}},
                    {"delivery_challan_number": {"$regex": f"{search_text}", "$options": "i"}},
                ]

            today = datetime.datetime.now()
            seven_days_ago = today + datetime.timedelta(days=7)
            
            result = {"labels": [], "datasets": [], "total": 0, "page_size": 15}
            pipeline = [
                    {
                        "$addFields": {
                            "certificate_expiry_date": {
                                "$dateFromString": {
                                    "dateString": "$certificate_expiry",
                                    "format": "%d-%m-%Y"
                                }
                            }
                        }
                    },
                    {
                        "$match": {
                            "$and": [
                                {"certificate_expiry_date": {"$lte": seven_days_ago}},
                                query
                            ]
                        }
                    },
                    {
                        "$addFields": {
                            "days_to_go": {
                                "$divide": [
                                    {
                                        "$subtract": ["$certificate_expiry_date", datetime.datetime.now() + datetime.timedelta(days=-1)]
                                    },
                                    1000 * 60 * 60 * 24
                                ]
                            }
                        }
                    },
                    {
                        "$sort": {"vehicle_number": 1, "created_at": -1}
                    },
                    {
                        "$group": {
                            "_id": "$vehicle_number",
                            "latest_record": {"$first": "$$ROOT"}
                        }
                    },
                    {
                        "$project": {  
                            "latest_record": -1
                        }
                    },
                    {
                        "$replaceRoot": {"newRoot": "$latest_record"}
                    },
                    {
                        "$sort": {"created_at": -1}
                    }
                ]
            count_pipeline = pipeline.copy()
            count_pipeline.append({"$count": "total_count"})
            count_result = list(Gmrdata.objects.aggregate(count_pipeline))
            total_count = count_result[0]["total_count"] if count_result else 0
            
            if page_no and page_size:
                pipeline.append({"$skip": skip})
                pipeline.append({"$limit": limit})
        
            results = list(Gmrdata.objects.aggregate(pipeline))
            results_sorted = sorted(results, key=lambda record: record["days_to_go"], reverse=False)
            result["labels"] = ["vehicle_number", "vehicle_chassis_number", "expiry_date", "fitness_file", "days_to_go"]
            finalData = []
            for record in results_sorted:
                dictData = {}
                # dictData["sr_no"] = count
                dictData["vehicle_number"] = record["vehicle_number"]
                dictData["vehicle_chassis_number"] = record["vehicle_chassis_number"]
                dictData["expiry_date"] = record["certificate_expiry"]
                dictData["fitness_file"] = record["fitness_file"]

                days_to_go = record["days_to_go"]
                total_seconds = int(days_to_go * 24 * 60 * 60)  # Convert days to seconds
                delta = timedelta(seconds=total_seconds)

                days = delta.days
                hours, remainder = divmod(delta.seconds, 3600)
                minutes, seconds = divmod(remainder, 60)
                if days < 0:
                    dictData["days_to_go"] = 0
                else:
                    dictData["days_to_go"] = f"{days}"
                # count -= 1

                finalData.append(dictData)

            result["datasets"] = finalData
            result["total"] = total_count
            return result
        elif type and type == "download":
            del type
            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            today = datetime.datetime.now()
            seven_days_ago = today + datetime.timedelta(days=7)
            
            result = {"labels": [], "datasets": [], "total": 0, "page_size": 15}

            pipeline = [
                    {
                        "$addFields": {
                            "certificate_expiry_date": {
                                "$dateFromString": {
                                    "dateString": "$certificate_expiry",
                                    "format": "%d-%m-%Y"
                                }
                            }
                        }
                    },
                    {
                        "$match": {
                            "$and": [
                                {"certificate_expiry_date": {"$lte": seven_days_ago}},
                            ]
                        }
                    },
                    {
                        "$addFields": {
                            "days_to_go": {
                                "$divide": [
                                    {
                                        "$subtract": ["$certificate_expiry_date", datetime.datetime.now() + datetime.timedelta(days=-1)]
                                    },
                                    1000 * 60 * 60 * 24
                                ]
                            }
                        }
                    },
                    {
                        "$sort": {"vehicle_number": 1, "created_at": -1}
                    },
                    {
                        "$group": {
                            "_id": "$vehicle_number",
                            "latest_record": {"$first": "$$ROOT"}
                        }
                    },
                    {
                        "$project": {  
                            "latest_record": -1
                        }
                    },
                    {
                        "$replaceRoot": {"newRoot": "$latest_record"}
                    },
                    {
                        "$sort": {"created_at": -1}
                    }
                ]
            count_pipeline = pipeline.copy()
            count_pipeline.append({"$count": "total_count"})
            count_result = list(Gmrdata.objects.aggregate(count_pipeline))
            total_count = count_result[0]["total_count"] if count_result else 0
            
            if page_no and page_size:
                pipeline.append({"$skip": skip})
                pipeline.append({"$limit": limit})
            
            results = list(Gmrdata.objects.aggregate(pipeline))
            # sorting data in ascending order i.e reverse=False
            # results_sorted = sorted(results, key=lambda record: record["latest_record"]["days_to_go"], reverse=False)
            results_sorted = sorted(results, key=lambda record: record["days_to_go"], reverse=False)
            count = len(results_sorted)
            path = os.path.join(
                "static_server",
                "gmr_ai",
                file,
                "Vehicle_expiry_{}.xlsx".format(
                    datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                ),
            )
            filename = os.path.join(os.getcwd(), path)
            workbook = xlsxwriter.Workbook(filename)
            workbook.use_zip64()
            cell_format2 = workbook.add_format()
            cell_format2.set_bold()
            cell_format2.set_font_size(10)
            cell_format2.set_align("center")
            cell_format2.set_align("vjustify")

            worksheet = workbook.add_worksheet()
            worksheet.set_column("A:AZ", 20)
            worksheet.set_default_row(50)
            cell_format = workbook.add_format()
            cell_format.set_font_size(10)
            cell_format.set_align("center")
            cell_format.set_align("vcenter")
            headers = ["sr_no", "vehicle_number", "vehicle_chassis_number", "expiry_date", "days_to_go"]
            finalData = []

            for index, header in enumerate(headers):
                worksheet.write(0, index, header, cell_format2)

            row = 1
            for record in results_sorted:
                worksheet.write(row, 0, count, cell_format)
                worksheet.write(row, 1, record["vehicle_number"])
                worksheet.write(row, 2, record["vehicle_chassis_number"])
                worksheet.write(row, 3, record["certificate_expiry"])
                # worksheet.write(row, 4, record["fitness_file"])
                

                days_to_go = record["days_to_go"]
                total_seconds = int(days_to_go * 24 * 60 * 60)  # Convert days to seconds
                delta = timedelta(seconds=total_seconds)

                days = delta.days
                hours, remainder = divmod(delta.seconds, 3600)
                minutes, seconds = divmod(remainder, 60)
                if days < 0:
                    console_logger.debug("The number is negative.")
                    worksheet.write(row, 4, 0)
                else:
                    console_logger.debug("The number is positive.")
                    worksheet.write(row, 4, days)
                count -= 1
                row += 1
            workbook.close()

            return {
                "Type": "vehicle_fitness_expiry",
                "Datatype": "Report",
                "File_Path": path,
            }

    except Exception as e:
        console_logger.debug("----- Fetch Report Name Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

@router.get("/fetch/rail", tags=["Rail Map"])
def endpoint_to_fetch_railway_data(response: Response, currentPage: Optional[int] = None, perPage: Optional[int] = None, search_text: Optional[str] = None, start_timestamp: Optional[str] = None, end_timestamp: Optional[str] = None, month_date: Optional[str] = None, type: Optional[str] = "display"):
    try:
        result = {        
                "labels": [],
                "datasets": [],
                "total" : 0,
                "page_size": 15
        }
        if type and type == "display":
            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage

            data = Q()

            # based on condition for timestamp playing with & and | 
            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(placement_date__gte = f"{start_date}")

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M","Asia/Kolkata")
                data &= Q(placement_date__lte = f"{end_date}")

            if search_text:
                if search_text.isdigit():
                    data &= Q(rr_no__icontains=search_text) | Q(po_no__icontains=search_text)
                else:
                    data &= (Q(source__icontains=search_text))

            if month_date:
                start_date = f'{month_date}-01'
                startd_date=datetime.datetime.strptime(f"{start_date}T00:00","%Y-%m-%dT%H:%M")
                end_date = (datetime.datetime.strptime(start_date, "%Y-%m-%d") + relativedelta(day=31)).strftime("%Y-%m-%d")
                data &= Q(placement_date__gte = startd_date.strftime("%Y-%m-%dT%H:%M"))
                data &= Q(placement_date__lte = f"{end_date}T23:59")

            offset = (page_no - 1) * page_len
            logs = (
                RailData.objects(data, avery_placement_date=None)
                .order_by("-placement_date")
                .skip(offset)
                .limit(page_len)
            )   
            if any(logs):
                for log in logs:
                    result["labels"] = list(log.simplepayload().keys())
                    result["datasets"].append(log.simplepayload())
                result["total"]= len(RailData.objects(data, avery_placement_date=None))
            return result
        elif type and type == "download":
            del type

            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            # Constructing the base for query
            data = Q()

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(placement_date__gte = f"{start_date}")

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M","Asia/Kolkata",False)
                data &= Q(placement_date__lte = f"{end_date}")
            
            if search_text:
                if search_text.isdigit():
                    data &= Q(arv_cum_do_number__icontains = search_text) | Q(delivery_challan_number__icontains = search_text)
                else:
                    data &= Q(vehicle_number__icontains = search_text)

            usecase_data = RailData.objects(data, avery_placement_date=None).order_by("-created_at")
            count = len(usecase_data)
            path = None
            logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
            if usecase_data:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "Rail_Report_{}.xlsx".format(
                            datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format()
                    cell_format2.set_bold()
                    cell_format2.set_font_size(10)
                    cell_format2.set_align("center")
                    cell_format2.set_align("vcenter")
                    cell_format2.set_text_wrap(True)
                    cell_format2.set_border(1)

                    header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
                    date_format = workbook.add_format({'align': 'center', 'font_size': 12, "bold": True})
                    report_name_format = workbook.add_format({'align': 'center', 'font_size': 15, "bold": True})

                    header_format.set_align("vcenter")
                    date_format.set_align("vcenter")
                    report_name_format.set_align("vcenter")
                    header_format.set_border(1)
                    date_format.set_border(1)
                    report_name_format.set_border(1)

                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)
                    cell_format = workbook.add_format()
                    cell_format.set_font_size(10)
                    cell_format.set_align("center")
                    cell_format.set_align("vcenter")
                    cell_format.set_text_wrap(True)
                    cell_format.set_border(1)

                    worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                    
                    # Merge cells for the main header and place it in the center
                    main_header = "GMR Warora Energy Limited"  # Set your main header text here
                    worksheet.merge_range("A1:AC1", main_header, header_format)  # Merge cells A1 to H1 for the header
                    
                    # Write the current date on the left side (A2)
                    worksheet.write("A2", f"Date: {end_date.strftime('%d-%m-%Y')}", date_format)
                    worksheet.merge_range("C2:AC2", f"Rail Coal Journey", report_name_format)

                    headers = [
                        "Sr.No",
                        "RR No",
                        "RR Qty",
                        "Po No",
                        "Po Date",
                        "Line Item",
                        "Source",
                        "Placement Date",
                        "Completion Date",
                        "Drawn Date",
                        "Total ul wt",
                        "Boxes Supplied",
                        "Total Secl Gross Wt",
                        "Total Secl Tare Wt",
                        "Total Secl Net Wt",
                        "Total Secl Ol Wt",
                        "Boxes Loaded",
                        "Total Rly Gross Wt",
                        "Total Rly_Tare Wt",
                        "Total Rly Net Wt",
                        "Total Rly Ol Wt",
                        "Total Secl Chargable Wt",
                        "Total Rly Chargable Wt",
                        "Freight",
                        "Gst",
                        "Pola",
                        "Total Freight",
                        "Source Type",
                        "Created At"
                    ]
                   
                    for index, header in enumerate(headers):
                        worksheet.write(2, index, header, cell_format2)

                    for row, query in enumerate(usecase_data, start=3):
                        console_logger.debug(result)
                        result = query.simplepayload()
                        worksheet.write(row, 0, count, cell_format)     
                        worksheet.write(row, 1, str(result["rr_no"]), cell_format)                      
                        worksheet.write(row, 2, str(result["rr_qty"]), cell_format)                      
                        worksheet.write(row, 3, str(result["po_no"]), cell_format)                      
                        worksheet.write(row, 4, str(result["po_date"]), cell_format)                      
                        worksheet.write(row, 5, str(result["line_item"]), cell_format)                      
                        worksheet.write(row, 6, str(result["source"]), cell_format)                      
                        worksheet.write(row, 7, str(result["placement_date"]), cell_format)                      
                        worksheet.write(row, 8, str(result["completion_date"]), cell_format)                      
                        worksheet.write(row, 9, str(result["drawn_date"]), cell_format)                      
                        worksheet.write(row, 10, float(result["total_ul_wt"]), cell_format)                      
                        worksheet.write(row, 11, str(result["boxes_supplied"]), cell_format)                      
                        worksheet.write(row, 12, float(result["total_secl_gross_wt"]), cell_format)                      
                        worksheet.write(row, 13, float(result["total_secl_tare_wt"]), cell_format)                      
                        worksheet.write(row, 14, float(result["total_secl_net_wt"]), cell_format)                      
                        worksheet.write(row, 15, float(result["total_secl_ol_wt"]), cell_format)                      
                        worksheet.write(row, 16, str(result["boxes_loaded"]), cell_format)                      
                        worksheet.write(row, 17, float(result["total_rly_gross_wt"]), cell_format)                      
                        worksheet.write(row, 18, float(result["total_rly_tare_wt"]), cell_format)                      
                        worksheet.write(row, 19, float(result["total_rly_net_wt"]), cell_format)                      
                        worksheet.write(row, 20, float(result["total_rly_ol_wt"]), cell_format)                      
                        worksheet.write(row, 21, float(result["total_secl_chargable_wt"]), cell_format)                      
                        worksheet.write(row, 22, float(result["total_rly_chargable_wt"]), cell_format)                      
                        worksheet.write(row, 23, float(result["freight"]), cell_format)                      
                        worksheet.write(row, 24, float(result["gst"]), cell_format)
                        if result.get("pola") == "Not found":                    
                            worksheet.write(row, 25, str(result["pola"]), cell_format)
                        else:
                            worksheet.write(row, 25, float(result["pola"]), cell_format)             
                        worksheet.write(row, 26, float(result["total_freight"]), cell_format)                      
                        worksheet.write(row, 27, str(result["source_type"]), cell_format)                      
                        worksheet.write(row, 28, str(result["created_at"]), cell_format)                   
                        
                        count-=1
                        
                    workbook.close()
                    console_logger.debug("Successfully {} report generated".format(service_id))
                    console_logger.debug("sent data {}".format(path))

                    return {
                            "Type": "gmr_rail_journey_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                            }
                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            else:
                console_logger.error("No data found")
                return {
                        "Type": "gmr_rail_journey_download_event",
                        "Datatype": "Report",
                        "File_Path": path,
                        }
    except Exception as e:
        console_logger.debug("----- Fetch Report Name Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e
    

@router.get("/fetch/avery/rail", tags=["Rail Map"])
def endpoint_to_fetch_railway_data_avery(response: Response, currentPage: Optional[int] = None, perPage: Optional[int] = None, search_text: Optional[str] = None, start_timestamp: Optional[str] = None, end_timestamp: Optional[str] = None, month_date: Optional[str] = None, type: Optional[str] = "display"):
    """
    Function that fetches avery data from railway db  
    Parameters:
        currentPage: Page No | 
        perPage: How much data on particular page |
        search_text: Text to search | 
        start_timestamp: start timestamp | 
        end_timestamp: end timestamp | 
        month_date: Single Month, for eg: 2024-11, 2024-10 |
        type: Display, Download

    Returns:
        Set of data
    """
    try:
        result = {        
                "labels": [],
                "datasets": [],
                "total" : 0,
                "page_size": 15
        }
        if type and type == "display":
            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage

            data = Q()

            # based on condition for timestamp playing with & and | 
            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(avery_placement_date__gte = start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M","Asia/Kolkata",False)
                data &= Q(avery_placement_date__lte = end_date)

            if search_text:
                if search_text.isdigit():
                    data &= Q(rr_no__icontains=search_text) | Q(po_no__icontains=search_text)
                else:
                    data &= (Q(source__icontains=search_text))

            if month_date:
                start_date = f'{month_date}-01'
                startd_date=datetime.datetime.strptime(f"{start_date}T00:00:00","%Y-%m-%dT%H:%M:%S")
                end_date = (datetime.datetime.strptime(start_date, "%Y-%m-%d") + relativedelta(day=31)).strftime("%Y-%m-%d")
                data &= Q(avery_placement_date__gte = startd_date.strftime("%Y-%m-%dT%H:%M:%S"))
                data &= Q(avery_placement_date__lte = f"{end_date}T23:59:59")
            
            offset = (page_no - 1) * page_len
            logs = (
                RailData.objects(data)
                .order_by("-avery_placement_date", "-avery_completion_date")
                .skip(offset)
                .limit(page_len)
            )

            listData = []
            if any(logs):
                for log in logs:
                    payload = log.averyPayloadMain()
                    # if len(payload.get("po_date")) > 10:
                    #     payload["po_date"] = str(datetime.datetime.fromtimestamp(int(payload["po_date"]) / 1000).strftime("%Y-%m-%d"))
                    if payload.get("po_date"):
                        if 10 < len(payload.get("po_date")) <= 13:
                            payload["po_date"] = str(datetime.datetime.fromtimestamp(int(payload["po_date"]) / 1000).strftime("%Y-%m-%d"))
                    payload['total_gwel_gross_wt'] = 0
                    payload['total_gwel_tare_wt'] = 0
                    payload['total_gwel_net_wt'] = 0
                    try:
                        fetchsapDatarails = sapRecordsRail.objects.get(rr_no=log.rr_no)
                        if fetchsapDatarails.adho_sanrachna_vikas:
                            payload["adho_sanrachna_vikas"] = fetchsapDatarails.adho_sanrachna_vikas
                        else:
                            payload["adho_sanrachna_vikas"] = 0
                        if fetchsapDatarails.assessable_value:
                            payload["assessable_value"] = fetchsapDatarails.assessable_value
                        else:
                            payload["assessable_value"] = 0
                        if fetchsapDatarails.dmf:
                            payload["dmf"] = fetchsapDatarails.dmf
                        else:
                            payload["dmf"] = 0
                        if fetchsapDatarails.evac_facility_charge:
                            payload["evac_facility_charge"] = fetchsapDatarails.evac_facility_charge
                        else:
                            payload["evac_facility_charge"] = 0
                        if fetchsapDatarails.gross_bill_value:
                            payload["gross_bill_value"] = fetchsapDatarails.gross_bill_value
                        else:
                            payload["gross_bill_value"] = 0
                        if fetchsapDatarails.gst_comp_cess:
                            payload["gst_comp_cess"] = fetchsapDatarails.gst_comp_cess
                        else:
                            payload["gst_comp_cess"] = 0
                        if fetchsapDatarails.igst:
                            payload["igst"] = fetchsapDatarails.igst
                        else:
                            payload["igst"] = 0
                        if fetchsapDatarails.less_underloading_charges:
                            payload["less_underloading_charges"] = fetchsapDatarails.   less_underloading_charges
                        else:
                            payload["less_underloading_charges"] = 0
                        if fetchsapDatarails.net_value:
                            payload["net_value"] = fetchsapDatarails.net_value
                        else:
                            payload["net_value"] = 0
                        if fetchsapDatarails.nmet_charges:
                            payload["nmet_charges"] = fetchsapDatarails.nmet_charges
                        else:
                            payload["nmet_charges"] = 0
                        if fetchsapDatarails.pariyavaran_upkar:
                            payload["pariyavaran_upkar"] = fetchsapDatarails.pariyavaran_upkar
                        else:
                            payload["pariyavaran_upkar"] = 0
                        if fetchsapDatarails.sizing_charges:
                            payload["sizing_charges"] = fetchsapDatarails.sizing_charges
                        else:
                            payload["sizing_charges"] = 0
                        if fetchsapDatarails.royality_charges:
                            payload["royality_charges"] = fetchsapDatarails.royality_charges
                        else:
                            payload["royality_charges"] = 0
                        if fetchsapDatarails.total_amount:
                            payload["total_amount"] = fetchsapDatarails.total_amount
                        else:
                            payload["total_amount"] = 0
                    except DoesNotExist as e:
                        payload["adho_sanrachna_vikas"] = 0
                        payload["assessable_value"] = 0
                        payload["dmf"] = 0
                        payload["evac_facility_charge"] = 0
                        payload["gross_bill_value"] = 0
                        payload["gst_comp_cess"] = 0
                        payload["igst"] = 0
                        payload["less_underloading_charges"] = 0
                        payload["net_value"] = 0
                        payload["nmet_charges"] = 0
                        payload["pariyavaran_upkar"] = 0
                        payload["sizing_charges"] = 0
                        payload["royality_charges"] = 0
                        payload["total_amount"] = 0
                    
                    if log.avery_rly_data:
                        total_wagon_gross_wt = 0
                        total_wagon_tare_wt = 0
                        total_wagon_net_wt = 0
                        for singleRailData in log.avery_rly_data:
                            # Handle wagon_gross_wt
                            if singleRailData.gwel_gross_wt:
                                try:
                                    total_wagon_gross_wt += float(singleRailData.gwel_gross_wt)
                                except ValueError:
                                    print(f"Warning: Invalid data for wagon_gross_wt: {singleRailData.wagon_gross_wt}")

                            # Handle wagon_tare_wt
                            if singleRailData.gwel_tare_wt:
                                try:
                                    total_wagon_tare_wt += float(singleRailData.gwel_tare_wt)
                                except ValueError:
                                    print(f"Warning: Invalid data for wagon_tare_wt: {singleRailData.wagon_tare_wt}")

                            # Handle wagon_net_wt
                            if singleRailData.gwel_net_wt:
                                try:
                                    total_wagon_net_wt += float(singleRailData.gwel_net_wt)
                                except ValueError:
                                    print(f"Warning: Invalid data for wagon_net_wt: {singleRailData.wagon_net_wt}")
                            payload['total_gwel_gross_wt'] = round(total_wagon_gross_wt, 2)
                            payload['total_gwel_tare_wt'] = round(total_wagon_tare_wt, 2)
                            payload['total_gwel_net_wt'] = round(total_wagon_net_wt, 2)
                            # payload["GWEL_received_wagons"] = gwel_received_wagons
                            # payload["GWEL_pending_wagons"] = int(log.boxes_loaded) - int(gwel_received_wagons)
                    # result["labels"] = list(payload.keys())
                    result["datasets"].append(payload)
                    # listData.append(payload)
                result["labels"] = ["rr_no", "rr_qty", "po_no", "po_date", "line_item", "source", "GWEL_placement_date", "GWEL_completion_date", "GWEL_received_wagons", "GWEL_pending_wagons", "boxes_loaded", "total_secl_gross_wt", "total_secl_tare_wt", "total_secl_net_wt", "total_rly_gross_wt", "total_rly_tare_wt", "total_rly_net_wt", "total_gwel_gross_wt", "total_gwel_tare_wt", "total_gwel_net_wt", "source_type", "month", "rr_date", "siding", "mine", "grade", "adho_sanrachna_vikas", "assessable_value", "dmf", "evac_facility_charge", "gross_bill_value", "gst_comp_cess", "igst", "less_underloading_charges", "net_value", "nmet_charges", "pariyavaran_upkar", "sizing_charges", "royality_charges", "total_amount", "freight", "gst", "pola", "total_freight", "sd", "created_at"]
                result["total"] = RailData.objects(data).count()
            return result
        elif type and type == "download":
            del type

            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            # Constructing the base for query
            data = Q()

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M").strftime("%Y-%m-%dT%H:%M")
                data &= Q(avery_placement_date__gte=start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M","Asia/Kolkata",False).strftime("%Y-%m-%dT%H:%M")
                data &= Q(avery_placement_date__lte=end_date)
            
            if search_text:
                if search_text.isdigit():
                    data &= Q(rr_no__icontains=search_text) | Q(po_no__icontains=search_text)
                else:
                    data &= (Q(source__icontains=search_text))
            usecase_data = RailData.objects(data).order_by("-avery_placement_date", "-avery_completion_date")
            count = len(usecase_data)
            path = None
            logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
            if usecase_data:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "Rail_Avery_Report_{}.xlsx".format(
                            datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format()
                    cell_format2.set_bold()
                    cell_format2.set_font_size(10)
                    cell_format2.set_align("center")
                    cell_format2.set_align("vcenter")
                    cell_format2.set_text_wrap(True)
                    cell_format2.set_border(1)

                    header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
                    date_format = workbook.add_format({'align': 'center', 'font_size': 12, "bold": True})
                    report_name_format = workbook.add_format({'align': 'center', 'font_size': 15, "bold": True})

                    header_format.set_align("vcenter")
                    date_format.set_align("vcenter")
                    report_name_format.set_align("vcenter")
                    header_format.set_border(1)
                    date_format.set_border(1)
                    report_name_format.set_border(1)

                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)
                    cell_format = workbook.add_format()
                    cell_format.set_font_size(10)
                    cell_format.set_align("center")
                    cell_format.set_align("vcenter")
                    cell_format.set_text_wrap(True)
                    cell_format.set_border(1)

                    worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                    
                    # Merge cells for the main header and place it in the center
                    main_header = "GMR Warora Energy Limited"  # Set your main header text here
                    worksheet.merge_range("A1:AO1", main_header, header_format)  # Merge cells A1 to H1 for the header
                    
                    # Write the current date on the left side (A2)
                    worksheet.write("A2", f"Date: {datetime.datetime.strptime(end_date, '%Y-%m-%dT%H:%M').strftime('%d-%m-%Y')}", date_format)
                    worksheet.merge_range("C2:AO2", f"GWEL-Rail Coal Journey", report_name_format)

                    headers = ["RR No.", 
                               "RR Qty", 
                               "PO NO", 
                               "PO Date", 
                               "Line Item", 
                               "Source", 
                               "GWEL Placement Date", 
                               "GWEL Completion Date", 
                               "GWEL Received Wagons", 
                               "GWEL Pending Wagons", 
                               "Boxes Loaded", 
                               "Total Secl Gross WT", 
                               "Total Secl Tare WT", 
                               "Total Secl Net WT", 
                               "Total Rly Gross WT", 
                               "Total Rly Tare WT", 
                               "Total Rly Net WT", 
                               "Total GWEL Gross WT", 
                               "Total GWEL Tare WT", 
                               "Total Gwel Net WT", 
                               "Source Type", 
                               "Month", 
                               "RR Date", 
                               "Siding", 
                               "Mine", 
                               "Grade", 
                               "Adho Sanrachna Vikas", 
                               "Assessable Value", 
                               "DMF", 
                               "Evac Facility Charge", 
                               "Gross Bill Value", 
                               "Gst Comp Cess", 
                               "IGST", 
                               "Less Underloading Charges", 
                               "Net Value", 
                               "Nmet Charges", 
                               "Pariyavaran Upkar", 
                               "Sizing charges", 
                               "Royality Charges", 
                               "Total Amount", 
                               "Created At"]
                   
                    for index, header in enumerate(headers):
                        worksheet.write(2, index, header, cell_format2)

                    for row, query in enumerate(usecase_data, start=3):
                        result = query.averyPayloadMain()
                        if payload.get("po_date"):
                            if 10 <= len(payload.get("po_date")) <= 13:
                                result["po_date"] = str(datetime.datetime.fromtimestamp(int(result["po_date"]) / 1000).strftime("%Y-%m-%d"))
                        result['total_gwel_gross_wt'] = 0
                        result['total_gwel_tare_wt'] = 0
                        result['total_gwel_net_wt'] = 0
                        # result["GWEL_received_wagons"] = 0
                        # result["GWEL_pending_wagons"] = 0

                        try:
                            fetchsapDatarails = sapRecordsRail.objects.get(rr_no=query.rr_no)
                            if fetchsapDatarails.adho_sanrachna_vikas:
                                result["adho_sanrachna_vikas"] = fetchsapDatarails.adho_sanrachna_vikas
                            else:
                                result["adho_sanrachna_vikas"] = 0
                            if fetchsapDatarails.assessable_value:
                                result["assessable_value"] = fetchsapDatarails.assessable_value
                            else:
                                result["assessable_value"] = 0
                            if fetchsapDatarails.dmf:
                                result["dmf"] = fetchsapDatarails.dmf
                            else:
                                result["dmf"] = 0
                            if fetchsapDatarails.evac_facility_charge:
                                result["evac_facility_charge"] = fetchsapDatarails.evac_facility_charge
                            else:
                                result["evac_facility_charge"] = 0
                            if fetchsapDatarails.gross_bill_value:
                                result["gross_bill_value"] = fetchsapDatarails.gross_bill_value
                            else:
                                result["gross_bill_value"] = 0
                            if fetchsapDatarails.gst_comp_cess:
                                result["gst_comp_cess"] = fetchsapDatarails.gst_comp_cess
                            else:
                                result["gst_comp_cess"] = 0
                            if fetchsapDatarails.igst:
                                result["igst"] = fetchsapDatarails.igst
                            else:
                                result["igst"] = 0
                            if fetchsapDatarails.less_underloading_charges:
                                result["less_underloading_charges"] = fetchsapDatarails.   less_underloading_charges
                            else:
                                result["less_underloading_charges"] = 0
                            if fetchsapDatarails.net_value:
                                result["net_value"] = fetchsapDatarails.net_value
                            else:
                                result["net_value"] = 0
                            if fetchsapDatarails.nmet_charges:
                                result["nmet_charges"] = fetchsapDatarails.nmet_charges
                            else:
                                result["nmet_charges"] = 0
                            if fetchsapDatarails.pariyavaran_upkar:
                                result["pariyavaran_upkar"] = fetchsapDatarails.pariyavaran_upkar
                            else:
                                result["pariyavaran_upkar"] = 0
                            if fetchsapDatarails.sizing_charges:
                                result["sizing_charges"] = fetchsapDatarails.sizing_charges
                            else:
                                result["sizing_charges"] = 0
                            if fetchsapDatarails.royality_charges:
                                result["royality_charges"] = fetchsapDatarails.royality_charges
                            else:
                                result["royality_charges"] = 0
                            if fetchsapDatarails.total_amount:
                                result["total_amount"] = fetchsapDatarails.total_amount
                            else:
                                result["total_amount"] = 0
                        except DoesNotExist as e:
                            result["adho_sanrachna_vikas"] = 0
                            result["assessable_value"] = 0
                            result["dmf"] = 0
                            result["evac_facility_charge"] = 0
                            result["gross_bill_value"] = 0
                            result["gst_comp_cess"] = 0
                            result["igst"] = 0
                            result["less_underloading_charges"] = 0
                            result["net_value"] = 0
                            result["nmet_charges"] = 0
                            result["pariyavaran_upkar"] = 0
                            result["sizing_charges"] = 0
                            result["royality_charges"] = 0
                            result["total_amount"] = 0

                        if query.avery_rly_data:
                            total_wagon_gross_wt = 0
                            total_wagon_tare_wt = 0
                            total_wagon_net_wt = 0
                            for singleRailData in query.avery_rly_data:
                                # Handle wagon_gross_wt
                                if hasattr(singleRailData, 'wagon_gross_wt'):
                                # if singleRailData.wagon_gross_wt:
                                    try:
                                        total_wagon_gross_wt += float(singleRailData.wagon_gross_wt)
                                    except ValueError:
                                        print(f"Warning: Invalid data for wagon_gross_wt: {singleRailData.wagon_gross_wt}")

                                # Handle wagon_tare_wt
                                if hasattr(singleRailData, 'wagon_tare_wt'):
                                # if singleRailData.wagon_tare_wt:
                                    try:
                                        total_wagon_tare_wt += float(singleRailData.wagon_tare_wt)
                                    except ValueError:
                                        print(f"Warning: Invalid data for wagon_tare_wt: {singleRailData.wagon_tare_wt}")

                                # Handle wagon_net_wt
                                # if singleRailData.wagon_net_wt:
                                if hasattr(singleRailData, 'wagon_net_wt'):
                                    try:
                                        total_wagon_net_wt += float(singleRailData.wagon_net_wt)
                                    except ValueError:
                                        print(f"Warning: Invalid data for wagon_net_wt: {singleRailData.wagon_net_wt}")
                                result['total_gwel_gross_wt'] = round(total_wagon_gross_wt, 2)
                                result['total_gwel_tare_wt'] = round(total_wagon_tare_wt, 2)
                                result['total_gwel_net_wt'] = round(total_wagon_net_wt, 2)
                                # result["GWEL_received_wagons"] = gwel_received_wagons
                                # result["GWEL_pending_wagons"] = int(query.boxes_loaded) - int(gwel_received_wagons)
                        # result['total_wagon_gross_wt'] = 0
                        # result['total_wagon_tare_wt'] = 0
                        # result['total_wagon_net_wt'] = 0
                        # result["GWEL_received_wagons"] = 0
                        # result["GWEL_pending_wagons"] = 0
                        # worksheet.write(row, 0, count, cell_format)     
                        worksheet.write(row, 0, str(result["rr_no"]), cell_format)                      
                        worksheet.write(row, 1, str(result["rr_qty"]), cell_format)                      
                        worksheet.write(row, 2, str(result["po_no"]), cell_format)                      
                        worksheet.write(row, 3, str(result["po_date"]), cell_format)                      
                        worksheet.write(row, 4, str(result["line_item"]), cell_format)                      
                        worksheet.write(row, 5, str(result["source"]), cell_format)                      
                        worksheet.write(row, 6, str(result["GWEL_placement_date"]), cell_format)                      
                        worksheet.write(row, 7, str(result["GWEL_completion_date"]), cell_format)
                        worksheet.write(row, 8, str(result["GWEL_received_wagons"]), cell_format)                      
                        worksheet.write(row, 9, str(result["GWEL_pending_wagons"]), cell_format)                      
                        worksheet.write(row, 10, str(result["boxes_loaded"]), cell_format)                      
                        worksheet.write(row, 11, str(result["total_secl_gross_wt"]), cell_format)                      
                        worksheet.write(row, 12, str(result["total_secl_tare_wt"]), cell_format)                      
                        worksheet.write(row, 13, str(result["total_secl_net_wt"]), cell_format)                    
                        worksheet.write(row, 14, str(result["total_rly_gross_wt"]), cell_format)                      
                        worksheet.write(row, 15, str(result["total_rly_tare_wt"]), cell_format)                      
                        worksheet.write(row, 16, str(result["total_rly_net_wt"]), cell_format)                    
                        worksheet.write(row, 17, str(result["total_gwel_gross_wt"]), cell_format)                      
                        worksheet.write(row, 18, str(result["total_gwel_tare_wt"]), cell_format)                      
                        worksheet.write(row, 19, str(result["total_gwel_net_wt"]), cell_format)                      
                        worksheet.write(row, 20, str(result["source_type"]), cell_format)                      
                        worksheet.write(row, 21, str(result["month"]), cell_format)                      
                        worksheet.write(row, 22, str(result["rr_date"]), cell_format)                      
                        worksheet.write(row, 23, str(result["siding"]), cell_format)                      
                        worksheet.write(row, 24, str(result["mine"]), cell_format)                      
                        worksheet.write(row, 25, str(result["grade"]), cell_format)                 
                        worksheet.write(row, 26, str(result["adho_sanrachna_vikas"]), cell_format)                 
                        worksheet.write(row, 27, str(result["assessable_value"]), cell_format)                 
                        worksheet.write(row, 28, str(result["dmf"]), cell_format)                 
                        worksheet.write(row, 29, str(result["evac_facility_charge"]), cell_format)                 
                        worksheet.write(row, 30, str(result["gross_bill_value"]), cell_format)                 
                        worksheet.write(row, 31, str(result["gst_comp_cess"]), cell_format)                 
                        worksheet.write(row, 32, str(result["igst"]), cell_format)                 
                        worksheet.write(row, 33, str(result["less_underloading_charges"]), cell_format)                 
                        worksheet.write(row, 34, str(result["net_value"]), cell_format)                 
                        worksheet.write(row, 35, str(result["nmet_charges"]), cell_format)                 
                        worksheet.write(row, 36, str(result["pariyavaran_upkar"]), cell_format)                 
                        worksheet.write(row, 37, str(result["sizing_charges"]), cell_format)                 
                        worksheet.write(row, 38, str(result["royality_charges"]), cell_format)                 
                        worksheet.write(row, 39, str(result["total_amount"]), cell_format)        
                        worksheet.write(row, 40, str(result["created_at"]), cell_format)                   
                        
                        count-=1
                        
                    workbook.close()
                    console_logger.debug("Successfully {} report generated".format(service_id))
                    console_logger.debug("sent data {}".format(path))

                    return {
                            "Type": "gmr_rail_avery_journey_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                            }
                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            else:
                console_logger.error("No data found")
                return {
                        "Type": "gmr_rail_journey_download_event",
                        "Datatype": "Report",
                        "File_Path": path,
                        }
    except Exception as e:
        console_logger.debug("----- Fetch Report Name Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/fetch/avery/rcr", tags=["Rail Map"])
def endpoint_to_fetch_avery_rcr_data(response: Response, currentPage: Optional[int] = None, perPage: Optional[int] = None, search_text: Optional[str] = None, start_timestamp: Optional[str] = None, end_timestamp: Optional[str] = None, month_date: Optional[str] = None, type: Optional[str] = "display"):
    """
    Function that fetches avery data from rcr db  
    Parameters:
        currentPage: Page No | 
        perPage: How much data on particular page |
        search_text: Text to search | 
        start_timestamp: start timestamp | 
        end_timestamp: end timestamp | 
        month_date: Single Month, for eg: 2024-11, 2024-10 |
        type: Display, Download

    Returns:
        Set of data
    """
    try:
        result = {        
                "labels": [],
                "datasets": [],
                "total" : 0,
                "page_size": 15
        }
        if type and type == "display":
            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage

            data = Q()

            # based on condition for timestamp playing with & and | 
            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(avery_placement_date__gte = start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M","Asia/Kolkata",False)
                data &= Q(avery_placement_date__lte = end_date)

            if search_text:
                if search_text.isdigit():
                    data &= Q(rr_no__icontains=search_text) | Q(po_no__icontains=search_text)
                else:
                    data &= (Q(source__icontains=search_text))

            if month_date:
                start_date = f'{month_date}-01'
                startd_date=datetime.datetime.strptime(f"{start_date}T00:00","%Y-%m-%dT%H:%M")
                end_date = (datetime.datetime.strptime(start_date, "%Y-%m-%d") + relativedelta(day=31)).strftime("%Y-%m-%d")
                data &= Q(avery_placement_date__gte = startd_date.strftime("%Y-%m-%dT%H:%M"))
                data &= Q(avery_placement_date__lte = f"{end_date}T23:59")
            offset = (page_no - 1) * page_len

            logs = (
                RcrData.objects(data)
                .order_by("-avery_placement_date", "-avery_completion_date")
                .skip(offset)
                .limit(page_len)
            )  
            if any(logs):
                for log in logs:
                    payload = log.averyPayloadMain()
                    if payload.get("po_date"):
                        if 10 < len(payload.get("po_date")) <= 13:
                            payload["po_date"] = str(datetime.datetime.fromtimestamp(int(payload["po_date"]) / 1000).strftime("%Y-%m-%d"))
                    payload['total_gwel_gross_wt'] = 0
                    payload['total_gwel_tare_wt'] = 0
                    payload['total_gwel_net_wt'] = 0
                    try:
                        fetchSapRcrRecords = sapRecordsRCR.objects.get(rr_no=log.rr_no)
                        if fetchSapRcrRecords.secl_mode_transport:
                            payload["secl_mode_transport"] = fetchSapRcrRecords.secl_mode_transport
                        else:
                            payload["secl_mode_transport"] = 0
                        if fetchSapRcrRecords.area:
                            payload["area"] = fetchSapRcrRecords.area
                        else:
                            payload["area"] = ""
                        if fetchSapRcrRecords.secl_basic_price:
                            payload["secl_basic_price"] = fetchSapRcrRecords.secl_basic_price
                        else:
                            payload["secl_basic_price"] = 0
                        if fetchSapRcrRecords.secl_sizing_charges:
                            payload["secl_sizing_charges"] = fetchSapRcrRecords.secl_sizing_charges
                        else:
                            payload["secl_sizing_charges"] = 0
                        if fetchSapRcrRecords.secl_stc_charges:
                            payload["secl_stc_charges"] = fetchSapRcrRecords.secl_stc_charges
                        else:
                            payload["secl_stc_charges"] = 0
                        if fetchSapRcrRecords.secl_evac_facility_charges:
                            payload["secl_evac_facility_charges"] = fetchSapRcrRecords.secl_evac_facility_charges
                        else:
                            payload["secl_evac_facility_charges"] = 0
                        if fetchSapRcrRecords.secl_nmet_charges:
                            payload["secl_nmet_charges"] = fetchSapRcrRecords.secl_nmet_charges
                        else:
                            payload["secl_nmet_charges"] = 0
                        if fetchSapRcrRecords.secl_dmf:
                            payload["secl_dmf"] = fetchSapRcrRecords.secl_dmf
                        else:
                            payload["secl_dmf"] = 0
                        if fetchSapRcrRecords.secl_adho_sanrachna_vikas:
                            payload["secl_adho_sanrachna_vikas"] = fetchSapRcrRecords.secl_adho_sanrachna_vikas
                        else:
                            payload["secl_adho_sanrachna_vikas"] = 0
                        if fetchSapRcrRecords.secl_pariyavaran_upkar:
                            payload["secl_pariyavaran_upkar"] = fetchSapRcrRecords.secl_pariyavaran_upkar
                        else:
                            payload["secl_pariyavaran_upkar"] = 0
                        if fetchSapRcrRecords.secl_terminal_tax:
                            payload["secl_terminal_tax"] = fetchSapRcrRecords.secl_terminal_tax
                        else:
                            payload["secl_terminal_tax"] = 0
                        if fetchSapRcrRecords.secl_assessable_tax:
                            payload["secl_assessable_tax"] = fetchSapRcrRecords.secl_assessable_tax
                        else:
                            payload["secl_assessable_tax"] = 0
                        if fetchSapRcrRecords.secl_igst:
                            payload["secl_igst"] = fetchSapRcrRecords.secl_igst
                        else:
                            payload["secl_igst"] = 0
                        if fetchSapRcrRecords.secl_gst_comp_cess:
                            payload["secl_gst_comp_cess"] = fetchSapRcrRecords.secl_gst_comp_cess
                        else:
                            payload["secl_gst_comp_cess"] = 0
                        if fetchSapRcrRecords.sap_po:
                            payload["sap_po"] = fetchSapRcrRecords.sap_po
                        else:
                            payload["sap_po"] = 0
                    except DoesNotExist as e:
                        payload["secl_mode_transport"] = 0
                        payload["area"] = ""
                        payload["secl_basic_price"] = 0
                        payload["secl_sizing_charges"] = 0
                        payload["secl_stc_charges"] = 0
                        payload["secl_evac_facility_charges"] = 0
                        payload["secl_nmet_charges"] = 0
                        payload["secl_dmf"] = 0
                        payload["secl_adho_sanrachna_vikas"] = 0
                        payload["secl_pariyavaran_upkar"] = 0
                        payload["secl_terminal_tax"] = 0
                        payload["secl_assessable_tax"] = 0
                        payload["secl_igst"] = 0
                        payload["secl_gst_comp_cess"] = 0
                        payload["sap_po"] = 0

                        
                    if log.avery_rly_data:
                        total_wagon_gross_wt = 0
                        total_wagon_tare_wt = 0
                        total_wagon_net_wt = 0
                        for singleRailData in log.avery_rly_data:
                            # Handle wagon_gross_wt
                            if singleRailData.gwel_gross_wt:
                                try:
                                    total_wagon_gross_wt += float(singleRailData.gwel_gross_wt)
                                except ValueError:
                                    print(f"Warning: Invalid data for wagon_gross_wt: {singleRailData.wagon_gross_wt}")

                            # Handle wagon_tare_wt
                            if singleRailData.gwel_tare_wt:
                                try:
                                    total_wagon_tare_wt += float(singleRailData.gwel_tare_wt)
                                except ValueError:
                                    print(f"Warning: Invalid data for wagon_tare_wt: {singleRailData.wagon_tare_wt}")

                            # Handle wagon_net_wt
                            if singleRailData.gwel_net_wt:
                                try:
                                    total_wagon_net_wt += float(singleRailData.gwel_net_wt)
                                except ValueError:
                                    print(f"Warning: Invalid data for wagon_net_wt: {singleRailData.wagon_net_wt}")
                            payload['total_gwel_gross_wt'] = total_wagon_gross_wt
                            payload['total_gwel_tare_wt'] = total_wagon_tare_wt
                            payload['total_gwel_net_wt'] = total_wagon_net_wt
                            # payload["GWEL_received_wagons"] = gwel_received_wagons
                            # payload["GWEL_pending_wagons"] = int(log.boxes_loaded) - int(gwel_received_wagons)
                    # result["labels"] = list(payload.keys())
                    result["labels"] = ["rr_no", "rr_qty", "po_no", "po_date", "line_item", "source", "GWEL_placement_date", "GWEL_completion_date", "GWEL_received_wagons", "GWEL_pending_wagons", "boxes_loaded", "total_secl_gross_wt", "total_secl_tare_wt", "total_secl_net_wt", "total_rly_gross_wt", "total_rly_tare_wt", "total_rly_net_wt", "total_gwel_gross_wt", "total_gwel_tare_wt", "total_gwel_net_wt", "source_type", "month", "rr_date", "siding", "mine", "grade", "secl_mode_transport", "area", "secl_basic_price", "secl_sizing_charges", "secl_stc_charges", "secl_evac_facility_charges", "secl_nmet_charges", "secl_dmf", "secl_adho_sanrachna_vikas", "secl_pariyavaran_upkar", "secl_terminal_tax", "secl_assessable_tax",
                    "secl_igst", "secl_gst_comp_cess", "sap_po", "freight", "gst", "pola", "total_freight", "sd", "created_at"]
                    result["datasets"].append(payload)
                    result["total"]= len(RcrData.objects(data))
            return result
        elif type and type == "download":
            del type

            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            # Constructing the base for query
            data = Q()

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M").strftime("%Y-%m-%dT%H:%M")
                data &= Q(avery_placement_date__gte=start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M","Asia/Kolkata",False).strftime("%Y-%m-%dT%H:%M")
                data &= Q(avery_placement_date__lte=end_date)
            
            if search_text:
                if search_text.isdigit():
                    data &= Q(rr_no__icontains=search_text) | Q(po_no__icontains=search_text)
                else:
                    data &= (Q(source__icontains=search_text))
            usecase_data = RcrData.objects(data).order_by("-avery_placement_date", "-avery_completion_date")
            count = len(usecase_data)
            path = None
            logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
            if usecase_data:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "Rcr_Avery_Report_{}.xlsx".format(
                            datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format()
                    cell_format2.set_bold()
                    cell_format2.set_font_size(10)
                    cell_format2.set_align("center")
                    cell_format2.set_align("vcenter")
                    cell_format2.set_text_wrap(True)
                    cell_format2.set_border(1)

                    header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
                    date_format = workbook.add_format({'align': 'center', 'font_size': 12, "bold": True})
                    report_name_format = workbook.add_format({'align': 'center', 'font_size': 15, "bold": True})

                    header_format.set_align("vcenter")
                    date_format.set_align("vcenter")
                    report_name_format.set_align("vcenter")
                    header_format.set_border(1)
                    date_format.set_border(1)
                    report_name_format.set_border(1)

                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)
                    cell_format = workbook.add_format()
                    cell_format.set_font_size(10)
                    cell_format.set_align("center")
                    cell_format.set_align("vcenter")
                    cell_format.set_text_wrap(True)
                    cell_format.set_border(1)

                    worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                    
                    # Merge cells for the main header and place it in the center
                    main_header = "GMR Warora Energy Limited"  # Set your main header text here
                    worksheet.merge_range("A1:AP1", main_header, header_format)  # Merge cells A1 to H1 for the header
                    
                    # Write the current date on the left side (A2)
                    worksheet.write("A2", f"Date: {datetime.datetime.strptime(end_date, '%Y-%m-%dT%H:%M').strftime('%d-%m-%Y')}", date_format)
                    worksheet.merge_range("C2:AP2", f"GWEL-RCR Coal Journey", report_name_format)

                    headers = [
                            "RR No.", 
                            "RR Qty", 
                            "PO No.", 
                            "PO Date", 
                            "Line Item", 
                            "Source", 
                            "GWEL Placement Date", 
                            "GWEL Completion Date", 
                            "GWEL Received Wagons", 
                            "GWEL Pending Wagons", 
                            "Boxes Loaded", 
                            "Total Secl Gross WT", 
                            "Total Secl Tare WT", 
                            "Total Secl Net WT", 
                            "Total Rly Gross WT", 
                            "Total RLY Tare WT", 
                            "Total Rly Net WT", 
                            "Total Gwel Gross WT", 
                            "Total Gwel Tare WT", 
                            "Total Gwel Net WT", 
                            "Source Type", 
                            "Month", 
                            "RR Date", 
                            "Siding", 
                            "Mine", 
                            "Grade", 
                            "Secl Mode Transport", 
                            "Area", 
                            "Secl Basic Price", 
                            "Secl Sizing Charges", 
                            "Secl Stc Charges", 
                            "Secl Evac Facility Charges", 
                            "Secl Nmet Charges", 
                            "Secl Dmf", 
                            "Secl Adho Sanrachna Vikas", 
                            "Secl Pariyavaran Upkar", 
                            "Secl Terminal Tax", 
                            "Secl Assessable Tax",
                            "Secl IGST", 
                            "Secl GSt Comp Cess", 
                            "Sap PO", 
                            "Created At"]
                   
                    for index, header in enumerate(headers):
                        worksheet.write(2, index, header, cell_format2)

                    for row, query in enumerate(usecase_data, start=3):
                        result = query.averyPayloadMain()
                        if result.get("po_date"):
                            # if len(result.get("po_date")) > 10:
                            if 10 < len(result.get("po_date")) <= 13:
                                result["po_date"] = str(datetime.datetime.fromtimestamp(int(result["po_date"]) / 1000).strftime("%Y-%m-%d"))
                        result['total_gwel_gross_wt'] = 0
                        result['total_gwel_tare_wt'] = 0
                        result['total_gwel_net_wt'] = 0

                        try:
                            fetchSapRcrRecords = sapRecordsRCR.objects.get(rr_no=query.rr_no)
                            if fetchSapRcrRecords.secl_mode_transport:
                                result["secl_mode_transport"] = fetchSapRcrRecords.secl_mode_transport
                            else:
                                result["secl_mode_transport"] = 0
                            if fetchSapRcrRecords.area:
                                result["area"] = fetchSapRcrRecords.area
                            else:
                                result["area"] = ""
                            if fetchSapRcrRecords.secl_basic_price:
                                result["secl_basic_price"] = fetchSapRcrRecords.secl_basic_price
                            else:
                                result["secl_basic_price"] = 0
                            if fetchSapRcrRecords.secl_sizing_charges:
                                result["secl_sizing_charges"] = fetchSapRcrRecords.secl_sizing_charges
                            else:
                                result["secl_sizing_charges"] = 0
                            if fetchSapRcrRecords.secl_stc_charges:
                                result["secl_stc_charges"] = fetchSapRcrRecords.secl_stc_charges
                            else:
                                result["secl_stc_charges"] = 0
                            if fetchSapRcrRecords.secl_evac_facility_charges:
                                result["secl_evac_facility_charges"] = fetchSapRcrRecords.secl_evac_facility_charges
                            else:
                                result["secl_evac_facility_charges"] = 0
                            if fetchSapRcrRecords.secl_nmet_charges:
                                result["secl_nmet_charges"] = fetchSapRcrRecords.secl_nmet_charges
                            else:
                                result["secl_nmet_charges"] = 0
                            if fetchSapRcrRecords.secl_dmf:
                                result["secl_dmf"] = fetchSapRcrRecords.secl_dmf
                            else:
                                result["secl_dmf"] = 0
                            if fetchSapRcrRecords.secl_adho_sanrachna_vikas:
                                result["secl_adho_sanrachna_vikas"] = fetchSapRcrRecords.secl_adho_sanrachna_vikas
                            else:
                                result["secl_adho_sanrachna_vikas"] = 0
                            if fetchSapRcrRecords.secl_pariyavaran_upkar:
                                result["secl_pariyavaran_upkar"] = fetchSapRcrRecords.secl_pariyavaran_upkar
                            else:
                                result["secl_pariyavaran_upkar"] = 0
                            if fetchSapRcrRecords.secl_terminal_tax:
                                result["secl_terminal_tax"] = fetchSapRcrRecords.secl_terminal_tax
                            else:
                                result["secl_terminal_tax"] = 0
                            if fetchSapRcrRecords.secl_assessable_tax:
                                result["secl_assessable_tax"] = fetchSapRcrRecords.secl_assessable_tax
                            else:
                                result["secl_assessable_tax"] = 0
                            if fetchSapRcrRecords.secl_igst:
                                result["secl_igst"] = fetchSapRcrRecords.secl_igst
                            else:
                                result["secl_igst"] = 0
                            if fetchSapRcrRecords.secl_gst_comp_cess:
                                result["secl_gst_comp_cess"] = fetchSapRcrRecords.secl_gst_comp_cess
                            else:
                                result["secl_gst_comp_cess"] = 0
                            if fetchSapRcrRecords.sap_po:
                                result["sap_po"] = fetchSapRcrRecords.sap_po
                            else:
                                result["sap_po"] = 0
                        except DoesNotExist as e:
                            result["secl_mode_transport"] = 0
                            result["area"] = ""
                            result["secl_basic_price"] = 0
                            result["secl_sizing_charges"] = 0
                            result["secl_stc_charges"] = 0
                            result["secl_evac_facility_charges"] = 0
                            result["secl_nmet_charges"] = 0
                            result["secl_dmf"] = 0
                            result["secl_adho_sanrachna_vikas"] = 0
                            result["secl_pariyavaran_upkar"] = 0
                            result["secl_terminal_tax"] = 0
                            result["secl_assessable_tax"] = 0
                            result["secl_igst"] = 0
                            result["secl_gst_comp_cess"] = 0
                            result["sap_po"] = 0

                        if query.avery_rly_data:
                            total_wagon_gross_wt = 0
                            total_wagon_tare_wt = 0
                            total_wagon_net_wt = 0
                            for singleRailData in query.avery_rly_data:
                                # Handle wagon_gross_wt
                                if hasattr(singleRailData, 'wagon_gross_wt'):
                                # if singleRailData.wagon_gross_wt:
                                    try:
                                        total_wagon_gross_wt += float(singleRailData.wagon_gross_wt)
                                    except ValueError:
                                        print(f"Warning: Invalid data for wagon_gross_wt: {singleRailData.wagon_gross_wt}")

                                # Handle wagon_tare_wt
                                if hasattr(singleRailData, 'wagon_tare_wt'):
                                # if singleRailData.wagon_tare_wt:
                                    try:
                                        total_wagon_tare_wt += float(singleRailData.wagon_tare_wt)
                                    except ValueError:
                                        print(f"Warning: Invalid data for wagon_tare_wt: {singleRailData.wagon_tare_wt}")

                                # Handle wagon_net_wt
                                # if singleRailData.wagon_net_wt:
                                if hasattr(singleRailData, 'wagon_net_wt'):
                                    try:
                                        total_wagon_net_wt += float(singleRailData.wagon_net_wt)
                                    except ValueError:
                                        print(f"Warning: Invalid data for wagon_net_wt: {singleRailData.wagon_net_wt}")
                                result['total_gwel_gross_wt'] = total_wagon_gross_wt
                                result['total_gwel_tare_wt'] = total_wagon_tare_wt
                                result['total_gwel_net_wt'] = total_wagon_net_wt
                                # result["GWEL_received_wagons"] = gwel_received_wagons
                                # result["GWEL_pending_wagons"] = int(query.boxes_loaded) - int(gwel_received_wagons)
                        # result['total_wagon_gross_wt'] = 0
                        # result['total_wagon_tare_wt'] = 0
                        # result['total_wagon_net_wt'] = 0
                        # result["GWEL_received_wagons"] = 0
                        # result["GWEL_pending_wagons"] = 0
                        # worksheet.write(row, 0, count, cell_format)     
                        worksheet.write(row, 0, str(result["rr_no"]), cell_format)                      
                        worksheet.write(row, 1, str(result["rr_qty"]), cell_format)                      
                        worksheet.write(row, 2, str(result["po_no"]), cell_format)                      
                        worksheet.write(row, 3, str(result["po_date"]), cell_format)                      
                        worksheet.write(row, 4, str(result["line_item"]), cell_format)                      
                        worksheet.write(row, 5, str(result["source"]), cell_format)                      
                        worksheet.write(row, 6, str(result["GWEL_placement_date"]), cell_format)                      
                        worksheet.write(row, 7, str(result["GWEL_completion_date"]), cell_format)
                        worksheet.write(row, 8, str(result["GWEL_received_wagons"]), cell_format)                      
                        worksheet.write(row, 9, str(result["GWEL_pending_wagons"]), cell_format)                      
                        worksheet.write(row, 10, str(result["boxes_loaded"]), cell_format)
                        if result.get("total_secl_gross_wt"):               
                            worksheet.write(row, 11, float(result["total_secl_gross_wt"]), cell_format)
                        else:                  
                            worksheet.write(row, 11, 0, cell_format)
                        if result.get("total_secl_tare_wt"):        
                            worksheet.write(row, 12, float(result["total_secl_tare_wt"]), cell_format)                      
                        else:
                            worksheet.write(row, 12, 0, cell_format)
                        if result.get("total_secl_net_wt"):             
                            worksheet.write(row, 13, float(result["total_secl_net_wt"]), cell_format)                    
                        else:
                            worksheet.write(row, 13, 0, cell_format)                    
                        if result.get("total_rly_gross_wt"):
                            worksheet.write(row, 14, float(result["total_rly_gross_wt"]), cell_format)                      
                        else:
                            worksheet.write(row, 14, 0, cell_format)
                        if result.get("total_rly_tare_wt"):                  
                            worksheet.write(row, 15, float(result["total_rly_tare_wt"]), cell_format)
                        else:                     
                            worksheet.write(row, 15, 0, cell_format)
                        if result.get("total_rly_net_wt"):             
                            worksheet.write(row, 16, float(result["total_rly_net_wt"]), cell_format)                    
                        else:
                            worksheet.write(row, 16, 0, cell_format)
                        if result.get("total_gwel_gross_wt"):             
                            worksheet.write(row, 17, float(result["total_gwel_gross_wt"]), cell_format)
                        else:               
                            worksheet.write(row, 17, 0, cell_format)
                        if result.get("total_gwel_tare_wt"):               
                            worksheet.write(row, 18, float(result["total_gwel_tare_wt"]), cell_format)                      
                        else:
                            worksheet.write(row, 18, 0, cell_format)
                        if result.get("total_gwel_net_wt"):        
                            worksheet.write(row, 19, float(result["total_gwel_net_wt"]), cell_format)                      
                        else:
                            worksheet.write(row, 19, 0, cell_format)                      
                        worksheet.write(row, 20, str(result["source_type"]), cell_format)                      
                        worksheet.write(row, 21, str(result["month"]), cell_format)                      
                        worksheet.write(row, 22, str(result["rr_date"]), cell_format)                      
                        worksheet.write(row, 23, str(result["siding"]), cell_format)                      
                        worksheet.write(row, 24, str(result["mine"]), cell_format)                      
                        worksheet.write(row, 25, str(result["grade"]), cell_format)                  
                        worksheet.write(row, 26, str(result["secl_mode_transport"]), cell_format)                  
                        worksheet.write(row, 27, str(result["area"]), cell_format)                  
                        worksheet.write(row, 28, float(result["secl_basic_price"]), cell_format)                  
                        worksheet.write(row, 29, float(result["secl_sizing_charges"]), cell_format)                  
                        worksheet.write(row, 30, float(result["secl_stc_charges"]), cell_format)                  
                        worksheet.write(row, 31, float(result["secl_evac_facility_charges"]), cell_format)                  
                        worksheet.write(row, 32, float(result["secl_nmet_charges"]), cell_format)                  
                        worksheet.write(row, 33, float(result["secl_dmf"]), cell_format)                  
                        worksheet.write(row, 34, float(result["secl_adho_sanrachna_vikas"]), cell_format)                  
                        worksheet.write(row, 35, float(result["secl_pariyavaran_upkar"]), cell_format)                  
                        worksheet.write(row, 36, float(result["secl_terminal_tax"]), cell_format)                  
                        worksheet.write(row, 37, float(result["secl_assessable_tax"]), cell_format)                  
                        worksheet.write(row, 38, float(result["secl_igst"]), cell_format)                  
                        worksheet.write(row, 39, float(result["secl_gst_comp_cess"]), cell_format)                  
                        worksheet.write(row, 40, str(result["sap_po"]), cell_format)               
                        worksheet.write(row, 41, str(result["created_at"]), cell_format)                   
                        
                        count-=1
                        
                    workbook.close()
                    console_logger.debug("Successfully {} report generated".format(service_id))
                    console_logger.debug("sent data {}".format(path))

                    return {
                            "Type": "gmr_rcr_avery_journey_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                            }
                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            else:
                console_logger.error("No data found")
                return {
                        "Type": "gmr_rcr_avery_journey_download_event",
                        "Datatype": "Report",
                        "File_Path": path,
                        }
    except Exception as e:
        console_logger.debug("----- Fetch Report Name Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/fetch/singlerail", tags=["Rail Map"])
def endpoint_to_fetch_railway_data_single(response: Response, rrno: str):
    """
    Function that fetches single railway data using rrno
    Parameters:
        rr_no

    Returns:
        single dictionary data for particular rr_no
    """
    try:
        fetchRailData = RailData.objects.get(rr_no=rrno)
        return fetchRailData.payload()
    except DoesNotExist as e:
        raise HTTPException(status_code=404, detail="No data found")
    except Exception as e:
        console_logger.debug("----- Fetch Railway Data Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        # console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

@router.get("/fetch/singlercravery", tags=["Rail Map"])
def endpoint_to_fetch_rcravery_data(response: Response, rrno: str):
    """
    Function that fetches single rcravery data using rrno
    Parameters:
        rr_no

    Returns:
        single dictionary data for particular rr_no
    """
    try:
        fetchRcrData = RcrData.objects.get(rr_no=rrno)
        return fetchRcrData.payload()
    except DoesNotExist as e:
        raise HTTPException(status_code=404, detail="No data found")
    except Exception as e:
        console_logger.debug("----- Fetch RailwayRcr Data Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        # console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


    
@router.get("/fetch/allminenames", tags=["Rail Map"])
def endpoint_to_fetch_rail_mines(response: Response):
    try:
        # mine_names = short_mine_collection.find({},{"coal_journey":"rail"})
        mine_names = short_mine_collection.find({})
        dictData = {}
        # railData = []
        # roadData = []
        railData = {}
        roadData = {}
        for single_data in mine_names:
            if single_data.get("coal_journey") == "Rail":
                railData[single_data.get("mine_name")] = single_data.get("source_type")
                # railData.append(single_data.get("mine_name"))
            if single_data.get("coal_journey") == "Road":
                # roadData.append(single_data.get("mine_name"))
                roadData[single_data.get("mine_name")] = single_data.get("source_type")
        
        dictData["road"] = roadData
        dictData["rail"] = railData

        return dictData
    except Exception as e:
        console_logger.debug("----- Fetch Rail Mines Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        # console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/update/rail", tags=["Railway"])
def endpoint_to_insert_rail_data(response: Response, payload: RailwayData):
    try:
        # Extract data from payload
        final_data = payload.dict()

        # Fetch existing RailData document
        fetchRailData = RailData.objects.get(rr_no=final_data.get("rr_no"))

        if fetchRailData:
            # Update top-level fields in the RailData document
            for key, value in final_data.items():
                if key != 'secl_rly_data' and hasattr(fetchRailData, key):
                    setattr(fetchRailData, key, value)

            # Update secl_rly_data
            for new_data in final_data.get('secl_rly_data', []):
                updated = False
                for secl_data in fetchRailData.secl_rly_data:
                    if secl_data.wagon_no == new_data['wagon_no']:
                        for key, value in new_data.items():
                            setattr(secl_data, key, value)
                        updated = True
                        break
                if not updated:
                    fetchRailData.secl_rly_data.append(SeclRailData(**new_data))

            fetchRailData.save()
        return {"detail": "success"}
    except Exception as e:
        console_logger.debug("----- Fetch Report Name Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        # console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


# @router.post("/insert/rail", tags=["Railway"])
# def endpoint_to_insert_rail_data(response: Response, payload: RailwayData, rr_no: Optional[str] = None):
#     try:
#         # Extract data from payload
#         final_data = payload.dict()

#         try:
#             fetchRailData = RailData.objects.get(rr_no=rr_no)
#             # Update top-level fields in the RailData document
#             for key, value in final_data.items():
#                 if key != 'secl_rly_data' and hasattr(fetchRailData, key):
#                     setattr(fetchRailData, key, value)

#             # Update secl_rly_data
#             for new_data in final_data.get('secl_rly_data', []):
#                 updated = False
#                 for secl_data in fetchRailData.secl_rly_data:
#                     if secl_data.wagon_no == new_data['wagon_no']:
#                         for key, value in new_data.items():
#                             setattr(secl_data, key, value)
#                         updated = True
#                         break
#                 if not updated:
#                     fetchRailData.secl_rly_data.append(SeclRailData(**new_data))

#             fetchRailData.save()
#             return {"detail": "success"}
#         except DoesNotExist as e:
#             final_data = payload.dict()
#             secl_list_data = []
#             for single_data in final_data.get("secl_rly_data"):
#                 secl_rly_dict_data = {
#                     "indexing": single_data.get("indexing"),
#                     "wagon_owner": single_data.get("wagon_owner"),
#                     "wagon_type": single_data.get("wagon_type"),
#                     "wagon_no": single_data.get("wagon_no"),
#                     "secl_cc_wt": single_data.get("secl_cc_wt"),
#                     "secl_gross_wt": single_data.get("secl_gross_wt"),
#                     "secl_tare_wt": single_data.get("secl_tare_wt"),
#                     "secl_net_wt": single_data.get("secl_net_wt"),
#                     "secl_ol_wt": single_data.get("secl_ol_wt"),
#                     "secl_ul_wt":single_data.get("secl_ul_wt"),
#                     "secl_chargable_wt": single_data.get("secl_chargable_wt"),
#                     "rly_cc_wt": single_data.get("rly_cc_wt"),
#                     "rly_gross_wt": single_data.get("rly_gross_wt"),
#                     "rly_tare_wt": single_data.get("rly_tare_wt"),
#                     "rly_net_wt": single_data.get("rly_net_wt"),
#                     "rly_permissible_cc_wt": single_data.get("rly_permissible_cc_wt"),
#                     "rly_ol_wt": single_data.get("rly_ol_wt"),
#                     "rly_norm_rate": single_data.get("rly_norm_rate"),
#                     "rly_pun_rate": single_data.get("rly_pun_rate"),
#                     "rly_chargable_wt": single_data.get("rly_chargable_wt"),
#                     "rly_sliding_adjustment": single_data.get("rly_sliding_adjustment"),
#                 }
#                 secl_list_data.append(secl_rly_dict_data)
#             rail_data = RailData(
#                 rr_no=final_data.get("rr_no"),
#                 rr_qty=final_data.get("rr_qty"),
#                 po_no=final_data.get("po_no"),
#                 po_date=final_data.get("po_date"),
#                 line_item=final_data.get("line_item"),
#                 source=final_data.get("source"),
#                 placement_date=final_data.get("placement_date"),
#                 completion_date=final_data.get("completion_date"),
#                 drawn_date=final_data.get("drawn_date"),
#                 total_ul_wt=final_data.get("total_ul_wt"),
#                 boxes_supplied=final_data.get("boxes_supplied"),
#                 total_secl_gross_wt=final_data.get("total_secl_gross_wt"),
#                 total_secl_tare_wt=final_data.get("total_secl_tare_wt"),
#                 total_secl_net_wt=final_data.get("total_secl_net_wt"),
#                 total_secl_ol_wt=final_data.get("total_secl_ol_wt"),
#                 boxes_loaded=final_data.get("boxes_loaded"),
#                 total_rly_gross_wt=final_data.get("total_rly_gross_wt"),
#                 total_rly_tare_wt=final_data.get("total_rly_tare_wt"),
#                 total_rly_net_wt=final_data.get("total_rly_net_wt"),
#                 total_rly_ol_wt=final_data.get("total_rly_ol_wt"),
#                 total_secl_chargable_wt=final_data.get("total_secl_chargable_wt"),
#                 total_rly_chargable_wt=final_data.get("total_rly_chargable_wt"),
#                 freight=final_data.get("freight"),
#                 gst=final_data.get("gst"),
#                 pola=final_data.get("pola"),
#                 total_freight=final_data.get("total_freight"),
#                 source_type=final_data.get("source_type"),
#                 secl_rly_data=secl_list_data,
#             )     
#             rail_data.save()
#             return {"message": "Data inserted successfully"}

#     except Exception as e:
#         console_logger.debug("----- Fetch Report Name Error -----",e)
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#         console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#         return e


# Helper function to determine the rake_no
# def calculate_rake_no(month, placement_date):
#     # try:
#     # Convert strings to date objects
#     # month_start_date = datetime.strptime(month, '%b %d, %Y')  # Assuming month is in this format
#     month_start_date = datetime.datetime.strptime(month, '%Y-%m-%d')  # Assuming month is in this format
#     placement_date_obj = datetime.datetime.strptime(placement_date, '%Y-%m-%d')

#     # Calculate the 3rd date of the next month
#     next_month_start_date = month_start_date + timedelta(days=30)
#     next_month_3rd = next_month_start_date.replace(day=3)
#     console_logger.debug(month_start_date.replace(day=4))
#     console_logger.debug(placement_date_obj)
#     console_logger.debug(next_month_3rd)
#     # Check if placement_date falls between 4th of the current month and 3rd of the next month
#     if month_start_date.replace(day=4) <= placement_date_obj <= next_month_3rd:
#         return "1"
#     else:
#         return "rev1"
    # except ValueError:
    #     return "rev1"  # Default to rev1 if date parsing fails

def calculate_rake_no(month, placement_date, existing_rake_nos):
    try:
        month_start_date = datetime.datetime.strptime(month, '%Y-%m-%d')
        placement_date_obj = datetime.datetime.strptime(placement_date, '%Y-%m-%d')
        next_month_start_date = month_start_date + datetime.timedelta(days=32)
        console_logger.debug(next_month_start_date)
        next_month_3rd = next_month_start_date.replace(day=3)
        console_logger.debug(month_start_date.replace(day=4))
        console_logger.debug(placement_date_obj)
        console_logger.debug(next_month_3rd)
        console_logger.debug(existing_rake_nos)
        if month_start_date.replace(day=4) <= placement_date_obj <= next_month_3rd:
            rake_no_base = "1"
        else:
            rake_no_base = "rev1"
        filtered_rake_nos = [rake for rake in existing_rake_nos if rake is not None]
        console_logger.debug(rake_no_base)
        if "rev" in rake_no_base:
            # Filter out rake numbers that start with "rev"
            rev_list = [x for x in filtered_rake_nos if x.startswith("rev")]
            if rev_list:
                # Extract the numeric part and find the maximum value
                max_rev_number = max(int(x.split("rev")[1]) for x in rev_list)
                # Increment the maximum value
                rake_no_base = f"rev{max_rev_number + 1}"
            else:
                rake_no_base = "rev1"
        else:
            console_logger.debug("rev is absent")
            number_list = [int(x) for x in filtered_rake_nos if x.isdigit()]
            if number_list:
                max_number = max(number_list)
                rake_no_base = str(max_number + 1)
            else:
                rake_no_base = "1" 
        return rake_no_base
    except Exception as e:
        console_logger.debug("----- Calculate Rake No Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/insert/rail", tags=["Railway"])
def endpoint_to_insert_rail_data(response: Response, payload: RailwayData, id: Optional[str] = None):
    try:
        final_data = payload.dict()
        try:
            if id:
                fetchRailData = RailData.objects.get(id=ObjectId(id))
            else:
                fetchRailData = RailData.objects.get(rr_no=final_data.get("rr_no"))
            try:
                fetchSaprecordsRail = sapRecordsRail.objects.get(rr_no=final_data.get("rr_no"))
            except DoesNotExist as e:
                fetchSaprecordsRail = None
            if fetchSaprecordsRail:
                if fetchSaprecordsRail.month:
                    fetchRailData.month = datetime.datetime.strptime(fetchSaprecordsRail.month, '%b %d, %Y').strftime('%Y-%m-%d')
                fetchRailData.rr_date = fetchSaprecordsRail.rr_date
                fetchRailData.siding = fetchSaprecordsRail.siding
                fetchRailData.mine = fetchSaprecordsRail.mine
                fetchRailData.grade = fetchSaprecordsRail.grade
                fetchRailData.rr_qty = fetchSaprecordsRail.rr_qty
                fetchRailData.po_amount = fetchSaprecordsRail.po_amount
            
            fetchRailData.placement_date = final_data.get("placement_date")
            fetchRailData.completion_date = final_data.get("completion_date")
            fetchRailData.drawn_date = final_data.get("drawn_date")
            fetchRailData.total_ul_wt = final_data.get("total_ul_wt")
            fetchRailData.boxes_supplied = final_data.get("boxes_supplied")
            fetchRailData.total_secl_gross_wt = final_data.get("total_secl_gross_wt")
            fetchRailData.total_secl_tare_wt = final_data.get("total_secl_tare_wt")
            fetchRailData.total_secl_net_wt = final_data.get("total_secl_net_wt")
            fetchRailData.total_secl_ol_wt = final_data.get("total_secl_ol_wt")
            fetchRailData.boxes_loaded = final_data.get("boxes_loaded")
            fetchRailData.total_rly_gross_wt = final_data.get("total_rly_gross_wt")
            fetchRailData.total_rly_tare_wt = final_data.get("total_rly_tare_wt")
            fetchRailData.total_rly_net_wt = final_data.get("total_rly_net_wt")
            fetchRailData.total_rly_ol_wt = final_data.get("total_rly_ol_wt")
            fetchRailData.total_secl_chargable_wt = final_data.get("total_secl_chargable_wt")
            fetchRailData.total_rly_chargable_wt = final_data.get("total_rly_chargable_wt")
            fetchRailData.freight = final_data.get("freight")
            fetchRailData.gst = final_data.get("gst")
            if final_data.get("pola") == "Not found":
                fetchRailData.pola = ""
            else:
                fetchRailData.pola = final_data.get("pola")
            if final_data.get("sd") == "Not found":
                fetchRailData.sd = ""
            else:
                fetchRailData.sd = final_data.get("sd")
            fetchRailData.total_freight = final_data.get("total_freight")
            fetchRailData.source_type = final_data.get("source_type")
            # fetchRailData.month = final_data.get("month")

            for key, value in final_data.items():
                if key != 'secl_rly_data' and hasattr(fetchRailData, key):
                    setattr(fetchRailData, key, value)


            # if fetchRailData.placement_date:
            #     console_logger.debug(fetchRailData.placement_date)
            #     console_logger.debug(datetime.datetime.strptime(fetchSaprecordsRail.month, '%b %d, %Y').strftime('%Y-%m-%d'))
            #     console_logger.debug(datetime.datetime.strptime(fetchRailData.placement_date, '%Y-%m-%dT%H:%M').strftime('%Y-%m-%d'))
            #     # Set rake_no based on month and placement_date
            #     fetchRailData.rake_no = calculate_rake_no(datetime.datetime.strptime(fetchSaprecordsRail.month, '%b %d, %Y').strftime('%Y-%m-%d'), fetchRailData.placement_date.strftime('%Y-%m-%d'))

            for new_data in final_data.get('secl_rly_data', []):
                updated = False
                for secl_data in fetchRailData.secl_rly_data:
                    if secl_data.wagon_no == new_data['wagon_no']:
                        for key, value in new_data.items():
                            setattr(secl_data, key, value)
                        updated = True
                        break
                if not updated:
                    fetchRailData.secl_rly_data.append(SeclRailData(**new_data))
            listAveryData = []
            for new_data in final_data.get('secl_rly_data', []):
                dictAveryData = {}
                dictAveryData["indexing"] = new_data.get("indexing")
                dictAveryData["wagon_owner"] = new_data.get("wagon_owner")
                dictAveryData["wagon_type"] = new_data.get("wagon_type")
                dictAveryData["wagon_no"] = new_data.get("wagon_no")
                listAveryData.append(AveryRailData(**dictAveryData))
        
            fetchRailData.avery_rly_data = listAveryData
            fetchRailData.save()

            return {"detail": "success"}
        except DoesNotExist as e:
            final_data = payload.dict()
            secl_list_data = []
            for single_data in final_data.get("secl_rly_data"):
                secl_rly_dict_data = {
                    "indexing": single_data.get("indexing"),
                    "wagon_owner": single_data.get("wagon_owner"),
                    "wagon_type": single_data.get("wagon_type"),
                    "wagon_no": single_data.get("wagon_no"),
                    "secl_cc_wt": single_data.get("secl_cc_wt"),
                    "secl_gross_wt": single_data.get("secl_gross_wt"),
                    "secl_tare_wt": single_data.get("secl_tare_wt"),
                    "secl_net_wt": single_data.get("secl_net_wt"),
                    "secl_ol_wt": single_data.get("secl_ol_wt"),
                    "secl_ul_wt":single_data.get("secl_ul_wt"),
                    "secl_chargable_wt": single_data.get("secl_chargable_wt"),
                    "rly_cc_wt": single_data.get("rly_cc_wt"),
                    "rly_gross_wt": single_data.get("rly_gross_wt"),
                    "rly_tare_wt": single_data.get("rly_tare_wt"),
                    "rly_net_wt": single_data.get("rly_net_wt"),
                    "rly_permissible_cc_wt": single_data.get("rly_permissible_cc_wt"),
                    "rly_ol_wt": single_data.get("rly_ol_wt"),
                    "rly_norm_rate": single_data.get("rly_norm_rate"),
                    "rly_pun_rate": single_data.get("rly_pun_rate"),
                    "rly_chargable_wt": single_data.get("rly_chargable_wt"),
                    "rly_sliding_adjustment": single_data.get("rly_sliding_adjustment"),
                }
                secl_list_data.append(secl_rly_dict_data)

            avery_list_data = []
            for single_data in final_data.get("secl_rly_data"):
                avery_rly_dict_data = {
                    "indexing": single_data.get("indexing"),
                    "wagon_owner": single_data.get("wagon_owner"),
                    "wagon_type": single_data.get("wagon_type"),
                    "wagon_no": single_data.get("wagon_no"),
                }
                avery_list_data.append(avery_rly_dict_data)
            try:
                fetchSaprecordsRail = sapRecordsRail.objects.get(rr_no=final_data.get("rr_no"))
            except DoesNotExist as e:
                fetchSaprecordsRail = None
            console_logger.debug(final_data.get("sd"))
            rail_data = RailData(
                rr_no=final_data.get("rr_no"),
                # rr_qty=final_data.get("rr_qty"),
                rr_qty=fetchSaprecordsRail.rr_qty if fetchSaprecordsRail and fetchSaprecordsRail.rr_qty else "",
                po_no=final_data.get("po_no"),
                po_date=final_data.get("po_date"),
                line_item=final_data.get("line_item"),
                source=final_data.get("source"),
                placement_date=final_data.get("placement_date"),
                completion_date=final_data.get("completion_date"),
                drawn_date=final_data.get("drawn_date"),
                total_ul_wt=final_data.get("total_ul_wt"),
                boxes_supplied=final_data.get("boxes_supplied"),
                total_secl_gross_wt=final_data.get("total_secl_gross_wt"),
                total_secl_tare_wt=final_data.get("total_secl_tare_wt"),
                total_secl_net_wt=final_data.get("total_secl_net_wt"),
                total_secl_ol_wt=final_data.get("total_secl_ol_wt"),
                boxes_loaded=final_data.get("boxes_loaded"),
                total_rly_gross_wt=final_data.get("total_rly_gross_wt"),
                total_rly_tare_wt=final_data.get("total_rly_tare_wt"),
                total_rly_net_wt=final_data.get("total_rly_net_wt"),
                total_rly_ol_wt=final_data.get("total_rly_ol_wt"),
                total_secl_chargable_wt=final_data.get("total_secl_chargable_wt"),
                total_rly_chargable_wt=final_data.get("total_rly_chargable_wt"),
                freight=final_data.get("freight"),
                gst=final_data.get("gst"),
                pola=final_data.get("pola") if final_data.get("pola") != "Not found" else "",
                sd=final_data.get("sd") if final_data.get("sd") != "Not found" else "",
                total_freight=final_data.get("total_freight"),
                source_type=final_data.get("source_type"),
                month = final_data.get("month"),    # modified by faisal
                # month=datetime.datetime.strptime(fetchSaprecordsRail.month, '%b %d, %Y').strftime('%Y-%m-%d') if fetchSaprecordsRail and fetchSaprecordsRail.month else "",
                secl_rly_data=secl_list_data,
                avery_rly_data=avery_list_data,
                rr_date=fetchSaprecordsRail.rr_date if fetchSaprecordsRail and fetchSaprecordsRail.rr_date else "",
                siding=fetchSaprecordsRail.siding if fetchSaprecordsRail and fetchSaprecordsRail.siding else "",
                mine=fetchSaprecordsRail.mine if fetchSaprecordsRail and fetchSaprecordsRail.mine else "",
                grade=fetchSaprecordsRail.grade if fetchSaprecordsRail and fetchSaprecordsRail.grade else "",
                # rr_qty=fetchSaprecordsRail.get("rr_qty") if fetchSaprecordsRail.get("rr_qty") else "",
                po_amount=fetchSaprecordsRail.po_amount if fetchSaprecordsRail and fetchSaprecordsRail.po_amount else "",
            ) 
            existing_rake_nos = [data.rake_no for data in RailData.objects()]

            if final_data.get("placement_date") and fetchSaprecordsRail and fetchSaprecordsRail.month:
                placement_date_obj = datetime.datetime.strptime(final_data.get("placement_date"), '%Y-%m-%dT%H:%M')
                rail_data.rake_no = calculate_rake_no(datetime.datetime.strptime(fetchSaprecordsRail.month, '%b %d, %Y').strftime('%Y-%m-%d'),
                                                      placement_date_obj.strftime('%Y-%m-%d'),
                                                      existing_rake_nos)
            rail_data.save()
            return {"message": "Data inserted successfully"}

    except Exception as e:
        console_logger.debug("----- Fetch Report Name Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/fetch/rcr", tags=["Railway"])
def endpoint_to_fetch_rcr_data(response: Response, currentPage: Optional[int] = None, perPage: Optional[int] = None, search_text: Optional[str] = None, start_timestamp: Optional[str] = None, end_timestamp: Optional[str] = None, month_date: Optional[str] = None, type: Optional[str] = "display"):
    try:
        result = {        
                "labels": [],
                "datasets": [],
                "total" : 0,
                "page_size": 15
        }
        if type and type == "display":
            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage

            data = Q()

            # based on condition for timestamp playing with & and | 
            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(placement_date__gte = start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M","Asia/Kolkata",False)
                data &= Q(placement_date__lte = end_date)

            if search_text:
                if search_text.isdigit():
                    data &= Q(rr_no__icontains=search_text) | Q(po_no__icontains=search_text)
                else:
                    data &= (Q(source__icontains=search_text))

            if month_date:
                start_date = f'{month_date}-01'
                startd_date=datetime.datetime.strptime(f"{start_date}T00:00","%Y-%m-%dT%H:%M")
                end_date = (datetime.datetime.strptime(start_date, "%Y-%m-%d") + relativedelta(day=31)).strftime("%Y-%m-%d")
                data &= Q(placement_date__gte = startd_date.strftime("%Y-%m-%dT%H:%M"))
                data &= Q(placement_date__lte = f"{end_date}T23:59")

            offset = (page_no - 1) * page_len
            logs = (
                RcrData.objects(data)
                .order_by("-placement_date")
                .skip(offset)
                .limit(page_len)
            )   
            if any(logs):
                for log in logs:
                    result["labels"] = list(log.simplepayload().keys())
                    result["datasets"].append(log.simplepayload())
                result["total"]= len(RailData.objects(data))
            return result
        elif type and type == "download":
            del type

            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            # Constructing the base for query
            data = Q()

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(created_at__gte = start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M","Asia/Kolkata",False)
                data &= Q(created_at__lte = end_date)
            
            if search_text:
                if search_text.isdigit():
                    data &= Q(arv_cum_do_number__icontains = search_text) | Q(delivery_challan_number__icontains = search_text)
                else:
                    data &= Q(vehicle_number__icontains = search_text)

            usecase_data = RcrData.objects(data).order_by("-created_at")
            count = len(usecase_data)
            path = None
            logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
            if usecase_data:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "Rcr_Report_{}.xlsx".format(
                            datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format()
                    cell_format2.set_bold()
                    cell_format2.set_font_size(10)
                    cell_format2.set_align("center")
                    cell_format2.set_align("vcenter")
                    cell_format2.set_text_wrap(True)

                    header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
                    date_format = workbook.add_format({'align': 'left', 'font_size': 12, "bold": True})
                    report_name_format = workbook.add_format({'align': 'left', 'font_size': 12, "bold": True})

                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)
                    cell_format = workbook.add_format()
                    cell_format.set_font_size(10)
                    cell_format.set_align("center")
                    cell_format.set_align("vcenter")
                    cell_format.set_text_wrap(True)

                    worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                    
                    # Merge cells for the main header and place it in the center
                    main_header = "GMR Warora Energy Limited"  # Set your main header text here
                    worksheet.merge_range("A1:AC1", main_header, header_format)  # Merge cells A1 to H1 for the header
                    
                    # Write the current date on the left side (A2)
                    worksheet.write("A2", f"Date: {end_date.strftime('%d-%m-%Y')}", date_format)
                    worksheet.merge_range("C2:H2", f"Report Name: Rail Coal Journey", report_name_format)

                    headers = [
                        "Sr.No",
                        "RR No",
                        "RR Qty",
                        "Po No",
                        "Po Date",
                        "Line Item",
                        "Source",
                        "Placement Date",
                        "Completion Date",
                        "Drawn Date",
                        "Total ul wt",
                        "Boxes Supplied",
                        "Total Secl Gross Wt",
                        "Total Secl Tare Wt",
                        "Total Secl Net Wt",
                        "Total Secl Ol Wt",
                        "Boxes Loaded",
                        "Total Rly Gross Wt",
                        "Total Rly_Tare Wt",
                        "Total Rly Net Wt",
                        "Total Rly Ol Wt",
                        "Total Secl Chargable Wt",
                        "Total Rly Chargable Wt",
                        "Freight",
                        "Gst",
                        "Pola",
                        "Total Freight",
                        "Source Type",
                        "Created At"
                    ]
                   
                    for index, header in enumerate(headers):
                        worksheet.write(2, index, header, cell_format2)

                    for row, query in enumerate(usecase_data, start=3):
                        result = query.simplepayload()
                        worksheet.write(row, 0, count, cell_format)     
                        worksheet.write(row, 1, str(result["rr_no"]))                      
                        worksheet.write(row, 2, str(result["rr_qty"]))                      
                        worksheet.write(row, 3, str(result["po_no"]))                      
                        worksheet.write(row, 4, str(result["po_date"]))                      
                        worksheet.write(row, 5, str(result["line_item"]))                      
                        worksheet.write(row, 6, str(result["source"]))                      
                        worksheet.write(row, 7, str(result["placement_date"]))                      
                        worksheet.write(row, 8, str(result["completion_date"]))                      
                        worksheet.write(row, 9, str(result["drawn_date"]))                      
                        worksheet.write(row, 10, str(result["total_ul_wt"]))                      
                        worksheet.write(row, 11, str(result["boxes_supplied"]))                      
                        worksheet.write(row, 12, str(result["total_secl_gross_wt"]))                      
                        worksheet.write(row, 13, str(result["total_secl_tare_wt"]))                      
                        worksheet.write(row, 14, str(result["total_secl_net_wt"]))                      
                        worksheet.write(row, 15, str(result["total_secl_ol_wt"]))                      
                        worksheet.write(row, 16, str(result["boxes_loaded"]))                      
                        worksheet.write(row, 17, str(result["total_rly_gross_wt"]))                      
                        worksheet.write(row, 18, str(result["total_rly_tare_wt"]))                      
                        worksheet.write(row, 19, str(result["total_rly_net_wt"]))                      
                        worksheet.write(row, 20, str(result["total_rly_ol_wt"]))                      
                        worksheet.write(row, 21, str(result["total_secl_chargable_wt"]))                      
                        worksheet.write(row, 22, str(result["total_rly_chargable_wt"]))                      
                        worksheet.write(row, 23, str(result["freight"]))                      
                        worksheet.write(row, 24, str(result["gst"]))                      
                        worksheet.write(row, 25, str(result["pola"]))                      
                        worksheet.write(row, 26, str(result["total_freight"]))                      
                        worksheet.write(row, 27, str(result["source_type"]))                      
                        worksheet.write(row, 28, str(result["created_at"]))                   
                        
                        count-=1
                        
                    workbook.close()
                    console_logger.debug("Successfully {} report generated".format(service_id))
                    console_logger.debug("sent data {}".format(path))

                    return {
                            "Type": "gmr_rcr_journey_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                            }
                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            else:
                console_logger.error("No data found")
                return {
                        "Type": "gmr_rcr_journey_download_event",
                        "Datatype": "Report",
                        "File_Path": path,
                        }
    except Exception as e:
        console_logger.debug("----- Fetch Rcr Name Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

@router.get("/fetch/singlercr", tags=["Railway"])
def endpoint_to_fetch_railway_data_rcr(response: Response, rrno: str):
    try:
        fetchRailData = RcrData.objects.get(rr_no=rrno)
        return fetchRailData.payload()
    except DoesNotExist as e:
        raise HTTPException(status_code=404, detail="No data found")
    except Exception as e:
        console_logger.debug("----- Fetch Railway Data Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

@router.post("/insert/rcr", tags=["Railway"])
def endpoint_to_insert_rcr_data(response: Response, payload: RailwayData, id: Optional[str] = None):
    try:
        final_data = payload.dict()
        try:
            if id:
                fetchRailData = RcrData.objects.get(id=ObjectId(id))
            else:
                fetchRailData = RcrData.objects.get(rr_no=final_data.get("rr_no"))
            try:
                fetchSaprecordsRail = sapRecordsRCR.objects.get(rr_no=final_data.get("rr_no"))
            except DoesNotExist as e:
                fetchSaprecordsRail = None
            
            if fetchSaprecordsRail:
                if fetchSaprecordsRail.month:
                    fetchRailData.month = datetime.datetime.strptime(fetchSaprecordsRail.month, '%b %d, %Y').strftime('%Y-%m-%d')
                fetchRailData.rr_date = fetchSaprecordsRail.rr_date
                fetchRailData.area = fetchSaprecordsRail.area
                fetchRailData.mine = fetchSaprecordsRail.mine
                fetchRailData.grade = fetchSaprecordsRail.grade
                fetchRailData.rr_qty = fetchSaprecordsRail.rr_qty
                fetchRailData.po_amount = fetchSaprecordsRail.po_amount
            
            fetchRailData.placement_date = final_data.get("placement_date")
            fetchRailData.completion_date = final_data.get("completion_date")
            fetchRailData.drawn_date = final_data.get("drawn_date")
            fetchRailData.total_ul_wt = final_data.get("total_ul_wt")
            fetchRailData.boxes_supplied = final_data.get("boxes_supplied")
            fetchRailData.total_secl_gross_wt = final_data.get("total_secl_gross_wt")
            fetchRailData.total_secl_tare_wt = final_data.get("total_secl_tare_wt")
            fetchRailData.total_secl_net_wt = final_data.get("total_secl_net_wt")
            fetchRailData.total_secl_ol_wt = final_data.get("total_secl_ol_wt")
            fetchRailData.boxes_loaded = final_data.get("boxes_loaded")
            fetchRailData.total_rly_gross_wt = final_data.get("total_rly_gross_wt")
            fetchRailData.total_rly_tare_wt = final_data.get("total_rly_tare_wt")
            fetchRailData.total_rly_net_wt = final_data.get("total_rly_net_wt")
            fetchRailData.total_rly_ol_wt = final_data.get("total_rly_ol_wt")
            fetchRailData.total_secl_chargable_wt = final_data.get("total_secl_chargable_wt")
            fetchRailData.total_rly_chargable_wt = final_data.get("total_rly_chargable_wt")
            fetchRailData.freight = final_data.get("freight")
            fetchRailData.gst = final_data.get("gst")
            if final_data.get("pola") == "Not found":
                fetchRailData.pola = ""
            else:
                fetchRailData.pola = final_data.get("pola")
            if final_data.get("sd") == "Not found":
                fetchRailData.sd = ""
            else:
                fetchRailData.sd = final_data.get("sd")
            fetchRailData.total_freight = final_data.get("total_freight")
            fetchRailData.source_type = final_data.get("source_type")
            # fetchRailData.month = final_data.get("month")
            
            for key, value in final_data.items():
                if key != 'secl_rly_data' and hasattr(fetchRailData, key):
                    setattr(fetchRailData, key, value)

            # if fetchRailData.placement_date:
            #     console_logger.debug(fetchRailData.placement_date)
            #     console_logger.debug(datetime.datetime.strptime(fetchSaprecordsRail.month, '%b %d, %Y').strftime('%Y-%m-%d'))
            #     console_logger.debug(datetime.datetime.strptime(fetchRailData.placement_date, '%Y-%m-%dT%H:%M').strftime('%Y-%m-%d'))
            #     # Set rake_no based on month and placement_date
            #     fetchRailData.rake_no = calculate_rake_no(datetime.datetime.strptime(fetchSaprecordsRail.month, '%b %d, %Y').strftime('%Y-%m-%d'), fetchRailData.placement_date.strftime('%Y-%m-%d'))

            for new_data in final_data.get('secl_rly_data', []):
                updated = False
                for secl_data in fetchRailData.secl_rly_data:
                    if secl_data.wagon_no == new_data['wagon_no']:
                        for key, value in new_data.items():
                            setattr(secl_data, key, value)
                        updated = True
                        break
                if not updated:
                    fetchRailData.secl_rly_data.append(SeclRailData(**new_data))
            listAveryData = []
            for new_data in final_data.get('secl_rly_data', []):
                dictAveryData = {}
                dictAveryData["indexing"] = new_data.get("indexing")
                dictAveryData["wagon_owner"] = new_data.get("wagon_owner")
                dictAveryData["wagon_type"] = new_data.get("wagon_type")
                dictAveryData["wagon_no"] = new_data.get("wagon_no")
                listAveryData.append(AveryRailData(**dictAveryData))
        
            fetchRailData.avery_rly_data = listAveryData
            fetchRailData.save()

            return {"detail": "success"}
        except DoesNotExist as e:
            final_data = payload.dict()
            secl_list_data = []
            for single_data in final_data.get("secl_rly_data"):
                secl_rly_dict_data = {
                    "indexing": single_data.get("indexing"),
                    "wagon_owner": single_data.get("wagon_owner"),
                    "wagon_type": single_data.get("wagon_type"),
                    "wagon_no": single_data.get("wagon_no"),
                    "secl_cc_wt": single_data.get("secl_cc_wt"),
                    "secl_gross_wt": single_data.get("secl_gross_wt"),
                    "secl_tare_wt": single_data.get("secl_tare_wt"),
                    "secl_net_wt": single_data.get("secl_net_wt"),
                    "secl_ol_wt": single_data.get("secl_ol_wt"),
                    "secl_ul_wt":single_data.get("secl_ul_wt"),
                    "secl_chargable_wt": single_data.get("secl_chargable_wt"),
                    "rly_cc_wt": single_data.get("rly_cc_wt"),
                    "rly_gross_wt": single_data.get("rly_gross_wt"),
                    "rly_tare_wt": single_data.get("rly_tare_wt"),
                    "rly_net_wt": single_data.get("rly_net_wt"),
                    "rly_permissible_cc_wt": single_data.get("rly_permissible_cc_wt"),
                    "rly_ol_wt": single_data.get("rly_ol_wt"),
                    "rly_norm_rate": single_data.get("rly_norm_rate"),
                    "rly_pun_rate": single_data.get("rly_pun_rate"),
                    "rly_chargable_wt": single_data.get("rly_chargable_wt"),
                    "rly_sliding_adjustment": single_data.get("rly_sliding_adjustment"),
                }
                secl_list_data.append(secl_rly_dict_data)

            avery_list_data = []
            for single_data in final_data.get("secl_rly_data"):
                avery_rly_dict_data = {
                    "indexing": single_data.get("indexing"),
                    "wagon_owner": single_data.get("wagon_owner"),
                    "wagon_type": single_data.get("wagon_type"),
                    "wagon_no": single_data.get("wagon_no"),
                }
                avery_list_data.append(avery_rly_dict_data)
            try:
                fetchSaprecordsRail = sapRecordsRCR.objects.get(rr_no=final_data.get("rr_no"))
            except DoesNotExist as e:
                fetchSaprecordsRail = None

            rail_data = RcrData(
                rr_no=final_data.get("rr_no"),
                # rr_qty=final_data.get("rr_qty"),
                rr_qty=fetchSaprecordsRail.rr_qty if fetchSaprecordsRail and fetchSaprecordsRail.rr_qty else "",
                po_no=final_data.get("po_no"),
                po_date=final_data.get("po_date"),
                line_item=final_data.get("line_item"),
                source=final_data.get("source"),
                placement_date=final_data.get("placement_date"),
                completion_date=final_data.get("completion_date"),
                drawn_date=final_data.get("drawn_date"),
                total_ul_wt=final_data.get("total_ul_wt"),
                boxes_supplied=final_data.get("boxes_supplied"),
                total_secl_gross_wt=final_data.get("total_secl_gross_wt"),
                total_secl_tare_wt=final_data.get("total_secl_tare_wt"),
                total_secl_net_wt=final_data.get("total_secl_net_wt"),
                total_secl_ol_wt=final_data.get("total_secl_ol_wt"),
                boxes_loaded=final_data.get("boxes_loaded"),
                total_rly_gross_wt=final_data.get("total_rly_gross_wt"),
                total_rly_tare_wt=final_data.get("total_rly_tare_wt"),
                total_rly_net_wt=final_data.get("total_rly_net_wt"),
                total_rly_ol_wt=final_data.get("total_rly_ol_wt"),
                total_secl_chargable_wt=final_data.get("total_secl_chargable_wt"),
                total_rly_chargable_wt=final_data.get("total_rly_chargable_wt"),
                freight=final_data.get("freight"),
                gst=final_data.get("gst"),
                pola=final_data.get("pola") if final_data.get("pola") != "Not found" else "",
                sd=final_data.get("sd") if final_data.get("sd") != "Not found" else "",
                total_freight=final_data.get("total_freight"),
                source_type=final_data.get("source_type"),
                month = final_data.get("month"),    # modified by faisal
                # month=datetime.datetime.strptime(fetchSaprecordsRail.month, '%b %d, %Y').strftime('%Y-%m-%d') if fetchSaprecordsRail and fetchSaprecordsRail.month else "",
                secl_rly_data=secl_list_data,
                avery_rly_data=avery_list_data,
                rr_date=fetchSaprecordsRail.rr_date if fetchSaprecordsRail and fetchSaprecordsRail.rr_date else "",
                siding=fetchSaprecordsRail.siding if fetchSaprecordsRail and fetchSaprecordsRail.siding else "",
                mine=fetchSaprecordsRail.mine if fetchSaprecordsRail and fetchSaprecordsRail.mine else "",
                grade=fetchSaprecordsRail.grade if fetchSaprecordsRail and fetchSaprecordsRail.grade else "",
                # rr_qty=fetchSaprecordsRail.get("rr_qty") if fetchSaprecordsRail.get("rr_qty") else "",
                po_amount=fetchSaprecordsRail.po_amount if fetchSaprecordsRail and fetchSaprecordsRail.po_amount else "",
            ) 
            existing_rake_nos = [data.rake_no for data in RailData.objects()]

            if final_data.get("placement_date") and fetchSaprecordsRail and fetchSaprecordsRail.month:
                placement_date_obj = datetime.datetime.strptime(final_data.get("placement_date"), '%Y-%m-%dT%H:%M')
                rail_data.rake_no = calculate_rake_no(datetime.datetime.strptime(fetchSaprecordsRail.month, '%b %d, %Y').strftime('%Y-%m-%d'),
                                                      placement_date_obj.strftime('%Y-%m-%d'),
                                                      existing_rake_nos)
            rail_data.save()
            return {"message": "Data inserted successfully"}

    except Exception as e:
        console_logger.debug("----- Fetch Report Name Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/load_bunker_data", tags=["Coal bunker"])
def save_bunker_data(start_date: Optional[str] = None, end_date: Optional[str] = None, shift_name: Optional[str] = None):
    success = False
    try:
        global consumption_headers, proxies
        entry = UsecaseParameters.objects.first()
        historian_ip = entry.Parameters.get('gmr_api', {}).get('roi1', {}).get('Coal Consumption IP') if entry else None
        headers_data = {
            'accept': 'application/json',
        }

        params = {
            'start_date': start_date,
            'end_date': end_date,
        }

        # params = {
        #     'start_date': "2024-08-29T00:00:00",
        #     'end_date': "2024-08-29T23:59:00",
        # }
        
        try:
            response = requests.get(f'http://{ip}/api/v1/host/bunker_extract_data', params=params, headers=headers_data)
            data = json.loads(response.text)
            try:
                for item in data["Data"]:
                    if item["Data"] is not None:
                        tag_id = item["Data"]["TagID"]
                        unit = "Unit1" if tag_id == 15274 else "Unit2"
                        sum = str(int(float(item["Data"]["SUM"])) / 1000)
                        created_date = item["Data"]["CreatedDate"]

                        if bunkerAnalysis.objects.filter(tagid = tag_id, created_date=created_date):
                            console_logger.debug("data there bunkerAnalysis")
                            pass
                        else:
                            console_logger.debug("adding data")
                            bunkerAnalysis(
                                tagid = tag_id,
                                units = unit,
                                bunkering = sum,
                                shift_name = shift_name,
                                created_date = created_date,
                                ID = bunkerAnalysis.objects.count() + 1).save()
                    
                        success = "completed"
                        console_logger.debug("successful")
                    else:
                        success = "No data found"
                        console_logger.debug("No data found")
            except KeyError:
                console_logger.debug("No Data Found!")
        except requests.exceptions.Timeout:
            console_logger.debug("Request Timed Out!")
        except requests.exceptions.ConnectionError:
            console_logger.debug("Connection Error")
    
    except Exception as e:
        success = False
        console_logger.debug("----- Bunker Consumption Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e
    finally:
        console_logger.debug(f"success:{success}")
        SchedulerResponse("save consumption data", f"{success}")
        return {"message" : "Successful"}


@router.get("/coal_bunker_graph", tags=["Coal bunker"])
def coal_bunker_graph(response:Response, type: Optional[str] = "Daily",
                              Month: Optional[str] = None, 
                              Daily: Optional[str] = None, Year: Optional[str] = None):
    try:
        data={}
        UTC_OFFSET_TIMEDELTA = datetime.datetime.utcnow() - datetime.datetime.now()

        basePipeline = [
            {
                "$match": {
                    "created_date": {
                        "$gte": None,
                    },
                },
            },
            {
                "$project": {
                    "ts": {
                        "$hour": {"date": "$created_date"},
                    },
                    "tagid": "$tagid",
                    "bunkering": "$bunkering",
                    "_id": 0
                },
            },
            {
                "$group": {
                    "_id": {
                        "ts": "$ts",
                        "tagid": "$tagid"
                    },
                    "data": {
                        "$push": "$bunkering"
                    }
                }
            },
        ]
        
        if type == "Daily":

            date=Daily
            end_date =f'{date} 23:59:59'
            start_date = f'{date} 00:00:00'
            format_data = "%Y-%m-%d %H:%M:%S"
            endd_date=datetime.datetime.strptime(end_date,format_data)
            startd_date=datetime.datetime.strptime(start_date,format_data)

            basePipeline[0]["$match"]["created_date"]["$lte"] = (endd_date)
            basePipeline[0]["$match"]["created_date"]["$gte"] = (startd_date)
            

            result = {
                "data": {
                    "labels": [str(i) for i in range(1, 25)],
                    "datasets": [
                        {"label": "Unit 1", "data": [0 for i in range(1, 25)]},             
                        {"label": "Unit 2", "data": [0 for i in range(1, 25)]},             
                    ],
                }
            }

        elif type == "Week":
            basePipeline[0]["$match"]["created_date"]["$gte"] = (
                datetime.datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
                + UTC_OFFSET_TIMEDELTA
                - datetime.timedelta(days=7)
            )
            basePipeline[1]["$project"]["ts"] = {"$dayOfMonth": "$created_date"}
            result = {
                "data": {
                    "labels": [
                        (
                            basePipeline[0]["$match"]["created_date"]["$gte"]
                            + datetime.timedelta(days=i + 1)
                        ).strftime("%d")
                        for i in range(1, 8)
                    ],
                    "datasets": [
                        {"label": "Unit 1", "data": [0 for i in range(1, 8)]},              
                        {"label": "Unit 2", "data": [0 for i in range(1, 8)]},              
                    ],
                }
            }

        elif type == "Month":

            date=Month
            format_data = "%Y - %m-%d"

            start_date = f'{date}-01'
            startd_date=datetime.datetime.strptime(start_date,format_data)
            
            end_date = startd_date + relativedelta( day=31)
            end_label = (end_date).strftime("%d")

            basePipeline[0]["$match"]["created_date"]["$lte"] = (end_date)
            basePipeline[0]["$match"]["created_date"]["$gte"] = (startd_date)
            basePipeline[1]["$project"]["ts"] = {"$dayOfMonth": "$created_date"}
            result = {
                "data": {
                    "labels": [
                        (
                            basePipeline[0]["$match"]["created_date"]["$gte"]
                            + datetime.timedelta(days=i + 1)
                        ).strftime("%d")
                        for i in range(-1, (int(end_label))-1)
                    ],
                    "datasets": [
                        {"label": "Unit 1", "data": [0 for i in range(-1, (int(end_label))-1)]},        
                        {"label": "Unit 2", "data": [0 for i in range(-1, (int(end_label))-1)]},        
                    ],
                }
            }

        elif type == "Year":

            date=Year
            end_date =f'{date}-12-31 23:59:59'
            start_date = f'{date}-01-01 00:00:00'
            format_data = "%Y-%m-%d %H:%M:%S"
            endd_date=datetime.datetime.strptime(end_date,format_data)
            startd_date=datetime.datetime.strptime(start_date,format_data)

            basePipeline[0]["$match"]["created_date"]["$lte"] = (
                endd_date
            )
            basePipeline[0]["$match"]["created_date"]["$gte"] = (
                startd_date          
            )

            basePipeline[1]["$project"]["ts"] = {"$month": "$created_date"}
            result = {
                "data": {
                    "labels": [
                        (
                            basePipeline[0]["$match"]["created_date"]["$gte"]
                            + relativedelta(months=i)
                        ).strftime("%m")
                        for i in range(0, 12)
                    ],
                    "datasets": [
                        {"label": "Unit 1", "data": [0 for i in range(0, 12)]},                     
                        {"label": "Unit 2", "data": [0 for i in range(0, 12)]},                     
                    ],
                }
            }
        output = bunkerAnalysis.objects().aggregate(basePipeline)
        outputDict = {}

        for data in output:
            if "_id" in data:
                ts = data["_id"]["ts"]
                tag_id = data["_id"]["tagid"]

                data_list = data.get('data', [])
                sum_list = []
                for item in data_list:
                    try:
                        sum_value = float(item)
                        sum_list.append(sum_value)
                    except ValueError:
                        pass
                
                if ts not in outputDict:
                    outputDict[ts] = {tag_id: sum_list}
                else:
                    if tag_id not in outputDict[ts]:
                        outputDict[ts][tag_id] = sum_list
                    else:
                        outputDict[ts][tag_id].append(sum_list)

        modified_labels = [i for i in range(1, 25)]

        for index, label in enumerate(result["data"]["labels"]):
            if type == "Week":
                modified_labels = [
                    (
                        basePipeline[0]["$match"]["created_date"]["$gte"]
                        + datetime.timedelta(days=i + 1)
                    ).strftime("%d-%m-%Y,%a")
                    for i in range(1, 8)
                ]
            
            elif type == "Month":
                modified_labels = [
                    (
                        basePipeline[0]["$match"]["created_date"]["$gte"]
                        + datetime.timedelta(days=i + 1)
                    ).strftime("%d/%m")
                    for i in range(-1, (int(end_label))-1)
                ]

            elif type == "Year":
                modified_labels = [
                    (
                        basePipeline[0]["$match"]["created_date"]["$gte"]
                        + relativedelta(months=i)
                    ).strftime("%b %y")
                    for i in range(0, 12)
                ]

            if int(label) in outputDict:
                for key, val in outputDict[int(label)].items():
                    total_sum = sum(val)
                    if key == 15274:
                        result["data"]["datasets"][0]["data"][index] = total_sum
                    if key == 15275:
                        result["data"]["datasets"][1]["data"][index] = total_sum

        result["data"]["labels"] = copy.deepcopy(modified_labels)
        return result
    
    except Exception as e:
        console_logger.debug(e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/coal_bunker_table", tags=["Coal bunker"])
def coal_bunker_table(response:Response, specified_date: Optional[str] = None):
    try:
        DataExecutionsHandler = DataExecutions()
        response =  DataExecutionsHandler.bunker_coal_table_email(specified_date=specified_date)
        return response
    except Exception as e:
        success = False
        console_logger.debug("----- Coal Testing Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e

@router.get("/coal_bunker_analysis", tags=["Coal bunker"])
def coal_bunker_analysis(response:Response, specified_date: Optional[str] = None):
    try:
        DataExecutionsHandler = DataExecutions()
        response = DataExecutionsHandler.bunker_coal_analysis(specified_date=specified_date)
        return response
    except Exception as e:
        success = False
        console_logger.debug("----- Coal Testing Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e

@router.get("/fetch/coalbunkerdata", tags=["Coal bunker"])
def fetch_coal_bunker_data(response: Response, currentPage: Optional[int] = None,perPage: Optional[int] = None, start_timestamp: Optional[str] = None, end_timestamp: Optional[str] = None, search_text: Optional[str] = None, date: Optional[str] = None, type: Optional[str] = "display"):
    try:
        DataExecutionsHandler = DataExecutions()
        response = DataExecutionsHandler.bunker_coal_data(currentPage=currentPage, perPage=perPage, start_timestamp=start_timestamp, end_timestamp=end_timestamp, search_text=search_text, type=type, date=date)
        return response
    except Exception as e:
        success = False
        console_logger.debug("----- Coal Bunker Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e

@router.post("/update/coalbunkerdata", tags=["Coal bunker"])
def update_coal_bunker_data(response: Response, Data: BunkerAnalysisData):
    try:
        payload = Data.dict()
        DataExecutionsHandler = DataExecutions()
        response = DataExecutionsHandler.update_coalbunker_analysis_data(payload=payload)
        return response
    except Exception as e:
        success = False
        console_logger.debug("----- Coal Testing Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e

# def get_current_financial_year_data(Month):
#     current_date = datetime.datetime.now(datetime.timezone.utc)    
#     start_month = current_date.replace(month=Month,day=1, hour=0,minute=0,second=0,microsecond=0)
#     end_month = start_month.replace(month=Month+1) - datetime.timedelta(days=1)
    
#     return start_month, end_month

def get_current_financial_year_data(Month):
    current_date = datetime.datetime.now(datetime.timezone.utc)    
    start_month = current_date.replace(month=Month, day=1, hour=0, minute=0, second=0, microsecond=0)
    if Month == 12:
        end_month = start_month.replace(month=1, year=current_date.year + 1) - datetime.timedelta(days=1)
    else:
        end_month = start_month.replace(month=Month + 1) - datetime.timedelta(days=1)

    return start_month, end_month

def bunkerQualityAnanlysis():
    try:
        if "BunkerQualityAnalysis" not in collectionList:
            BQA = gmrDB.create_collection("BunkerQualityAnalysis")
        else:
            BQA = gmrDB.get_collection("BunkerQualityAnalysis")
        index_name = "po_index"
        BQA.create_index(index_name)

        for month in range(1,13):
            start_day, end_day = get_current_financial_year_data(month)
            # logger.debug((start_day,end_day))
            filter={
                'sample_date': {
                    '$gte': start_day, 
                    '$lte': end_day
                }
            }
            result = BQA.find(
            filter=filter
            )
            df = pd.DataFrame(list(result))
            if df.empty:
                continue
            # Create a column for Cumulative WT, where the first row starts as WT
            df['cumulative_wt'] = 0.0  # Initialize the column with zeros
            df['cumulative_wt'].iloc[0] = df['bunkered_qty'].iloc[0]  # First value matches WT
            
            df['wt_gcv'] = 0.0
            df['wt_gcv'].iloc[0] = df['bunkered_qty'].iloc[0] * df['arb_gcv'].iloc[0]
            
            df['cumulative_wt_gcv'] = 0.0
            df['cumulative_wt_gcv'].iloc[0] = df['wt_gcv'].iloc[0]
            
            df['bunker_wt_gcv'] = 0.0
            df['bunker_wt_gcv'].iloc[0] = df['cumulative_wt_gcv'].iloc[0] / df['cumulative_wt'].iloc[0]
            
            # Loop over the rows starting from the second row to calculate cumulative values
            for i in range(1, len(df)):
                df['cumulative_wt'].iloc[i] = df['bunkered_qty'].iloc[i] + df['cumulative_wt'].iloc[i-1]
                df['wt_gcv'].iloc[i] = df['bunkered_qty'].iloc[i] * df['arb_gcv'].iloc[i]
                df['cumulative_wt_gcv'].iloc[i] = df['wt_gcv'].iloc[i] + df['cumulative_wt_gcv'].iloc[i-1]
                df['bunker_wt_gcv'].iloc[i] = df['cumulative_wt_gcv'].iloc[i] / df['cumulative_wt'].iloc[i]
            
            bulk = []
            
            for idx, row in df.iterrows():
                filter_query = {'sample_date':row['sample_date'],'unit_no':row['unit_no']}
                update_query = {
                '$set': {
                    'cumulative_wt': row['cumulative_wt'],
                    'wt_gcv': row['wt_gcv'],
                    'cumulative_wt_gcv': row['cumulative_wt_gcv'],
                    'bunker_wt_gcv': row['bunker_wt_gcv']  # Insert or update the 'wt_gcv' field
                    }
                }
                bulk.append(UpdateOne(filter_query,update_query))
                BQA.update_many(filter_query, update_query, upsert=True)
            BQA.bulk_write(bulk) 

    except Exception as e:
        success = False
        console_logger.debug("----- Coal Testing Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e

def get_month_ends_data(Month):
    current_date = datetime.datetime.now(datetime.timezone.utc)
    start_month = current_date.replace(month=Month, day=1, hour=0, minute=0, second=0, microsecond=0)
    end_month = start_month.replace(month=Month + 1) - datetime.timedelta(days=1)
    
    return end_month
    


def summarybyMonth():
    try:
        if "BunkerQualityAnalysis" not in collectionList or "BunkerQualitySummary" not in collectionList:
            BQA = gmrDB.create_collection("BunkerQualityAnalysis")
            BQS = gmrDB.create_collection("BunkerQualitySummary")
        else:
            BQA = gmrDB.get_collection("BunkerQualityAnalysis")
            BQS = gmrDB.get_collection("BunkerQualitySummary")
            index_name = "po_index"
            BQA.create_index(index_name)
        
        monthends = []
        for month in range(1, 12):
            monthends.append(get_month_ends_data(month))
        
        # result = BQA.aggregate([ 
        #     { 
        #         "$facet": {
        #             "WeightedGCV": [
        #                 {
        #                     '$match': {
        #                         'sample_date': {
        #                             '$in': monthends,
        #                         }
        #                     }
        #                 },
        #                 {
        #                     '$sort': {
        #                         'bunker_wt_gcv': -1
        #                     }
        #                 },
        #                 {
        #                     '$group': {
        #                         '_id': {
        #                             'sample_date': '$sample_date'
        #                         },
        #                         'bunker_wt_gcv': {
        #                             '$max': '$bunker_wt_gcv'
        #                         }
        #                     }
        #                 },
        #             ],
        #             "DomesticQty": [
        #                 {
        #                     '$project': {
        #                         'sample_date': 1,  
        #                         'bunkered_qty': 1  
        #                     }
        #                 },
        #                 {
        #                     '$group': {
        #                         '_id': {
        #                             '$dateToString': {
        #                                 'date': '$sample_date',
        #                                 'format': '%Y-%m'
        #                             }
        #                         },
        #                         'domestic_qty': {
        #                             '$sum': '$bunkered_qty' 
        #                         }
        #                     }
        #                 }
        #             ]
        #         }
        #     }
        # ])
        
        # wgcv = []
        # dqty = []
        
        # for col in result:
        #     wgcv.extend(col.get("WeightedGCV", []))
        #     dqty.extend(col.get("DomesticQty", []))
        
        # data = []
        # for i in wgcv:
        #     for s in dqty:
        #         if s.get("_id") == i.get("_id").get("sample_date").strftime("%Y-%m"):
        #             data.append({
        #                 "date": i.get("_id").get("sample_date"), 
        #                 "domestic_qty": s.get("domestic_qty"), 
        #                 "wt_gcv": i.get("bunker_wt_gcv"),
        #                 "imported_qty": 0, 
        #                 "total_qty": s.get("domestic_qty") + 0, 
        #                 "cum_total_qty": s.get("domestic_qty") + 0
        #             })

        pipeline = [
                {
                    "$facet": {
                    "WeightedGCV": [
                        {
                        "$sort": { "sample_date": -1 }
                        },
                        {
                        "$group": {
                            "_id": {
                            "year": { "$year": "$sample_date" },
                            "month": { "$month": "$sample_date" }
                            },
                            "last_document": { "$first": "$$ROOT" }
                        }
                        },
                        {
                        "$replaceRoot": { "newRoot": "$last_document" }
                        },
                        {
                        "$sort": { "sample_date": 1 }  
                        }
                    ],
                    "DomesticQty": [
                        {
                        "$project": {
                            "sample_date": 1,
                            "bunkered_qty": 1
                        }
                        },
                        {
                        "$group": {
                            "_id": { "$dateToString": { "date": "$sample_date", "format": "%Y-%m" } },
                            "domestic_qty": { "$sum": "$bunkered_qty" }
                        }
                        }
                    ]
                    }
                }
                ]

        result = BQA.aggregate(pipeline=pipeline)
        d = []
        
        for col in result:    
            wgcv =  col.get("WeightedGCV")
            dqty = col.get("DomesticQty")
                
        data = []
        for i in wgcv:
            for s in dqty:
                if s.get("_id") == i.get("sample_date").strftime("%Y-%m"):
                    # data.append({"date": i.get("sample_date") if i.get("sample_date") in monthends else i.get("sample_date").replace(day=1, month=(i.get("sample_date").month) + 1) - datetime.timedelta(days=1), 
                    data.append({"date": i.get("sample_date"), 
                                "domestic_qty":s.get("domestic_qty"),
                                "wt_gcv":i.get("bunker_wt_gcv"), 
                                "imported_qty":0, 
                                "total_qty":s.get("domestic_qty")+0, 
                                "cum_total_qty":s.get("domestic_qty")+0 
                            })
                    if data[-1]["date"] > datetime.datetime.now():
                        data[-1]["date"] = i.get("sample_date")  
        df = pl.DataFrame(data).sort(by='date', descending=False)

        df = df.with_columns([
            pl.lit(0.0).alias("cum_total_qty"),
            pl.lit(0.0).alias("weighted_domestic_gcv"),
            pl.lit(0.0).alias("cum_weighted_domestic_gcv"),
            pl.lit(0.0).alias("weighted_gcv")
        ])
        
        
        df = df.with_columns([
            pl.lit(df['total_qty'][0]).alias("cum_total_qty"),
            pl.lit(df['wt_gcv'][0] * df['total_qty'][0]).alias("weighted_domestic_gcv"),
            pl.lit(df['wt_gcv'][0] * df['total_qty'][0]).alias("cum_weighted_domestic_gcv"),
            pl.lit((df['wt_gcv'][0] * df['total_qty'][0]) / df['total_qty'][0]).alias("weighted_gcv")
        ])
        
        # Perform calculations for cumulative columns
        cum_total_qty_list = [df['cum_total_qty'][0]]
        weighted_domestic_gcv_list = [df['weighted_domestic_gcv'][0]]
        cum_weighted_domestic_gcv_list = [df['cum_weighted_domestic_gcv'][0]]
        weighted_gcv_list = [df['weighted_gcv'][0]]

        for i in range(1, len(df)):
            cum_total_qty = df['total_qty'][i] + cum_total_qty_list[i-1]
            wt_gcv = df['wt_gcv'][i] if df['wt_gcv'][i] is not None else 0
            total_qty = df['total_qty'][i] if df['total_qty'][i] is not None else 0
            weighted_domestic_gcv = wt_gcv * total_qty
            # weighted_domestic_gcv = df['wt_gcv'][i] * df['total_qty'][i]
            cum_weighted_domestic_gcv = weighted_domestic_gcv + cum_weighted_domestic_gcv_list[i-1]
            weighted_gcv = cum_weighted_domestic_gcv / cum_total_qty

            cum_total_qty_list.append(cum_total_qty)
            weighted_domestic_gcv_list.append(weighted_domestic_gcv)
            cum_weighted_domestic_gcv_list.append(cum_weighted_domestic_gcv)
            weighted_gcv_list.append(weighted_gcv)

        # Update the DataFrame with the calculated lists
        df = df.with_columns([
            pl.Series("cum_total_qty", cum_total_qty_list),
            pl.Series("weighted_domestic_gcv", weighted_domestic_gcv_list),
            pl.Series("cum_weighted_domestic_gcv", cum_weighted_domestic_gcv_list),
            pl.Series("weighted_gcv", weighted_gcv_list)
        ])

        bulk = []
        
        for row in df.iter_rows(named=True):
            filter_query = {"date": row["date"]}

            date_obj = row["date"].date()
    
            first_day_of_month = datetime.datetime.combine(date_obj.replace(day=1), datetime.datetime.min.time())
            last_day_of_month = datetime.datetime.combine(date_obj.replace(day=monthrange(date_obj.year, date_obj.month)[1]), datetime.datetime.min.time())

            month_filter_query = {
                "date": {
                    "$gte": first_day_of_month,
                    "$lte": last_day_of_month
                }
            }

            bulk.append(DeleteMany(month_filter_query))

            update_query = {
                "$set": {
                    "domestic_qty": row['domestic_qty'],
                    "wt_gcv": row['wt_gcv'],
                    "imported_qty": row['imported_qty'],
                    "total_qty": row['total_qty'],
                    "cum_total_qty": row['cum_total_qty'],
                    "weighted_domestic_gcv": row['weighted_domestic_gcv'],
                    "cum_weighted_domestic_gcv": row['cum_weighted_domestic_gcv'],
                    "weighted_gcv": row['weighted_gcv']
                }
            }
            bulk.append(UpdateOne(filter_query, update_query, upsert=True))
        
        # print(bulk)
        BQS.bulk_write(bulk)

    except Exception as e:
        success = False
        console_logger.debug("----- Coal Testing Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e

def gcvAnalysisCoalReceiptSummary():
    try:
        if "RecieptCoalQualityAnalysis" not in collectionList:
            RCA = gmrDB.create_collection("RecieptCoalQualityAnalysis")
        else:
            RCA = gmrDB.get_collection("RecieptCoalQualityAnalysis")
            BQS = gmrDB.get_collection("BunkerQualitySummary")
        
    except Exception as e:
        success = False
        console_logger.debug("----- Coal Testing Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e


@router.get("/load_coal_bunker_data", tags=["Coal bunker"])
def extract_bunker_data(start_date: Optional[str] = None, end_date: Optional[str] = None):
    try:
        DataExecutionsHandler = DataExecutions()
        response = DataExecutionsHandler.fetchcoalBunkerData(start_date=start_date, end_date=end_date)
        bunkerQualityAnanlysis()
        summarybyMonth()
        
        # summarybyWeek()
        # endpoint_to_fetch_coal_recipt_summary()

        endpoint_to_insert_gcv_analysis()
        return {"detail": "success"}
    except Exception as e:
        success = False
        console_logger.debug("----- Coal Testing Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e 


@router.get("/fetch/coalbunker", tags=["Coal bunker"])
def fetch_bunker_data(response: Response, currentPage: Optional[int] = None, perPage: Optional[int] = None, search_text: Optional[str] = None, start_timestamp: Optional[str] = None, end_timestamp: Optional[str] = None, month_date: Optional[str] = None, type: Optional[str] = "display"):
    try:
        DataExecutionsHandler = DataExecutions()
        response = DataExecutionsHandler.fetchcoalBunkerDbData(currentPage=currentPage, perPage=perPage, search_text=search_text, start_timestamp=start_timestamp, end_timestamp=end_timestamp, month_date=month_date, type=type)
        return response
    except Exception as e:
        success = False
        console_logger.debug("----- Coal Testing Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e 


@router.get("/fetch/testing/bunker", tags=["extra"])
def fetch_bunker_data_test(response: Response):
    try:
        fetchBunkerData = BunkerData.objects()
        # bunker_generate_report(fetchBunkerdata=fetchBunkerData)
        bunker_single_generate_report(fetchBunkerdata=fetchBunkerData)
        return {"detail": "success"}
    except Exception as e:
        success = False
        console_logger.debug("----- Coal Testing Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e 


def update_dictionary(data):
    # Check if key 2 is empty and key 3 contains a space
    if data[2] == '' and ' ' in data[3]:
        # Split the value at key 3 by the first space
        parts = data[3].split(' ', 1)
        data[2] = parts[0]
        data[3] = parts[1]
    return data


@router.post("/pdf_railway_data_upload", tags=["Extra"])
async def extract_data_railway(response: Response, pdf_upload: Optional[UploadFile] = File(None)):
    try:
        if pdf_upload is None:
            return {"error": "No file uploaded"}
        contents = await pdf_upload.read()

        # Check if the file is empty
        if not contents:
            return {"error": "Uploaded file is empty"}
        
        # Verify file format (PDF)
        if not pdf_upload.filename.endswith('.pdf'):
            return {"error": "Uploaded file is not a PDF"}

        file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
        target_directory = f"static_server/gmr_ai/{file}"
        os.umask(0)
        os.makedirs(target_directory, exist_ok=True, mode=0o777)

        file_extension = pdf_upload.filename.split(".")[-1]
        file_name = f'pdf_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.{file_extension}'
        full_path = os.path.join(os.getcwd(), target_directory, file_name)
        with open(full_path, "wb") as file_object:
            file_object.write(contents)

        outdata = outbond(full_path)
        upper_data = extract_pdf_data(full_path)

        rr_no = upper_data.get('RR_NO')
        if rr_no:
            sap_record = sapRecordsRail.objects(rr_no=rr_no).first()
            if sap_record:
                upper_data.update({
                    "LINE_ITEM": sap_record.line_item,
                    "RR_Qty": sap_record.rr_qty,
                    "PO_NO": sap_record.sap_po,
                })
            else:
                upper_data.update({
                    "LINE_ITEM": None,
                    "RR_Qty":None,
                    "PO_NO":None,
                })

        key_mappings = {
            0: "sr_no",
            1: "wagon_owner",
            2: "wagon_type",
            3: "wagon_no",
            4: "rly_cc_wt",
            5: "rly_tare_wt",
            6: "no_of_art",
            7: "cmdt_code",
            8: "rly_gross_wt",
            9: "rly_sliding_adjustment",
            10: "dip_wt",
            11: "actl_wt",
            12: "rly_permissible_cc_wt",
            13: "rly_ol_wt",
            14: "rly_norm_rate",
            15: "rly_pun_rate",
            16: "rly_chargable_wt",
        }
        # Extract the relevant records starting from the specified entry
        start_index = None
        for i, record in enumerate(outdata):
            # finding and printing starting where value start from 1
            if record.get(0) == "1":
                start_index = i
                break

        if start_index is None:
            raise ValueError("Starting record not found in the data")

        # Process and transform the records
        transformed_data = []
        for record in outdata[start_index:]:
            updated_record_data = update_dictionary(record)
            transformed_record = {key_mappings[k]: v for k, v in updated_record_data.items()}
            transformed_data.append(transformed_record)

        header_data = {
            "header_data": upper_data,
            "table_data": transformed_data,
        }

        return header_data
    except Exception as e:
        success = False
        console_logger.debug("----- Coal Testing Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e


@router.post("/pdf_rcr_data_upload", tags=["Extra"])
async def extract_data_railway_rcr(response: Response, pdf_upload: Optional[UploadFile] = File(None)):
    try:
        if pdf_upload is None:
            return {"error": "No file uploaded"}
        contents = await pdf_upload.read()

        # Check if the file is empty
        if not contents:
            return {"error": "Uploaded file is empty"}
        
        # Verify file format (PDF)
        if not pdf_upload.filename.endswith('.pdf'):
            return {"error": "Uploaded file is not a PDF"}

        file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
        target_directory = f"static_server/gmr_ai/{file}"
        os.umask(0)
        os.makedirs(target_directory, exist_ok=True, mode=0o777)

        file_extension = pdf_upload.filename.split(".")[-1]
        file_name = f'pdf_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.{file_extension}'
        full_path = os.path.join(os.getcwd(), target_directory, file_name)
        with open(full_path, "wb") as file_object:
            file_object.write(contents)

        outdata = outbond(full_path)
        upper_data = extract_pdf_data(full_path)

        rr_no = upper_data.get('RR_NO')
        if rr_no:
            sap_record = sapRecordsRCR.objects(rr_no=rr_no).first()
            if sap_record:
                # upper_data.update({
                #     "LINE_ITEM": sap_record.line_item,
                #     "RR_Qty": sap_record.do_qty,
                #     "SOURCE": sap_record.source,
                #     "PO_NO": sap_record.sap_po,
                #     "RR_Qty": sap_record.do_qty
                # })
                upper_data.update({
                    "LINE_ITEM": sap_record.line_item,
                    "RR_Qty": sap_record.rr_qty,
                    "PO_NO": sap_record.sap_po,
                })
            else:
                upper_data.update({
                    "LINE_ITEM": None,
                    "RR_Qty":None,
                    "PO_NO":None,
                })

        key_mappings = {
            0: "sr_no",
            1: "wagon_owner",
            2: "wagon_type",
            3: "wagon_no",
            4: "rly_cc_wt",
            5: "rly_tare_wt",
            6: "no_of_art",
            7: "cmdt_code",
            8: "rly_gross_wt",
            9: "rly_sliding_adjustment",
            10: "dip_wt",
            11: "actl_wt",
            12: "rly_permissible_cc_wt",
            13: "rly_ol_wt",
            14: "rly_norm_rate",
            15: "rly_pun_rate",
            16: "rly_chargable_wt",
        }
        # Extract the relevant records starting from the specified entry
        start_index = None
        for i, record in enumerate(outdata):
            # finding and printing starting where value start from 1
            if record.get(0) == "1":
                start_index = i
                break

        if start_index is None:
            raise ValueError("Starting record not found in the data")

        # Process and transform the records
        transformed_data = []
        for record in outdata[start_index:]:
            updated_record_data = update_dictionary(record)
            transformed_record = {key_mappings[k]: v for k, v in updated_record_data.items()}
            transformed_data.append(transformed_record)

        header_data = {
            "header_data": upper_data,
            "table_data": transformed_data,
        }

        return header_data
    except Exception as e:
        success = False
        console_logger.debug("----- Sap Records Rcr Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e


@router.get("/rail/rake/count", tags=["Rail Map"])
def daywise_rake_scanned_count(response:Response):
    try:
        DataExecutionsHandler = DataExecutions()
        response = DataExecutionsHandler.rakeScannedOutData()
        return response

    except Exception as e:
        console_logger.debug("----- Vehicle Scanned Count Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/fetch/rail/excel", tags=["excel test"])
def fetch_excel_data_rail(response:Response, start_date: str, end_date: str, filter_type: str):
    try:
        DataExecutionsHandler = DataExecutions()
        response = DataExecutionsHandler.download_coal_test_excel(start_date=start_date, end_date=end_date, filter_type=filter_type)
        return response
    except Exception as e:
        console_logger.debug("----- Vehicle Scanned Count Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

@router.get("/fetch/road/coal", tags=["excel test"])
def fetch_data_road_logistics(response:Response, specified_date: str):
    try:
        DataExecutionsHandler = DataExecutions()
        response = DataExecutionsHandler.download_road_coal_logistics(specified_date=specified_date)
        return response
    except Exception as e:
        console_logger.debug("----- Vehicle Scanned Count Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/fetch/rail/coal", tags=["excel test"])
def fetch_data_rail_logistics(response:Response, specified_date: str):
    try:
        DataExecutionsHandler = DataExecutions()
        response = DataExecutionsHandler.download_rail_coal_logistics(specified_date=specified_date)
        return response
    except Exception as e:
        console_logger.debug("----- Vehicle Scanned Count Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/bunker/report", tags=["bunker"])
def fetch_pdf_report_bunker(response:Response, certificate_no: str):
    try:
        DataExecutionsHandler = DataExecutions()
        response = DataExecutionsHandler.display_pdf_report_bunker_addons(certificate_no=certificate_no)
        return response
    except Exception as e:
        console_logger.debug("----- Vehicle Scanned Count Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/update/scheduler/status", tags=["scheduler"])
def update_pdf_report_bunker(response:Response, scheduler_name: str, active: bool):
    try:
        DataExecutionsHandler = DataExecutions()
        response = DataExecutionsHandler.update_schheduler_status(scheduler_name=scheduler_name, active=active)
        return response
    except Exception as e:
        console_logger.debug("----- Update Scheduler Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/fetch/scheduler/status", tags=["scheduler"])
def display_pdf_report_bunker(response:Response):
    try:
        DataExecutionsHandler = DataExecutions()
        response = DataExecutionsHandler.fetch_scheduler_status()
        return response
    except Exception as e:
        console_logger.debug("----- Fetch Scheduler Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

    

# @router.get("/fetch/road/excel", tags=["excel test"])
# def fetch_excel_data_road(response:Response, start_date: str, end_date: str, filter_type: str):
#     try:
#         DataExecutionsHandler = DataExecutions()
#         response = DataExecutionsHandler.coal_test_road_excel(start_date=start_date, end_date=end_date, filter_type=filter_type)
#         return response
#     except Exception as e:
#         console_logger.debug("----- Vehicle Scanned Count Error -----",e)
#         response.status_code = 400
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#         console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#         return e


@router.post("/send_email", tags=["Generate_Email"])
def generate_email(response: Response, email:dict):
    try:
        url = f"http://{ip}/api/v1/host/send-email/"

        headers = {'Content-Type': 'application/json'}

        payload = json.dumps(email)
        response = requests.request("POST", url, headers=headers, data=payload)
        response.status_code = response.status_code
        return response.json()
    
    except Exception as e:
        console_logger.debug("----- Email Generation Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/truck_tare_email_alert", tags=["Generate_Email"])
def generate_truck_tare_email_alert(response: Response, data: TruckEmailTrigger):
    try:
        payload = data.dict()
        reportSchedule = ReportScheduler.objects()
        if reportSchedule[6].active == False:
            console_logger.debug("scheduler is off")
            return {"detail": "scheduler is off"}
        elif reportSchedule[6].active == True:
            console_logger.debug("inside Truck Tare Difference Alert")
            response_code, fetch_email = fetch_email_data()
            if response_code == 200:
                console_logger.debug(reportSchedule[6].recipient_list)
                subject = f"Truck Tare Difference Alert Report {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                table_data = "<tr>"
                table_data += f"<td>{payload['details'][0]['vehicle_number']}</td>"
                table_data += f"<td>{payload['details'][0]['current_gwel_tare_time']}</td>"
                table_data += f"<td>{payload['details'][0]['current_gwel_tare_wt']}</td>"
                table_data += f"<td>{payload['details'][0]['min_GWEL_Tare_Wt']}</td>"
                table_data += f"<td>{payload['details'][0]['max_GWEL_Tare_Wt']}</td>"
                table_data += f"<td>{payload['details'][0]['difference']}</td>"
                table_data += "<tr>"

                body = f"""
                        <b>Truck Tare Difference Alert Report for Date: {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d %H:%M:%S'),'%Y-%m-%d %H:%M:%S').strftime('%d %B %Y %H:%M:%S')}</b>
                        <br>
                        <br>
                        <!doctype html>
                        <html>
                        <head>
                            <meta charset="utf-8">
                            <title>Truck Tare Difference Alert</title>
                        </head>
                        <body>
                            <table border='1'>
                                <tr>
                                    <th>Vehicle Number</th>
                                    <th>Current GWEL Tare Time</th>
                                    <th>Current GWEL Tare Wt(MT)</th>
                                    <th>Min GWEL Tare Wt(MT)</th>
                                    <th>Max GWEL Tare Wt(MT)</th>
                                    <th>Difference(MT)</th>
                                </tr>
                                {table_data}
                            </table>
                        </body>
                        </html>"""
                checkEmailDevelopment = EmailDevelopmentCheck.objects()
                if checkEmailDevelopment[0].development == "local":
                    console_logger.debug("inside local")
                    send_email(fetch_email.get("Smtp_user"), subject, fetch_email.get("Smtp_password"), fetch_email.get("Smtp_host"), fetch_email.get("Smtp_port"), reportSchedule[6].recipient_list, body, "", reportSchedule[6].cc_list, reportSchedule[6].bcc_list)
                elif checkEmailDevelopment[0].development == "prod":
                    console_logger.debug("inside prod")
                    send_data = {
                        "sender_email": fetch_email.get("Smtp_user"),
                        "subject": subject,
                        "password": fetch_email.get("Smtp_password"),
                        "smtp_host": fetch_email.get("Smtp_host"),
                        "smtp_port": fetch_email.get("Smtp_port"),
                        "receiver_email": reportSchedule[6].recipient_list,
                        "body": body,
                        "file_path": "",
                        "cc_list": reportSchedule[6].cc_list,
                        "bcc_list": reportSchedule[6].bcc_list
                    }
                    # console_logger.debug(send_data)
                    generate_email(Response, email=send_data)
        return {"detail": "success"}
    except Exception as e:
        console_logger.debug("----- Email Generation Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/road/mine_wise_vehicle_count", tags=["Road Map"])
def minewise_day_vehicle_scanned_count(response:Response, specified_time: str):
    try:
        timezone = pytz.timezone('Asia/Kolkata')
        basePipeline = [
            {
                '$match': {
                    'created_at': {
                        '$gte': None,
                        '$lte': None,
                    }
                }
            }, {
                '$group': {
                    '_id': '$mine', 
                    'vehicle_count': {
                        '$sum': 1
                    }
                }
            }
        ]

        date = specified_time
        end_date = f'{date} 23:59:59'
        start_date = f'{date} 00:00:00'
        format_data = "%Y-%m-%d %H:%M:%S"
        endd_date = convert_to_utc_format(end_date.__str__(), format_data)
        startd_date = convert_to_utc_format(start_date.__str__(), format_data)

        basePipeline[0]["$match"]["created_at"]["$lte"] = endd_date
        basePipeline[0]["$match"]["created_at"]["$gte"] = startd_date

        output = Gmrdata.objects().aggregate(basePipeline)
        listdata = []
        for data in output:
            outputDict = {}
            outputDict['mine_name'] = data.get("_id")
            outputDict['vehicle_count'] = data.get("vehicle_count")
            listdata.append(outputDict)


        return listdata

    except Exception as e:
        console_logger.debug("----- Vehicle Scanned Count Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/coalstocktracker", tags=["Coal Tracker"])
def endpoint_to_fetch_coal_stock_tracker(response: Response, specified_date: str):
    try:
        specified_change_date = datetime.datetime.strftime(datetime.datetime.strptime(specified_date, "%Y-%m-%d"), "%d-%m-%Y")

        url = "https://gateway.grid-india.in/POSOCO/reports/1.0/WebAccessAPI/GetUtilityExternalSharedData?apikey=fdcfa9a0-3e10-45cc-ac8b-a0b076b0b21f"

        payload = json.dumps({
            "Date": specified_change_date,
            "SchdRevNo": -1,
            "UserName": "usr_GMR_WARORA",
            "UtilAcronymList": []
        })
        headers = {
            'Content-Type': 'application/json',
            'Authorization': 'Basic dXNyX0dNUl9XQVJPUkE6V2Jlc0ludGVAMDIwNzIwMzg='
        }

        response = requests.request("POST", url, headers=headers, data=payload)

        if response.status_code == 200:
            console_logger.debug(response.text)

    except Exception as e:
        console_logger.debug("----- Vehicle Scanned Count Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

def endpoint_to_generate_embedded_images_using_lat_long(lat, long):
    try:
        from staticmap import StaticMap, IconMarker
        m = StaticMap(1500, 800, 80)
        icon_flag = IconMarker((long, lat), 'icon.png', 12, 32)
        m.add_marker(icon_flag)
        image = m.render()
        file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
        target_directory = f"static_server/gmr_ai/{file}"
        os.umask(0)
        os.makedirs(target_directory, exist_ok=True, mode=0o777)
        file_name = f'map_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.png'
        full_path = os.path.join(os.getcwd(), target_directory, file_name)
        image.save(full_path)
        return full_path
    except Exception as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/geofence_email_alert", tags=["Generate_Email"])
def generate_truck_tare_email_alert(response: Response, data: geofenceEmailTrigger):
    try:
        console_logger.debug((data.dict()))
        payload = data.dict()
        # wholeLatLong = payload.get('lat_long').split(", ")
        # 0=lat, 1=long
        # map_image_path = endpoint_to_generate_embedded_images_using_lat_long(wholeLatLong[0], wholeLatLong[1])
        # <div>
        #     <img src='cid:image1'>
        # </div>
        reportSchedule = ReportScheduler.objects()
        if reportSchedule[7].active == False:
            console_logger.debug("scheduler is off")
            return {"detail": "scheduler is off"}
        elif reportSchedule[7].active == True:
            console_logger.debug("inside Truck Tare Difference Alert")
            response_code, fetch_email = fetch_email_data()
            if response_code == 200:
                console_logger.debug(reportSchedule[7].recipient_list)
                subject = f"Geofence Alert {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                title_data = ""
                if payload.get("geo_fence") == "outside":
                    title_data = f"<b>Vehicle Number {payload.get('vehicle_number')} is outside the geofenced area.</b>"
                elif payload.get("geo_fence") == "inside":
                    title_data = f"<b>Vehicle Number {payload.get('vehicle_number')} is inside the geofenced area.</b>"
                body = f"""
                        <b>Geofence Alert: {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d %H:%M:%S'),'%Y-%m-%d %H:%M:%S').strftime('%d %B %Y %H:%M:%S')}</b>
                        <br>
                        <br>
                        <!doctype html>
                        <html>
                        <head>
                            <meta charset="utf-8">
                            <title>Geofence Alert</title>
                        </head>
                        <body>
                            {title_data}
                            <br>
                            <br>
                            <table border='1'>
                                <tr>
                                    <th>Vehicle Number</th>
                                    <th>Current Lat Long</th>
                                    <th>Map Location</th>
                                    <th>Source Location</th>
                                </tr>
                                <tr>
                                    <td>{payload.get('vehicle_number')}</td>
                                    <td>{payload.get('lat_long')}</td>
                                    <td><a href='https://maps.google.com/?q={payload.get('lat_long')}'>Location</a></td>
                                    <td>{payload.get('mine_name')}</td>
                                </tr>
                            </table>
                            
                            

                        </body>
                        </html>"""
                checkEmailDevelopment = EmailDevelopmentCheck.objects()
                if checkEmailDevelopment[0].development == "local":
                    console_logger.debug("inside local")
                    send_email(fetch_email.get("Smtp_user"), subject, fetch_email.get("Smtp_password"), fetch_email.get("Smtp_host"), fetch_email.get("Smtp_port"), reportSchedule[7].recipient_list, body, "", reportSchedule[7].cc_list, reportSchedule[7].bcc_list)
                elif checkEmailDevelopment[0].development == "prod":
                    console_logger.debug("inside prod")
                    send_data = {
                        "sender_email": fetch_email.get("Smtp_user"),
                        "subject": subject,
                        "password": fetch_email.get("Smtp_password"),
                        "smtp_host": fetch_email.get("Smtp_host"),
                        "smtp_port": fetch_email.get("Smtp_port"),
                        "receiver_email": reportSchedule[7].recipient_list,
                        "body": body,
                        "file_path": "",
                        "cc_list": reportSchedule[7].cc_list,
                        "bcc_list": reportSchedule[7].bcc_list
                    }
                    # console_logger.debug(send_data)
                    generate_email(Response, email=send_data)
        return {"detail": "success"}
    except Exception as e:
        console_logger.debug("----- Email Generation Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


# @router.post("/insert/shift/schedule", tags=["scheduler"])
# def endpoint_to_insert_shift_schedule(response: Response, data: ShiftMainData, report_name: str):
#     try:
#         inputData = data.dict()pyongo
#         fetchAllScheduler = shiftScheduler.objects(report_name=report_name)
#         if fetchAllScheduler:
#             fetchAllScheduler.delete()
        
#         if fetchAllScheduler.time != "":
#             time_format = "%H:%M"
#             given_time = datetime.datetime.strptime(fetchAllScheduler.time, time_format)

#             time_to_subtract = datetime.timedelta(hours=5, minutes=30)

#             new_time = given_time - time_to_subtract
#             new_time_str = new_time.strftime(time_format)
#             hh, mm = new_time_str.split(":")

#         for single_data in inputData.get("data"):
#             if single_data.get("filter") == "shift_schedule":
#                 shiftScheduler(shift_name = single_data.get('shift_name'), start_shift_time = single_data.get("start_shift_time"), end_shift_time = single_data.get("end_shift_time"), scheduling=single_data.get("scheduling"), report_name=report_name).save()
#                 console_logger.debug(single_data.get('shift_name'))
#                 console_logger.debug(single_data.get("start_shift_time"))
#                 console_logger.debug(single_data.get("end_shift_time"),)
#                 # Parse end_shift_time
#                 end_shift_time = datetime.datetime.strptime(single_data.get("end_shift_time"), time_format)
#                 # Adjust for timezone by subtracting the specified duration
#                 end_shift_time_ist = end_shift_time - time_to_subtract
#                 # Convert the adjusted time back to hours and minutes
#                 end_shift_hh, end_shift_mm = end_shift_time_ist.strftime(time_format).split(":")
#                 # Schedule the background task
#                 backgroundTaskHandler.run_job(
#                     task_name=single_data.get('shift_name'),
#                     func=bunker_scheduler,
#                     trigger="cron",
#                     **{"day": "*", "hour": end_shift_hh, "minute": end_shift_mm}, 
#                     func_kwargs={
#                         "shift_name": single_data.get('shift_name'), 
#                         "start_time": single_data.get("start_shift_time"), 
#                         "end_time": single_data.get("end_shift_time")
#                     }
#                 ) 
#             elif single_data.get("filter") == "daily":

#                 backgroundTaskHandler.run_job(task_name=report_name, func=send_report_generate, trigger="cron", **{"day": "*", "hour": hh, "minute": mm}, func_kwargs={"report_name":payload.report_name}, max_instances=1)
#                 # backgroundTaskHandler.run_job(task_name=reportScheduler.report_name, func=send_report_generate, trigger="cron", **{"day": "*", "second": 2})
#             elif single_data.get("filter") == "weekly":
#                 # backgroundTaskHandler.run_job(task_name=reportScheduler.report_name, func=send_report_generate, trigger="cron", **{"week": reportScheduler.schedule}) # week (int|str) - ISO week (1-53)
#                 backgroundTaskHandler.run_job(task_name=report_name, func=send_report_generate, trigger="cron", **{"day_of_week": reportScheduler.schedule, "hour": hh, "minute": mm}, func_kwargs={"report_name":payload.report_name}, max_instances=1)
#             elif single_data.get("filter") == "monthly":
#                 # backgroundTaskHandler.run_job(task_name=reportScheduler.report_name, func=send_report_generate, trigger="cron", **{"month": reportScheduler.schedule}) # month (int|str) - month (1-12)
#                 backgroundTaskHandler.run_job(task_name=report_name, func=send_report_generate, trigger="cron", **{"day": reportScheduler.schedule, "hour": hh, "minute": mm}, func_kwargs={"report_name":payload.report_name}, max_instances=1)
        
#         return {"details": "success"}
#     except Exception as e:
#         console_logger.debug("----- Email Generation Error -----",e)
#         response.status_code = 400
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#         console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#         return e
    
@router.post("/insert/shift/schedule", tags=["scheduler"])
def endpoint_to_insert_shift_schedule(response: Response, data: ShiftMainData, report_name: str):
    try:
        dataName = data.dict()
        for single_data in dataName.get("data"):
            try:
                shiftSchedulerData = shiftScheduler.objects.get(report_name=report_name, shift_name=single_data.get("shift_name"))
                shiftSchedulerData.start_shift_time = single_data.get("start_shift_time")
                shiftSchedulerData.end_shift_time = single_data.get("end_shift_time")
                shiftSchedulerData.filter = single_data.get("filter")
                shiftSchedulerData.schedule = single_data.get("schedule")
                shiftSchedulerData.time = single_data.get("time")
                shiftSchedulerData.duration = single_data.get("duration")
                shiftSchedulerData.save()

            except DoesNotExist as e:
                shiftSchedulerData = shiftScheduler(report_name=report_name,shift_name=single_data.get("shift_name"), start_shift_time=single_data.get("start_shift_time"), end_shift_time=single_data.get("end_shift_time"), filter = single_data.get("filter"), schedule = single_data.get("schedule"), time=single_data.get("time"), duration=single_data.get("duration"))
                shiftSchedulerData.save()
        
            time_format = "%H:%M"

            time_to_subtract = datetime.timedelta(hours=5, minutes=30)
            console_logger.debug(single_data.get("filter"))
            if single_data.get("filter") == "daily" or single_data.get("filter") == "weekly" or single_data.get("filter") == "monthly":
                if single_data.get("duration") != "":
                    splitDuration = single_data.get("duration").split(":")
                    hours = int(splitDuration[1])
                    minutes = int(splitDuration[2])
                    trigger_time = datetime.datetime.strptime(shiftSchedulerData.time, "%H:%M")
                    duration_timedelta = timedelta(hours=hours, minutes=minutes)
                    calculation_time = trigger_time - duration_timedelta
                    given_time = datetime.datetime.strptime(calculation_time.strftime("%H:%M"), time_format)
                    new_time = given_time - time_to_subtract
                    new_time_str = new_time.strftime(time_format)
                    hh, mm = new_time_str.split(":")
                else:
                    given_time = datetime.datetime.strptime(shiftSchedulerData.time, time_format)
                    new_time = given_time - time_to_subtract
                    new_time_str = new_time.strftime(time_format)
                    hh, mm = new_time_str.split(":")

            if single_data.get("filter") == "shift_schedule":
                console_logger.debug(single_data.get('shift_name'))
                console_logger.debug(single_data.get("start_shift_time"))
                console_logger.debug(single_data.get("end_shift_time"),)
                # Parse end_shift_time
                end_shift_time = datetime.datetime.strptime(single_data.get("end_shift_time"), time_format)
                # Adjust for timezone by subtracting the specified duration
                end_shift_time_ist = end_shift_time - time_to_subtract
                # Convert the adjusted time back to hours and minutes
                end_shift_hh, end_shift_mm = end_shift_time_ist.strftime(time_format).split(":")
                # Schedule the background task
                backgroundTaskHandler.run_job(
                    task_name=single_data.get('shift_name'),
                    func=bunker_scheduler,
                    trigger="cron",
                    **{"day": "*", "hour": end_shift_hh, "minute": end_shift_mm}, 
                    func_kwargs={
                        "shift_name": single_data.get('shift_name'), 
                        "start_time": single_data.get("start_shift_time"), 
                        "end_time": single_data.get("end_shift_time")
                    }
                ) 
            elif single_data.get("filter") == "daily":
                console_logger.debug(hh)
                console_logger.debug(mm)
                backgroundTaskHandler.run_job(task_name=report_name, func=shiftSchedulerfunc, trigger="cron", **{"day": "*", "hour": hh, "minute": mm}, func_kwargs={"report_name":report_name, "duration": single_data.get("duration")}, max_instances=1)
                # backgroundTaskHandler.run_job(task_name=reportScheduler.report_name, func=send_report_generate, trigger="cron", **{"day": "*", "second": 2})
            elif single_data.get("filter") == "weekly":
                backgroundTaskHandler.run_job(task_name=report_name, func=shiftSchedulerfunc, trigger="cron", **{"day_of_week": shiftSchedulerData.schedule, "hour": hh, "minute": mm}, func_kwargs={"report_name":report_name, "duration": single_data.get("duration")}, max_instances=1)
            elif single_data.get("filter") == "monthly":
                backgroundTaskHandler.run_job(task_name=report_name, func=shiftSchedulerfunc, trigger="cron", **{"day": shiftSchedulerData.schedule, "hour": hh, "minute": mm}, func_kwargs={"report_name":report_name, "duration": single_data.get("duration")}, max_instances=1)
        
        return {"details": "success"}
    except Exception as e:
        console_logger.debug("----- Email Generation Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

def shiftSchedulerfunc(**kwargs):
    try:
        console_logger.debug(("scheduler report generate",kwargs))
        if kwargs["report_name"] == "receipt_coal_quality_analysis":
            durationSplit = kwargs["duration"].split(":")
            end_date = datetime.date.today().strftime("%Y-%m-%d")
            start_date = (datetime.date.today() - timedelta(int(f"{durationSplit[0]}"))).strftime("%Y-%m-%d")
            coal_test(start_date=start_date, end_date=end_date)
        elif kwargs["report_name"] == "bombcalorimeter_fetch_gcv":
            endpoint_to_fetch_coal_quality_gcv()
        elif kwargs["report_name"] == "bunker_quality_analysis":
            todays_date = datetime.date.today().strftime("%Y-%m-%d")
            two_months_back = datetime.date.today() - relativedelta(months=2)
            two_months_back_formatted = two_months_back.strftime("%Y-%m-%d")
            # DataExecutionsHandler = DataExecutions()
            # DataExecutionsHandler.fetchcoalBunkerData(start_date=two_months_back_formatted, end_date=todays_date)
            extract_bunker_data(start_date=two_months_back_formatted, end_date=todays_date)
        elif kwargs["report_name"] == "rail_avery_data":
            start_date = datetime.date.today().strftime("%Y/%m/%d")
            one_months_back = datetime.date.today() - relativedelta(months=1)
            end_date = one_months_back.strftime("%Y/%m/%d")
            endpoint_to_update_averydata(start_date=start_date, end_date=end_date)
        elif kwargs["report_name"] == "form15_data":
            today_date = datetime.datetime.today()
            datem = datetime.datetime(today_date.year, today_date.month, 1)
            endpoint_to_automate_form15_data(month=datem.strftime("Y-%m"))
            # print(datem.strftime("%Y-%m"))
            
    
        return "success"

    except Exception as e:
        console_logger.debug("----- Shift Scheduler Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

@router.get("/sync/limbs/bombcalorimeter", tags=["Coal Testing"])
def endpoint_to_sync_limbs_bombcalorimter(start_date: Optional[str] = None, end_date: Optional[str] = None):
    try:

        coal_test(start_date=start_date, end_date=end_date)
        endpoint_to_fetch_coal_quality_gcv()
        return {"detail": "success"}
    except Exception as e:
        console_logger.debug("----- Email Generation Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/fetch/shift/schedule", tags=["scheduler"])
def endpoint_to_fetch_shift_schedule(response: Response):
    try:
        listData = []
        try:
            fetchShiftSchedule = shiftScheduler.objects()
            for single_schedule in fetchShiftSchedule:
                listData.append(single_schedule.payload())
            return listData
        except DoesNotExist as e:
            return {"details": "No data found"}
    except Exception as e:
        console_logger.debug("----- Email Generation Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/fetch/single/shiftschedule", tags=["scheduler"])
def endpoint_to_fetch_single_shift_schedule(response: Response, report_name: str):
    try:
        fetchSingleShiftSchedule = shiftScheduler.objects(report_name=report_name)
        listData = []
        if fetchSingleShiftSchedule:
            for singleShiftSchedule in fetchSingleShiftSchedule:
                listData.append(singleShiftSchedule.payload())
        return listData
    except Exception as e:
        console_logger.debug("----- Email Generation Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/emailtriggerplatform", tags=["email"])
def end_point_to_update_email_trigger(response: Response, development: str):
    try:
        try:
            checkEmailDevelopment = EmailDevelopmentCheck.objects()
            if checkEmailDevelopment:
                checkEmailDevelopment.delete()
                EmailDevelopmentCheck(development=development).save()
        except DoesNotExist as e:
            EmailDevelopmentCheck(development=development).save()
        
        return {"detail": "success"}
    except Exception as e:
        console_logger.debug("----- Email Trigger Platform Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/fetchemailtriggerplatform", tags=["email"])
def end_point_to_fetch_email_trigger(response: Response):
    try:
        checkEmailDevelopment = EmailDevelopmentCheck.objects()
        return {"detail": checkEmailDevelopment[0].development}
    except Exception as e:
        console_logger.debug("----- Email Trigger Platform Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


def bunker_scheduler(**kwargs):
    try:
        time_format = "%H:%M"
        # Time to subtract: 5 hours and 30 minutes
        time_to_subtract = datetime.timedelta(hours=5, minutes=30)
        start_shift_hh, start_shift_mm = kwargs["start_time"].split(":")
        end_shift_hh, end_shift_mm = kwargs["end_time"].split(":")
        start_date = datetime.datetime.now().strftime("%Y-%m-%d")
        start_ddate = f"{start_date}T{start_shift_hh}:{start_shift_mm}:00"
        end_ddate = f"{start_date}T{end_shift_hh}:{end_shift_mm}:00"
        console_logger.debug(kwargs["shift_name"])
        save_bunker_data(start_ddate, end_ddate, kwargs["shift_name"])
    except Exception as e:
        console_logger.debug("----- Email Generation Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

@router.get("/coal/qualitygcv", tags=["Coal Testing"])
def endpoint_to_fetch_coal_quality_gcv():
    try:
        # uncomment later when bombcalorimeter will start working
        # DataExecutionsHandler = DataExecutions()
        # response = DataExecutionsHandler.fetch_coal_quality_gcv()
        # return response
        return "success"
    except Exception as e:
        console_logger.debug("----- Coal Quality GCV Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/insert/scheduler/shifts", tags=["Coal Testing"])
def endpoint_to_insert_shifts_scheduler(response: Response, shift_scheduler: str):
    try:
        DataExecutionsHandler = DataExecutions()
        response = DataExecutionsHandler.insertShiftScheduler(shift_scheduler=shift_scheduler)
        return response
    except Exception as e:
        console_logger.debug("----- Insert Shift Scheduler Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

@router.get("/fetch/scheduler/shifts", tags=["Coal Testing"])
def endpoint_to_fetch_shifts_scheduler(response: Response):
    try:
        DataExecutionsHandler = DataExecutions()
        response = DataExecutionsHandler.fetchShiftScheduler()
        return response
    except Exception as e:
        console_logger.debug("----- Fetch Shift Scheduler Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

# @router.get("/testing_consumption_report")
# def process_today_data(specified_date):
#     try:
#         today = specified_date
#         today_start = f'{today} 00:00:00'
#         today_end = f'{today} 23:59:59'
#         today_start = datetime.datetime.strptime(today_start, "%Y-%m-%d %H:%M:%S")
#         today_end = datetime.datetime.strptime(today_end, "%Y-%m-%d %H:%M:%S")
#         pipeline = [
#             {
#                 "$match": {
#                     "created_date": {
#                         "$gte": today_start,
#                         "$lt": today_end
#                     },
#                     "tagid": { "$in": [2, 16, 3536, 3538] }
#                 }
#             },
#             {
#                 "$project": {
#                     "hour": { "$hour": "$created_date" },
#                     "tagid": 1,
#                     "sum": { "$toDouble": "$sum" }
#                 }
#             },
#             {
#                 "$group": {
#                     "_id": {
#                         "hour": "$hour",
#                         "tagid": "$tagid"
#                     },
#                     "total_sum": { "$sum": "$sum" }
#                 }
#             },
#             {
#                 "$sort": { "_id.hour": 1 }
#             }
#         ]

#         results = list(Historian.objects.aggregate(pipeline))

#         unit_data = {
#             "Unit 1": {
#                 "label": list(range(24)),
#                 "generation_tag": [0] * 24,
#                 "consumption_tag": [0] * 24,
#                 "specific_coal": [0] * 24,
#                 "total_generation_sum": 0,
#                 "total_consumption_sum": 0,
#                 "total_specific_coal": 0
#             },
#             "Unit 2": {
#                 "label": list(range(24)),
#                 "generation_tag": [0] * 24,
#                 "consumption_tag": [0] * 24,
#                 "specific_coal": [0] * 24,
#                 "total_generation_sum": 0,
#                 "total_consumption_sum": 0,
#                 "total_specific_coal": 0
#             }
#         }

#         for result in results:
#             hour = result["_id"]["hour"]
#             tag_id = result["_id"]["tagid"]
#             sum_value = result["total_sum"]

#             if tag_id == 2:
#                 unit_data["Unit 1"]["generation_tag"][hour] = sum_value
#                 unit_data["Unit 1"]["total_generation_sum"] += sum_value
#             elif tag_id == 16:
#                 unit_data["Unit 1"]["consumption_tag"][hour] = sum_value
#                 unit_data["Unit 1"]["total_consumption_sum"] += sum_value
#             elif tag_id == 3536:
#                 unit_data["Unit 2"]["generation_tag"][hour] = sum_value
#                 unit_data["Unit 2"]["total_generation_sum"] += sum_value
#             elif tag_id == 3538:
#                 unit_data["Unit 2"]["consumption_tag"][hour] = sum_value
#                 unit_data["Unit 2"]["total_consumption_sum"] += sum_value
        
#         for unit in ["Unit 1", "Unit 2"]:
#             for hour in range(24):
#                 generation = unit_data[unit]["generation_tag"][hour]
#                 consumption = unit_data[unit]["consumption_tag"][hour]
#                 if generation > 0:
#                     unit_data[unit]["specific_coal"][hour] = round(consumption / generation, 2)
#                     unit_data[unit]["total_specific_coal"] += round(consumption / generation, 2)
#                 else:
#                     unit_data[unit]["specific_coal"][hour] = 0
#                     unit_data[unit]["total_specific_coal"] += 0

#         return unit_data
#     except Exception as e:
#         console_logger.debug("----- Error -----",e)
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#         console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#         return e
    

@router.get("/testing_consumption_report")
def process_today_data(specified_date):
    try:
        today = specified_date
        today_start = f'{today} 00:00:00'
        today_end = f'{today} 23:59:59'
        today_start = datetime.datetime.strptime(today_start, "%Y-%m-%d %H:%M:%S")
        today_end = datetime.datetime.strptime(today_end, "%Y-%m-%d %H:%M:%S")

        pipeline = [
            {
                "$match": {
                    "created_date": {
                        "$gte": today_start,
                        "$lt": today_end
                    },
                    "tagid": {"$in": [2, 16, 3536, 3538]}
                }
            },
            {
                "$sort": {
                    "created_date": -1
                }
            },
            {
                "$group": {
                    "_id": {
                        "ts": {"$hour": {"date": "$created_date"}},
                        "tagid": "$tagid",
                        "created_date": "$created_date"
                    },
                    "latest_sum": {"$first": {"$toDouble": "$sum"}}
                }
            },
            {
                "$project": {
                    "ts": "$_id.ts",
                    "tagid": "$_id.tagid",
                    "sum": "$latest_sum",
                    "_id": 0
                }
            },
            # Group the final data by ts and tagid
            {
                "$group": {
                    "_id": {
                        "ts": "$ts",
                        "tagid": "$tagid"
                    },
                    "data": {
                        "$push": "$sum"
                    }
                }
            },
            # Sort by the timestamp to ensure proper ordering
            {
                "$sort": { "_id.ts": 1 }
            }
        ]

        results = list(Historian.objects.aggregate(pipeline))

        unit_data = {
            "Unit 1": {
                "label": list(range(24)),
                "generation_tag": [0] * 24,
                "consumption_tag": [0] * 24,
                "specific_coal": [0] * 24,
                "total_generation_sum": 0,
                "total_consumption_sum": 0,
                "total_specific_coal": 0
            },
            "Unit 2": {
                "label": list(range(24)),
                "generation_tag": [0] * 24,
                "consumption_tag": [0] * 24,
                "specific_coal": [0] * 24,
                "total_generation_sum": 0,
                "total_consumption_sum": 0,
                "total_specific_coal": 0
            }
        }

        # Process the results and map them to Unit 1 and Unit 2 based on tagid
        for result in results:
            hour = result["_id"]["ts"]
            tag_id = result["_id"]["tagid"]
            sum_values = result["data"][0]  # This will now contain the sum 

            if tag_id == 2:
                unit_data["Unit 1"]["generation_tag"][hour] = sum_values
                unit_data["Unit 1"]["total_generation_sum"] += sum_values
            elif tag_id == 16:
                unit_data["Unit 1"]["consumption_tag"][hour] = sum_values
                unit_data["Unit 1"]["total_consumption_sum"] += sum_values
            elif tag_id == 3536:
                unit_data["Unit 2"]["generation_tag"][hour] = sum_values
                unit_data["Unit 2"]["total_generation_sum"] += sum_values
            elif tag_id == 3538:
                unit_data["Unit 2"]["consumption_tag"][hour] = sum_values
                unit_data["Unit 2"]["total_consumption_sum"] += sum_values
        
        # Calculate specific coal consumption based on generation and consumption
        for unit in ["Unit 1", "Unit 2"]:
            non_zero_hours = 0
            for hour in range(24):
                generation = unit_data[unit]["generation_tag"][hour]
                consumption = unit_data[unit]["consumption_tag"][hour]
                if generation > 0:
                    unit_data[unit]["specific_coal"][hour] = round(consumption / generation, 2)
                    unit_data[unit]["total_specific_coal"] += round(consumption / generation, 2)
                    non_zero_hours += 1
                else:
                    unit_data[unit]["specific_coal"][hour] = 0
                    unit_data[unit]["total_specific_coal"] += 0

            if non_zero_hours > 0:
                unit_data[unit]["average_generation"] = round(unit_data[unit]["total_generation_sum"] / non_zero_hours, 2)
                unit_data[unit]["average_consumption"] = round(unit_data[unit]["total_consumption_sum"] / non_zero_hours, 2)
                unit_data[unit]["average_specific_coal"] = round(unit_data[unit]["total_specific_coal"] / non_zero_hours, 2)

        return unit_data
    except Exception as e:
        console_logger.debug("----- Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return {"error": str(e)}


@router.get("/coal_consumption_pdf_report", tags=["PDF Report"])
def endpoint_to_generate_coal_consumption_report(response: Response, specified_date: Optional[str]=None):
    try:
        fetchtableData = process_today_data(specified_date)
        fetchData = generate_report_consumption(specified_date, fetchtableData)
        return fetchData
    except Exception as e:
        console_logger.debug("----- Coal Consumption PDF Report Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

    
@router.post("/tarealert", tags=["Email Alert"])
def endpoint_to_send_tare_email_alert(response: Response, data: RequestData):
    try:
        payload = data.dict()
        reportSchedule = ReportScheduler.objects()
        if reportSchedule[10].active == False:
            console_logger.debug("scheduler is off")
            return {"detail": "scheduler is off"}
        elif reportSchedule[10].active == True:
            console_logger.debug("inside Tare Email Alert")
            response_code, fetch_email = fetch_email_data()
            if response_code == 200:
                console_logger.debug(reportSchedule[7].recipient_list)
                subject = f"Tare Alert {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d'),'%Y-%m-%d').strftime('%d %B %Y')}"
                title_data = "<b>Tare Weight is not in between +/-500kg</b>"
                body = f"""
                        <b>Tare Alert: {datetime.datetime.strptime(datetime.datetime.today().strftime('%Y-%m-%d %H:%M:%S'),'%Y-%m-%d %H:%M:%S').strftime('%d %B %Y %H:%M:%S')}</b>
                        <br>
                        <br>
                        <!doctype html>
                        <html>
                        <head>
                            <meta charset="utf-8">
                            <title>Tare Alert</title>
                        </head>
                        <body>
                            {title_data}
                            <br>
                            <br>
                            <table border='1'>
                                <tr>
                                    <th>Mine</th>
                                    <th>Vehicle No</th>
                                    <th>Delivery Challan No</th>
                                    <th>DO No</th>
                                    <th>Vehicle Chassis No</th>
                                    <th>Fitness Expiry</th>
                                    <th>DC Date</th>
                                    <th>Challan Net Wt(MT)</th>
                                    <th>Challan Tare Wt(MT)</th>
                                    <th>GWEL Tare Wt(MT)</th>
                                    <th>Total Net Amount</th>
                                </tr>
                                <tr>
                                    <td>{payload.get('Mine_Name')}</td>
                                    <td>{payload.get('Vehicle_Truck_Registration_No')}</td>
                                    <td>{payload.get('Delivery_Challan_Number')}</td>
                                    <td>{payload.get('ARV_Cum_DO_Number')}</td>
                                    <td>{payload.get('Chassis_No')}</td>
                                    <td>{payload.get('Certificate_will_expire_on')}</td>
                                    <td>{payload.get('Delivery_Challan_Date')}</td>
                                    <td>{payload.get('Net_Qty')}</td>
                                    <td>{payload.get('Tare_Qty')}</td>
                                    <td>{payload.get('Actual_Tare_Qty')}</td>
                                    <td>{payload.get('Total_Net_Amount_of_Figures')}</td>
                                </tr>
                            </table>
                        </body>
                        </html>"""
                checkEmailDevelopment = EmailDevelopmentCheck.objects()
                if checkEmailDevelopment[0].development == "local":
                    console_logger.debug("inside local")
                    send_email(fetch_email.get("Smtp_user"), subject, fetch_email.get("Smtp_password"), fetch_email.get("Smtp_host"), fetch_email.get("Smtp_port"), reportSchedule[10].recipient_list, body, "", reportSchedule[10].cc_list, reportSchedule[10].bcc_list)
                elif checkEmailDevelopment[0].development == "prod":
                    console_logger.debug("inside prod")
                    send_data = {
                        "sender_email": fetch_email.get("Smtp_user"),
                        "subject": subject,
                        "password": fetch_email.get("Smtp_password"),
                        "smtp_host": fetch_email.get("Smtp_host"),
                        "smtp_port": fetch_email.get("Smtp_port"),
                        "receiver_email": reportSchedule[10].recipient_list,
                        "body": body,
                        "file_path": "",
                        "cc_list": reportSchedule[10].cc_list,
                        "bcc_list": reportSchedule[10].bcc_list
                    }
                    # console_logger.debug(send_data)
                    generate_email(Response, email=send_data)
        return {"detail": "success"}
    except Exception as e:
        console_logger.debug("----- Coal Consumption PDF Report Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


# Helper function to handle regex extraction with error handling
def extract_with_regex(pattern, text, group_index=1):
    try:
        # match = re.search(pattern, text, re.MULTILINE)
        # if match:
        #     return match.group(group_index).strip()
        # return None
        match = re.search(pattern, text, re.MULTILINE)
        if match:
            return match.group(group_index).strip()
        return None
    except Exception as e:
        console_logger.debug("----- Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

def extract_with_regex_scheme(text):
    try:
        # pattern = r"([\w\s\(\)\-]+)\s*Scheme Name\s*:"
        # match = re.search(pattern, text, re.DOTALL)
        # if match:
        #     text=' '.join(match.group(1).split()).strip()
        #     return text
        # return None
        pattern = r"([\w\s\(\)\-]+)\s*Scheme Name\s*:"
        match = re.search(pattern, text, re.DOTALL)
        if match:
            text=' '.join(match.group(1).split()).strip()
            return text
        return None
    except Exception as e:
        console_logger.debug("----- Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

# def extract_invoice_data(text):
#     data = {}    
#     pattern = r'([\w\s]+(?:\([^)]*\))?)\s+([\d.]+)\s+([\d.]+)'   

#     matches = re.findall(pattern, text)
#     for match in matches:
#         description, rate, amount = match
        
#         key = description.lower().replace(' ', '_').replace('(', '').replace(')', '').strip('_')

#         key = key.replace('\n', '')
        
#         if 'basic_price' in key and not 'royalty_charges__14_of_basic_price':
#             key = 'basic_price'
#         elif 'sizing_charges' in key:
#             key = 'sizing_charges'
#         elif 'stc_charges' in key:
#             key = 'stc_charges'
#         elif 'evac_facility_charge' in key:
#             key = 'evac_facility_charge'
#         elif 'royalty_charges' in key:
#             key = 'royalty_charges'
#         elif 'nmet_charges' in key:
#             key = 'nmet'
#         elif 'dmf' in key:
#             key = 'dmf'
#         elif 'cgst' in key:
#             key = 'cgst'
#         elif 'sgst' in key:
#             key = 'sgst'
#         elif 'gst_comp_cess' in key:
#             key = 'gst'
#         elif 'so_value' in key and 'grand_total' in key:
#             key = 'so_value'
#         elif 'less_emd' in key:
#             key = 'emd'
#         elif 'so_value' in key and 'rounded' in key:
#             key = 'so_rounded'
        
#         data[f'{key}_rate'] = float(rate)
#         data[f'{key}_amount'] = float(amount)
    
#     return data


# Function to extract specific fields using regex
# def extract_fields(pdf_path):
#     try:
#         reader = PdfReader(pdf_path)
#         text = ""
#         for page in reader.pages:
#             text += page.extract_text()
#         fields = {}
#         # Adjusted regex patterns for more accurate extraction
#         fields["do_no"] = extract_with_regex(r'(\d+)\s*Sales Order Number', text)
#         if fields["do_no"]:        
#             fields["do_date"] = extract_with_regex(r"([\w\s,]+)(?=\s*Sales Order Date)", text)
#             fields["start_date"] = extract_with_regex(r"([\w\s,]+)(?=\s*Sales Order Valid From)", text)
#             fields["end_date"] = extract_with_regex(r"([\w\s,]+)(?=\s*Sales Order Valid To)", text)
#             fields["slno"] = extract_with_regex(r"([\w\s,]+)(?=\s*Month)", text)
#             if fields["slno"]:
#                 fields["slno"]
#             else:
#                 fields["slno"] = extract_with_regex(r'Month\s*:\s*(\d+)', text)
#             # fields["Scheme Name"] = extract_with_regex(r"(.*?)\s*Scheme Name\s*:", text) 
#             fields["consumer_type"] = extract_with_regex_scheme(text) 

#             fields["grade"] = extract_with_regex(r"(\w+)\s+Grade Desc\s*:", text)
#             fields["Size"] = extract_with_regex(r"([\-\d\s\w]+)\s+Size\s*:", text)        
#             # Improved pattern for Plant extraction
#             fields["mine"] = extract_with_regex(r'Line Item Plant Material Material Description HSN Code Unit of Measure Quantity\s*\n10\s+([^\d]+)', text)
            
#             # Extract Line Item and Quantity more generally
#             fields["line_item"] = extract_with_regex(r'Line Item\s+Plant\s+Material.*\n(\d+)', text)
#             fields["do_qty"] = extract_with_regex(r'\b(\d{1,3}(?:,\d{3})*)\b\s*$', text)        
#             # New field: Total Net Amount
#             fields["po_amount"] = extract_with_regex(r"Total\s+([\d,]+\.\d{2})", text)   
#         else:        
#             fields["do_no"] = extract_with_regex(r'Sales Order Number\s+:\s+(\d+)', text)
#             fields["do_date"] = extract_with_regex(r'Sales Order Date\s+:\s+([A-Za-z]+\s+\d{1,2},\s+\d{4})', text)
#             fields["start_date"] = extract_with_regex(r'Sales Order Valid From\s+:\s+([A-Za-z]+\s+\d{1,2},\s+\d{4})', text)
#             fields["end_date"] = extract_with_regex(r'Sales Order Valid To\s+:\s+([A-Za-z]+\s+\d{1,2},\s+\d{4})', text)
#             fields["slno"] = extract_with_regex(r"([\w\s,]+)(?=\s*Month)", text)
#             if fields["slno"]:
#                 fields["slno"]
#             else:
#                 fields["slno"] = extract_with_regex(r'Month\s*:\s*(\d+)', text)
#             fields["consumer_type"] = extract_with_regex(r"Scheme Name\s*:\s*(.*\s*.*)", text) 
#             fields["consumer_type"] = re.sub(r'\n', '', fields["consumer_type"])
#             fields["grade"] = extract_with_regex(r"(?i)Grade Desc\s*:\s*(.*)" , text)
#             fields["Size"] = extract_with_regex(r"Size\s*:\s*(\S+)\s*", text)+" MM"        
#             # Improved pattern for Plant extraction
#             fields["mine"] = extract_with_regex(r'Line Item Plant Material Material Description HSN Code Unit of Measure Quantity\s*\n10\s+([^\d]+)', text)
#             # Extract Line Item and Quantity more generally
#             fields["line_item"] = extract_with_regex(r'Line Item\s+Plant\s+Material.*\n(\d+)', text)
#             fields["do_qty"] = extract_with_regex(r'\b(\d{1,3}(?:,\d{3})*)\b\s*$', text)        
#             # New field: Total Net Amount
#             fields["po_amount"] = extract_with_regex(r"Total\s+([\d,]+\.\d{2})", text)
#         return fields
#     except Exception as e:
#         console_logger.debug("----- Error ----- {}".format(e))
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#         console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#         return e

# new
def extract_fields(pdf_path):
    try:
        reader = PdfReader(pdf_path)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
        fields = {}

        fields["do_no"] = extract_with_regex(r'(\d+)\s*Sales Order Number', text)
        if fields["do_no"]:        
            fields["do_date"] = extract_with_regex(r"([\w\s,]+)(?=\s*Sales Order Date)", text)
            fields["start_date"] = extract_with_regex(r"([\w\s,]+)(?=\s*Sales Order Valid From)", text)
            fields["end_date"] = extract_with_regex(r"([\w\s,]+)(?=\s*Sales Order Valid To)", text)
            fields["slno"] = extract_with_regex(r"([\w\s,]+)(?=\s*Month)", text)
            if fields["slno"]:
                fields["slno"]
            else:
                fields["slno"] = extract_with_regex(r'Month\s*:\s*(\d+)', text)
            # fields["Scheme Name"] = extract_with_regex(r"(.*?)\s*Scheme Name\s*:", text) 
            fields["consumer_type"] = extract_with_regex_scheme(text) 
            fields["mode_of_transport"] = extract_with_regex(r'(\w+)\s+Mode of Transport\s*:', text)
            fields["grade"] = extract_with_regex(r"(\w+)\s+Grade Desc\s*:", text)
            fields["size"] = extract_with_regex(r"([\-\d\s\w]+)\s+Size\s*:", text)    
            # Improved pattern for Plant extraction
            fields["mine"] = extract_with_regex(r'Line Item Plant Material Material Description HSN Code Unit of Measure Quantity\s*\n10\s+([^\d]+)', text)
            
            # Extract Line Item and Quantity more generally
            fields["line_item"] = extract_with_regex(r'Line Item\s+Plant\s+Material.*\n(\d+)', text)
            fields["Material Description"] = extract_with_regex(r'(\S+\s+[-~]\d+\s*MM)', text)
            if fields["Material Description"]:
                fields["Material Description"]=fields["Material Description"].replace('\n', '')
            fields["do_qty"] = extract_with_regex(r'\b(\d{1,3}(?:,\d{3})*)\b\s*$', text)        
            # New field: Total Net Amount
            fields["po_amount"] = extract_with_regex(r"Total\s+([\d,]+\.\d{2})", text)   
            fields['table'] = extract_invoice_data(text)
        return fields
    except Exception as e:
        console_logger.debug("----- Road Sap Upload Error -----",e)
        # response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


def extract_invoice_data(pdf_path):
    try:
        with pdfplumber.open(pdf_path) as pdf:
            content = ""
            for page in pdf.pages:
                content += page.extract_text() + "\n"
        # Extract required information
        fields = {
            'do_no': r'Sales Order Number\s*:\s*(\d+)',
            'consumer_type': r'Scheme Name\s*:\s*([A-Za-z0-9 ,\-]+)(?=\s*Area\s*:)',
            'do_date':r'Sales Order Date\s*:\s*((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2},\s+\d{4})',
            'grade': r'Grade Desc\s*:\s*(.+)',
            'start_date': r'Sales Order Valid From\s*:\s*([A-Za-z0-9 ,]+?)(?=\s*[A-Z][a-zA-Z\s]+:)',
            'end_date': r'Sales Order Valid To\s*:\s*([A-Za-z0-9 ,]+?)(?=\s*[A-Z][a-zA-Z\s]+:)',
            'size': r'Size\s*:(.+)',
            'bid_id': r'Bid ID\s*:\s*(.+?)(?=\s+Mode)',
            'mode_of_transport': r'Mode of Transport\s*:\s*(.+?)(?=\s+Name)',
            'slno': r'Month\s*:\s*(\d+)',
            'type_of_consumer': r'Type of Consumer\s*:\s*(.+?)(?=\s+No\.)',
            'mine': r'Line Item.*?\n\d+\s+(.*?)\s+(\d{10})',
            'material_description': r'\d{10}\s+(.+?)(?=\s+27011200)',
            'do_qty': r'TE\s+([\d,]+)',
            'line_item': r'Line Item.*?Quantity\s*\n(\d+)'
        }
        extracted_data = {}
        for key, pattern in fields.items():
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                extracted_data[key] = match.group(1).strip()
            else:
                extracted_data[key] = "Not found"
        
        # Extract table data
        table_pattern = r'(Pricing Description.+?)(?=\nLess EMD|\Z)'
        table_match = re.search(table_pattern, content, re.DOTALL)
        
        if table_match:
            table_text = table_match.group(1)
            rows = re.findall(r'(.+?)\s+([\d.]+)\s+([\d.]+)', table_text)
            df = pd.DataFrame(rows, columns=['Pricing Description', 'Rate Per Ton(INR)', 'Amount(INR)'])
            extracted_data['Table'] = df
        else:
            extracted_data['Table'] = "Table not found"
        
        bank_details_pattern = r'BANK DETAILS.*?Total\s+([\d.]+)'
        bank_details_match = re.search(bank_details_pattern, content, re.DOTALL | re.IGNORECASE)
        if bank_details_match:
            extracted_data['po_amount'] = bank_details_match.group(1)
        else:
            extracted_data['po_amount'] = "Not found"
        
        return extracted_data
    except Exception as e:
        console_logger.debug("----- Road Sap Upload Error -----",e)
        # response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e



def get_saprecordstable_data(loopdata, data_name):
    try:
        for key, value in loopdata.items():
            if key == data_name:
                return value
        return None
    except Exception as e:
        console_logger.debug("----- Road Sap Upload Error -----",e)
        # response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e



# @router.post("/road/sapupload", tags=["Coal Testing"])
# async def endpoint_to_upload_sap_data(response: Response, pdf_upload: List[UploadFile] = File(...)):
#     try:
#         for UploadedFile in pdf_upload:

#             f_name = UploadedFile.filename
#             contents = UploadedFile.file
#             #     if pdf_upload is None:
#             #         return {"error": "No file uploaded"}
#             # contents = await pdf_upload.read()

#             # Check if the file is empty
#             if not contents:
#                 return {"error": "Uploaded file is empty"}

#             # Verify file format (PDF)
#             if not f_name.endswith(('.pdf','.PDF')):
#                 response.status_code = 400
#                 return {"error": "Uploaded file is not a PDF"}
            
#             file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
#             target_directory = f"static_server/gmr_ai/{file}"
#             os.umask(0)
#             os.makedirs(target_directory, exist_ok=True, mode=0o777)

#             file_extension = f_name.split(".")[-1]
#             file_name = f'sap_upload_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.{file_extension}'
#             full_path = os.path.join(os.getcwd(), target_directory, file_name)
#             with open(full_path, "wb") as file_object:
#                 # file_object.write(file.file)
#                 shutil.copyfileobj(contents, file_object)

#             # fetchPdfData = extract_fields(full_path)

#             fetchPdfData = extract_invoice_data(full_path)

#             console_logger.debug(fetchPdfData)


#             if fetchPdfData:
#                 try:
#                     checkSaprecords = SapRecords.objects.get(do_no=fetchPdfData.get("do_no"))
#                     checkSaprecords.do_date = datetime.datetime.strptime(fetchPdfData.get("do_date"), '%b %d, %Y').strftime('%Y-%m-%d')
#                     checkSaprecords.start_date = datetime.datetime.strptime(fetchPdfData.get("start_date"), '%b %d, %Y').strftime('%Y-%m-%d')
#                     checkSaprecords.end_date = datetime.datetime.strptime(fetchPdfData.get("end_date"), '%b %d, %Y').strftime('%Y-%m-%d')
#                     checkSaprecords.slno = fetchPdfData.get("slno")
#                     if fetchPdfData.get("do_no")[:3] == "111":
#                         checkSaprecords.consumer_type = "WCL FSA Coal"
#                     elif fetchPdfData.get("do_no")[:3] == "112":
#                         checkSaprecords.consumer_type = "WCL E Auction Coal"
#                     elif fetchPdfData.get("do_no")[:3] == "114":
#                         if "Linkage" in fetchPdfData.get("consumer_type"):
#                             checkSaprecords.consumer_type = "WCL Shakti B(iii) Round 5 Coal"
#                         else:
#                             checkSaprecords.consumer_type = "WCL Shakti Auction Coal"
#                     checkSaprecords.grade = f'{fetchPdfData.get("grade")}{fetchPdfData.get("size")}'
#                     checkSaprecords.mine_name = fetchPdfData.get("mine")
#                     # checkSaprecords.line_item = fetchPdfData.get("line_item")
#                     checkSaprecords.do_qty = fetchPdfData.get("do_qty").replace(",", "")
#                     checkSaprecords.po_amount = fetchPdfData.get("po_amount")
#                     checkSaprecords.save()

#                 except DoesNotExist as e:
#                     # insertSapRecords = SapRecords(do_no=fetchPdfData.get("do_no"), do_date=fetchPdfData.get("do_date"), start_date=datetime.datetime.strptime(fetchPdfData.get("start_date"), '%b %d, %Y').strftime('%Y-%m-%d'), end_date=datetime.datetime.strptime(fetchPdfData.get("end_date"), '%b %d, %Y').strftime('%Y-%m-%d'), slno=fetchPdfData.get("slno"), consumer_type=fetchPdfData.get("consumer_type"), grade=f'{fetchPdfData.get("grade")} {fetchPdfData.get("size")}', mine_name=fetchPdfData.get("mine"), line_item=fetchPdfData.get("line_item"), do_qty=fetchPdfData.get("do_qty").replace(",", ""), po_amount=fetchPdfData.get("po_amount"))
#                     if fetchPdfData.get("do_no") is not None:
#                         if fetchPdfData.get("do_no")[:3] == "111":
#                             consumer_type = "WCL FSA Coal"
#                         elif fetchPdfData.get("do_no")[:3] == "112":
#                             consumer_type = "WCL E Auction Coal"
#                         elif fetchPdfData.get("do_no")[:3] == "114":
#                             if "Linkage" in fetchPdfData.get("consumer_type"):
#                                 consumer_type = "WCL Shakti B(iii) Round 5 Coal"
#                             else:
#                                 consumer_type = "WCL Shakti Auction Coal"
                        
#                         insertSapRecords = SapRecords(
#                             do_no=fetchPdfData.get("do_no"),
#                             do_date=datetime.datetime.strptime(fetchPdfData.get("do_date"), '%b %d, %Y').strftime('%Y-%m-%d'),
#                             start_date=datetime.datetime.strptime(fetchPdfData.get("start_date"), '%b %d, %Y').strftime('%Y-%m-%d'), 
#                             end_date=datetime.datetime.strptime(fetchPdfData.get("end_date"), '%b %d, %Y').strftime('%Y-%m-%d'), 
#                             slno=fetchPdfData.get("slno"), 
#                             consumer_type=consumer_type, 
#                             grade=f'{fetchPdfData.get("grade")}{fetchPdfData.get("size")}', 
#                             mine_name=fetchPdfData.get("mine"), 
#                             do_qty=fetchPdfData.get("do_qty").replace(",", ""), 
#                             po_amount=fetchPdfData.get("po_amount")
#                         )
#                         insertSapRecords.save()

                
#                 try:
#                     checkGmrData = Gmrdata.objects(arv_cum_do_number=fetchPdfData.get("do_no"))
#                     for singleCheckGmrData in checkGmrData:
#                         singleCheckGmrData.do_date = datetime.datetime.strptime(fetchPdfData.get("do_date"), '%b %d, %Y').strftime('%Y-%m-%d')
#                         singleCheckGmrData.start_date = datetime.datetime.strptime(fetchPdfData.get("start_date"), '%b %d, %Y').strftime('%Y-%m-%d')
#                         singleCheckGmrData.end_date = datetime.datetime.strptime(fetchPdfData.get("end_date"), '%b %d, %Y').strftime('%Y-%m-%d')
#                         singleCheckGmrData.slno = fetchPdfData.get("slno")
#                         if fetchPdfData.get("do_no")[:3] == "111":
#                             singleCheckGmrData.type_consumer = "WCL FSA Coal"
#                         elif fetchPdfData.get("do_no")[:3] == "112":
#                             singleCheckGmrData.type_consumer = "WCL E Auction Coal"
#                         elif fetchPdfData.get("do_no")[:3] == "114":
#                             if "Linkage" in fetchPdfData.get("consumer_type"):
#                                 singleCheckGmrData.type_consumer = "WCL Shakti B(iii) Round 5 Coal"
#                             else:
#                                 singleCheckGmrData.type_consumer = "WCL Shakti Auction Coal"
#                         singleCheckGmrData.type_consumer = fetchPdfData.get("consumer_type")
#                         singleCheckGmrData.grade = f'{fetchPdfData.get("grade")}{fetchPdfData.get("size")}'
#                         singleCheckGmrData.mine = fetchPdfData.get("mine")
#                         # singleCheckGmrData.line_item = fetchPdfData.get("line_item")
#                         singleCheckGmrData.po_qty = fetchPdfData.get("do_qty").replace(",", "")
#                         singleCheckGmrData.po_amount = fetchPdfData.get("po_amount")
#                         singleCheckGmrData.save()
#                 except DoesNotExist as e:
#                     pass

#         return {"detail": "success"}
#     except Exception as e:
#         console_logger.debug("----- Road Sap Upload Error -----",e)
#         response.status_code = 400
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#         console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#         return e


@router.post("/road/sapupload", tags=["Coal Testing"])
async def endpoint_to_upload_sap_data(response: Response, pdf_upload: List[UploadFile] = File(...)):
    try:
        listData = []
        for UploadedFile in pdf_upload:

            f_name = UploadedFile.filename
            contents = UploadedFile.file

            # Check if the file is empty
            if not contents:
                return {"error": "Uploaded file is empty"}

            # Verify file format (PDF)
            if not f_name.endswith(('.pdf','.PDF')):
                response.status_code = 400
                return {"error": "Uploaded file is not a PDF"}
            
            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            file_extension = f_name.split(".")[-1]
            file_name = f'sap_upload_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.{file_extension}'
            full_path = os.path.join(os.getcwd(), target_directory, file_name)
            with open(full_path, "wb") as file_object:
                shutil.copyfileobj(contents, file_object)

            fetchPdfData = extract_invoice_data(full_path)

            listData.append(fetchPdfData)

        return listData
    except Exception as e:
        console_logger.debug("----- Road Sap Upload Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/upload/road/sapupload", tags=["Coal Testing"])
def endpoint_to_update_road_sap_upload(response: Response, data: roadSapUpload):
    try:
        multyData = data.dict()
        
        for dataLoad in multyData["data"]:
            try:
                checkSaprecords = SapRecords.objects.get(do_no=dataLoad.get("do_no"))
                checkSaprecords.do_date = datetime.datetime.strptime(dataLoad.get("do_date"), '%b %d, %Y').strftime('%Y-%m-%d')
                checkSaprecords.start_date = datetime.datetime.strptime(dataLoad.get("start_date"), '%b %d, %Y').strftime('%Y-%m-%d')
                checkSaprecords.end_date = datetime.datetime.strptime(dataLoad.get("end_date"), '%b %d, %Y').strftime('%Y-%m-%d')
                checkSaprecords.slno = dataLoad.get("slno")
                checkSaprecords.consumer_type = dataLoad.get("consumer_type")
                checkSaprecords.grade = dataLoad.get("grade")
                checkSaprecords.mine_name = dataLoad.get("mine_name")
                checkSaprecords.do_qty = dataLoad.get("do_qty")
                checkSaprecords.po_amount = dataLoad.get("po_amount")
                # particulars start
                checkSaprecords.basic_price = float(dataLoad.get("Table").get("Amount(INR)").get("0"))
                checkSaprecords.sizing_charges = float(dataLoad.get("Table").get("Amount(INR)").get("1"))
                checkSaprecords.stc_charges = float(dataLoad.get("Table").get("Amount(INR)").get("2"))
                checkSaprecords.evac_facility_charges = float(dataLoad.get("Table").get("Amount(INR)").get("3"))
                checkSaprecords.royality_charges = float(dataLoad.get("Table").get("Amount(INR)").get("4"))
                checkSaprecords.nmet_charges = float(dataLoad.get("Table").get("Amount(INR)").get("5"))
                checkSaprecords.dmf = float(dataLoad.get("Table").get("Amount(INR)").get("6"))
                checkSaprecords.cgst = float(dataLoad.get("Table").get("Amount(INR)").get("7"))
                checkSaprecords.sgst = float(dataLoad.get("Table").get("Amount(INR)").get("8"))
                checkSaprecords.gst_comp_cess = float(dataLoad.get("Table").get("Amount(INR)").get("9"))
                checkSaprecords.so_value_grand_total = float(dataLoad.get("Table").get("Amount(INR)").get("10"))
                # particulars end
                checkSaprecords.save()
            except DoesNotExist as e:
                insertSapRecords = SapRecords(
                    do_no=dataLoad.get("do_no"),
                    do_date=datetime.datetime.strptime(dataLoad.get("do_date"), '%b %d, %Y').strftime('%Y-%m-%d'),
                    start_date=datetime.datetime.strptime(dataLoad.get("start_date"), '%b %d, %Y').strftime('%Y-%m-%d'), 
                    end_date=datetime.datetime.strptime(dataLoad.get("end_date"), '%b %d, %Y').strftime('%Y-%m-%d'), 
                    slno=dataLoad.get("slno"), 
                    consumer_type=dataLoad.get("consumer_type"), 
                    grade=dataLoad.get("grade"), 
                    mine_name=dataLoad.get("mine_name"), 
                    do_qty=dataLoad.get("do_qty"), 
                    po_amount=dataLoad.get("po_amount"),
                    # particulars start
                    basic_price = float(dataLoad.get("Table").get("Amount(INR)").get("0")),
                    sizing_charges = float(dataLoad.get("Table").get("Amount(INR)").get("1")),
                    stc_charges = float(dataLoad.get("Table").get("Amount(INR)").get("2")),
                    evac_facility_charges = float(dataLoad.get("Table").get("Amount(INR)").get("3")),
                    royality_charges = float(dataLoad.get("Table").get("Amount(INR)").get("4")),
                    nmet_charges = float(dataLoad.get("Table").get("Amount(INR)").get("5")),
                    dmf = float(dataLoad.get("Table").get("Amount(INR)").get("6")),
                    cgst = float(dataLoad.get("Table").get("Amount(INR)").get("7")),
                    sgst = float(dataLoad.get("Table").get("Amount(INR)").get("8")),
                    gst_comp_cess = float(dataLoad.get("Table").get("Amount(INR)").get("9")),
                    so_value_grand_total = float(dataLoad.get("Table").get("Amount(INR)").get("10")),
                    # particulars end
                )
                insertSapRecords.save() 

            try:
                checkGmrData = Gmrdata.objects(arv_cum_do_number=dataLoad.get("do_no"))
                for singleCheckGmrData in checkGmrData:
                    singleCheckGmrData.do_date = datetime.datetime.strptime(dataLoad.get("do_date"), '%b %d, %Y').strftime('%Y-%m-%d')
                    singleCheckGmrData.start_date = datetime.datetime.strptime(dataLoad.get("start_date"), '%b %d, %Y').strftime('%Y-%m-%d')
                    singleCheckGmrData.end_date = datetime.datetime.strptime(dataLoad.get("end_date"), '%b %d, %Y').strftime('%Y-%m-%d')
                    singleCheckGmrData.slno = dataLoad.get("slno")
                    singleCheckGmrData.type_consumer = dataLoad.get("consumer_type")
                    singleCheckGmrData.grade = dataLoad.get("grade")
                    singleCheckGmrData.mine = dataLoad.get("mine_name")
                    singleCheckGmrData.po_qty = dataLoad.get("do_qty")
                    singleCheckGmrData.po_amount = dataLoad.get("po_amount")
                    singleCheckGmrData.save()
            except DoesNotExist as e:
                pass
    except Exception as e:
        console_logger.debug("----- Update Road Sap Upload Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e



@router.post("/rcr/do_sales_order", tags=["Coal Testing"])
async def endpoint_to_upload_rcr_do_sales_order(response: Response, pdf_upload: List[UploadFile] = File(...)):
    try:
        listData = []
        for UploadedFile in pdf_upload:

            f_name = UploadedFile.filename
            contents = UploadedFile.file

            # Check if the file is empty
            if not contents:
                return {"error": "Uploaded file is empty"}

            # Verify file format (PDF)
            if not f_name.endswith(('.pdf','.PDF')):
                response.status_code = 400
                return {"error": "Uploaded file is not a PDF"}
            
            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            file_extension = f_name.split(".")[-1]
            file_name = f'sap_upload_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.{file_extension}'
            full_path = os.path.join(os.getcwd(), target_directory, file_name)
            with open(full_path, "wb") as file_object:
                shutil.copyfileobj(contents, file_object)

            fetchPdfData = extract_invoice_data(full_path)

            # console_logger.debug(fetchPdfData)
            testData = fetchPdfData.get("Table").get("Amount(INR)").to_dict()

            fetchPdfData["po_amount"] = testData.get(14)

            listData.append(fetchPdfData)

        return listData
    except Exception as e:
        console_logger.debug("----- Road Sap Upload Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/upload/rcr/do_sales_order", tags=["Coal Testing"])
def endpoint_to_update_road_sap_upload(response: Response, data: roadSapUpload):
    try:
        multyData = data.dict()
        console_logger.debug(multyData)
        for dataLoad in multyData["data"]:
            try:
                checkSaprecords = SapRecordsRcrRoad.objects.get(do_no=dataLoad.get("do_no"))
                checkSaprecords.do_date = datetime.datetime.strptime(dataLoad.get("do_date"), '%b %d, %Y').strftime('%Y-%m-%d')
                checkSaprecords.start_date = datetime.datetime.strptime(dataLoad.get("start_date"), '%b %d, %Y').strftime('%Y-%m-%d')
                checkSaprecords.end_date = datetime.datetime.strptime(dataLoad.get("end_date"), '%b %d, %Y').strftime('%Y-%m-%d')
                checkSaprecords.slno = dataLoad.get("slno")
                checkSaprecords.consumer_type = dataLoad.get("consumer_type")
                checkSaprecords.grade = dataLoad.get("grade")
                checkSaprecords.mine_name = dataLoad.get("mine_name")
                checkSaprecords.do_qty = dataLoad.get("do_qty")
                checkSaprecords.po_amount = dataLoad.get("po_amount")
                checkSaprecords.basic_charges = float(dataLoad.get("Table").get("Amount(INR)").get("0"))
                checkSaprecords.sizing_charges = float(dataLoad.get("Table").get("Amount(INR)").get("1"))
                checkSaprecords.stc_charges = float(dataLoad.get("Table").get("Amount(INR)").get("2"))
                checkSaprecords.evac_facility_charges = float(dataLoad.get("Table").get("Amount(INR)").get("3"))
                checkSaprecords.royality_charges = float(dataLoad.get("Table").get("Amount(INR)").get("4"))
                checkSaprecords.nmet_charges = float(dataLoad.get("Table").get("Amount(INR)").get("5"))
                checkSaprecords.dmf = float(dataLoad.get("Table").get("Amount(INR)").get("6"))
                checkSaprecords.adho_sanrachna_vikas = float(dataLoad.get("Table").get("Amount(INR)").get("7"))
                checkSaprecords.pariyavarn_upkar = float(dataLoad.get("Table").get("Amount(INR)").get("8"))
                checkSaprecords.terminal_tax = float(dataLoad.get("Table").get("Amount(INR)").get("9"))
                checkSaprecords.assessable_value = float(dataLoad.get("Table").get("Amount(INR)").get("10"))
                checkSaprecords.igst = float(dataLoad.get("Table").get("Amount(INR)").get("11"))
                checkSaprecords.gst_comp_cess = float(dataLoad.get("Table").get("Amount(INR)").get("12"))
                checkSaprecords.requisite_payment = float(dataLoad.get("Table").get("Amount(INR)").get("13"))
                checkSaprecords.save()
            except DoesNotExist as e:
                insertSapRecords = SapRecordsRcrRoad(
                    do_no=dataLoad.get("do_no"),
                    do_date=datetime.datetime.strptime(dataLoad.get("do_date"), '%b %d, %Y').strftime('%Y-%m-%d'),
                    start_date=datetime.datetime.strptime(dataLoad.get("start_date"), '%b %d, %Y').strftime('%Y-%m-%d'), 
                    end_date=datetime.datetime.strptime(dataLoad.get("end_date"), '%b %d, %Y').strftime('%Y-%m-%d'), 
                    slno=dataLoad.get("slno"), 
                    consumer_type=dataLoad.get("consumer_type"), 
                    grade=dataLoad.get("grade"), 
                    mine_name=dataLoad.get("mine_name"), 
                    do_qty=dataLoad.get("do_qty"), 
                    po_amount=dataLoad.get("po_amount"),
                    basic_charges = float(dataLoad.get("Table").get("Amount(INR)").get("0")),
                    sizing_charges = float(dataLoad.get("Table").get("Amount(INR)").get("1")),
                    stc_charges = float(dataLoad.get("Table").get("Amount(INR)").get("2")),
                    evac_facility_charges = float(dataLoad.get("Table").get("Amount(INR)").get("3")),
                    royality_charges = float(dataLoad.get("Table").get("Amount(INR)").get("4")),
                    nmet_charges = float(dataLoad.get("Table").get("Amount(INR)").get("5")),
                    dmf = float(dataLoad.get("Table").get("Amount(INR)").get("6")),
                    adho_sanrachna_vikas = float(dataLoad.get("Table").get("Amount(INR)").get("7")),
                    pariyavarn_upkar = float(dataLoad.get("Table").get("Amount(INR)").get("8")),
                    terminal_tax = float(dataLoad.get("Table").get("Amount(INR)").get("9")),
                    assessable_value = float(dataLoad.get("Table").get("Amount(INR)").get("10")),
                    igst = float(dataLoad.get("Table").get("Amount(INR)").get("11")),
                    gst_comp_cess = float(dataLoad.get("Table").get("Amount(INR)").get("12")),
                    requisite_payment = float(dataLoad.get("Table").get("Amount(INR)").get("13")),
                )
                insertSapRecords.save() 

            try:
                checkGmrData = RcrRoadData.objects(do_number=dataLoad.get("do_no"))
                for singleCheckGmrData in checkGmrData:
                    singleCheckGmrData.do_date = datetime.datetime.strptime(dataLoad.get("do_date"), '%b %d, %Y').strftime('%Y-%m-%d')
                    singleCheckGmrData.start_date = datetime.datetime.strptime(dataLoad.get("start_date"), '%b %d, %Y').strftime('%Y-%m-%d')
                    singleCheckGmrData.end_date = datetime.datetime.strptime(dataLoad.get("end_date"), '%b %d, %Y').strftime('%Y-%m-%d')
                    singleCheckGmrData.slno = dataLoad.get("slno")
                    singleCheckGmrData.type_consumer = dataLoad.get("consumer_type")
                    singleCheckGmrData.grade = dataLoad.get("grade")
                    singleCheckGmrData.mine = dataLoad.get("mine")
                    singleCheckGmrData.po_qty = dataLoad.get("do_qty")
                    singleCheckGmrData.po_amount = dataLoad.get("po_amount")
                    singleCheckGmrData.save()
            except DoesNotExist as e:
                pass
    except Exception as e:
        console_logger.debug("----- Update Rcr Sap Upload Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e




# @router.get("/fetch/rake/quota", tags=["Rail Map"])
# def end_point_to_fetch_rake_quota_test(response: Response, 
#                 currentPage: Optional[int] = None,
#                 perPage: Optional[int] = None,
#                 # search_text: Optional[str] = None,
#                 month_date: Optional[str] = None,
#                 start_timestamp: Optional[str] = None,
#                 end_timestamp: Optional[str] = None,
#                 type: Optional[str] = "display"):
#     try:
#         data = Q()
#         result = {        
#                 "labels": [],
#                 "datasets": [],
#                 "total": 0,
#                 "page_size": 15
#         }

#         if type and type == "display":

#             page_no = 1
#             page_len = result["page_size"]

#             if currentPage:
#                 page_no = currentPage

#             if perPage:
#                 page_len = perPage
#                 result["page_size"] = perPage

#             if month_date:
#                 month_check = datetime.datetime.strptime(month_date, "%Y-%m").strftime("%m-%Y")
#                 data &= Q(month__iexact = month_check)

#             if start_timestamp:
#                 start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M", "Asia/Kolkata", False)
#                 data &= Q(created_at__gte=start_date)

#             if end_timestamp:
#                 end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M", "Asia/Kolkata", False)
#                 data &= Q(created_at__lte=end_date)

#             offset = (page_no - 1) * page_len
#             logs = (
#                 rakeQuota.objects(data)
#                 .order_by("year", "month")
#                 .skip(offset)
#                 .limit(page_len)
#             )
#             listData = []
#             if logs:
#                 for log in logs:
#                     # dictData = {"SrNo": 0, "month": "", 'valid_upto': "", "rake_alloted": "", "source_type": "","rakes_loaded_till_date": 0, "rakes_loaded_on_date": 0, "previous_month_rake": 0, "rakes_received_on_date": 0, "total_rakes_received_for_month": 0, "balance_rakes_to_receive": 0, "no_of_rakes_in_transist": 0, "rakes_previous_month_quota_received": 0}
#                     dictData = {"month": "", "source_type": "", "rakes_previous_month_quota_received": 0, "rake_planned_for_the_month": "","rakes_loaded_till_date": 0, "rakes_loaded_on_date": 0, "previous_month_rake": 0, "rakes_received_on_date": 0, "total_rakes_received_for_month": 0, "balance_rakes_to_receive": 0, "no_of_rakes_in_transist": 0, }
#                     rake_year = log.year
#                     rake_month = log.month
#                     # Convert rake_month to match RailData drawn_date format
#                     month_year = f"{rake_year}-{rake_month[:2].upper()}"
#                     # date_obj = datetime.datetime.strptime(month_year, "%Y-%b")
#                     date_obj = datetime.datetime.strptime(month_year, "%Y-%m")

#                     # Get the previous month
#                     prev_date_obj = date_obj - datetime.timedelta(days=1)
#                     # prev_month_year = prev_date_obj.strftime("%Y-%b")
#                     prev_month_year = prev_date_obj.strftime("%Y-%m")

#                     # Format the date object to the desired format
#                     formatted_date = date_obj.strftime("%Y-%m")
#                     # Query RailData based on drawn_date month-year match
#                     # rail_logs = RailData.objects.filter(drawn_date__icontains=formatted_date)
#                     rail_logs = RailData.objects.filter(placement_date__icontains=formatted_date)
#                     dictData["month"] = datetime.datetime.strptime(log.month, "%m-%Y").strftime("%b-%Y")
#                     dictData["valid_upto"] = log.valid_upto
#                     dictData["rake_planned_for_the_month"] = log.rake_alloted

#                     # Calculate balance rakes to receive
#                     balance_rakes_to_receive = int(log.rake_alloted) - int(dictData["total_rakes_received_for_month"])
#                     dictData["balance_rakes_to_receive"] = balance_rakes_to_receive

#                     prev_date_obj = datetime.datetime.strptime(log.month, "%m-%Y")

#                     last_month = date_obj.month-1
#                     last_year = date_obj.year

#                     if last_month == 0:
#                         last_month = 12
#                         last_year -= 1

#                     last_month_date_obj = datetime.datetime(last_year, last_month, 1)

#                     # Convert back to the "%b-%Y" format
#                     last_month_str = last_month_date_obj.strftime("%m-%Y")
#                     # last_month = datetime.datetime.strptime(last_month_str, "%b-%Y")
#                     last_month = datetime.datetime.strptime(last_month_str, "%m-%Y")

#                     # Query for the previous month's data
#                     prev_month_log = rakeQuota.objects.filter(
#                         month=f'{last_month.strftime("%b").upper()}-{last_month.strftime("%Y")}',
#                         year=last_month.strftime("%Y")
#                     ).first()
#                     # If there is a previous month log, add its rake_alloted to the current month's rake_alloted
#                     if prev_month_log:
#                         # dictData["previous_month_rake"] += prev_month_log.rake_alloted
#                         # dictData["previous_month_rake"] = int(log.rake_alloted) + int(prev_month_log.rake_alloted)
#                         dictData["previous_month_rake"] = int(prev_month_log.rake_alloted)

#                     if rail_logs:
#                         for rail_log in rail_logs:
#                             # source_type = rail_log.source_type
#                             # console_logger.debug(rail_log.source_type)
#                             if rail_log.source_type != "":
#                                 dictData["source_type"] = rail_log.source_type
#                             # Count the rakes loaded till the drawn_date for the specific rr_no
#                             dictData["rakes_loaded_till_date"] = RailData.objects.filter(
#                                 # rr_no=rail_log.rr_no,
#                                 drawn_date__lte=f"{formatted_date}-30T23:59"
#                             ).count()
#                             # Get today's date in UTC
#                             today_utc = datetime.datetime.utcnow().replace(hour=0, minute=0, second=0)
#                             end_of_day_utc = today_utc + timedelta(hours=23, minutes=59, seconds=59)

#                             # Query to filter data based on the current date
#                             dictData["rakes_loaded_on_date"] = RailData.objects.filter(
#                                 # rr_no=rail_log.rr_no,
#                                 drawn_date__gte=today_utc,
#                                 drawn_date__lte=end_of_day_utc
#                             ).count()
#                             dictData["no_of_rakes_in_transist"] = dictData["rakes_loaded_till_date"] - dictData["total_rakes_received_for_month"]

#                             # dictData["balance_rakes_to_receive"] = log.rake_alloted - dictData["total_rakes_received_for_month"]
#                             # dictData["rakes_loaded_on_date"] = RailData.objects.filter(
#                             #     rr_no=rail_log.rr_no,
#                             #     drawn_date__gte=f"{datetime.datetime.today().strftime('%Y-%m-%d')}T00:00",
#                             #     drawn_date__lte=f"{datetime.datetime.today().strftime('%Y-%m-%d')}T23:59"
#                             # ).count()
#                     listData.append(dictData)
#                 # After building the listData, add the balance_rakes_to_receive to the next available month
#                 for i, current_month_data in enumerate(listData):
#                     for j in range(i + 1, len(listData)):
#                         next_month_data = listData[j]
#                         # console_logger.debug(next_month_data["month"])
#                         # console_logger.debug(current_month_data["month"])
#                         # Compare months to find the next available month
#                         if next_month_data["month"] > current_month_data["month"]:
#                             next_month_data["rakes_previous_month_quota_received"] = current_month_data["balance_rakes_to_receive"]
#                             break  # Stop after updating the first available next month

#             # Append to the result dataset
#                 result["labels"] = list(dictData.keys())
#                 result["datasets"] = listData
#                 result["total"] = len(rakeQuota.objects(data))
#             return result

#         elif type == "download":
#             file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
#             target_directory = f"static_server/gmr_ai/{file}"
#             os.umask(0)
#             os.makedirs(target_directory, exist_ok=True, mode=0o777)

#             headers = ["month", "year", 'valid_upto', "rake_alloted", "source_type", "rakes_planned_for_month","rakes_loaded_till_date", "rakes_loaded_on_date", "previous_month_rake", "rakes_received_on_date", "total_rakes_received_for_month", "balance_rakes_to_receive", "no_of_rakes_in_transist"]

#             if month_date:
#                 month_check = datetime.datetime.strptime(month_date, "%Y-%m").strftime("%b-%Y").upper()
#                 data &= Q(month__iexact = month_check)
            
#             if start_timestamp:
#                 start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M", "Asia/Kolkata", False)
#                 data &= Q(created_at__gte=start_date)

#             if end_timestamp:
#                 end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M", "Asia/Kolkata", False)
#                 data &= Q(created_at__lte=end_date)

#             usecase_data = rakeQuota.objects(data).order_by("-created_at")
#             count = len(usecase_data)
#             path = None

#             if usecase_data:
#                 try: 
#                     path = os.path.join(
#                         "static_server",
#                         "gmr_ai",
#                         file,
#                         "Rake_Quota_{}.xlsx".format(
#                             datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
#                         ),
#                     )
#                     filename = os.path.join(os.getcwd(), path)
#                     workbook = xlsxwriter.Workbook(filename)
#                     workbook.use_zip64()
#                     cell_format2 = workbook.add_format()
#                     cell_format2.set_bold()
#                     cell_format2.set_font_size(10)
#                     cell_format2.set_align("center")
#                     cell_format2.set_align("vcenter")

#                     worksheet = workbook.add_worksheet()
#                     worksheet.set_column("A:AZ", 20)
#                     worksheet.set_default_row(50)
#                     cell_format = workbook.add_format()
#                     cell_format.set_font_size(10)
#                     cell_format.set_align("center")
#                     cell_format.set_align("vcenter")

#                     for index, header in enumerate(headers):
#                         worksheet.write(0, index, header, cell_format2)
                    
#                     for row, query in enumerate(usecase_data, start=1):
#                         rake_month = query.month
#                         rake_year = query.year
#                         month_year = f"{rake_year}-{rake_month[:3].upper()}"
#                         date_obj = datetime.datetime.strptime(month_year, "%Y-%b")
#                         formatted_date = date_obj.strftime("%Y-%m")
#                         rail_logs = RailData.objects.filter(drawn_date__icontains=formatted_date)
#                         # worksheet.write(row, 0, row, cell_format)
#                         worksheet.write(row, 0, str(query.month), cell_format)
#                         worksheet.write(row, 1, str(query.year), cell_format)
#                         worksheet.write(row, 2, str(query.valid_upto), cell_format)
#                         worksheet.write(row, 3, str(query.rake_alloted), cell_format)
#                         prev_date_obj = datetime.datetime.strptime(query.month, "%b-%Y")

#                         last_month = date_obj.month-1
#                         last_year = date_obj.year

#                         if last_month == 0:
#                             last_month = 12
#                             last_year -= 1

#                         last_month_date_obj = datetime.datetime(last_year, last_month, 1)

#                         # Convert back to the "%b-%Y" format
#                         last_month_str = last_month_date_obj.strftime("%b-%Y")
#                         last_month = datetime.datetime.strptime(last_month_str, "%b-%Y")

#                         prev_month_log = rakeQuota.objects.filter(
#                             month=f'{last_month.strftime("%b").upper()}-{last_month.strftime("%Y")}',
#                             year=last_month.strftime("%Y")
#                         ).first()
#                         # If there is a previous month log, add its rake_alloted to the current month's rake_alloted
#                         if prev_month_log:
#                             previous_month_rake = int(query.rake_alloted) + int(prev_month_log.rake_alloted)
#                             if previous_month_rake:
#                                 worksheet.write(row, 7, str(previous_month_rake), cell_format)
#                         else:
#                             worksheet.write(row, 8, 0, cell_format)
#                         total_rakes_received_for_month = 0
#                         if rail_logs:
#                             for rail_log in rail_logs:
#                                 source_type = rail_log.source_type
#                                 if source_type != "":
#                                     worksheet.write(row, 4, str(source_type), cell_format)
#                                 else:
#                                     worksheet.write(row, 4, "-", cell_format)
#                                 # Count the rakes loaded till the drawn_date for the specific rr_no
#                                 rakes_loaded_till_date = RailData.objects.filter(
#                                     # rr_no=rail_log.rr_no,
#                                     drawn_date__lte=f"{formatted_date}-30T23:59"
#                                 ).count()
#                                 if rakes_loaded_till_date:
#                                     worksheet.write(row, 5, str(rakes_loaded_till_date), cell_format)
#                                 else:
#                                     worksheet.write(row, 5, 0, cell_format)

#                                 rakes_loaded_on_date = RailData.objects.filter(
#                                     # rr_no=rail_log.rr_no,
#                                     drawn_date__gte=f"{datetime.datetime.today().strftime('%Y-%m-%d')}T00:00",
#                                     drawn_date__lte=f"{datetime.datetime.today().strftime('%Y-%m-%d')}T23:59"
#                                 ).count()
#                                 if rakes_loaded_on_date != 0:
#                                     worksheet.write(row, 6, str(rakes_loaded_on_date), cell_format)
#                                 else:
#                                     worksheet.write(row, 6, 0, cell_format)

#                                 worksheet.write(row, 8, 0, cell_format)
#                                 worksheet.write(row, 9, 0, cell_format)
#                                 worksheet.write(row, 10, 0, cell_format)
#                                 worksheet.write(row, 11, 0, cell_format)
#                                 # worksheet.write(row, 12, 0, cell_format)
#                                 worksheet.write(row, 12, rakes_loaded_till_date - total_rakes_received_for_month, cell_format)
#                         else:
#                             worksheet.write(row, 5, "-", cell_format)
#                             worksheet.write(row, 6, 0, cell_format)
#                             worksheet.write(row, 7, 0, cell_format)
#                             worksheet.write(row, 9, 0, cell_format)
#                             worksheet.write(row, 10, 0, cell_format)
#                             worksheet.write(row, 11, 0, cell_format)
#                             worksheet.write(row, 12, 0, cell_format)
#                             # worksheet.write(row, 13, 0, cell_format)
                                    
#                         count -= 1
                    
#                     workbook.close()
#                     console_logger.debug("Successfully {} report generated".format(service_id))
#                     console_logger.debug("sent data {}".format(path))

#                     return {
#                         "Type": "Rake_quota_download_event",
#                         "Datatype": "Report",
#                         "File_Path": path,
#                     }

#                 except Exception as e:
#                     console_logger.debug(e)
#                     exc_type, exc_obj, exc_tb = sys.exc_info()
#                     fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#                     console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#                     console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
                
#             else:
#                 console_logger.error("No data found")
#                 return {
#                     "Type": "Rake_quota_download_event",
#                     "Datatype": "Report",
#                     "File_Path": path,
#                 }

#     except Exception as e:
#         console_logger.debug("----- Fetch Rake Quota Error -----",e)
#         response.status_code = 400
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#         console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#         return e


def convert_to_date(month_str):
    return datetime.datetime.strptime(month_str, "%b-%Y")

@router.get("/fetch/rake/quota", tags=["Rail Map"])
def end_point_to_fetch_rake_quota_test_old(response: Response, 
                currentPage: Optional[int] = None,
                perPage: Optional[int] = None,
                # search_text: Optional[str] = None,
                # month_date: Optional[str] = None,
                start_timestamp: Optional[str] = None,
                end_timestamp: Optional[str] = None,
                type: Optional[str] = "display"):
    try:
        data = Q()
        result = {        
                "labels": [],
                "datasets": [],
                "total": 0,
                "page_size": 15
        }

        if type and type == "display":

            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage

            # if month_date:
            #     month_start = datetime.datetime.strptime(month_date, "%Y-%m").replace(day=4)
            #     month_end = (month_start + timedelta(days=31)).replace(day=3)
            #     month_end = month_end.replace(hour=23, minute=59, second=59)

            #     rail_logs = RailData.objects(
            #         placement_date__gte=month_start.strftime("%Y-%m-%dT%H:%M"),
            #         placement_date__lte=month_end.strftime("%Y-%m-%dT%H:%M"),
            #     )
            #     month_check = []
            #     placement_dates = []
                
            #     for log in rail_logs:
            #         if log.month:
            #             if len(log.month) == 7:
            #                 date = datetime.datetime.strptime(log.month, "%Y-%m").strftime("%Y-%m")
            #             elif len(log.month) == 10:
            #                 date = datetime.datetime.strptime(log.month, "%Y-%m-%d").strftime("%Y-%m")
            #             placement_dates.append(date)

            #         month_check.extend(list(set(placement_dates)))
            #     current_month = month_start.strftime("%m-%Y")
            #     month_check.append(current_month)
            #     data &= Q(month__in=month_check)

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M", "Asia/Kolkata", False)
                data &= Q(created_at__gte=start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M", "Asia/Kolkata", False)
                data &= Q(created_at__lte=end_date)

            offset = (page_no - 1) * page_len
            logs = (
                rakeQuota.objects(data)
                .order_by("year", "-month")
                .skip(offset)
                .limit(page_len)
            )
            listData = []
            if logs:
                for log in logs:
                    dictData = {"month": "", "source_type": "", "rakes_previous_month_quota_received": 0, 
                                "rake_planned_for_the_month": "","rakes_loaded_till_date": 0, 
                                "rakes_loaded_on_date": 0, "rakes_received_on_date": 0, 
                                "total_rakes_received_for_month": 0, "balance_rakes_to_receive": 0, 
                                "no_of_rakes_in_transist": 0, "expected_rakes_date": 0, 
                                "expected_rakes_value": 0}
                    rake_year = log.year
                    rake_month = log.month

                    month_year = f"{rake_year}-{rake_month[:2].upper()}"
                    date_obj = datetime.datetime.strptime(month_year, "%Y-%m")
                    prev_date_obj = date_obj - datetime.timedelta(days=1)
                    prev_month_year = prev_date_obj.strftime("%Y-%m")

                    formatted_date = date_obj.strftime("%Y-%m")
                    rail_logs = RailData.objects.filter(placement_date__icontains=formatted_date)
                    dictData["month"] = datetime.datetime.strptime(log.month, "%m-%Y").strftime("%b-%Y")
                    # dictData["valid_upto"] = log.valid_upto
                    dictData["rake_planned_for_the_month"] = int(log.rake_alloted)
                    if log.cancelled_rakes:
                        dictData["cancelled_rakes"] = int(log.cancelled_rakes)
                    else:
                        dictData["cancelled_rakes"] = 0
                    if log.remarks:
                        dictData["remarks"] = log.remarks
                    else:
                        dictData["remarks"] = ""
                    if log.source_type:
                        dictData["source_type"] = log.source_type
                    if log.expected_rakes:
                        dictData["expected_rakes_date"] = list(log.expected_rakes.keys())[0]
                        dictData["expected_rakes_value"] = list(log.expected_rakes.values())[0]
                    
                    currentDate = datetime.datetime.now()

                    # Get the first day of the current month
                    start_date_time = datetime.datetime(currentDate.year, currentDate.month, 1)

                    # Get the last day of the current month
                    _, last_day = calendar.monthrange(currentDate.year, currentDate.month)
                    end_date_time = datetime.datetime(currentDate.year, currentDate.month, last_day, 23, 59, 59)
                    # Ensure 'avery_placement_date' is stored as a string, then cast it to a datetime object for comparison
                    dictData["total_rakes_received_for_month"] = RailData.objects.filter(
                        Q(month__icontains=formatted_date) & 
                        Q(avery_placement_date__ne=None) &
                        Q(source_type__iexact = log.source_type)
                        # Q(avery_placement_date__gte=start_date_time.strftime("%Y-%m-%d %H:%M:%S")) &
                        # Q(avery_placement_date__lte=end_date_time.strftime("%Y-%m-%d %H:%M:%S"))
                    ).count()
                    if log.cancelled_rakes:
                        balance_rakes_to_receive = int(log.rake_alloted) - int(dictData["total_rakes_received_for_month"]) - int(log.cancelled_rakes)
                    else:
                        balance_rakes_to_receive = int(log.rake_alloted) - int(dictData["total_rakes_received_for_month"])
                    dictData["balance_rakes_to_receive"] = balance_rakes_to_receive

                    today_date = datetime.date.today()

                    # Get the start of the month
                    start_date = today_date.replace(day=1)

                    # Get the last day of the month
                    last_day = calendar.monthrange(today_date.year, today_date.month)[1]
                    end_date = today_date.replace(day=last_day)

                    startd_date = f"{start_date} 00:00:00"
                    endd_date = f"{end_date} 23:59:59"

                    current_date = datetime.datetime.now()
                    current_month_year = current_date.strftime('%B-%Y')  # Format: 'Month-Year' (e.g., 'November-2024')

                    # Check if formatted_date matches the current month and year
                    
                    dictData["rakes_previous_month_quota_received"] = RailData.objects.filter(
                        month__icontains=formatted_date,
                        avery_placement_date__gte=startd_date,
                        avery_placement_date__lte=endd_date,
                        source_type__iexact=log.source_type
                    ).count()

                    prev_date_obj = datetime.datetime.strptime(log.month, "%m-%Y")

                    last_month = date_obj.month-1
                    last_year = date_obj.year

                    if last_month == 0:
                        last_month = 12
                        last_year -= 1

                    last_month_date_obj = datetime.datetime(last_year, last_month, 1)

                    last_month_str = last_month_date_obj.strftime("%m-%Y")
                    last_month = datetime.datetime.strptime(last_month_str, "%m-%Y")

                    if rail_logs:
                        for rail_log in rail_logs:
                            # if rail_log.source_type != "":
                            #     dictData["source_type"] = rail_log.source_type
                            
                            today_utc = datetime.datetime.utcnow().replace(hour=0, minute=0, second=0)
                            end_of_day_utc = today_utc + timedelta(hours=23, minutes=59, seconds=59)

                            dictData["rakes_loaded_till_date"] = RailData.objects.filter(
                                month__icontains=formatted_date,
                                source_type__iexact = log.source_type
                            ).count()

                            dictData["rakes_received_on_date"] = RailData.objects.filter(
                                month__icontains=formatted_date,
                                avery_placement_date__gte=today_utc.strftime("%Y-%m-%d %H:%M:%S"),
                                avery_placement_date__lte=end_of_day_utc.strftime("%Y-%m-%d %H:%M:%S"),
                                source_type__iexact = log.source_type
                            ).count()

                            dictData["rakes_loaded_on_date"] = RailData.objects.filter(
                                drawn_date__gte=today_utc,
                                drawn_date__lte=end_of_day_utc,
                                month__icontains=formatted_date,
                                source_type__iexact = log.source_type
                            ).count()

                            dictData["no_of_rakes_in_transist"] = dictData["rakes_loaded_till_date"] - dictData["total_rakes_received_for_month"]

                    listData.append(dictData)
                # for i, current_month_data in enumerate(listData):
                #     for j in range(i + 1, len(listData)):
                #         next_month_data = listData[j]
                #         if next_month_data["month"] > current_month_data["month"]:
                #             next_month_data["rakes_previous_month_quota_received"] = current_month_data["balance_rakes_to_receive"]
                #             break

                listData.sort(key=lambda x: convert_to_date(x['month']))

                # Filter out entries where balance_rakes_to_receive is 0 or less
                filtered_data = [entry for entry in listData if entry['balance_rakes_to_receive'] > 0]

                # console_logger.debug(listData)

                # Update rakes_previous_month_quota_received with balance_rakes_to_receive from previous month
                # for i in range(1, len(filtered_data)):
                #     # console_logger.debug(i)
                #     # console_logger.debug(i-1)
                #     console_logger.debug(filtered_data[i - 1]['balance_rakes_to_receive'])
                #     console_logger.debug(filtered_data[i]['rakes_previous_month_quota_received'])
                #     filtered_data[i]['rakes_previous_month_quota_received'] = filtered_data[i - 1]['balance_rakes_to_receive']

                # console_logger.debug(listData)
                filtered_data.reverse()

                current_date = datetime.datetime.now()
                current_month_year = current_date.strftime('%b-%Y')

                for check_entry in filtered_data:
                    if check_entry['month'] == current_month_year:
                        check_entry['rakes_previous_month_quota_received'] = 0

                dataList = {}

                dataList['data'] = filtered_data

                # Initialize a dictionary to store the totals
                totals = {
                    'rakes_previous_month_quota_received': 0,
                    'rake_planned_for_the_month': 0,
                    'rakes_loaded_till_date': 0,
                    'rakes_loaded_on_date': 0,
                    'rakes_received_on_date': 0,
                    'total_rakes_received_for_month': 0,
                    'balance_rakes_to_receive': 0,
                    'no_of_rakes_in_transist': 0,
                    'expected_rakes_date': 0,
                    'expected_rakes_value': 0,
                }

                # Loop through each record in data and accumulate totals
                for record in filtered_data:
                    totals['rakes_previous_month_quota_received'] += record['rakes_previous_month_quota_received']
                    totals['rake_planned_for_the_month'] += int(record['rake_planned_for_the_month'])
                    totals['rakes_loaded_till_date'] += record['rakes_loaded_till_date']
                    totals['rakes_loaded_on_date'] += record['rakes_loaded_on_date']
                    totals['rakes_received_on_date'] += record['rakes_received_on_date']
                    totals['total_rakes_received_for_month'] += record['total_rakes_received_for_month']
                    totals['balance_rakes_to_receive'] += record['balance_rakes_to_receive']
                    totals['no_of_rakes_in_transist'] += record['no_of_rakes_in_transist']
                    totals['expected_rakes_date'] += record['expected_rakes_date']
                    totals['expected_rakes_value'] += record['expected_rakes_value']

                # Add the totals to dataList
                dataList['rake_total'] = totals

                result["labels"] = list(dictData.keys())
                result["datasets"] = dataList
                result["total"] = len(rakeQuota.objects(data))
            return result

        elif type == "download":
            file = datetime.datetime.now().strftime("%d-%m-%Y")
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            headers = ["month", "source_type", "rakes_previous_month_quota_received", "rake_planned_for_the_month",
                    "rakes_loaded_till_date", "rakes_loaded_on_date", "rakes_received_on_date", "total_rakes_received_for_month",
                    "balance_rakes_to_receive", "no_of_rakes_in_transist", "expected_rakes_date", "expected_rakes_value"]
            # if month_date:
            #     month_start = datetime.datetime.strptime(month_date, "%Y-%m").replace(day=4)
            #     month_end = (month_start + timedelta(days=31)).replace(day=3)
            #     month_end = month_end.replace(hour=23, minute=59, second=59)

            #     rail_logs = RailData.objects(
            #         placement_date__gte=month_start.strftime("%Y-%m-%dT%H:%M"),
            #         placement_date__lte=month_end.strftime("%Y-%m-%dT%H:%M"),
            #     )
            #     month_check = []
            #     placement_dates = []
                
            #     for log in rail_logs:
            #         if log.month:
            #             if len(log.month) == 7:
            #                 date = datetime.datetime.strptime(log.month, "%Y-%m").strftime("%Y-%m")
            #             elif len(log.month) == 10:
            #                 date = datetime.datetime.strptime(log.month, "%Y-%m-%d").strftime("%Y-%m")
            #         placement_dates.append(date)
            #         month_check.extend(list(set(placement_dates)))
            #     current_month = month_start.strftime("%m-%Y")
            #     month_check.append(current_month)
            #     data &= Q(month__in=month_check)

            usecase_data = rakeQuota.objects.filter(data).order_by("year", "-month")
            count = len(usecase_data)
            path = None

            if usecase_data:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        f"Rake_Quota_{datetime.datetime.now().strftime('%Y-%m-%d:%H:%M:%S')}.xlsx",
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format({'bold': True, 'font_size': 10, 'align': 'center', 'valign': 'vcenter'})
                    cell_format = workbook.add_format({'font_size': 10, 'align': 'center', 'valign': 'vcenter'})

                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)

                    for index, header in enumerate(headers):
                        worksheet.write(0, index, header, cell_format2)

                    listData = []
                    for row, query in enumerate(usecase_data, start=1):
                        dictData = {
                            "month": "", "source_type": "", "rakes_previous_month_quota_received": 0, 
                            "rake_planned_for_the_month": "", "rakes_loaded_till_date": 0, "rakes_loaded_on_date": 0, 
                            "rakes_received_on_date": 0, "total_rakes_received_for_month": 0, 
                            "balance_rakes_to_receive": 0, "no_of_rakes_in_transist": 0, "expected_rakes_date": 0, "expected_rakes_value": 0
                        }
                        
                        month_year = f"{query.year}-{query.month[:2].upper()}"
                        date_obj = datetime.datetime.strptime(month_year, "%Y-%m")
                        formatted_date = date_obj.strftime("%Y-%m")
                        rail_logs = RailData.objects.filter(drawn_date__icontains=formatted_date)

                        dictData["month"] = month_year
                        dictData["rake_planned_for_the_month"] = query.rake_alloted
                        if query.expected_rakes:
                            dictData["expected_rakes_date"] = list(query.expected_rakes.keys())[0]
                            dictData["expected_rakes_value"] = list(query.expected_rakes.values())[0]
                        if rail_logs:
                            for rail_log in rail_logs:
                                if rail_log.source_type != "":
                                    dictData["source_type"] = rail_log.source_type
                                rakes_loaded_till_date = RailData.objects.filter(
                                    month__icontains=formatted_date,
                                    source_type__iexact = log.source_type
                                ).count()
                                dictData["rakes_loaded_till_date"] = rakes_loaded_till_date
                                rakes_loaded_on_date = RailData.objects.filter(
                                    drawn_date__gte=f"{datetime.datetime.today().strftime('%Y-%m-%d')}T00:00",
                                    drawn_date__lte=f"{datetime.datetime.today().strftime('%Y-%m-%d')}T23:59",
                                    month__icontains=formatted_date,
                                    source_type__iexact = log.source_type
                                ).count()
                                dictData["rakes_loaded_on_date"] = rakes_loaded_on_date
                                dictData["no_of_rakes_in_transist"] = rakes_loaded_till_date - dictData["total_rakes_received_for_month"]
                    
                        balance_rakes_to_receive = int(query.rake_alloted) - dictData["total_rakes_received_for_month"]
                        dictData["balance_rakes_to_receive"] = balance_rakes_to_receive

                        prev_month_log = rakeQuota.objects.filter(
                            month=(date_obj.replace(day=1) - datetime.timedelta(days=1)).strftime("%b-%Y").upper()
                        ).first()

                        rakes_previous_month_quota_received = RailData.objects.filter(
                            month__icontains=formatted_date,
                            avery_placement_date__gte=startd_date,
                            avery_placement_date__lte=endd_date,
                            source_type__iexact=log.source_type
                        ).count()
                        dictData["rakes_previous_month_quota_received"] = rakes_previous_month_quota_received

                        current_date = datetime.datetime.now()
                        current_month_year = current_date.strftime('%b-%Y')

                        for check_entry in dictData:
                            if check_entry['month'] == current_month_year:
                                check_entry['rakes_previous_month_quota_received'] = 0

                        # if prev_month_log:
                        #     dictData["rakes_previous_month_quota_received"] = int(prev_month_log.rake_alloted)

                        worksheet.write(row, 0, dictData["month"], cell_format)
                        worksheet.write(row, 1, dictData["source_type"], cell_format)
                        worksheet.write(row, 2, dictData["rakes_previous_month_quota_received"], cell_format)
                        worksheet.write(row, 3, dictData["rake_planned_for_the_month"], cell_format)
                        worksheet.write(row, 4, dictData["rakes_loaded_till_date"], cell_format)
                        worksheet.write(row, 5, dictData["rakes_loaded_on_date"], cell_format)
                        worksheet.write(row, 6, dictData["rakes_received_on_date"], cell_format)
                        worksheet.write(row, 7, dictData["total_rakes_received_for_month"], cell_format)
                        worksheet.write(row, 8, dictData["balance_rakes_to_receive"], cell_format)
                        worksheet.write(row, 9, dictData["no_of_rakes_in_transist"], cell_format)
                        worksheet.write(row, 10, dictData["expected_rakes_date"], cell_format)
                        worksheet.write(row, 11, dictData["expected_rakes_value"], cell_format)

                        listData.append(dictData)

                    workbook.close()
                    console_logger.debug(f"Successfully {service_id} report generated")
                    console_logger.debug(f"Sent data {path}")

                    return {
                        "Type": "Rake_quota_download_event",
                        "Datatype": "Report",
                        "File_Path": path,
                    }

                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
                
            else:
                console_logger.error("No data found")
                return {
                    "Type": "Rake_quota_download_event",
                    "Datatype": "Report",
                    "File_Path": path,
                }

    except Exception as e:
        console_logger.debug("----- Fetch Rake Quota Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/fetch/rake/quota_completed", tags=["Rail Map"])
def end_point_to_fetch_rake_quota_completed(response: Response, 
                currentPage: Optional[int] = None,
                perPage: Optional[int] = None,
                # search_text: Optional[str] = None,
                # month_date: Optional[str] = None,
                start_timestamp: Optional[str] = None,
                end_timestamp: Optional[str] = None,
                type: Optional[str] = "display"):
    try:
        data = Q()
        result = {        
                "labels": [],
                "datasets": [],
                "total": 0,
                "page_size": 15
        }

        if type and type == "display":

            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage

            # if month_date:
            #     month_start = datetime.datetime.strptime(month_date, "%Y-%m").replace(day=4)
            #     month_end = (month_start + timedelta(days=31)).replace(day=3)
            #     month_end = month_end.replace(hour=23, minute=59, second=59)

            #     rail_logs = RailData.objects(
            #         placement_date__gte=month_start.strftime("%Y-%m-%dT%H:%M"),
            #         placement_date__lte=month_end.strftime("%Y-%m-%dT%H:%M"),
            #     )
            #     month_check = []
            #     placement_dates = []
                
            #     for log in rail_logs:
            #         if log.month:
            #             if len(log.month) == 7:
            #                 date = datetime.datetime.strptime(log.month, "%Y-%m").strftime("%Y-%m")
            #             elif len(log.month) == 10:
            #                 date = datetime.datetime.strptime(log.month, "%Y-%m-%d").strftime("%Y-%m")
            #             placement_dates.append(date)

            #         month_check.extend(list(set(placement_dates)))
            #     current_month = month_start.strftime("%m-%Y")
            #     month_check.append(current_month)
            #     data &= Q(month__in=month_check)

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M", "Asia/Kolkata", False)
                data &= Q(created_at__gte=start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M", "Asia/Kolkata", False)
                data &= Q(created_at__lte=end_date)

            offset = (page_no - 1) * page_len
            logs = (
                rakeQuota.objects(data)
                .order_by("year", "-month")
                .skip(offset)
                .limit(page_len)
            )
            listData = []
            if logs:
                for log in logs:
                    dictData = {"month": "", "source_type": "", "rakes_previous_month_quota_received": 0, 
                                "rake_planned_for_the_month": "","rakes_loaded_till_date": 0, 
                                "rakes_loaded_on_date": 0, "rakes_received_on_date": 0, 
                                "total_rakes_received_for_month": 0, "balance_rakes_to_receive": 0, 
                                "no_of_rakes_in_transist": 0, "expected_rakes_date": 0, 
                                "expected_rakes_value": 0}
                    rake_year = log.year
                    rake_month = log.month

                    month_year = f"{rake_year}-{rake_month[:2].upper()}"
                    date_obj = datetime.datetime.strptime(month_year, "%Y-%m")
                    prev_date_obj = date_obj - datetime.timedelta(days=1)
                    prev_month_year = prev_date_obj.strftime("%Y-%m")

                    formatted_date = date_obj.strftime("%Y-%m")
                    rail_logs = RailData.objects.filter(placement_date__icontains=formatted_date)
                    dictData["month"] = datetime.datetime.strptime(log.month, "%m-%Y").strftime("%b-%Y")
                    # dictData["valid_upto"] = log.valid_upto
                    dictData["rake_planned_for_the_month"] = int(log.rake_alloted)
                    if log.cancelled_rakes:
                        dictData["cancelled_rakes"] = int(log.cancelled_rakes)
                    else:
                        dictData["cancelled_rakes"] = 0
                    if log.remarks:
                        dictData["remarks"] = log.remarks
                    else:
                        dictData["remarks"] = ""
                    if log.source_type:
                        dictData["source_type"] = log.source_type
                    if log.expected_rakes:
                        dictData["expected_rakes_date"] = list(log.expected_rakes.keys())[0]
                        dictData["expected_rakes_value"] = list(log.expected_rakes.values())[0]
                    
                    currentDate = datetime.datetime.now()

                    # Get the first day of the current month
                    start_date_time = datetime.datetime(currentDate.year, currentDate.month, 1)

                    # Get the last day of the current month
                    _, last_day = calendar.monthrange(currentDate.year, currentDate.month)
                    end_date_time = datetime.datetime(currentDate.year, currentDate.month, last_day, 23, 59, 59)
                    # Ensure 'avery_placement_date' is stored as a string, then cast it to a datetime object for comparison
                    dictData["total_rakes_received_for_month"] = RailData.objects.filter(
                        Q(month__icontains=formatted_date) & 
                        Q(avery_placement_date__ne=None) &
                        Q(source_type__iexact = log.source_type)
                        # Q(avery_placement_date__gte=start_date_time.strftime("%Y-%m-%d %H:%M:%S")) &
                        # Q(avery_placement_date__lte=end_date_time.strftime("%Y-%m-%d %H:%M:%S"))
                    ).count()
                    if log.cancelled_rakes:
                        balance_rakes_to_receive = int(log.rake_alloted) - int(dictData["total_rakes_received_for_month"]) - int(log.cancelled_rakes)
                    else:
                        balance_rakes_to_receive = int(log.rake_alloted) - int(dictData["total_rakes_received_for_month"])
                    dictData["balance_rakes_to_receive"] = balance_rakes_to_receive

                    today_date = datetime.date.today()

                    # Get the start of the month
                    start_date = today_date.replace(day=1)

                    # Get the last day of the month
                    last_day = calendar.monthrange(today_date.year, today_date.month)[1]
                    end_date = today_date.replace(day=last_day)

                    startd_date = f"{start_date} 00:00:00"
                    endd_date = f"{end_date} 23:59:59"

                    current_date = datetime.datetime.now()
                    current_month_year = current_date.strftime('%B-%Y')  # Format: 'Month-Year' (e.g., 'November-2024')

                    # Check if formatted_date matches the current month and year
                    
                    dictData["rakes_previous_month_quota_received"] = RailData.objects.filter(
                        month__icontains=formatted_date,
                        avery_placement_date__gte=startd_date,
                        avery_placement_date__lte=endd_date,
                        source_type__iexact=log.source_type
                    ).count()

                    prev_date_obj = datetime.datetime.strptime(log.month, "%m-%Y")

                    last_month = date_obj.month-1
                    last_year = date_obj.year

                    if last_month == 0:
                        last_month = 12
                        last_year -= 1

                    last_month_date_obj = datetime.datetime(last_year, last_month, 1)

                    last_month_str = last_month_date_obj.strftime("%m-%Y")
                    last_month = datetime.datetime.strptime(last_month_str, "%m-%Y")

                    if rail_logs:
                        for rail_log in rail_logs:
                            # if rail_log.source_type != "":
                            #     dictData["source_type"] = rail_log.source_type
                            
                            today_utc = datetime.datetime.utcnow().replace(hour=0, minute=0, second=0)
                            end_of_day_utc = today_utc + timedelta(hours=23, minutes=59, seconds=59)

                            dictData["rakes_loaded_till_date"] = RailData.objects.filter(
                                month__icontains=formatted_date,
                                source_type__iexact = log.source_type
                            ).count()

                            dictData["rakes_received_on_date"] = RailData.objects.filter(
                                month__icontains=formatted_date,
                                avery_placement_date__gte=today_utc.strftime("%Y-%m-%d %H:%M:%S"),
                                avery_placement_date__lte=end_of_day_utc.strftime("%Y-%m-%d %H:%M:%S"),
                                source_type__iexact = log.source_type
                            ).count()

                            dictData["rakes_loaded_on_date"] = RailData.objects.filter(
                                drawn_date__gte=today_utc,
                                drawn_date__lte=end_of_day_utc,
                                month__icontains=formatted_date,
                                source_type__iexact = log.source_type
                            ).count()

                            dictData["no_of_rakes_in_transist"] = dictData["rakes_loaded_till_date"] - dictData["total_rakes_received_for_month"]

                    listData.append(dictData)
                # for i, current_month_data in enumerate(listData):
                #     for j in range(i + 1, len(listData)):
                #         next_month_data = listData[j]
                #         if next_month_data["month"] > current_month_data["month"]:
                #             next_month_data["rakes_previous_month_quota_received"] = current_month_data["balance_rakes_to_receive"]
                #             break

                listData.sort(key=lambda x: convert_to_date(x['month']))

                # Filter out entries where balance_rakes_to_receive is 0 or less
                # filtered_data = [entry for entry in listData if entry['balance_rakes_to_receive'] > 0]
                filtered_data = listData

                # console_logger.debug(listData)

                # Update rakes_previous_month_quota_received with balance_rakes_to_receive from previous month
                # for i in range(1, len(filtered_data)):
                #     # console_logger.debug(i)
                #     # console_logger.debug(i-1)
                #     console_logger.debug(filtered_data[i - 1]['balance_rakes_to_receive'])
                #     console_logger.debug(filtered_data[i]['rakes_previous_month_quota_received'])
                #     filtered_data[i]['rakes_previous_month_quota_received'] = filtered_data[i - 1]['balance_rakes_to_receive']

                # console_logger.debug(listData)
                filtered_data.reverse()

                current_date = datetime.datetime.now()
                current_month_year = current_date.strftime('%b-%Y')

                for check_entry in filtered_data:
                    if check_entry['month'] == current_month_year:
                        check_entry['rakes_previous_month_quota_received'] = 0

                dataList = {}

                dataList['data'] = filtered_data

                # Initialize a dictionary to store the totals
                totals = {
                    'rakes_previous_month_quota_received': 0,
                    'rake_planned_for_the_month': 0,
                    'rakes_loaded_till_date': 0,
                    'rakes_loaded_on_date': 0,
                    'rakes_received_on_date': 0,
                    'total_rakes_received_for_month': 0,
                    'balance_rakes_to_receive': 0,
                    'no_of_rakes_in_transist': 0,
                    'expected_rakes_date': 0,
                    'expected_rakes_value': 0,
                }

                # Loop through each record in data and accumulate totals
                for record in listData:
                    totals['rakes_previous_month_quota_received'] += record['rakes_previous_month_quota_received']
                    totals['rake_planned_for_the_month'] += int(record['rake_planned_for_the_month'])
                    totals['rakes_loaded_till_date'] += record['rakes_loaded_till_date']
                    totals['rakes_loaded_on_date'] += record['rakes_loaded_on_date']
                    totals['rakes_received_on_date'] += record['rakes_received_on_date']
                    totals['total_rakes_received_for_month'] += record['total_rakes_received_for_month']
                    totals['balance_rakes_to_receive'] += record['balance_rakes_to_receive']
                    totals['no_of_rakes_in_transist'] += record['no_of_rakes_in_transist']
                    totals['expected_rakes_date'] += record['expected_rakes_date']
                    totals['expected_rakes_value'] += record['expected_rakes_value']

                # Add the totals to dataList
                dataList['rake_total'] = totals

                result["labels"] = list(dictData.keys())
                result["datasets"] = dataList
                result["total"] = len(rakeQuota.objects(data))
            return result

        elif type == "download":
            file = datetime.datetime.now().strftime("%d-%m-%Y")
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            headers = ["month", "source_type", "rakes_previous_month_quota_received", "rake_planned_for_the_month",
                    "rakes_loaded_till_date", "rakes_loaded_on_date", "rakes_received_on_date", "total_rakes_received_for_month",
                    "balance_rakes_to_receive", "no_of_rakes_in_transist", "expected_rakes_date", "expected_rakes_value"]
            # if month_date:
            #     month_start = datetime.datetime.strptime(month_date, "%Y-%m").replace(day=4)
            #     month_end = (month_start + timedelta(days=31)).replace(day=3)
            #     month_end = month_end.replace(hour=23, minute=59, second=59)

            #     rail_logs = RailData.objects(
            #         placement_date__gte=month_start.strftime("%Y-%m-%dT%H:%M"),
            #         placement_date__lte=month_end.strftime("%Y-%m-%dT%H:%M"),
            #     )
            #     month_check = []
            #     placement_dates = []
                
            #     for log in rail_logs:
            #         if log.month:
            #             if len(log.month) == 7:
            #                 date = datetime.datetime.strptime(log.month, "%Y-%m").strftime("%Y-%m")
            #             elif len(log.month) == 10:
            #                 date = datetime.datetime.strptime(log.month, "%Y-%m-%d").strftime("%Y-%m")
            #         placement_dates.append(date)
            #         month_check.extend(list(set(placement_dates)))
            #     current_month = month_start.strftime("%m-%Y")
            #     month_check.append(current_month)
            #     data &= Q(month__in=month_check)

            usecase_data = rakeQuota.objects.filter(data).order_by("year", "-month")
            count = len(usecase_data)
            path = None

            if usecase_data:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        f"Rake_Quota_{datetime.datetime.now().strftime('%Y-%m-%d:%H:%M:%S')}.xlsx",
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format({'bold': True, 'font_size': 10, 'align': 'center', 'valign': 'vcenter'})
                    cell_format = workbook.add_format({'font_size': 10, 'align': 'center', 'valign': 'vcenter'})

                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)

                    for index, header in enumerate(headers):
                        worksheet.write(0, index, header, cell_format2)

                    listData = []
                    for row, query in enumerate(usecase_data, start=1):
                        dictData = {
                            "month": "", "source_type": "", "rakes_previous_month_quota_received": 0, 
                            "rake_planned_for_the_month": "", "rakes_loaded_till_date": 0, "rakes_loaded_on_date": 0, 
                            "rakes_received_on_date": 0, "total_rakes_received_for_month": 0, 
                            "balance_rakes_to_receive": 0, "no_of_rakes_in_transist": 0, "expected_rakes_date": 0, "expected_rakes_value": 0
                        }
                        
                        month_year = f"{query.year}-{query.month[:2].upper()}"
                        date_obj = datetime.datetime.strptime(month_year, "%Y-%m")
                        formatted_date = date_obj.strftime("%Y-%m")
                        rail_logs = RailData.objects.filter(drawn_date__icontains=formatted_date)

                        dictData["month"] = month_year
                        dictData["rake_planned_for_the_month"] = query.rake_alloted
                        if query.expected_rakes:
                            dictData["expected_rakes_date"] = list(query.expected_rakes.keys())[0]
                            dictData["expected_rakes_value"] = list(query.expected_rakes.values())[0]
                        if rail_logs:
                            for rail_log in rail_logs:
                                if rail_log.source_type != "":
                                    dictData["source_type"] = rail_log.source_type
                                rakes_loaded_till_date = RailData.objects.filter(
                                    month__icontains=formatted_date,
                                    source_type__iexact = log.source_type
                                ).count()
                                dictData["rakes_loaded_till_date"] = rakes_loaded_till_date
                                rakes_loaded_on_date = RailData.objects.filter(
                                    drawn_date__gte=f"{datetime.datetime.today().strftime('%Y-%m-%d')}T00:00",
                                    drawn_date__lte=f"{datetime.datetime.today().strftime('%Y-%m-%d')}T23:59",
                                    month__icontains=formatted_date,
                                    source_type__iexact = log.source_type
                                ).count()
                                dictData["rakes_loaded_on_date"] = rakes_loaded_on_date
                                dictData["no_of_rakes_in_transist"] = rakes_loaded_till_date - dictData["total_rakes_received_for_month"]
                    
                        balance_rakes_to_receive = int(query.rake_alloted) - dictData["total_rakes_received_for_month"]
                        dictData["balance_rakes_to_receive"] = balance_rakes_to_receive

                        prev_month_log = rakeQuota.objects.filter(
                            month=(date_obj.replace(day=1) - datetime.timedelta(days=1)).strftime("%b-%Y").upper()
                        ).first()

                        rakes_previous_month_quota_received = RailData.objects.filter(
                            month__icontains=formatted_date,
                            avery_placement_date__gte=startd_date,
                            avery_placement_date__lte=endd_date,
                            source_type__iexact=log.source_type
                        ).count()
                        dictData["rakes_previous_month_quota_received"] = rakes_previous_month_quota_received

                        current_date = datetime.datetime.now()
                        current_month_year = current_date.strftime('%b-%Y')

                        for check_entry in dictData:
                            if check_entry['month'] == current_month_year:
                                check_entry['rakes_previous_month_quota_received'] = 0

                        # if prev_month_log:
                        #     dictData["rakes_previous_month_quota_received"] = int(prev_month_log.rake_alloted)

                        worksheet.write(row, 0, dictData["month"], cell_format)
                        worksheet.write(row, 1, dictData["source_type"], cell_format)
                        worksheet.write(row, 2, dictData["rakes_previous_month_quota_received"], cell_format)
                        worksheet.write(row, 3, dictData["rake_planned_for_the_month"], cell_format)
                        worksheet.write(row, 4, dictData["rakes_loaded_till_date"], cell_format)
                        worksheet.write(row, 5, dictData["rakes_loaded_on_date"], cell_format)
                        worksheet.write(row, 6, dictData["rakes_received_on_date"], cell_format)
                        worksheet.write(row, 7, dictData["total_rakes_received_for_month"], cell_format)
                        worksheet.write(row, 8, dictData["balance_rakes_to_receive"], cell_format)
                        worksheet.write(row, 9, dictData["no_of_rakes_in_transist"], cell_format)
                        worksheet.write(row, 10, dictData["expected_rakes_date"], cell_format)
                        worksheet.write(row, 11, dictData["expected_rakes_value"], cell_format)

                        listData.append(dictData)

                    workbook.close()
                    console_logger.debug(f"Successfully {service_id} report generated")
                    console_logger.debug(f"Sent data {path}")

                    return {
                        "Type": "Rake_quota_download_event",
                        "Datatype": "Report",
                        "File_Path": path,
                    }

                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
                
            else:
                console_logger.error("No data found")
                return {
                    "Type": "Rake_quota_download_event",
                    "Datatype": "Report",
                    "File_Path": path,
                }

    except Exception as e:
        console_logger.debug("----- Fetch Rake Quota Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/fetch/rcrrake/quota", tags=["Rail Map"])
def end_point_to_fetch_rcr_rake_quota(response: Response, 
                currentPage: Optional[int] = None,
                perPage: Optional[int] = None,
                # search_text: Optional[str] = None,
                # month_date: Optional[str] = None,
                start_timestamp: Optional[str] = None,
                end_timestamp: Optional[str] = None,
                type: Optional[str] = "display"):
    try:
        data = Q()
        result = {        
                "labels": [],
                "datasets": [],
                "total": 0,
                "page_size": 15
        }

        if type and type == "display":

            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage

            # if month_date:
            #     month_start = datetime.datetime.strptime(month_date, "%Y-%m").replace(day=4)
            #     month_end = (month_start + timedelta(days=31)).replace(day=3)
            #     month_end = month_end.replace(hour=23, minute=59, second=59)

            #     rail_logs = RailData.objects(
            #         placement_date__gte=month_start.strftime("%Y-%m-%dT%H:%M"),
            #         placement_date__lte=month_end.strftime("%Y-%m-%dT%H:%M"),
            #     )
            #     month_check = []
            #     placement_dates = []
                
            #     for log in rail_logs:
            #         if log.month:
            #             if len(log.month) == 7:
            #                 date = datetime.datetime.strptime(log.month, "%Y-%m").strftime("%Y-%m")
            #             elif len(log.month) == 10:
            #                 date = datetime.datetime.strptime(log.month, "%Y-%m-%d").strftime("%Y-%m")
            #             placement_dates.append(date)

            #         month_check.extend(list(set(placement_dates)))
            #     current_month = month_start.strftime("%m-%Y")
            #     month_check.append(current_month)
            #     data &= Q(month__in=month_check)

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M", "Asia/Kolkata", False)
                data &= Q(created_at__gte=start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M", "Asia/Kolkata", False)
                data &= Q(created_at__lte=end_date)

            offset = (page_no - 1) * page_len
            logs = (
                rcrrakeQuota.objects(data)
                .order_by("year", "-month")
                .skip(offset)
                .limit(page_len)
            )
            listData = []
            if logs:
                for log in logs:
                    dictData = {"month": "", "source_type": "", "rakes_previous_month_quota_received": 0, 
                                "rake_planned_for_the_month": "","rakes_loaded_till_date": 0, 
                                "rakes_loaded_on_date": 0, "rakes_received_on_date": 0, 
                                "total_rakes_received_for_month": 0, "balance_rakes_to_receive": 0, 
                                "no_of_rakes_in_transist": 0, "expected_rakes_date": 0, 
                                "expected_rakes_value": 0}
                    rake_year = log.year
                    rake_month = log.month

                    month_year = f"{rake_year}-{rake_month[:2].upper()}"
                    date_obj = datetime.datetime.strptime(month_year, "%Y-%m")
                    prev_date_obj = date_obj - datetime.timedelta(days=1)
                    prev_month_year = prev_date_obj.strftime("%Y-%m")

                    formatted_date = date_obj.strftime("%Y-%m")
                    rail_logs = RcrData.objects.filter(placement_date__icontains=formatted_date)
                    dictData["month"] = datetime.datetime.strptime(log.month, "%m-%Y").strftime("%b-%Y")
                    # dictData["valid_upto"] = log.valid_upto
                    dictData["rake_planned_for_the_month"] = int(log.rake_alloted)
                    if log.cancelled_rakes:
                        dictData["cancelled_rakes"] = int(log.cancelled_rakes)
                    else:
                        dictData["cancelled_rakes"] = 0
                    if log.remarks:
                        dictData["remarks"] = log.remarks
                    else:
                        dictData["remarks"] = ""
                    if log.source_type:
                        dictData["source_type"] = log.source_type
                    if log.expected_rakes:
                        dictData["expected_rakes_date"] = list(log.expected_rakes.keys())[0]
                        dictData["expected_rakes_value"] = list(log.expected_rakes.values())[0]
                    
                    currentDate = datetime.datetime.now()

                    # Get the first day of the current month
                    start_date_time = datetime.datetime(currentDate.year, currentDate.month, 1)

                    # Get the last day of the current month
                    _, last_day = calendar.monthrange(currentDate.year, currentDate.month)
                    end_date_time = datetime.datetime(currentDate.year, currentDate.month, last_day, 23, 59, 59)
                    # Ensure 'avery_placement_date' is stored as a string, then cast it to a datetime object for comparison
                    dictData["total_rakes_received_for_month"] = RcrData.objects.filter(
                        Q(month__icontains=formatted_date) & 
                        Q(avery_placement_date__ne=None) &
                        Q(source_type__iexact = log.source_type)
                        # Q(avery_placement_date__gte=start_date_time.strftime("%Y-%m-%d %H:%M:%S")) &
                        # Q(avery_placement_date__lte=end_date_time.strftime("%Y-%m-%d %H:%M:%S"))
                    ).count()

                    if log.cancelled_rakes:
                        balance_rakes_to_receive = int(log.rake_alloted) - int(dictData["total_rakes_received_for_month"]) - int(log.cancelled_rakes)
                    else:
                        balance_rakes_to_receive = int(log.rake_alloted) - int(dictData["total_rakes_received_for_month"])
                    dictData["balance_rakes_to_receive"] = balance_rakes_to_receive

                    today_date = datetime.date.today()

                    # Get the start of the month
                    start_date = today_date.replace(day=1)

                    # Get the last day of the month
                    last_day = calendar.monthrange(today_date.year, today_date.month)[1]
                    end_date = today_date.replace(day=last_day)

                    startd_date = f"{start_date} 00:00:00"
                    endd_date = f"{end_date} 23:59:59"

                    current_date = datetime.datetime.now()
                    current_month_year = current_date.strftime('%B-%Y')  # Format: 'Month-Year' (e.g., 'November-2024')

                    # Check if formatted_date matches the current month and year
                    
                    dictData["rakes_previous_month_quota_received"] = RailData.objects.filter(
                        month__icontains=formatted_date,
                        avery_placement_date__gte=startd_date,
                        avery_placement_date__lte=endd_date,
                        source_type__iexact=log.source_type
                    ).count()

                    prev_date_obj = datetime.datetime.strptime(log.month, "%m-%Y")

                    last_month = date_obj.month-1
                    last_year = date_obj.year

                    if last_month == 0:
                        last_month = 12
                        last_year -= 1

                    last_month_date_obj = datetime.datetime(last_year, last_month, 1)

                    last_month_str = last_month_date_obj.strftime("%m-%Y")
                    last_month = datetime.datetime.strptime(last_month_str, "%m-%Y")

                    if rail_logs:
                        for rail_log in rail_logs:
                            # if rail_log.source_type != "":
                            #     dictData["source_type"] = rail_log.source_type
                            
                            today_utc = datetime.datetime.utcnow().replace(hour=0, minute=0, second=0)
                            end_of_day_utc = today_utc + timedelta(hours=23, minutes=59, seconds=59)

                            dictData["rakes_loaded_till_date"] = RcrData.objects.filter(
                                month__icontains=formatted_date,
                                source_type__iexact = log.source_type
                            ).count()

                            dictData["rakes_received_on_date"] = RcrData.objects.filter(
                                month__icontains=formatted_date,
                                avery_placement_date__gte=today_utc.strftime("%Y-%m-%d %H:%M:%S"),
                                avery_placement_date__lte=end_of_day_utc.strftime("%Y-%m-%d %H:%M:%S"),
                                source_type__iexact = log.source_type
                            ).count()

                            dictData["rakes_loaded_on_date"] = RcrData.objects.filter(
                                drawn_date__gte=today_utc,
                                drawn_date__lte=end_of_day_utc,
                                month__icontains=formatted_date,
                                source_type__iexact = log.source_type
                            ).count()
                            dictData["no_of_rakes_in_transist"] = dictData["rakes_loaded_till_date"] - dictData["total_rakes_received_for_month"]

                    listData.append(dictData)
                # for i, current_month_data in enumerate(listData):
                #     for j in range(i + 1, len(listData)):
                #         next_month_data = listData[j]
                #         if next_month_data["month"] > current_month_data["month"]:
                #             next_month_data["rakes_previous_month_quota_received"] = current_month_data["balance_rakes_to_receive"]
                #             break

                listData.sort(key=lambda x: convert_to_date(x['month']))

                # Filter out entries where balance_rakes_to_receive is 0 or less
                filtered_data = [entry for entry in listData if entry['balance_rakes_to_receive'] > 0]


                # Update rakes_previous_month_quota_received with balance_rakes_to_receive from previous month
                # for i in range(1, len(filtered_data)):
                #     filtered_data[i]['rakes_previous_month_quota_received'] = filtered_data[i - 1]['balance_rakes_to_receive']

                filtered_data.reverse()

                current_date = datetime.datetime.now()
                current_month_year = current_date.strftime('%b-%Y')

                for check_entry in filtered_data:
                    if check_entry['month'] == current_month_year:
                        check_entry['rakes_previous_month_quota_received'] = 0

                dataList = {}

                dataList['data'] = filtered_data

                # Initialize a dictionary to store the totals
                totals = {
                    'rakes_previous_month_quota_received': 0,
                    'rake_planned_for_the_month': 0,
                    'rakes_loaded_till_date': 0,
                    'rakes_loaded_on_date': 0,
                    'rakes_received_on_date': 0,
                    'total_rakes_received_for_month': 0,
                    'balance_rakes_to_receive': 0,
                    'no_of_rakes_in_transist': 0,
                    'expected_rakes_date': 0,
                    'expected_rakes_value': 0,
                }

                # Loop through each record in data and accumulate totals
                for record in filtered_data:
                    totals['rakes_previous_month_quota_received'] += record['rakes_previous_month_quota_received']
                    totals['rake_planned_for_the_month'] += int(record['rake_planned_for_the_month'])
                    totals['rakes_loaded_till_date'] += record['rakes_loaded_till_date']
                    totals['rakes_loaded_on_date'] += record['rakes_loaded_on_date']
                    totals['rakes_received_on_date'] += record['rakes_received_on_date']
                    totals['total_rakes_received_for_month'] += record['total_rakes_received_for_month']
                    totals['balance_rakes_to_receive'] += record['balance_rakes_to_receive']
                    totals['no_of_rakes_in_transist'] += record['no_of_rakes_in_transist']
                    totals['expected_rakes_value'] += record['expected_rakes_value']

                # Add the totals to dataList
                dataList['rake_total'] = totals

                # result["labels"] = list(dictData.keys())
                result["labels"] = [
                    "month",
                    "source_type",
                    "rakes_previous_month_quota_received",
                    "rake_planned_for_the_month",
                    "rakes_loaded_till_date",
                    "rakes_loaded_on_date",
                    "rakes_received_on_date",
                    "total_rakes_received_for_month",
                    "balance_rakes_to_receive",
                    "no_of_rakes_in_transist",
                    "expected_rakes_date",
                    "expected_rakes_value"
                ]
                result["datasets"] = dataList
                result["total"] = len(rcrrakeQuota.objects(data))
            return result

        elif type == "download":
            file = datetime.datetime.now().strftime("%d-%m-%Y")
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            headers = ["month", "source_type", "rakes_previous_month_quota_received", "rake_planned_for_the_month",
                    "rakes_loaded_till_date", "rakes_loaded_on_date", "rakes_received_on_date", "total_rakes_received_for_month",
                    "balance_rakes_to_receive", "no_of_rakes_in_transist", "expected_rakes_date", "expected_rakes_value"]
            # if month_date:
            #     month_start = datetime.datetime.strptime(month_date, "%Y-%m").replace(day=4)
            #     month_end = (month_start + timedelta(days=31)).replace(day=3)
            #     month_end = month_end.replace(hour=23, minute=59, second=59)

            #     rail_logs = RailData.objects(
            #         placement_date__gte=month_start.strftime("%Y-%m-%dT%H:%M"),
            #         placement_date__lte=month_end.strftime("%Y-%m-%dT%H:%M"),
            #     )
            #     month_check = []
            #     placement_dates = []
                
            #     for log in rail_logs:
            #         if log.month:
            #             if len(log.month) == 7:
            #                 date = datetime.datetime.strptime(log.month, "%Y-%m").strftime("%Y-%m")
            #             elif len(log.month) == 10:
            #                 date = datetime.datetime.strptime(log.month, "%Y-%m-%d").strftime("%Y-%m")
            #         placement_dates.append(date)
            #         month_check.extend(list(set(placement_dates)))
            #     current_month = month_start.strftime("%m-%Y")
            #     month_check.append(current_month)
            #     data &= Q(month__in=month_check)

            usecase_data = rcrrakeQuota.objects.filter(data).order_by("year", "-month")
            count = len(usecase_data)
            path = None

            if usecase_data:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        f"Rcr_Rake_Quota_{datetime.datetime.now().strftime('%Y-%m-%d:%H:%M:%S')}.xlsx",
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format({'bold': True, 'font_size': 10, 'align': 'center', 'valign': 'vcenter'})
                    cell_format = workbook.add_format({'font_size': 10, 'align': 'center', 'valign': 'vcenter'})

                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)

                    for index, header in enumerate(headers):
                        worksheet.write(0, index, header, cell_format2)

                    listData = []
                    for row, query in enumerate(usecase_data, start=1):
                        dictData = {
                            "month": "", "source_type": "", "rakes_previous_month_quota_received": 0, 
                            "rake_planned_for_the_month": "", "rakes_loaded_till_date": 0, "rakes_loaded_on_date": 0, 
                            "rakes_received_on_date": 0, "total_rakes_received_for_month": 0, 
                            "balance_rakes_to_receive": 0, "no_of_rakes_in_transist": 0, "expected_rakes_date": 0, "expected_rakes_value": 0
                        }
                        
                        month_year = f"{query.year}-{query.month[:2].upper()}"
                        date_obj = datetime.datetime.strptime(month_year, "%Y-%m")
                        formatted_date = date_obj.strftime("%Y-%m")
                        rail_logs = RcrData.objects.filter(drawn_date__icontains=formatted_date)

                        dictData["month"] = month_year
                        dictData["rake_planned_for_the_month"] = query.rake_alloted
                        if query.expected_rakes:
                            dictData["expected_rakes_date"] = list(query.expected_rakes.keys())[0]
                            dictData["expected_rakes_value"] = list(query.expected_rakes.values())[0]
                        if rail_logs:
                            for rail_log in rail_logs:
                                if rail_log.source_type != "":
                                    dictData["source_type"] = rail_log.source_type
                                rakes_loaded_till_date = RcrData.objects.filter(
                                    month__icontains=formatted_date,
                                    source_type__iexact = log.source_type
                                ).count()
                                dictData["rakes_loaded_till_date"] = rakes_loaded_till_date
                                rakes_loaded_on_date = RcrData.objects.filter(
                                    drawn_date__gte=f"{datetime.datetime.today().strftime('%Y-%m-%d')}T00:00",
                                    drawn_date__lte=f"{datetime.datetime.today().strftime('%Y-%m-%d')}T23:59",
                                    month__icontains=formatted_date,
                                    source_type__iexact = log.source_type
                                ).count()
                                dictData["rakes_loaded_on_date"] = rakes_loaded_on_date
                                dictData["no_of_rakes_in_transist"] = rakes_loaded_till_date - dictData["total_rakes_received_for_month"]

                        balance_rakes_to_receive = int(query.rake_alloted) - dictData["total_rakes_received_for_month"]
                        dictData["balance_rakes_to_receive"] = balance_rakes_to_receive

                        prev_month_log = rcrrakeQuota.objects.filter(
                            month=(date_obj.replace(day=1) - datetime.timedelta(days=1)).strftime("%b-%Y").upper()
                        ).first()

                        if prev_month_log:
                            dictData["rakes_previous_month_quota_received"] = int(prev_month_log.rake_alloted)

                        worksheet.write(row, 0, dictData["month"], cell_format)
                        worksheet.write(row, 1, dictData["source_type"], cell_format)
                        worksheet.write(row, 2, dictData["rakes_previous_month_quota_received"], cell_format)
                        worksheet.write(row, 3, dictData["rake_planned_for_the_month"], cell_format)
                        worksheet.write(row, 4, dictData["rakes_loaded_till_date"], cell_format)
                        worksheet.write(row, 5, dictData["rakes_loaded_on_date"], cell_format)
                        worksheet.write(row, 6, dictData["rakes_received_on_date"], cell_format)
                        worksheet.write(row, 7, dictData["total_rakes_received_for_month"], cell_format)
                        worksheet.write(row, 8, dictData["balance_rakes_to_receive"], cell_format)
                        worksheet.write(row, 9, dictData["no_of_rakes_in_transist"], cell_format)
                        worksheet.write(row, 10, dictData["expected_rakes_date"], cell_format)
                        worksheet.write(row, 11, dictData["expected_rakes_value"], cell_format)

                        listData.append(dictData)

                    workbook.close()
                    console_logger.debug(f"Successfully {service_id} report generated")
                    console_logger.debug(f"Sent data {path}")

                    return {
                        "Type": "Rcr_Rake_quota_download_event",
                        "Datatype": "Report",
                        "File_Path": path,
                    }

                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
                
            else:
                console_logger.error("No data found")
                return {
                    "Type": "Rcr_Rake_quota_download_event",
                    "Datatype": "Report",
                    "File_Path": path,
                }

    except Exception as e:
        console_logger.debug("----- Fetch Rcr Rake Quota Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e



@router.get("/fetch/rcrrake/quota/completed", tags=["Rail Map"])
def end_point_to_fetch_rcr_rake_quota_completed(response: Response, 
                currentPage: Optional[int] = None,
                perPage: Optional[int] = None,
                # search_text: Optional[str] = None,
                # month_date: Optional[str] = None,
                start_timestamp: Optional[str] = None,
                end_timestamp: Optional[str] = None,
                type: Optional[str] = "display"):
    try:
        data = Q()
        result = {        
                "labels": [],
                "datasets": [],
                "total": 0,
                "page_size": 15
        }

        if type and type == "display":

            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage

            # if month_date:
            #     month_start = datetime.datetime.strptime(month_date, "%Y-%m").replace(day=4)
            #     month_end = (month_start + timedelta(days=31)).replace(day=3)
            #     month_end = month_end.replace(hour=23, minute=59, second=59)

            #     rail_logs = RailData.objects(
            #         placement_date__gte=month_start.strftime("%Y-%m-%dT%H:%M"),
            #         placement_date__lte=month_end.strftime("%Y-%m-%dT%H:%M"),
            #     )
            #     month_check = []
            #     placement_dates = []
                
            #     for log in rail_logs:
            #         if log.month:
            #             if len(log.month) == 7:
            #                 date = datetime.datetime.strptime(log.month, "%Y-%m").strftime("%Y-%m")
            #             elif len(log.month) == 10:
            #                 date = datetime.datetime.strptime(log.month, "%Y-%m-%d").strftime("%Y-%m")
            #             placement_dates.append(date)

            #         month_check.extend(list(set(placement_dates)))
            #     current_month = month_start.strftime("%m-%Y")
            #     month_check.append(current_month)
            #     data &= Q(month__in=month_check)

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M", "Asia/Kolkata", False)
                data &= Q(created_at__gte=start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M", "Asia/Kolkata", False)
                data &= Q(created_at__lte=end_date)

            offset = (page_no - 1) * page_len
            logs = (
                rcrrakeQuota.objects(data)
                .order_by("year", "-month")
                .skip(offset)
                .limit(page_len)
            )
            listData = []
            if logs:
                for log in logs:
                    dictData = {"month": "", "source_type": "", "rakes_previous_month_quota_received": 0, 
                                "rake_planned_for_the_month": "","rakes_loaded_till_date": 0, 
                                "rakes_loaded_on_date": 0, "rakes_received_on_date": 0, 
                                "total_rakes_received_for_month": 0, "balance_rakes_to_receive": 0, 
                                "no_of_rakes_in_transist": 0, "expected_rakes_date": 0, 
                                "expected_rakes_value": 0}
                    rake_year = log.year
                    rake_month = log.month

                    month_year = f"{rake_year}-{rake_month[:2].upper()}"
                    date_obj = datetime.datetime.strptime(month_year, "%Y-%m")
                    prev_date_obj = date_obj - datetime.timedelta(days=1)
                    prev_month_year = prev_date_obj.strftime("%Y-%m")

                    formatted_date = date_obj.strftime("%Y-%m")
                    rail_logs = RcrData.objects.filter(placement_date__icontains=formatted_date)
                    dictData["month"] = datetime.datetime.strptime(log.month, "%m-%Y").strftime("%b-%Y")
                    # dictData["valid_upto"] = log.valid_upto
                    dictData["rake_planned_for_the_month"] = int(log.rake_alloted)
                    if log.cancelled_rakes:
                        dictData["cancelled_rakes"] = int(log.cancelled_rakes)
                    else:
                        dictData["cancelled_rakes"] = 0
                    if log.remarks:
                        dictData["remarks"] = log.remarks
                    else:
                        dictData["remarks"] = ""
                    if log.source_type:
                        dictData["source_type"] = log.source_type
                    if log.expected_rakes:
                        dictData["expected_rakes_date"] = list(log.expected_rakes.keys())[0]
                        dictData["expected_rakes_value"] = list(log.expected_rakes.values())[0]
                    
                    currentDate = datetime.datetime.now()

                    # Get the first day of the current month
                    start_date_time = datetime.datetime(currentDate.year, currentDate.month, 1)

                    # Get the last day of the current month
                    _, last_day = calendar.monthrange(currentDate.year, currentDate.month)
                    end_date_time = datetime.datetime(currentDate.year, currentDate.month, last_day, 23, 59, 59)
                    # Ensure 'avery_placement_date' is stored as a string, then cast it to a datetime object for comparison
                    dictData["total_rakes_received_for_month"] = RcrData.objects.filter(
                        Q(month__icontains=formatted_date) & 
                        Q(avery_placement_date__ne=None) &
                        Q(source_type__iexact = log.source_type)
                        # Q(avery_placement_date__gte=start_date_time.strftime("%Y-%m-%d %H:%M:%S")) &
                        # Q(avery_placement_date__lte=end_date_time.strftime("%Y-%m-%d %H:%M:%S"))
                    ).count()

                    if log.cancelled_rakes:
                        balance_rakes_to_receive = int(log.rake_alloted) - int(dictData["total_rakes_received_for_month"]) - int(log.cancelled_rakes)
                    else:
                        balance_rakes_to_receive = int(log.rake_alloted) - int(dictData["total_rakes_received_for_month"])
                    dictData["balance_rakes_to_receive"] = balance_rakes_to_receive

                    today_date = datetime.date.today()

                    # Get the start of the month
                    start_date = today_date.replace(day=1)

                    # Get the last day of the month
                    last_day = calendar.monthrange(today_date.year, today_date.month)[1]
                    end_date = today_date.replace(day=last_day)

                    startd_date = f"{start_date} 00:00:00"
                    endd_date = f"{end_date} 23:59:59"

                    current_date = datetime.datetime.now()
                    current_month_year = current_date.strftime('%B-%Y')  # Format: 'Month-Year' (e.g., 'November-2024')

                    # Check if formatted_date matches the current month and year
                    
                    dictData["rakes_previous_month_quota_received"] = RailData.objects.filter(
                        month__icontains=formatted_date,
                        avery_placement_date__gte=startd_date,
                        avery_placement_date__lte=endd_date,
                        source_type__iexact=log.source_type
                    ).count()

                    prev_date_obj = datetime.datetime.strptime(log.month, "%m-%Y")

                    last_month = date_obj.month-1
                    last_year = date_obj.year

                    if last_month == 0:
                        last_month = 12
                        last_year -= 1

                    last_month_date_obj = datetime.datetime(last_year, last_month, 1)

                    last_month_str = last_month_date_obj.strftime("%m-%Y")
                    last_month = datetime.datetime.strptime(last_month_str, "%m-%Y")

                    if rail_logs:
                        for rail_log in rail_logs:
                            # if rail_log.source_type != "":
                            #     dictData["source_type"] = rail_log.source_type
                            
                            today_utc = datetime.datetime.utcnow().replace(hour=0, minute=0, second=0)
                            end_of_day_utc = today_utc + timedelta(hours=23, minutes=59, seconds=59)

                            dictData["rakes_loaded_till_date"] = RcrData.objects.filter(
                                month__icontains=formatted_date,
                                source_type__iexact = log.source_type
                            ).count()

                            dictData["rakes_received_on_date"] = RcrData.objects.filter(
                                month__icontains=formatted_date,
                                avery_placement_date__gte=today_utc.strftime("%Y-%m-%d %H:%M:%S"),
                                avery_placement_date__lte=end_of_day_utc.strftime("%Y-%m-%d %H:%M:%S"),
                                source_type__iexact = log.source_type
                            ).count()

                            dictData["rakes_loaded_on_date"] = RcrData.objects.filter(
                                drawn_date__gte=today_utc,
                                drawn_date__lte=end_of_day_utc,
                                month__icontains=formatted_date,
                                source_type__iexact = log.source_type
                            ).count()
                            dictData["no_of_rakes_in_transist"] = dictData["rakes_loaded_till_date"] - dictData["total_rakes_received_for_month"]

                    listData.append(dictData)
                # for i, current_month_data in enumerate(listData):
                #     for j in range(i + 1, len(listData)):
                #         next_month_data = listData[j]
                #         if next_month_data["month"] > current_month_data["month"]:
                #             next_month_data["rakes_previous_month_quota_received"] = current_month_data["balance_rakes_to_receive"]
                #             break

                listData.sort(key=lambda x: convert_to_date(x['month']))

                # Filter out entries where balance_rakes_to_receive is 0 or less
                # filtered_data = [entry for entry in listData if entry['balance_rakes_to_receive'] > 0]

                filtered_data = listData


                # Update rakes_previous_month_quota_received with balance_rakes_to_receive from previous month
                # for i in range(1, len(filtered_data)):
                #     filtered_data[i]['rakes_previous_month_quota_received'] = filtered_data[i - 1]['balance_rakes_to_receive']

                filtered_data.reverse()

                current_date = datetime.datetime.now()
                current_month_year = current_date.strftime('%b-%Y')

                for check_entry in filtered_data:
                    if check_entry['month'] == current_month_year:
                        check_entry['rakes_previous_month_quota_received'] = 0

                dataList = {}

                dataList['data'] = filtered_data

                # Initialize a dictionary to store the totals
                totals = {
                    'rakes_previous_month_quota_received': 0,
                    'rake_planned_for_the_month': 0,
                    'rakes_loaded_till_date': 0,
                    'rakes_loaded_on_date': 0,
                    'rakes_received_on_date': 0,
                    'total_rakes_received_for_month': 0,
                    'balance_rakes_to_receive': 0,
                    'no_of_rakes_in_transist': 0,
                    'expected_rakes_date': 0,
                    'expected_rakes_value': 0,
                }

                # Loop through each record in data and accumulate totals
                for record in listData:
                    totals['rakes_previous_month_quota_received'] += record['rakes_previous_month_quota_received']
                    totals['rake_planned_for_the_month'] += int(record['rake_planned_for_the_month'])
                    totals['rakes_loaded_till_date'] += record['rakes_loaded_till_date']
                    totals['rakes_loaded_on_date'] += record['rakes_loaded_on_date']
                    totals['rakes_received_on_date'] += record['rakes_received_on_date']
                    totals['total_rakes_received_for_month'] += record['total_rakes_received_for_month']
                    totals['balance_rakes_to_receive'] += record['balance_rakes_to_receive']
                    totals['no_of_rakes_in_transist'] += record['no_of_rakes_in_transist']
                    totals['expected_rakes_value'] += record['expected_rakes_value']

                # Add the totals to dataList
                dataList['rake_total'] = totals

                # result["labels"] = list(dictData.keys())
                result["labels"] = [
                    "month",
                    "source_type",
                    "rakes_previous_month_quota_received",
                    "rake_planned_for_the_month",
                    "rakes_loaded_till_date",
                    "rakes_loaded_on_date",
                    "rakes_received_on_date",
                    "total_rakes_received_for_month",
                    "balance_rakes_to_receive",
                    "no_of_rakes_in_transist",
                    "expected_rakes_date",
                    "expected_rakes_value"
                ]
                result["datasets"] = dataList
                result["total"] = len(rcrrakeQuota.objects(data))
            return result

        elif type == "download":
            file = datetime.datetime.now().strftime("%d-%m-%Y")
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            headers = ["month", "source_type", "rakes_previous_month_quota_received", "rake_planned_for_the_month",
                    "rakes_loaded_till_date", "rakes_loaded_on_date", "rakes_received_on_date", "total_rakes_received_for_month",
                    "balance_rakes_to_receive", "no_of_rakes_in_transist", "expected_rakes_date", "expected_rakes_value"]
            # if month_date:
            #     month_start = datetime.datetime.strptime(month_date, "%Y-%m").replace(day=4)
            #     month_end = (month_start + timedelta(days=31)).replace(day=3)
            #     month_end = month_end.replace(hour=23, minute=59, second=59)

            #     rail_logs = RailData.objects(
            #         placement_date__gte=month_start.strftime("%Y-%m-%dT%H:%M"),
            #         placement_date__lte=month_end.strftime("%Y-%m-%dT%H:%M"),
            #     )
            #     month_check = []
            #     placement_dates = []
                
            #     for log in rail_logs:
            #         if log.month:
            #             if len(log.month) == 7:
            #                 date = datetime.datetime.strptime(log.month, "%Y-%m").strftime("%Y-%m")
            #             elif len(log.month) == 10:
            #                 date = datetime.datetime.strptime(log.month, "%Y-%m-%d").strftime("%Y-%m")
            #         placement_dates.append(date)
            #         month_check.extend(list(set(placement_dates)))
            #     current_month = month_start.strftime("%m-%Y")
            #     month_check.append(current_month)
            #     data &= Q(month__in=month_check)

            usecase_data = rcrrakeQuota.objects.filter(data).order_by("year", "-month")
            count = len(usecase_data)
            path = None

            if usecase_data:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        f"Rcr_Rake_Quota_{datetime.datetime.now().strftime('%Y-%m-%d:%H:%M:%S')}.xlsx",
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format({'bold': True, 'font_size': 10, 'align': 'center', 'valign': 'vcenter'})
                    cell_format = workbook.add_format({'font_size': 10, 'align': 'center', 'valign': 'vcenter'})

                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)

                    for index, header in enumerate(headers):
                        worksheet.write(0, index, header, cell_format2)

                    listData = []
                    for row, query in enumerate(usecase_data, start=1):
                        dictData = {
                            "month": "", "source_type": "", "rakes_previous_month_quota_received": 0, 
                            "rake_planned_for_the_month": "", "rakes_loaded_till_date": 0, "rakes_loaded_on_date": 0, 
                            "rakes_received_on_date": 0, "total_rakes_received_for_month": 0, 
                            "balance_rakes_to_receive": 0, "no_of_rakes_in_transist": 0, "expected_rakes_date": 0, "expected_rakes_value": 0
                        }
                        
                        month_year = f"{query.year}-{query.month[:2].upper()}"
                        date_obj = datetime.datetime.strptime(month_year, "%Y-%m")
                        formatted_date = date_obj.strftime("%Y-%m")
                        rail_logs = RcrData.objects.filter(drawn_date__icontains=formatted_date)

                        dictData["month"] = month_year
                        dictData["rake_planned_for_the_month"] = query.rake_alloted
                        if query.expected_rakes:
                            dictData["expected_rakes_date"] = list(query.expected_rakes.keys())[0]
                            dictData["expected_rakes_value"] = list(query.expected_rakes.values())[0]
                        if rail_logs:
                            for rail_log in rail_logs:
                                if rail_log.source_type != "":
                                    dictData["source_type"] = rail_log.source_type
                                rakes_loaded_till_date = RcrData.objects.filter(
                                    month__icontains=formatted_date,
                                    source_type__iexact = log.source_type
                                ).count()
                                dictData["rakes_loaded_till_date"] = rakes_loaded_till_date
                                rakes_loaded_on_date = RcrData.objects.filter(
                                    drawn_date__gte=f"{datetime.datetime.today().strftime('%Y-%m-%d')}T00:00",
                                    drawn_date__lte=f"{datetime.datetime.today().strftime('%Y-%m-%d')}T23:59",
                                    month__icontains=formatted_date,
                                    source_type__iexact = log.source_type
                                ).count()
                                dictData["rakes_loaded_on_date"] = rakes_loaded_on_date
                                dictData["no_of_rakes_in_transist"] = rakes_loaded_till_date - dictData["total_rakes_received_for_month"]

                        balance_rakes_to_receive = int(query.rake_alloted) - dictData["total_rakes_received_for_month"]
                        dictData["balance_rakes_to_receive"] = balance_rakes_to_receive

                        prev_month_log = rcrrakeQuota.objects.filter(
                            month=(date_obj.replace(day=1) - datetime.timedelta(days=1)).strftime("%b-%Y").upper()
                        ).first()

                        if prev_month_log:
                            dictData["rakes_previous_month_quota_received"] = int(prev_month_log.rake_alloted)

                        worksheet.write(row, 0, dictData["month"], cell_format)
                        worksheet.write(row, 1, dictData["source_type"], cell_format)
                        worksheet.write(row, 2, dictData["rakes_previous_month_quota_received"], cell_format)
                        worksheet.write(row, 3, dictData["rake_planned_for_the_month"], cell_format)
                        worksheet.write(row, 4, dictData["rakes_loaded_till_date"], cell_format)
                        worksheet.write(row, 5, dictData["rakes_loaded_on_date"], cell_format)
                        worksheet.write(row, 6, dictData["rakes_received_on_date"], cell_format)
                        worksheet.write(row, 7, dictData["total_rakes_received_for_month"], cell_format)
                        worksheet.write(row, 8, dictData["balance_rakes_to_receive"], cell_format)
                        worksheet.write(row, 9, dictData["no_of_rakes_in_transist"], cell_format)
                        worksheet.write(row, 10, dictData["expected_rakes_date"], cell_format)
                        worksheet.write(row, 11, dictData["expected_rakes_value"], cell_format)

                        listData.append(dictData)

                    workbook.close()
                    console_logger.debug(f"Successfully {service_id} report generated")
                    console_logger.debug(f"Sent data {path}")

                    return {
                        "Type": "Rcr_Rake_quota_download_event",
                        "Datatype": "Report",
                        "File_Path": path,
                    }

                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
                
            else:
                console_logger.error("No data found")
                return {
                    "Type": "Rcr_Rake_quota_download_event",
                    "Datatype": "Report",
                    "File_Path": path,
                }

    except Exception as e:
        console_logger.debug("----- Fetch Rcr Rake Quota Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


    
@router.get("/fetch/rake/quota/test", tags=["Rail Map"])
def end_point_to_fetch_rake_quota_test(response: Response, 
                currentPage: Optional[int] = None,
                perPage: Optional[int] = None,
                # search_text: Optional[str] = None,
                month_date: Optional[str] = None,
                start_timestamp: Optional[str] = None,
                end_timestamp: Optional[str] = None,
                type: Optional[str] = "display"):
    try:
        data = Q()
        result = {        
                "labels": [],
                "datasets": [],
                "total": 0,
                "page_size": 15
        }

        if type and type == "display":

            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage

            # if month_date:
            #     month_start = datetime.datetime.strptime(month_date, "%Y-%m").replace(day=4)
            #     month_end = (month_start + timedelta(days=31)).replace(day=3)
            #     month_end = month_end.replace(hour=23, minute=59, second=59)

            #     rail_logs = RailData.objects(
            #         placement_date__gte=month_start.strftime("%Y-%m-%dT%H:%M"),
            #         placement_date__lte=month_end.strftime("%Y-%m-%dT%H:%M"),
            #     )
            #     month_check = []
            #     placement_dates = []
                
            #     for log in rail_logs:
            #         if log.month:
            #             if len(log.month) == 7:
            #                 date = datetime.datetime.strptime(log.month, "%Y-%m").strftime("%Y-%m")
            #             elif len(log.month) == 10:
            #                 date = datetime.datetime.strptime(log.month, "%Y-%m-%d").strftime("%Y-%m")
            #             placement_dates.append(date)

            #         month_check.extend(list(set(placement_dates)))
            #     current_month = month_start.strftime("%m-%Y")
            #     month_check.append(current_month)
            #     data &= Q(month__in=month_check)

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M", "Asia/Kolkata", False)
                data &= Q(created_at__gte=start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M", "Asia/Kolkata", False)
                data &= Q(created_at__lte=end_date)

            offset = (page_no - 1) * page_len
            logs = (
                rakeQuota.objects(data)
                .order_by("year", "-month")
                .skip(offset)
                .limit(page_len)
            )
            listData = []
            if logs:
                for log in logs:
                    dictData = {"month": "", "source_type": "", "rakes_previous_month_quota_received": 0, 
                                "rake_planned_for_the_month": "","rakes_loaded_till_date": 0, 
                                "rakes_loaded_on_date": 0, "rakes_received_on_date": 0, 
                                "total_rakes_received_for_month": 0, "balance_rakes_to_receive": 0, 
                                "no_of_rakes_in_transist": 0, "expected_rakes_date": 0, 
                                "expected_rakes_value": 0}
                    rake_year = log.year
                    rake_month = log.month

                    month_year = f"{rake_year}-{rake_month[:2].upper()}"
                    date_obj = datetime.datetime.strptime(month_year, "%Y-%m")
                    prev_date_obj = date_obj - datetime.timedelta(days=1)
                    prev_month_year = prev_date_obj.strftime("%Y-%m")

                    formatted_date = date_obj.strftime("%Y-%m")
                    rail_logs = RailData.objects.filter(placement_date__icontains=formatted_date)
                    dictData["month"] = datetime.datetime.strptime(log.month, "%m-%Y").strftime("%b-%Y")
                    # dictData["valid_upto"] = log.valid_upto
                    dictData["rake_planned_for_the_month"] = int(log.rake_alloted)
                    if log.cancelled_rakes:
                        dictData["cancelled_rakes"] = int(log.cancelled_rakes)
                    else:
                        dictData["cancelled_rakes"] = 0
                    if log.remarks:
                        dictData["remarks"] = log.remarks
                    else:
                        dictData["remarks"] = ""
                    if log.source_type:
                        dictData["source_type"] = log.source_type
                    if log.expected_rakes:
                        dictData["expected_rakes_date"] = list(log.expected_rakes.keys())[0]
                        dictData["expected_rakes_value"] = list(log.expected_rakes.values())[0]
                    
                    currentDate = datetime.datetime.now()

                    # Get the first day of the current month
                    start_date_time = datetime.datetime(currentDate.year, currentDate.month, 1)

                    # Get the last day of the current month
                    _, last_day = calendar.monthrange(currentDate.year, currentDate.month)
                    end_date_time = datetime.datetime(currentDate.year, currentDate.month, last_day, 23, 59, 59)
                    # Ensure 'avery_placement_date' is stored as a string, then cast it to a datetime object for comparison
                    dictData["total_rakes_received_for_month"] = RailData.objects.filter(
                        Q(month__icontains=formatted_date) & 
                        Q(avery_placement_date__ne=None) &
                        Q(source_type__iexact = log.source_type)
                        # Q(avery_placement_date__gte=start_date_time.strftime("%Y-%m-%d %H:%M:%S")) &
                        # Q(avery_placement_date__lte=end_date_time.strftime("%Y-%m-%d %H:%M:%S"))
                    ).count()
                    if log.cancelled_rakes:
                        balance_rakes_to_receive = int(log.rake_alloted) - int(dictData["total_rakes_received_for_month"]) - int(log.cancelled_rakes)
                    else:
                        balance_rakes_to_receive = int(log.rake_alloted) - int(dictData["total_rakes_received_for_month"])
                    dictData["balance_rakes_to_receive"] = balance_rakes_to_receive

                    today_date = datetime.date.today()

                    # Get the start of the month
                    start_date = today_date.replace(day=1)

                    # Get the last day of the month
                    last_day = calendar.monthrange(today_date.year, today_date.month)[1]
                    end_date = today_date.replace(day=last_day)

                    startd_date = f"{start_date} 00:00:00"
                    endd_date = f"{end_date} 23:59:59"

                    current_date = datetime.datetime.now()
                    current_month_year = current_date.strftime('%B-%Y')  # Format: 'Month-Year' (e.g., 'November-2024')

                    # Check if formatted_date matches the current month and year
                    
                    dictData["rakes_previous_month_quota_received"] = RailData.objects.filter(
                        month__icontains=formatted_date,
                        avery_placement_date__gte=startd_date,
                        avery_placement_date__lte=endd_date,
                        source_type__iexact=log.source_type
                    ).count()

                    prev_date_obj = datetime.datetime.strptime(log.month, "%m-%Y")

                    last_month = date_obj.month-1
                    last_year = date_obj.year

                    if last_month == 0:
                        last_month = 12
                        last_year -= 1

                    last_month_date_obj = datetime.datetime(last_year, last_month, 1)

                    last_month_str = last_month_date_obj.strftime("%m-%Y")
                    last_month = datetime.datetime.strptime(last_month_str, "%m-%Y")

                    if rail_logs:
                        for rail_log in rail_logs:
                            # if rail_log.source_type != "":
                            #     dictData["source_type"] = rail_log.source_type
                            
                            today_utc = datetime.datetime.utcnow().replace(hour=0, minute=0, second=0)
                            end_of_day_utc = today_utc + timedelta(hours=23, minutes=59, seconds=59)

                            dictData["rakes_loaded_till_date"] = RailData.objects.filter(
                                month__icontains=formatted_date,
                                source_type__iexact = log.source_type
                            ).count()

                            dictData["rakes_received_on_date"] = RailData.objects.filter(
                                month__icontains=formatted_date,
                                avery_placement_date__gte=today_utc.strftime("%Y-%m-%d %H:%M:%S"),
                                avery_placement_date__lte=end_of_day_utc.strftime("%Y-%m-%d %H:%M:%S"),
                                source_type__iexact = log.source_type
                            ).count()

                            dictData["rakes_loaded_on_date"] = RailData.objects.filter(
                                drawn_date__gte=today_utc,
                                drawn_date__lte=end_of_day_utc,
                                month__icontains=formatted_date,
                                source_type__iexact = log.source_type
                            ).count()

                            dictData["no_of_rakes_in_transist"] = dictData["rakes_loaded_till_date"] - dictData["total_rakes_received_for_month"]

                    listData.append(dictData)
                # for i, current_month_data in enumerate(listData):
                #     for j in range(i + 1, len(listData)):
                #         next_month_data = listData[j]
                #         if next_month_data["month"] > current_month_data["month"]:
                #             next_month_data["rakes_previous_month_quota_received"] = current_month_data["balance_rakes_to_receive"]
                #             break

                listData.sort(key=lambda x: convert_to_date(x['month']))

                # Filter out entries where balance_rakes_to_receive is 0 or less
                filtered_data = [entry for entry in listData if entry['balance_rakes_to_receive'] > 0]

                # console_logger.debug(listData)

                # Update rakes_previous_month_quota_received with balance_rakes_to_receive from previous month
                # for i in range(1, len(filtered_data)):
                #     # console_logger.debug(i)
                #     # console_logger.debug(i-1)
                #     console_logger.debug(filtered_data[i - 1]['balance_rakes_to_receive'])
                #     console_logger.debug(filtered_data[i]['rakes_previous_month_quota_received'])
                #     filtered_data[i]['rakes_previous_month_quota_received'] = filtered_data[i - 1]['balance_rakes_to_receive']

                # console_logger.debug(listData)
                filtered_data.reverse()

                current_date = datetime.datetime.now()
                current_month_year = current_date.strftime('%b-%Y')

                for check_entry in filtered_data:
                    if check_entry['month'] == current_month_year:
                        check_entry['rakes_previous_month_quota_received'] = 0

                dataList = {}

                dataList['data'] = filtered_data

                # Initialize a dictionary to store the totals
                totals = {
                    'sum_rakes_previous_month_quota_received': 0,
                    'sum_rake_planned_for_the_month': 0,
                    'sum_rakes_loaded_till_date': 0,
                    'sum_rakes_loaded_on_date': 0,
                    'sum_rakes_received_on_date': 0,
                    'sum_total_rakes_received_for_month': 0,
                    'sum_balance_rakes_to_receive': 0,
                    'sum_no_of_rakes_in_transist': 0,
                    'sum_expected_rakes_date': 0,
                    'sum_expected_rakes_value': 0,
                    'sum_cancelled_rakes': 0,
                }

                # Loop through each record in data and accumulate totals
                for record in listData:
                    totals['sum_rakes_previous_month_quota_received'] += record['rakes_previous_month_quota_received']
                    totals['sum_rake_planned_for_the_month'] += int(record['rake_planned_for_the_month'])
                    totals['sum_rakes_loaded_till_date'] += record['rakes_loaded_till_date']
                    totals['sum_rakes_loaded_on_date'] += record['rakes_loaded_on_date']
                    totals['sum_rakes_received_on_date'] += record['rakes_received_on_date']
                    totals['sum_total_rakes_received_for_month'] += record['total_rakes_received_for_month']
                    totals['sum_balance_rakes_to_receive'] += record['balance_rakes_to_receive']
                    totals['sum_no_of_rakes_in_transist'] += record['no_of_rakes_in_transist']
                    totals['sum_expected_rakes_date'] += record['expected_rakes_date']
                    totals['sum_expected_rakes_value'] += record['expected_rakes_value']
                    totals['sum_cancelled_rakes'] += record['cancelled_rakes']

                # Add the totals to dataList
                dataList['rake_total'] = totals

                result["labels"] = list(dictData.keys())
                result["datasets"] = dataList
                result["total"] = len(rakeQuota.objects(data))
            return result
        elif type == "download":
            file = datetime.datetime.now().strftime("%d-%m-%Y")
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            headers = ["month", "source_type", "rakes_previous_month_quota_received", "rake_planned_for_the_month",
                    "rakes_loaded_till_date", "rakes_loaded_on_date", "rakes_received_on_date", "total_rakes_received_for_month",
                    "balance_rakes_to_receive", "no_of_rakes_in_transist", "expected_rakes_date", "expected_rakes_value"]
            if month_date:
                month_start = datetime.datetime.strptime(month_date, "%Y-%m").replace(day=4)
                month_end = (month_start + timedelta(days=31)).replace(day=3)
                month_end = month_end.replace(hour=23, minute=59, second=59)

                rail_logs = RailData.objects(
                    placement_date__gte=month_start.strftime("%Y-%m-%dT%H:%M"),
                    placement_date__lte=month_end.strftime("%Y-%m-%dT%H:%M"),
                )
                month_check = []
                placement_dates = []
                
                for log in rail_logs:
                    if log.month:
                        if len(log.month) == 7:
                            date = datetime.datetime.strptime(log.month, "%Y-%m").strftime("%Y-%m")
                        elif len(log.month) == 10:
                            date = datetime.datetime.strptime(log.month, "%Y-%m-%d").strftime("%Y-%m")
                    placement_dates.append(date)
                    month_check.extend(list(set(placement_dates)))
                current_month = month_start.strftime("%m-%Y")
                month_check.append(current_month)
                data &= Q(month__in=month_check)

            usecase_data = rakeQuota.objects.filter(data).order_by("year", "-month")
            count = len(usecase_data)
            path = None

            if usecase_data:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        f"Rake_Quota_{datetime.datetime.now().strftime('%Y-%m-%d:%H:%M:%S')}.xlsx",
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format({'bold': True, 'font_size': 10, 'align': 'center', 'valign': 'vcenter'})
                    cell_format = workbook.add_format({'font_size': 10, 'align': 'center', 'valign': 'vcenter'})

                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)

                    for index, header in enumerate(headers):
                        worksheet.write(0, index, header, cell_format2)

                    listData = []
                    for row, query in enumerate(usecase_data, start=1):
                        dictData = {
                            "month": "", "source_type": "", "rakes_previous_month_quota_received": 0, 
                            "rake_planned_for_the_month": "", "rakes_loaded_till_date": 0, "rakes_loaded_on_date": 0, 
                            "rakes_received_on_date": 0, "total_rakes_received_for_month": 0, 
                            "balance_rakes_to_receive": 0, "no_of_rakes_in_transist": 0, "expected_rakes_date": 0, "expected_rakes_value": 0
                        }
                        
                        month_year = f"{query.year}-{query.month[:2].upper()}"
                        date_obj = datetime.datetime.strptime(month_year, "%Y-%m")
                        formatted_date = date_obj.strftime("%Y-%m")
                        rail_logs = RailData.objects.filter(drawn_date__icontains=formatted_date)

                        dictData["month"] = month_year
                        dictData["rake_planned_for_the_month"] = query.rake_alloted
                        if query.expected_rakes:
                            dictData["expected_rakes_date"] = list(query.expected_rakes.keys())[0]
                            dictData["expected_rakes_value"] = list(query.expected_rakes.values())[0]
                        if rail_logs:
                            for rail_log in rail_logs:
                                if rail_log.source_type != "":
                                    dictData["source_type"] = rail_log.source_type
                                rakes_loaded_till_date = RailData.objects.filter(
                                    month__icontains=formatted_date
                                ).count()
                                dictData["rakes_loaded_till_date"] = rakes_loaded_till_date
                                rakes_loaded_on_date = RailData.objects.filter(
                                    drawn_date__gte=f"{datetime.datetime.today().strftime('%Y-%m-%d')}T00:00",
                                    drawn_date__lte=f"{datetime.datetime.today().strftime('%Y-%m-%d')}T23:59"
                                ).count()
                                dictData["rakes_loaded_on_date"] = rakes_loaded_on_date
                                dictData["no_of_rakes_in_transist"] = rakes_loaded_till_date - dictData["total_rakes_received_for_month"]

                        balance_rakes_to_receive = int(query.rake_alloted) - dictData["total_rakes_received_for_month"]
                        dictData["balance_rakes_to_receive"] = balance_rakes_to_receive

                        prev_month_log = rakeQuota.objects.filter(
                            month=(date_obj.replace(day=1) - datetime.timedelta(days=1)).strftime("%b-%Y").upper()
                        ).first()

                        if prev_month_log:
                            dictData["rakes_previous_month_quota_received"] = int(prev_month_log.rake_alloted)

                        worksheet.write(row, 0, dictData["month"], cell_format)
                        worksheet.write(row, 1, dictData["source_type"], cell_format)
                        worksheet.write(row, 2, dictData["rakes_previous_month_quota_received"], cell_format)
                        worksheet.write(row, 3, dictData["rake_planned_for_the_month"], cell_format)
                        worksheet.write(row, 4, dictData["rakes_loaded_till_date"], cell_format)
                        worksheet.write(row, 5, dictData["rakes_loaded_on_date"], cell_format)
                        worksheet.write(row, 6, dictData["rakes_received_on_date"], cell_format)
                        worksheet.write(row, 7, dictData["total_rakes_received_for_month"], cell_format)
                        worksheet.write(row, 8, dictData["balance_rakes_to_receive"], cell_format)
                        worksheet.write(row, 9, dictData["no_of_rakes_in_transist"], cell_format)
                        worksheet.write(row, 10, dictData["expected_rakes_date"], cell_format)
                        worksheet.write(row, 11, dictData["expected_rakes_value"], cell_format)

                        listData.append(dictData)

                    workbook.close()
                    console_logger.debug(f"Successfully {service_id} report generated")
                    console_logger.debug(f"Sent data {path}")

                    return {
                        "Type": "Rake_quota_download_event",
                        "Datatype": "Report",
                        "File_Path": path,
                    }

                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
                
            else:
                console_logger.error("No data found")
                return {
                    "Type": "Rake_quota_download_event",
                    "Datatype": "Report",
                    "File_Path": path,
                }

    except Exception as e:
        console_logger.debug("----- Fetch Rake Quota Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

@router.get("/fetch/rake/quota/test", tags=["Rail Map"])
def end_point_to_fetch_rcr_rake_quota_pdf(response: Response, 
                currentPage: Optional[int] = None,
                perPage: Optional[int] = None,
                # search_text: Optional[str] = None,
                month_date: Optional[str] = None,
                start_timestamp: Optional[str] = None,
                end_timestamp: Optional[str] = None,
                type: Optional[str] = "display"):
    try:
        data = Q()
        result = {        
                "labels": [],
                "datasets": [],
                "total": 0,
                "page_size": 15
        }

        if type and type == "display":

            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage

            # if month_date:
            #     month_start = datetime.datetime.strptime(month_date, "%Y-%m").replace(day=4)
            #     month_end = (month_start + timedelta(days=31)).replace(day=3)
            #     month_end = month_end.replace(hour=23, minute=59, second=59)

            #     rail_logs = RailData.objects(
            #         placement_date__gte=month_start.strftime("%Y-%m-%dT%H:%M"),
            #         placement_date__lte=month_end.strftime("%Y-%m-%dT%H:%M"),
            #     )
            #     month_check = []
            #     placement_dates = []
                
            #     for log in rail_logs:
            #         if log.month:
            #             if len(log.month) == 7:
            #                 date = datetime.datetime.strptime(log.month, "%Y-%m").strftime("%Y-%m")
            #             elif len(log.month) == 10:
            #                 date = datetime.datetime.strptime(log.month, "%Y-%m-%d").strftime("%Y-%m")
            #             placement_dates.append(date)

            #         month_check.extend(list(set(placement_dates)))
            #     current_month = month_start.strftime("%m-%Y")
            #     month_check.append(current_month)
            #     data &= Q(month__in=month_check)

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M", "Asia/Kolkata", False)
                data &= Q(created_at__gte=start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M", "Asia/Kolkata", False)
                data &= Q(created_at__lte=end_date)

            offset = (page_no - 1) * page_len
            logs = (
                rcrrakeQuota.objects(data)
                .order_by("year", "month")
                .skip(offset)
                .limit(page_len)
            )
            listData = []
            if logs:
                for log in logs:
                    dictData = {"month": "", "source_type": "", "rakes_previous_month_quota_received": 0, 
                                "rake_planned_for_the_month": "","rakes_loaded_till_date": 0, 
                                "rakes_loaded_on_date": 0, "rakes_received_on_date": 0, 
                                "total_rakes_received_for_month": 0, "balance_rakes_to_receive": 0, 
                                "no_of_rakes_in_transist": 0, "expected_rakes_date": 0, 
                                "expected_rakes_value": 0}
                    rake_year = log.year
                    rake_month = log.month

                    month_year = f"{rake_year}-{rake_month[:2].upper()}"
                    date_obj = datetime.datetime.strptime(month_year, "%Y-%m")
                    prev_date_obj = date_obj - datetime.timedelta(days=1)
                    prev_month_year = prev_date_obj.strftime("%Y-%m")

                    formatted_date = date_obj.strftime("%Y-%m")
                    rail_logs = RcrData.objects.filter(placement_date__icontains=formatted_date)
                    dictData["month"] = datetime.datetime.strptime(log.month, "%m-%Y").strftime("%b-%Y")
                    dictData["valid_upto"] = log.valid_upto
                    dictData["rake_planned_for_the_month"] = int(log.rake_alloted)
                    if log.cancelled_rakes:
                        dictData["cancelled_rakes"] = int(log.cancelled_rakes)
                    else:
                        dictData["cancelled_rakes"] = 0
                    if log.remarks:
                        dictData["remarks"] = log.remarks
                    else:
                        dictData["remarks"] = ""
                    if log.source_type:
                        dictData["source_type"] = log.source_type
                    if log.expected_rakes:
                        dictData["expected_rakes_date"] = list(log.expected_rakes.keys())[0]
                        dictData["expected_rakes_value"] = list(log.expected_rakes.values())[0]
                    
                    currentDate = datetime.datetime.now()

                    # Get the first day of the current month
                    start_date_time = datetime.datetime(currentDate.year, currentDate.month, 1)

                    # Get the last day of the current month
                    _, last_day = calendar.monthrange(currentDate.year, currentDate.month)
                    end_date_time = datetime.datetime(currentDate.year, currentDate.month, last_day, 23, 59, 59)

                    # Ensure 'avery_placement_date' is stored as a string, then cast it to a datetime object for comparison
                    dictData["total_rakes_received_for_month"] = RcrData.objects.filter(
                        Q(month__icontains=formatted_date) & 
                        Q(avery_placement_date__ne=None) &
                        Q(source_type__iexact = log.source_type)
                        # Q(avery_placement_date__gte=start_date_time.strftime("%Y-%m-%d %H:%M:%S")) &
                        # Q(avery_placement_date__lte=end_date_time.strftime("%Y-%m-%d %H:%M:%S"))
                    ).count()

                    if log.cancelled_rakes:
                        balance_rakes_to_receive = int(log.rake_alloted) - int(dictData["total_rakes_received_for_month"]) - int(log.cancelled_rakes)
                    else:
                        balance_rakes_to_receive = int(log.rake_alloted) - int(dictData["total_rakes_received_for_month"])
                    dictData["balance_rakes_to_receive"] = balance_rakes_to_receive

                    today_date = datetime.date.today()

                    # Get the start of the month
                    start_date = today_date.replace(day=1)

                    # Get the last day of the month
                    last_day = calendar.monthrange(today_date.year, today_date.month)[1]
                    end_date = today_date.replace(day=last_day)

                    startd_date = f"{start_date} 00:00:00"
                    endd_date = f"{end_date} 23:59:59"

                    current_date = datetime.datetime.now()
                    current_month_year = current_date.strftime('%B-%Y')  # Format: 'Month-Year' (e.g., 'November-2024')

                    # Check if formatted_date matches the current month and year
                    
                    dictData["rakes_previous_month_quota_received"] = RailData.objects.filter(
                        month__icontains=formatted_date,
                        avery_placement_date__gte=startd_date,
                        avery_placement_date__lte=endd_date,
                        source_type__iexact=log.source_type
                    ).count()

                    prev_date_obj = datetime.datetime.strptime(log.month, "%m-%Y")

                    last_month = date_obj.month-1
                    last_year = date_obj.year

                    if last_month == 0:
                        last_month = 12
                        last_year -= 1

                    last_month_date_obj = datetime.datetime(last_year, last_month, 1)

                    last_month_str = last_month_date_obj.strftime("%m-%Y")
                    last_month = datetime.datetime.strptime(last_month_str, "%m-%Y")

                    if rail_logs:
                        for rail_log in rail_logs:
                            # if rail_log.source_type != "":
                            #     dictData["source_type"] = rail_log.source_type
                            
                            today_utc = datetime.datetime.utcnow().replace(hour=0, minute=0, second=0)
                            end_of_day_utc = today_utc + timedelta(hours=23, minutes=59, seconds=59)

                            dictData["rakes_loaded_till_date"] = RcrData.objects.filter(
                                month__icontains=formatted_date,
                                source_type__iexact = log.source_type,
                            ).count()

                            # console_logger.debug(today_utc.strftime("%Y-%m-%d %H:%M:%S"))
                            # console_logger.debug(end_of_day_utc.strftime("%Y-%m-%d %H:%M:%S"))

                            dictData["rakes_received_on_date"] = RcrData.objects.filter(
                                month__icontains=formatted_date,
                                avery_placement_date__gte=today_utc.strftime("%Y-%m-%d %H:%M:%S"),
                                avery_placement_date__lte=end_of_day_utc.strftime("%Y-%m-%d %H:%M:%S"),
                                source_type__iexact = log.source_type,
                            ).count()

                            dictData["rakes_loaded_on_date"] = RcrData.objects.filter(
                                drawn_date__gte=today_utc,
                                drawn_date__lte=end_of_day_utc,
                                month__icontains=formatted_date,
                                source_type__iexact = log.source_type,
                            ).count()
                            dictData["no_of_rakes_in_transist"] = dictData["rakes_loaded_till_date"] - dictData["total_rakes_received_for_month"]

                    listData.append(dictData)
                # console_logger.debug(listData)
                # for i, current_month_data in enumerate(listData):
                #     for j in range(i + 1, len(listData)):
                #         next_month_data = listData[j]
                #         if next_month_data["month"] > current_month_data["month"]:
                #             next_month_data["rakes_previous_month_quota_received"] = current_month_data["balance_rakes_to_receive"]
                #             break

                listData.sort(key=lambda x: convert_to_date(x['month']))

                # Filter out entries where balance_rakes_to_receive is 0 or less
                filtered_data = [entry for entry in listData if entry['balance_rakes_to_receive'] > 0]

                # Update rakes_previous_month_quota_received with balance_rakes_to_receive from previous month
                # for i in range(1, len(filtered_data)):
                #     filtered_data[i]['rakes_previous_month_quota_received'] = filtered_data[i - 1]['balance_rakes_to_receive']

                filtered_data.reverse()

                current_date = datetime.datetime.now()
                current_month_year = current_date.strftime('%b-%Y')

                for check_entry in filtered_data:
                    if check_entry['month'] == current_month_year:
                        check_entry['rakes_previous_month_quota_received'] = 0

                # # Filter out entries where balance_rakes_to_receive is 0 or less
                # filtered_data = [entry for entry in listData if entry['balance_rakes_to_receive'] > 0]

                dataList = {}

                dataList['data'] = filtered_data

                # Initialize a dictionary to store the totals
                totals = {
                    'sum_rakes_previous_month_quota_received': 0,
                    'sum_rake_planned_for_the_month': 0,
                    'sum_rakes_loaded_till_date': 0,
                    'sum_rakes_loaded_on_date': 0,
                    'sum_rakes_received_on_date': 0,
                    'sum_total_rakes_received_for_month': 0,
                    'sum_balance_rakes_to_receive': 0,
                    'sum_no_of_rakes_in_transist': 0,
                    'sum_expected_rakes_date': 0,
                    'sum_expected_rakes_value': 0,
                    'sum_cancelled_rakes': 0,
                }

                # Loop through each record in data and accumulate totals
                for record in listData:
                    totals['sum_rakes_previous_month_quota_received'] += record['rakes_previous_month_quota_received']
                    totals['sum_rake_planned_for_the_month'] += int(record['rake_planned_for_the_month'])
                    totals['sum_rakes_loaded_till_date'] += record['rakes_loaded_till_date']
                    totals['sum_rakes_loaded_on_date'] += record['rakes_loaded_on_date']
                    totals['sum_rakes_received_on_date'] += record['rakes_received_on_date']
                    totals['sum_total_rakes_received_for_month'] += record['total_rakes_received_for_month']
                    totals['sum_balance_rakes_to_receive'] += record['balance_rakes_to_receive']
                    totals['sum_no_of_rakes_in_transist'] += record['no_of_rakes_in_transist']
                    totals['sum_expected_rakes_date'] += record['expected_rakes_date']
                    totals['sum_expected_rakes_value'] += record['expected_rakes_value']
                    totals['sum_cancelled_rakes'] += record['cancelled_rakes']

                # Add the totals to dataList
                dataList['rake_total'] = totals

                result["labels"] = list(dictData.keys())
                result["datasets"] = dataList
                result["total"] = len(rcrrakeQuota.objects(data))
            return result
        elif type == "download":
            file = datetime.datetime.now().strftime("%d-%m-%Y")
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            headers = ["month", "source_type", "rakes_previous_month_quota_received", "rake_planned_for_the_month",
                    "rakes_loaded_till_date", "rakes_loaded_on_date", "rakes_received_on_date", "total_rakes_received_for_month",
                    "balance_rakes_to_receive", "no_of_rakes_in_transist", "expected_rakes_date", "expected_rakes_value"]
            if month_date:
                month_start = datetime.datetime.strptime(month_date, "%Y-%m").replace(day=4)
                month_end = (month_start + timedelta(days=31)).replace(day=3)
                month_end = month_end.replace(hour=23, minute=59, second=59)

                rail_logs = RailData.objects(
                    placement_date__gte=month_start.strftime("%Y-%m-%dT%H:%M"),
                    placement_date__lte=month_end.strftime("%Y-%m-%dT%H:%M"),
                )
                month_check = []
                placement_dates = []
                
                for log in rail_logs:
                    if log.month:
                        if len(log.month) == 7:
                            date = datetime.datetime.strptime(log.month, "%Y-%m").strftime("%Y-%m")
                        elif len(log.month) == 10:
                            date = datetime.datetime.strptime(log.month, "%Y-%m-%d").strftime("%Y-%m")
                    placement_dates.append(date)
                    month_check.extend(list(set(placement_dates)))
                current_month = month_start.strftime("%m-%Y")
                month_check.append(current_month)
                data &= Q(month__in=month_check)

            usecase_data = rakeQuota.objects.filter(data).order_by("year", "-month")
            count = len(usecase_data)
            path = None

            if usecase_data:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        f"Rake_Quota_{datetime.datetime.now().strftime('%Y-%m-%d:%H:%M:%S')}.xlsx",
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format({'bold': True, 'font_size': 10, 'align': 'center', 'valign': 'vcenter'})
                    cell_format = workbook.add_format({'font_size': 10, 'align': 'center', 'valign': 'vcenter'})

                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)

                    for index, header in enumerate(headers):
                        worksheet.write(0, index, header, cell_format2)

                    listData = []
                    for row, query in enumerate(usecase_data, start=1):
                        dictData = {
                            "month": "", "source_type": "", "rakes_previous_month_quota_received": 0, 
                            "rake_planned_for_the_month": "", "rakes_loaded_till_date": 0, "rakes_loaded_on_date": 0, 
                            "rakes_received_on_date": 0, "total_rakes_received_for_month": 0, 
                            "balance_rakes_to_receive": 0, "no_of_rakes_in_transist": 0, "expected_rakes_date": 0, "expected_rakes_value": 0
                        }
                        
                        month_year = f"{query.year}-{query.month[:2].upper()}"
                        date_obj = datetime.datetime.strptime(month_year, "%Y-%m")
                        formatted_date = date_obj.strftime("%Y-%m")
                        rail_logs = RailData.objects.filter(drawn_date__icontains=formatted_date)

                        dictData["month"] = month_year
                        dictData["rake_planned_for_the_month"] = query.rake_alloted
                        if query.expected_rakes:
                            dictData["expected_rakes_date"] = list(query.expected_rakes.keys())[0]
                            dictData["expected_rakes_value"] = list(query.expected_rakes.values())[0]
                        if rail_logs:
                            for rail_log in rail_logs:
                                if rail_log.source_type != "":
                                    dictData["source_type"] = rail_log.source_type
                                rakes_loaded_till_date = RailData.objects.filter(
                                    month__icontains=formatted_date
                                ).count()
                                dictData["rakes_loaded_till_date"] = rakes_loaded_till_date
                                rakes_loaded_on_date = RailData.objects.filter(
                                    drawn_date__gte=f"{datetime.datetime.today().strftime('%Y-%m-%d')}T00:00",
                                    drawn_date__lte=f"{datetime.datetime.today().strftime('%Y-%m-%d')}T23:59"
                                ).count()
                                dictData["rakes_loaded_on_date"] = rakes_loaded_on_date
                                dictData["no_of_rakes_in_transist"] = rakes_loaded_till_date - dictData["total_rakes_received_for_month"]

                        balance_rakes_to_receive = int(query.rake_alloted) - dictData["total_rakes_received_for_month"]
                        dictData["balance_rakes_to_receive"] = balance_rakes_to_receive

                        prev_month_log = rakeQuota.objects.filter(
                            month=(date_obj.replace(day=1) - datetime.timedelta(days=1)).strftime("%b-%Y").upper()
                        ).first()

                        if prev_month_log:
                            dictData["rakes_previous_month_quota_received"] = int(prev_month_log.rake_alloted)

                        worksheet.write(row, 0, dictData["month"], cell_format)
                        worksheet.write(row, 1, dictData["source_type"], cell_format)
                        worksheet.write(row, 2, dictData["rakes_previous_month_quota_received"], cell_format)
                        worksheet.write(row, 3, dictData["rake_planned_for_the_month"], cell_format)
                        worksheet.write(row, 4, dictData["rakes_loaded_till_date"], cell_format)
                        worksheet.write(row, 5, dictData["rakes_loaded_on_date"], cell_format)
                        worksheet.write(row, 6, dictData["rakes_received_on_date"], cell_format)
                        worksheet.write(row, 7, dictData["total_rakes_received_for_month"], cell_format)
                        worksheet.write(row, 8, dictData["balance_rakes_to_receive"], cell_format)
                        worksheet.write(row, 9, dictData["no_of_rakes_in_transist"], cell_format)
                        worksheet.write(row, 10, dictData["expected_rakes_date"], cell_format)
                        worksheet.write(row, 11, dictData["expected_rakes_value"], cell_format)

                        listData.append(dictData)

                    workbook.close()
                    console_logger.debug(f"Successfully {service_id} report generated")
                    console_logger.debug(f"Sent data {path}")

                    return {
                        "Type": "Rake_quota_download_event",
                        "Datatype": "Report",
                        "File_Path": path,
                    }

                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
                
            else:
                console_logger.error("No data found")
                return {
                    "Type": "Rake_quota_download_event",
                    "Datatype": "Report",
                    "File_Path": path,
                }

    except Exception as e:
        console_logger.debug("----- Fetch Rake Quota Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/insert/rake/quota", tags=["Rail Map"])
def endpoint_to_insert_rake_quota(response:Response, data:rakeQuotaManual):
    try:
        payload = data.dict()
        insertRakeQuota = rakeQuota(
            month=payload.get("month"),
            year=payload.get("year"),
            valid_upto=payload.get("valid_upto"),
            coal_field=payload.get("coal_field"),
            rake_alloted=payload.get("rake_alloted"),
            rake_received=payload.get("rake_received"),
            due=payload.get("due"),
            grade=payload.get("grade"),
            ID=rakeQuota.objects.count() + 1)
        insertRakeQuota.save()
        return {"details": "success"}
    except Exception as e:
        console_logger.debug("----- Fetch Rake Quota Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


def extract_quota_rail(file):
    try:
        pdfFileObj = open(file, 'rb')
        pdfReader = PyPDF3.PdfFileReader(pdfFileObj)

        pageObj = pdfReader.getPage(0)
        mytext = pageObj.extractText()

        result={}
        date_match = re.search(r'([A-Z]{3}-\d{4})', mytext,re.DOTALL)
        result["date"]= date_match.group(1) if date_match else None

        no_rake_match = re.search(r'(\d+Rakes@\d+\s*BOXN)', mytext)
        result['no_rake'] = no_rake_match.group(1).replace('\n', '') if no_rake_match else None

        rakes_start_index = no_rake_match.start()
        result['coal_field'] = mytext[rakes_start_index-5:rakes_start_index].strip()


        grade_match = re.search(r"(\d{4}-\d{4}-\s*\w+)",mytext)
        result['coal_grade'] = grade_match.group(1).replace('\n', '') if grade_match else None

        valid_match = re.search(r"This Programme is Valid Upto:\s*(\d{2}-\d{2}-\d{4})",mytext)
        result['valid'] = valid_match.group(1) if valid_match else None

        return result
    except Exception as e:
        console_logger.debug("----- Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


# @router.post("/rail/rakequotaupload", tags=["Rail Map"])
# async def endpoint_to_upload_rake_data(response: Response, pdf_upload: Optional[UploadFile] = File(None)):
#     try:
#         if pdf_upload is None:
#             return {"error": "No file uploaded"}
#         contents = await pdf_upload.read()

#         if not contents:
#             return {"error": "Uploaded file is empty"}

#         if not pdf_upload.filename.endswith(('.pdf','.PDF')):
#             return {"error": "Uploaded file is not a PDF"}
        
#         file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
#         target_directory = f"static_server/gmr_ai/{file}"
#         os.umask(0)
#         os.makedirs(target_directory, exist_ok=True, mode=0o777)

#         file_extension = pdf_upload.filename.split(".")[-1]
#         file_name = f'rake_quota_upload_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.{file_extension}'
#         full_path = os.path.join(os.getcwd(), target_directory, file_name)
#         with open(full_path, "wb") as file_object:
#             file_object.write(contents)
#         fetchPdfData = extract_quota_rail(full_path)
#         console_logger.debug(fetchPdfData)
#         if fetchPdfData:
#             try:
#                 checkRakeRecords = rakeQuota.objects.get(month=fetchPdfData.get("date"))
#                 # checkRakeRecords.month = fetchPdfData.get("date")
#                 checkRakeRecords.month = datetime.datetime.strptime(fetchPdfData.get("date"), "%b-%Y").strftime("%m-%Y")
#                 checkRakeRecords.year = fetchPdfData.get("date").split("-")[1]
#                 checkRakeRecords.valid_upto = fetchPdfData.get("valid")
#                 checkRakeRecords.coal_field = fetchPdfData.get("coal_field")
#                 checkRakeRecords.rake_alloted = fetchPdfData.get("no_rake").split("Rakes")[0]
#                 checkRakeRecords.grade = fetchPdfData.get("coal_grade")
#                 checkRakeRecords.save()
#             except DoesNotExist as e:
#                 insertRakeRecords = rakeQuota(
#                     # month=fetchPdfData.get("date"),
#                     month=datetime.datetime.strptime(fetchPdfData.get("date"), "%b-%Y").strftime("%m-%Y"),
#                     year=fetchPdfData.get("date").split("-")[1],
#                     valid_upto=fetchPdfData.get("valid"),
#                     coal_field=fetchPdfData.get("coal_field"),
#                     rake_alloted=fetchPdfData.get("no_rake").split("Rakes")[0],
#                     grade=fetchPdfData.get("coal_grade"),
#                     ID=rakeQuota.objects.count() + 1)
#                 insertRakeRecords.save()
#         return {"details": "success"}
#     except Exception as e:
#         console_logger.debug("----- Rake Quota Upload Error -----",e)
#         response.status_code = 400
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#         console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#         return e

@router.post("/rail/rakequotaupload", tags=["Rail Map"])
async def endpoint_to_upload_rake_data(response: Response, pdf_upload: Optional[UploadFile] = File(None)):
    try:
        if pdf_upload is None:
            return {"error": "No file uploaded"}
        contents = await pdf_upload.read()

        if not contents:
            return {"error": "Uploaded file is empty"}

        if not pdf_upload.filename.endswith(('.pdf','.PDF')):
            return {"error": "Uploaded file is not a PDF"}
        
        file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
        target_directory = f"static_server/gmr_ai/{file}"
        os.umask(0)
        os.makedirs(target_directory, exist_ok=True, mode=0o777)

        file_extension = pdf_upload.filename.split(".")[-1]
        file_name = f'rake_quota_upload_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.{file_extension}'
        full_path = os.path.join(os.getcwd(), target_directory, file_name)
        with open(full_path, "wb") as file_object:
            file_object.write(contents)
        fetchPdfData = extract_quota_rail(full_path)
        # console_logger.debug(fetchPdfData)
        return fetchPdfData
    except Exception as e:
        console_logger.debug("----- Rake Quota Upload Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/rail/rcrrakequotaupload", tags=["Rail Map"])
async def endpoint_to_upload_rcr_rake_data(response: Response, pdf_upload: Optional[UploadFile] = File(None)):
    try:
        if pdf_upload is None:
            return {"error": "No file uploaded"}
        
        contents = await pdf_upload.read()

        if not contents:
            return {"error": "Uploaded file is empty"}

        if not pdf_upload.filename.endswith(('.pdf','.PDF')):
            return {"error": "Uploaded file is not a PDF"}
        
        file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
        target_directory = f"static_server/gmr_ai/{file}"
        os.umask(0)
        os.makedirs(target_directory, exist_ok=True, mode=0o777)

        file_extension = pdf_upload.filename.split(".")[-1]
        file_name = f'rcr_rake_quota_upload_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.{file_extension}'
        full_path = os.path.join(os.getcwd(), target_directory, file_name)
        with open(full_path, "wb") as file_object:
            file_object.write(contents)
        fetchPdfData = extract_quota_rail(full_path)
        # console_logger.debug(fetchPdfData)
        return fetchPdfData
    except Exception as e:
        console_logger.debug("----- Rcr Rake Quota Upload Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/update/rail/rakequotaupload", tags=["Rail Map"])
async def endpoint_to_upload_rake_data(response: Response, data: rakequotaUpload):
    try:
        payload = data.dict()
        console_logger.debug(payload)
        if payload:
            try:
                checkRakeRecords = rakeQuota.objects.get(month=payload.get("month"))
                # checkRakeRecords.month = fetchPdfData.get("date")
                # checkRakeRecords.month = datetime.datetime.strptime(payload.get("month"), "%b-%Y").strftime("%m-%Y")
                checkRakeRecords.year = payload.get("year")
                checkRakeRecords.valid_upto = payload.get("valid")
                checkRakeRecords.coal_field = payload.get("coal_field")
                checkRakeRecords.rake_alloted = payload.get("rake_alloted")
                checkRakeRecords.grade = payload.get("grade")
                checkRakeRecords.source_type = payload.get("source_type")
                checkRakeRecords.save()
            except DoesNotExist as e:
                insertRakeRecords = rakeQuota(
                    # month=fetchPdfData.get("date"),
                    month=payload.get("month"),
                    year=payload.get("year"),
                    valid_upto=payload.get("valid"),
                    coal_field=payload.get("coal_field"),
                    rake_alloted=payload.get("rake_alloted"),
                    grade=payload.get("grade"),
                    source_type=payload.get("source_type"),
                    ID=rakeQuota.objects.count() + 1)
                insertRakeRecords.save()
        return {"details": "success"}
    except Exception as e:
        console_logger.debug("----- Rake Quota Upload Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/update/rail/rcrrakequotaupload", tags=["Rail Map"])
async def endpoint_to_upload_rcr_rake_data(response: Response, data: rakequotaUpload):
    try:
        payload = data.dict()
        if payload:
            try:
                checkRakeRecords = rcrrakeQuota.objects.get(month=payload.get("month"))
                # checkRakeRecords.month = fetchPdfData.get("date")
                # checkRakeRecords.month = datetime.datetime.strptime(payload.get("month"), "%b-%Y").strftime("%m-%Y")
                checkRakeRecords.year = payload.get("year")
                checkRakeRecords.valid_upto = payload.get("valid")
                checkRakeRecords.coal_field = payload.get("coal_field")
                checkRakeRecords.rake_alloted = payload.get("rake_alloted")
                checkRakeRecords.grade = payload.get("grade")
                checkRakeRecords.source_type = payload.get("source_type")
                checkRakeRecords.save()
            except DoesNotExist as e:
                insertRakeRecords = rcrrakeQuota(
                    # month=fetchPdfData.get("date"),
                    month=payload.get("month"),
                    year=payload.get("year"),
                    valid_upto=payload.get("valid"),
                    coal_field=payload.get("coal_field"),
                    rake_alloted=payload.get("rake_alloted"),
                    grade=payload.get("grade"),
                    source_type=payload.get("source_type"),
                    ID=rcrrakeQuota.objects.count() + 1)
                insertRakeRecords.save()
        return {"details": "success"}
    except Exception as e:
        console_logger.debug("----- Rake Quota Upload Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/fetch/sourcetype/rake", tags=['Rail Map'])
def endpoint_to_fetch_source_type_rake(response: Response):
    try:
        listData = []
        checkSourceType = rakeQuota.objects.only("source_type")
        checkRcrSourceType = rcrrakeQuota.objects.only("source_type")
        for single_data in checkSourceType:
            if single_data.source_type:
                listData.append(single_data.source_type)
        for single_rcr_data in checkRcrSourceType:
            if single_rcr_data.source_type:
                listData.append(single_rcr_data.source_type)
        return list(set(listData))
    except Exception as e:
        console_logger.debug("----- Rake Quota Source Type Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e
    


def extract_with_regex_rcr(pattern, text, group_index=1):
    try:
        match = re.search(pattern, text, re.MULTILINE)
        if match:
            return match.group(group_index).strip()
        return None  # or return a default value, e.g., "Not Found"
    except Exception as e:
        console_logger.debug("----- Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


def extract_with_regex_scheme_rcr(text):
    try:
        pattern = r"([A-Za-z\s\(\)\-]+)\s*Scheme Name\s*:"
        match = re.search(pattern, text, re.DOTALL)
        if match:
            text = " ".join(match.group(1).split()).strip()
            return text
        return None
    except Exception as e:
        console_logger.debug("----- Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


def extract_fields_rcr_data(pdf_path):
    try:
        reader = PdfReader(pdf_path)
        text = ""
        for page in reader.pages:
            text += page.extract_text()

        fields = {}
        sales_order_no = extract_with_regex_rcr(r"(\d+)\s*Sales Order Number", text)

        if len(sales_order_no) > 4:
            fields["rr_no"] = sales_order_no
            fields["rr_date"] = extract_with_regex_rcr(
                r"([\w\s,]+)(?=\s*Sales Order Date)", text
            )
            fields["start_date"] = extract_with_regex_rcr(
                r"([\w\s,]+)(?=\s*Sales Order Valid From)", text
            )
            fields["end_date"] = extract_with_regex_rcr(
                r"([\w\s,]+)(?=\s*Sales Order Valid To)", text
            )
            month = extract_with_regex_rcr(r"([\w\s,]+)(?=\s*Month)", text)
            if len(month) > 6:
                fields["month"] = month[-6:]

            fields["consumer_type"] = extract_with_regex_scheme_rcr(text)

            fields["grade"] = extract_with_regex_rcr(r"(\w+)\s+Grade Desc\s*:", text)
            fields["size"] = extract_with_regex_rcr(r"([\-\d\s\w]+)\s+Size\s*:", text)
            # Improved pattern for Plant extraction
            fields["mine"] = extract_with_regex_rcr(
                r"Line Item Mine Material Material Description HSN Code Unit of Measure Quantity\s*\n10\s+([^\d]+)",
                text,
            )

            # Extract Line Item and Quantity more generally
            fields["line_item"] = extract_with_regex_rcr(
                r"Line Item\s+Mine\s+Material.*\n(\d+)", text
            )
            fields["rr_qty"] = extract_with_regex_rcr(r"\b(\d{1,3}(?:,\d{3})*)\b\s*$", text)
            # New field: Total Net Amount
            fields["po_amount"] = extract_with_regex_rcr(r"TOTAL\s*:\s*([\d,]+\.\d{2})", text)
        else:
            fields["rr_no"] = extract_with_regex_rcr(r"Sales Order Number\s+:\s+(\d+)", text)
            fields["rr_date"] = extract_with_regex_rcr(
                r"Sales Order Date\s+:\s+([A-Za-z]+\s+\d{1,2},\s+\d{4})", text
            )
            fields["start_date"] = extract_with_regex_rcr(
                r"Sales Order Valid From\s+:\s+([A-Za-z]+\s+\d{1,2},\s+\d{4})", text
            )
            fields["end_date"] = extract_with_regex_rcr(
                r"Sales Order Valid To\s+:\s+([A-Za-z]+\s+\d{1,2},\s+\d{4})", text
            )
            fields["month"] = extract_with_regex_rcr(r"Month\s*:\s*(\d+)", text)
            fields["consumer_type"] = extract_with_regex_rcr(r"Scheme Name\s*:\s*(.*)", text)
            fields["grade"] = extract_with_regex_rcr(r"(?i)Grade Desc\s*:\s*(.*)", text)
            fields["size"] = extract_with_regex_rcr(r"Size\s*:\s*(\S+)\s*", text) + " MM"
            # Improved pattern for Plant extraction
            fields["mine"] = extract_with_regex_rcr(
                r"Line Item Mine Material Material Description HSN Code Unit of Measure Quantity\s*\n10\s+([^\d]+)",
                text,
            )
            # Extract Line Item and Quantity more generally
            fields["line_item"] = extract_with_regex_rcr(
                r"Line Item\s+Mine\s+Material.*\n(\d+)", text
            )
            fields["rr_qty"] = extract_with_regex_rcr(r"\b(\d{1,3}(?:,\d{3})*)\b\s*$", text)
            # New field: Total Net Amount
            fields["po_amount"] = extract_with_regex_rcr(r"TOTAL\s*:\s*([\d,]+\.\d{2})", text)
        return fields
    except Exception as e:
        console_logger.debug("----- Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/rail/saprecordsrcr", tags=["Rail Map"])
async def endpoint_to_upload_sap_data(response: Response, pdf_upload: Optional[UploadFile] = File(None)):
    try:
        if pdf_upload is None:
            return {"error": "No file uploaded"}
        contents = await pdf_upload.read()

        # Check if the file is empty
        if not contents:
            return {"error": "Uploaded file is empty"}

        if not pdf_upload.filename.endswith(('.pdf','.PDF')):
            return {"error": "Uploaded file is not a PDF"}
        
        file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
        target_directory = f"static_server/gmr_ai/{file}"
        os.umask(0)
        os.makedirs(target_directory, exist_ok=True, mode=0o777)

        file_extension = pdf_upload.filename.split(".")[-1]
        file_name = f'sap_rcr_upload_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.{file_extension}'
        full_path = os.path.join(os.getcwd(), target_directory, file_name)
        with open(full_path, "wb") as file_object:
            file_object.write(contents)

        fetchRcrData = extract_fields_rcr_data(full_path)

        console_logger.debug(fetchRcrData)

        if fetchRcrData:
            try:
                checkRcrRecords = sapRecordsRCR.objects.get(rr_no=fetchRcrData.get("rr_no"))
                # checkRcrRecords.month = fetchRcrData.get("date")
                checkRcrRecords.rr_date = datetime.datetime.strptime(fetchRcrData.get("rr_date"), "%b %d, %Y").strftime("%Y-%m-%d")
                checkRcrRecords.start_date = datetime.datetime.strptime(fetchRcrData.get("start_date"), '%b %d, %Y').strftime('%Y-%m-%d')
                checkRcrRecords.end_date = datetime.datetime.strptime(fetchRcrData.get("end_date"), '%b %d, %Y').strftime('%Y-%m-%d')
                checkRcrRecords.month = fetchRcrData.get("month")
                checkRcrRecords.consumer_type = fetchRcrData.get("consumer_type")
                checkRcrRecords.grade = f'{fetchRcrData.get("grade")} {fetchRcrData.get("size")}'
                checkRcrRecords.mine = fetchRcrData.get("mine")
                checkRcrRecords.line_item = fetchRcrData.get("line_item")
                checkRcrRecords.rr_qty = fetchRcrData.get("rr_qty")
                checkRcrRecords.po_amount = fetchRcrData.get("po_amount")
                checkRcrRecords.save()
            except DoesNotExist as e:
                insertRcrRecords = sapRecordsRCR(
                    rr_no=fetchRcrData.get("rr_no"),
                    rr_date=datetime.datetime.strptime(fetchRcrData.get("rr_date"), '%b %d, %Y').strftime('%Y-%m-%d'),
                    start_date=datetime.datetime.strptime(fetchRcrData.get("start_date"), '%b %d, %Y').strftime('%Y-%m-%d'),
                    end_date=datetime.datetime.strptime(fetchRcrData.get("end_date"), '%b %d, %Y').strftime('%Y-%m-%d'),
                    month=fetchRcrData.get("month"),
                    consumer_type=fetchRcrData.get("consumer_type"),
                    grade=f'{fetchRcrData.get("grade")} {fetchRcrData.get("size")}',
                    mine=fetchRcrData.get("mine"),
                    line_item=fetchRcrData.get("line_item"),
                    rr_qty=fetchRcrData.get("rr_qty"),
                    po_amount=fetchRcrData.get("po_amount"),
                    # id=sapRecordsRCR.objects.count() + 1
                    )
                insertRcrRecords.save()
        return {"details": "success"}
        
    except Exception as e:
        console_logger.debug("----- RCR Sap Upload Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

def extract_values_rail(text):
  
    item_pattern = r'''
        l\s*Tax\s+([A-Z\s]+OC\s+\(\s*\d+\s*\))\s+    # Mine
        (\d+)\s+                   # Material
        (G\d+/[^M]+MM)\s+          # Grade/Size
        ([^2]+)\s+                 # Description
        (\d+)\s+                   # HSN Code
        (\w+)\s+                   # UOM
        ([\d.]+)\s+                # Billed Quantity
        ([\d.]+)\s+                # STC Charges
        ([\d.]+)\s+                # Basic Rate
        ([\d.]+)\s+                # Basic Price
        ([\d.]+\n?[\d.]*)\s+       # STC Price
        ([\d.]+\n?[\d.]*)\s+       # Forest Tax
        ([\d.]+\n?[\d.]*)          # Terminal Tax
    '''
    item_match = re.search(item_pattern, text, re.VERBOSE | re.DOTALL)
    
    if item_match:
        def clean_numeric(value):
            return float(value.replace('\n', '').strip())
        return {
            'mine': item_match.group(1).replace('\n', '').strip(),
            'material': item_match.group(2).strip(),
            'grade_size': item_match.group(3).strip(),
            'description': item_match.group(4).strip(),
            'hsn_code': item_match.group(5).strip(),
            'uom': item_match.group(6).strip(),
            'billed_quantity': clean_numeric(item_match.group(7)),
            'stc_charges': clean_numeric(item_match.group(8)),
            'basic_rate': clean_numeric(item_match.group(9)),
            'basic_price': clean_numeric(item_match.group(10)),
            'stc_price': clean_numeric(item_match.group(11)),
            'forest_tax': clean_numeric(item_match.group(12)),
            'terminal_tax': clean_numeric(item_match.group(13)) if item_match.group(13).strip() else 0.0
        }

    return None

def extract_with_regex_rail(pattern, text, group_index=1):
    try:
        match = re.search(pattern, text,re.MULTILINE)
        if match:
            return match.group(group_index).strip()
        return None  # or return a default value, e.g., "Not Found"
    except Exception as e:
        console_logger.debug("----- Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

def extract_fields_rail_old(pdf_path):
    try:
        reader = PdfReader(pdf_path)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
        fields = {}    
        # Adjusted regex patterns for more accurate extraction
        fields["sale_order_date"] = extract_with_regex(r'\s*(\w+\s+\d{1,2},\s+\d{4})\s*Sale Order Date', text)
        fields["rr_no"] = extract_with_regex(r'\s*:\s*(\d+)\s*RR_NO\s*:', text)
        fields["rr_date"] = extract_with_regex(r'\s*:\s*(\w+\s+\d{1,2},\s+\d{4})\s*RR_DATE', text)
        fields["siding"] = extract_with_regex(r'\s*:\s*\d+(.*?)Siding', text)
        fields["mine"] = extract_with_regex( r"TaxTermina\s*l Tax[\s\S]*?([A-Z\s]+\(\s*\d{4}\s*\))\d{10}", text).replace('\n', ' ')
        fields["grade_size"] = extract_with_regex(r'([A-Z\d/-]+\s*MM)', text)
        fields["billed_quantity"] = extract_with_regex(r'(\d{4}\.\d{3})', text)
        fields["total_amount"] = extract_with_regex(r'(\d+\.\d+)\s+Total Amount:', text)
        return fields
    except Exception as e:
        console_logger.debug("----- Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

def extract_invoice_data_rail(text):
    data = {} 
    pattern = r'([A-Za-z\s\(\d)%\-]+?)\s+([\d.]*?)\s+([\d.,]+)'

    # Find matches for description, rate, and amount
    matches = re.findall(pattern, text)
    for match in matches:
        description, rate, amount = match        

        # Clean up the description to use as the key
        key = description.lower().replace(" ","_").strip('_').strip()
        if not key or key.replace('_', '').strip().isdigit():
            continue


        # Normalizing specific keys (you can extend this for others if needed)
        if 'sizing_charges' in key:
            key = 'sizing_charges'
        elif 'evac_facility_charge' in key:
            key = 'evac_facility_charge'
        elif 'royalty_charges' in key:
            key = 'royalty_charges'
        elif 'nmet' in key:
            key = 'nmet'
        elif 'dmf' in key:
            key = 'dmf'
        elif 'adho' in key:
            key = 'adho_sanrachna_vikas'
        elif 'pariyavaran' in key:
            key = 'pariyavaran_upkar'
        elif 'assessable_value' in key:
            key = 'assessable_value'
        elif 'igst' in key:
            key = 'igst'
        elif 'gst_comp_cess' in key:
            key = 'gst_comp_cess'
        elif 'gross_bill_value' in key:
            key = 'gross_bill_value'
        elif ')' in key:
            key = 'less_underloading_charges'
        elif 'net_value' in key:
            key = 'net_value'

        amount = amount.replace(',', '')

        if rate == '':
            data[f'{key}_rate'] = 0.0
        else:
            data[f'{key}_rate'] = float(rate)

        # Add amount
        data[f'{key}_amount'] = float(amount)
    
    return data

def extract_fields_rail(pdf_path):
    try:
        reader = PdfReader(pdf_path)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
            # print(text)
        fields = {} 

        with open(pdf_path, 'rb') as pdfFileObj:
            pdfReader = PyPDF3.PdfFileReader(pdfFileObj)
            pageObj = pdfReader.pages[0]
            mytext = pageObj.extractText()  

        fields["legacy_fsa_no"] = extract_with_regex(r'Legacy FSA No\.\s*:\s*(\d+)', text)
        fields["invoice_number"] = extract_with_regex(r'\s*(\w+\d+)\s*Invoice Number', text)
        fields["sap_ref_inv_no"] = extract_with_regex(r'\s*(\w+\d+)\s*SAP ref. Inv. No', text)
        fields["invoice_date"] = extract_with_regex(r'([A-Za-z]{3} \d{1,2}, \d{4})\s+Invoice Date\s*:', text)
        fields["sales_order"] = extract_with_regex(r'(\d+)\s+Sales Order\s*:', text)
        fields["sale_order_date"] = extract_with_regex(r'\s*(\w+\s+\d{1,2},\s+\d{4})\s*Sale Order Date', text)
        fields["mode_of_disptach"] = extract_with_regex(r'(\w+)\s+Mode Of Dispatch\s*:', text)
        fields["rr_no"] = extract_with_regex(r'\s*:\s*(\d+)\s*RR_NO\s*:', text)
        fields["rr_date"] = extract_with_regex(r'\s*:\s*(\w+\s+\d{1,2},\s+\d{4})\s*RR_DATE', text)
        fields["siding"] = extract_with_regex(r'\s*:\s*\d+(.*?)Siding', text)
        fields["sanction_no"] = extract_with_regex(r'Sanction No\.\s*:\s*([A-Za-z0-9/]+)', text)
        fields["stc_slab"] = extract_with_regex(r'STC Slab\s*:\s*(\([0-9\-KM]+\))', text)
        fields["billed_quantity"] = extract_with_regex(r'(\d{4}\.\d{3})', text)
        fields["mine"] = extract_with_regex( r"TaxTermina\s*l Tax[\s\S]*?([A-Z\s]+\(\s*\d{4}\s*\))\d{10}", text).replace('\n', ' ')
        fields["grade_size"] = extract_with_regex(r'([A-Z\d/-]+\s*MM)', text)
        fields["total_amount"] = extract_with_regex(r'(\d+\.\d+)\s+Total Amount:', text)
        fields['bill'] = extract_values_rail(mytext)
        fields['table'] = extract_invoice_data_rail(text)

        return fields
    except Exception as e:
        console_logger.debug("----- Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/rail/saprecords", tags=["Rail Map"])
async def endpoint_to_upload_rail_data(response: Response, pdf_upload: List[UploadFile] = File(...)):
    try:
        # pdfname starts with for eg:"Inv 162005063.PDF"
        for UploadedFile in pdf_upload:

            f_name = UploadedFile.filename
            contents = UploadedFile.file
            # if pdf_upload is None:
            #     return {"error": "No file uploaded"}
            # contents = await pdf_upload.read()

            if not contents:
                return {"error": "Uploaded file is empty"}

            if not f_name.endswith(('.pdf','.PDF')):
                return {"error": "Uploaded file is not a PDF"}
            
            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            file_extension = f_name.split(".")[-1]
            file_name = f'sap_rail_upload_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.{file_extension}'
            full_path = os.path.join(os.getcwd(), target_directory, file_name)
            with open(full_path, "wb") as file_object:
                shutil.copyfileobj(contents, file_object)

            fetchRailData = extract_fields_rail(full_path)
            console_logger.debug(fetchRailData)
            if fetchRailData:
                try:
                    # checkRailSapRecords = sapRecordsRail.objects.get(rr_no=fetchRailData.get("rr_no"), siding=fetchRailData.get("siding"))
                    checkRailSapRecords = sapRecordsRail.objects.get(rr_no=fetchRailData.get("rr_no"))
                    checkRailSapRecords.update(
                        month = datetime.datetime.strptime(fetchRailData.get("sale_order_date"), "%b %d, %Y").strftime("%Y-%m-%d"),
                        rr_date = datetime.datetime.strptime(fetchRailData.get("rr_date"), '%b %d, %Y').strftime('%Y-%m-%d'),
                        invoice_date=datetime.datetime.strptime(fetchRailData.get("invoice_date"), "%b %d, %Y").date(),
                        invoice_no=fetchRailData.get("invoice_number"),
                        sale_date=datetime.datetime.strptime(fetchRailData.get("sale_order_date"), "%b %d, %Y").date(),
                        siding = fetchRailData.get("siding"),
                        mine = fetchRailData.get("mine"),
                        grade = fetchRailData.get("grade_size"),
                        rr_qty = fetchRailData.get("billed_quantity"),
                        po_amount = fetchRailData.get("total_amount"),
                        sizing_charges = float(fetchRailData.get("table").get("sizing_charges_amount")),
                        evac_facility_charge = float(fetchRailData.get("table").get("evac_facility_charge_amount")),
                        royality_charges = float(fetchRailData.get("table").get("royalty_charges_amount")),
                        nmet_charges = float(fetchRailData.get("table").get("nmet_amount")),
                        dmf = float(fetchRailData.get("table").get("dmf_amount")),
                        adho_sanrachna_vikas= float(fetchRailData.get("table").get("adho_sanrachna_vikas_amount")),
                        pariyavaran_upkar = float(fetchRailData.get("table").get("pariyavaran_upkar_amount")),
                        assessable_value = float(fetchRailData.get("table").get("assessable_value_amount")),
                        igst = float(fetchRailData.get("table").get("igst_amount")),
                        gst_comp_cess = float(fetchRailData.get("table").get("gst_comp_cess_amount")),
                        gross_bill_value = float(fetchRailData.get("table").get("gross_bill_value_amount")),
                        less_underloading_charges = float(fetchRailData.get("table").get("less_underloading_charges_amount")),
                        net_value = float(fetchRailData.get("table").get("net_value_rate")),
                        total_amount = float(fetchRailData.get("total_amount")),
                    )
                except DoesNotExist as e:
                    insertRailSapRecords = sapRecordsRail(
                        rr_no=fetchRailData.get("rr_no"),
                        month=fetchRailData.get("sale_order_date"),
                        rr_date=datetime.datetime.strptime(fetchRailData.get("rr_date"), '%b %d, %Y').strftime('%Y-%m-%d'),
                        siding=fetchRailData.get("siding"),
                        mine=fetchRailData.get("mine"),
                        grade=fetchRailData.get("grade_size"),
                        rr_qty=fetchRailData.get("billed_quantity"),
                        po_amount=fetchRailData.get("total_amount"),
                        invoice_date=datetime.datetime.strptime(fetchRailData.get("invoice_date"), "%b %d, %Y").date(),
                        invoice_no=fetchRailData.get("invoice_number"),
                        sale_date=datetime.datetime.strptime(fetchRailData.get("sale_order_date"), "%b %d, %Y").date(),
                        sizing_charges = float(fetchRailData.get("table").get("sizing_charges_amount")),
                        evac_facility_charge = float(fetchRailData.get("table").get("evac_facility_charge_amount")),
                        royality_charges = float(fetchRailData.get("table").get("royalty_charges_amount")),
                        nmet_charges = float(fetchRailData.get("table").get("nmet_amount")),
                        dmf = float(fetchRailData.get("table").get("dmf_amount")),
                        adho_sanrachna_vikas= float(fetchRailData.get("table").get("adho_sanrachna_vikas_amount")),
                        pariyavaran_upkar = float(fetchRailData.get("table").get("pariyavaran_upkar_amount")),
                        assessable_value = float(fetchRailData.get("table").get("assessable_value_amount")),
                        igst = float(fetchRailData.get("table").get("igst_amount")),
                        gst_comp_cess = float(fetchRailData.get("table").get("gst_comp_cess_amount")),
                        gross_bill_value = float(fetchRailData.get("table").get("gross_bill_value_amount")),
                        less_underloading_charges = float(fetchRailData.get("table").get("less_underloading_charges_amount")),
                        net_value = float(fetchRailData.get("table").get("net_value_rate")),
                        total_amount = float(fetchRailData.get("total_amount")),
                        )
                    insertRailSapRecords.save()
            
                try:
                    checkRailData = RailData.objects(rr_no=fetchRailData.get("rr_no"), siding=fetchRailData.get("siding"))
                    for singleCheckRailData in checkRailData:
                        singleCheckRailData.month = datetime.datetime.strptime(fetchRailData.get("sale_order_date"), '%b %d, %Y').strftime('%Y-%m-%d')
                        singleCheckRailData.rr_date = datetime.datetime.strptime(fetchRailData.get("rr_date"), '%b %d, %Y').strftime('%Y-%m-%d')
                        singleCheckRailData.siding = fetchRailData.get("siding")
                        singleCheckRailData.mine = fetchRailData.get("mine")
                        singleCheckRailData.grade = fetchRailData.get("grade_size")
                        singleCheckRailData.rr_qty = fetchRailData.get("billed_quantity")
                        singleCheckRailData.po_amount = fetchRailData.get("total_amount")
                        singleCheckRailData.save()
                except DoesNotExist as e:
                    pass
                # if "/" in fetchRailData.get("grade_size"):
                #         gradeData = fetchRailData.get("grade_size").split("/")
                #         finalGrade = f"{gradeData[0]}{gradeData[1]}"
                #     else:
                #         finalGrade = fetchRailData.get("grade_size")
                # try:
                #     checkGrnData = Grn.objects.get(do_no=fetchRailData.get("rr_no"), invoice_no=fetchRailData.get("invoice_number"))
                #     checkGrnData.update(
                #         invoice_date=str(datetime.datetime.strptime(fetchRailData.get("invoice_date"), "%b %d, %Y").date()),
                #         sale_date=str(datetime.datetime.strptime(fetchRailData.get("sale_order_date"), "%b %d, %Y").date()),
                #         grade=finalGrade,
                #         mine=fetchRailData.get("mine"),
                #         mode="rail",
                #         sizing_charges_rate=fetchRailData.get("table").get("sizing_charges_rate"),
                #         sizing_charges_amount=fetchRailData.get("table").get("sizing_charges_amount"),
                #         evac_facility_charge_rate=fetchRailData.get("table").get("evac_facility_charge_rate"),
                #         evac_facility_charge_amount=fetchRailData.get("table").get("evac_facility_charge_amount"),
                #         royalty_charges_rate=fetchRailData.get("table").get("royalty_charges_rate"),
                #         royalty_charges_amount= fetchRailData.get("table").get("royalty_charges_amount"),
                #         nmet_rate=fetchRailData.get("table").get("nmet_rate"),
                #         nmet_amount=fetchRailData.get("table").get("nmet_amount"),
                #         dmf_rate=fetchRailData.get("table").get("dmf_rate"),
                #         dmf_amount=fetchRailData.get("table").get("dmf_amount"),
                #         adho_sanrachna_vikas_rate=fetchRailData.get("table").get("adho_sanrachna_vikas_rate"),
                #         adho_sanrachna_vikas_amount=fetchRailData.get("table").get("adho_sanrachna_vikas_amount"),
                #         pariyavaran_upkar_rate=fetchRailData.get("table").get("pariyavaran_upkar_rate"),
                #         pariyavaran_upkar_amount=fetchRailData.get("table").get("pariyavaran_upkar_amount"),
                #         assessable_value_rate=fetchRailData.get("table").get("assessable_value_rate"),
                #         assessable_value_amount=fetchRailData.get("table").get("assessable_value_amount"),
                #         igst_rate=fetchRailData.get("table").get("igst_rate"),
                #         igst_amount=fetchRailData.get("table").get("igst_amount"),
                #         gst_comp_cess_rate=fetchRailData.get("table").get("gst_comp_cess_rate"),
                #         gst_comp_cess_amount=fetchRailData.get("table").get("gst_comp_cess_amount"),
                #         gross_bill_value_rate=fetchRailData.get("table").get("gross_bill_value_rate"),
                #         gross_bill_value_amount=fetchRailData.get("table").get("gross_bill_value_amount"),
                #         less_underloading_charges_rate=fetchRailData.get("table").get("less_underloading_charges_rate"),
                #         less_underloading_charges_amount=fetchRailData.get("table").get("less_underloading_charges_amount"),
                #         net_value_rate=fetchRailData.get("table").get("net_value_rate"),
                #         net_value_amount=fetchRailData.get("table").get("net_value_amount"),
                #     )
                    
                # except DoesNotExist as e:
                #     Grn(
                #         do_no=fetchRailData.get("rr_no"),
                #         invoice_date=str(datetime.datetime.strptime(fetchRailData.get("invoice_date"), "%b %d, %Y").date()),
                #         invoice_no=fetchRailData.get("invoice_number"),
                #         sale_date=str(datetime.datetime.strptime(fetchRailData.get("sale_order_date"), "%b %d, %Y").date()),
                #         grade=finalGrade,
                #         mine=fetchRailData.get("mine"),
                #         mode="rail",
                #         sizing_charges_rate=fetchRailData.get("table").get("sizing_charges_rate"),
                #         sizing_charges_amount=fetchRailData.get("table").get("sizing_charges_amount"),
                #         evac_facility_charge_rate=fetchRailData.get("table").get("evac_facility_charge_rate"),
                #         evac_facility_charge_amount=fetchRailData.get("table").get("evac_facility_charge_amount"),
                #         royalty_charges_rate=fetchRailData.get("table").get("royalty_charges_rate"),
                #         royalty_charges_amount= fetchRailData.get("table").get("royalty_charges_amount"),
                #         nmet_rate=fetchRailData.get("table").get("nmet_rate"),
                #         nmet_amount=fetchRailData.get("table").get("nmet_amount"),
                #         dmf_rate=fetchRailData.get("table").get("dmf_rate"),
                #         dmf_amount=fetchRailData.get("table").get("dmf_amount"),
                #         adho_sanrachna_vikas_rate=fetchRailData.get("table").get("adho_sanrachna_vikas_rate"),
                #         adho_sanrachna_vikas_amount=fetchRailData.get("table").get("adho_sanrachna_vikas_amount"),
                #         pariyavaran_upkar_rate=fetchRailData.get("table").get("pariyavaran_upkar_rate"),
                #         pariyavaran_upkar_amount=fetchRailData.get("table").get("pariyavaran_upkar_amount"),
                #         assessable_value_rate=fetchRailData.get("table").get("assessable_value_rate"),
                #         assessable_value_amount=fetchRailData.get("table").get("assessable_value_amount"),
                #         igst_rate=fetchRailData.get("table").get("igst_rate"),
                #         igst_amount=fetchRailData.get("table").get("igst_amount"),
                #         gst_comp_cess_rate=fetchRailData.get("table").get("gst_comp_cess_rate"),
                #         gst_comp_cess_amount=fetchRailData.get("table").get("gst_comp_cess_amount"),
                #         gross_bill_value_rate=fetchRailData.get("table").get("gross_bill_value_rate"),
                #         gross_bill_value_amount=fetchRailData.get("table").get("gross_bill_value_amount"),
                #         less_underloading_charges_rate=fetchRailData.get("table").get("less_underloading_charges_rate"),
                #         less_underloading_charges_amount=fetchRailData.get("table").get("less_underloading_charges_amount"),
                #         net_value_rate=fetchRailData.get("table").get("net_value_rate"),
                #         net_value_amount=fetchRailData.get("table").get("net_value_amount"),
                #     ).save()

        return {"details": "success"}      
    except Exception as e:
        console_logger.debug("----- Rake Quota Upload Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e



@router.get("/fetch/saprcr", tags=["Rail Map"])
def endpoint_to_fetch_saprcr_data(response: Response, currentPage: Optional[int] = None, perPage: Optional[int] = None, search_text: Optional[str] = None, start_timestamp: Optional[str] = None, end_timestamp: Optional[str] = None, month_date: Optional[str] = None, type: Optional[str] = "display"):
    try:
        result = {        
                "labels": [],
                "datasets": [],
                "total" : 0,
                "page_size": 15
        }
        if type and type == "display":
            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage

            data = Q()

            # based on condition for timestamp playing with & and | 
            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(created_at__gte = start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M","Asia/Kolkata",False)
                data &= Q(created_at__lte = end_date)

            if search_text:
                if search_text.isdigit():
                    data &= Q(rr_no__icontains=search_text)
                else:
                    data &= (Q(mine__icontains=search_text))

            if month_date:
                start_date = f'{month_date}-01'
                startd_date=datetime.datetime.strptime(f"{start_date}T00:00","%Y-%m-%dT%H:%M")
                end_date = (datetime.datetime.strptime(start_date, "%Y-%m-%d") + relativedelta(day=31)).strftime("%Y-%m-%d")
                # endd_date= f"{end_date}T23:59"
                data &= Q(placement_date__gte = startd_date.strftime("%Y-%m-%dT%H:%M"))
                data &= Q(placement_date__lte = f"{end_date}T23:59")

            offset = (page_no - 1) * page_len
            # listData = []
            logs = (
                RcrData.objects(data, avery_placement_date=None)
                .order_by("-created_at")
                .skip(offset)
                .limit(page_len)
            )   
            if any(logs):
                for log in logs:
                    result["labels"] = list(log.simplepayload().keys())
                    result["datasets"].append(log.simplepayload())
                result["total"]= len(RcrData.objects(data))
                return result
            else:
                return result
        elif type and type == "download":
            del type

            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            # Constructing the base for query
            data = Q()

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(created_at__gte = start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M","Asia/Kolkata",False)
                data &= Q(created_at__lte = end_date)
            
            if search_text:
                if search_text.isdigit():
                    data &= Q(rr_no__icontains=search_text)
                else:
                    data &= (Q(mine__icontains=search_text))

            usecase_data = RcrData.objects(data, avery_placement_date=None).order_by("-created_at")
            count = len(usecase_data)
            path = None
            logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
            if usecase_data:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "Rail_rcr_Report_{}.xlsx".format(
                            datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format()
                    cell_format2.set_bold()
                    cell_format2.set_font_size(10)
                    cell_format2.set_align("center")
                    cell_format2.set_align("vcenter")
                    cell_format2.set_text_wrap(True)
                    cell_format2.set_border(1)

                    header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
                    date_format = workbook.add_format({'align': 'center', 'font_size': 12, "bold": True})
                    report_name_format = workbook.add_format({'align': 'center', 'font_size': 15, "bold": True})

                    header_format.set_align("vcenter")
                    date_format.set_align("vcenter")
                    report_name_format.set_align("vcenter")
                    header_format.set_border(1)
                    date_format.set_border(1)
                    report_name_format.set_border(1)

                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)
                    cell_format = workbook.add_format()
                    cell_format.set_font_size(10)
                    cell_format.set_align("center")
                    cell_format.set_align("vcenter")

                    cell_format.set_text_wrap(True)
                    cell_format.set_border(1)

                    worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                    
                    # Merge cells for the main header and place it in the center
                    main_header = "GMR Warora Energy Limited"  # Set your main header text here
                    worksheet.merge_range("A1:AK1", main_header, header_format)  # Merge cells A1 to H1 for the header
                    
                    # Write the current date on the left side (A2)
                    worksheet.write("A2", f"Date: {end_date.strftime('%d-%m-%Y')}", date_format)
                    worksheet.merge_range("C2:AK2", f"Rcr Rail Coal Journey", report_name_format)

                    headers = [
                        "srno",
                        "rr_no",
                        "rr_qty",
                        "po_no",
                        "po_date",
                        "line_item",
                        "source",
                        "placement_date",
                        "completion_date",
                        "avery_placement_date",
                        "avery_completion_date",
                        "drawn_date",
                        "total_ul_wt",
                        "boxes_supplied",
                        "total_secl_gross_wt",
                        "total_secl_tare_wt",
                        "total_secl_net_wt",
                        "total_secl_ol_wt",
                        "boxes_loaded",
                        "total_rly_gross_wt",
                        "total_rly_tare_wt",
                        "total_rly_net_wt",
                        "total_rly_ol_wt",
                        "total_secl_chargable_wt",
                        "total_rly_chargable_wt",
                        "freight",
                        "gst",
                        "pola",
                        "total_freight",
                        "source_type",
                        "month",
                        "rr_date",
                        "siding",
                        "mine",
                        "grade",
                        "po_amount",
                        "rake_no",
                        "created_at"
                    ]
                   
                    for index, header in enumerate(headers):
                        worksheet.write(2, index, header, cell_format2)

                    for row, query in enumerate(usecase_data, start=3):
                        result = query.payload()
                        worksheet.write(row, 0, count, cell_format)     
                        worksheet.write(row, 1, str(result["rr_no"]), cell_format)                      
                        worksheet.write(row, 2, str(result["rr_qty"]), cell_format)                      
                        worksheet.write(row, 3, str(result["po_no"]), cell_format)                      
                        worksheet.write(row, 4, str(result["po_date"]), cell_format)                      
                        worksheet.write(row, 5, str(result["line_item"]), cell_format)                      
                        worksheet.write(row, 6, str(result["source"]), cell_format)                      
                        worksheet.write(row, 7, str(result["placement_date"]), cell_format)                      
                        worksheet.write(row, 8, str(result["completion_date"]), cell_format)                      
                        worksheet.write(row, 9, str(result["avery_placement_date"]), cell_format)                      
                        worksheet.write(row, 10, str(result["avery_completion_date"]), cell_format)                      
                        worksheet.write(row, 11, str(result["drawn_date"]), cell_format)                      
                        worksheet.write(row, 12, str(result["total_ul_wt"]), cell_format)                   
                        worksheet.write(row, 13, str(result["boxes_supplied"]), cell_format)                   
                        worksheet.write(row, 14, str(result["total_secl_gross_wt"]), cell_format)                   
                        worksheet.write(row, 15, str(result["total_secl_tare_wt"]), cell_format)                   
                        worksheet.write(row, 16, str(result["total_secl_net_wt"]), cell_format)                   
                        worksheet.write(row, 17, str(result["total_secl_ol_wt"]), cell_format)                   
                        worksheet.write(row, 18, str(result["boxes_loaded"]), cell_format)                   
                        worksheet.write(row, 19, str(result["total_rly_gross_wt"]), cell_format)                   
                        worksheet.write(row, 20, str(result["total_rly_tare_wt"]), cell_format)                   
                        worksheet.write(row, 21, str(result["total_rly_net_wt"]), cell_format)                   
                        worksheet.write(row, 22, str(result["total_rly_ol_wt"]), cell_format)                   
                        worksheet.write(row, 23, str(result["total_secl_chargable_wt"]), cell_format)                   
                        worksheet.write(row, 24, str(result["total_rly_chargable_wt"]), cell_format)                   
                        worksheet.write(row, 25, str(result["freight"]), cell_format)                   
                        worksheet.write(row, 26, str(result["pola"]), cell_format)                   
                        worksheet.write(row, 27, str(result["total_freight"]), cell_format)                   
                        worksheet.write(row, 28, str(result["source_type"]), cell_format)                   
                        worksheet.write(row, 29, str(result["month"]), cell_format)                   
                        worksheet.write(row, 30, str(result["rr_date"]), cell_format)                   
                        worksheet.write(row, 31, str(result["mine"]), cell_format)                   
                        worksheet.write(row, 32, str(result["grade"]), cell_format)                   
                        worksheet.write(row, 33, str(result["po_amount"]), cell_format)                   
                        worksheet.write(row, 34, str(result["rake_no"]), cell_format)                   
                        worksheet.write(row, 35, str(result["created_at"]), cell_format)                   
                        
                        count-=1
                        
                    workbook.close()
                    console_logger.debug("Successfully {} report generated".format(service_id))
                    console_logger.debug("sent data {}".format(path))

                    return {
                            "Type": "gmr_rcr_rail_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                            }
                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            else:
                console_logger.error("No data found")
                return {
                        "Type": "gmr_rcr_rail_download_event",
                        "Datatype": "Report",
                        "File_Path": path,
                        }
    except Exception as e:
        console_logger.debug("----- Fetch RCR Report Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/fetch/rcrroaddata", tags=["Rail Map"])
def endpoint_to_fetch_rcr_road_data(response: Response, currentPage: Optional[int] = None, perPage: Optional[int] = None, search_text: Optional[str] = None, start_timestamp: Optional[str] = None, end_timestamp: Optional[str] = None, month_date: Optional[str] = None, type: Optional[str] = "display"):
    try:
        result = {        
                "labels": [],
                "datasets": [],
                "total" : 0,
                "page_size": 15
        }
        if type and type == "display":
            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage

            data = Q()

            # based on condition for timestamp playing with & and | 
            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(tar_wt_date__gte = start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M","Asia/Kolkata",False)
                data &= Q(tar_wt_date__lte = end_date)

            if search_text:
                if search_text.isdigit():
                    data &= Q(do_number__icontains=search_text)
                else:
                    data &= (Q(mine__icontains=search_text))

            if month_date:
                start_date = f'{month_date}-01'
                startd_date=datetime.datetime.strptime(f"{start_date}T00:00","%Y-%m-%dT%H:%M")
                end_date = (datetime.datetime.strptime(start_date, "%Y-%m-%d") + relativedelta(day=31)).strftime("%Y-%m-%d")
                # endd_date= f"{end_date}T23:59"
                data &= Q(tar_wt_date__gte = startd_date.strftime("%Y-%m-%dT%H:%M"))
                data &= Q(tar_wt_date__lte = f"{end_date}T23:59")

            offset = (page_no - 1) * page_len
            # listData = []
            logs = (
                RcrRoadData.objects(data)
                .order_by("-created_at")
                .skip(offset)
                .limit(page_len)
            )   
            if any(logs):
                for log in logs:
                    result["labels"] = list(log.payload().keys())
                    result["datasets"].append(log.payload())
                result["total"]= len(RcrRoadData.objects(data))
                return result
            else:
                return result
        elif type and type == "download":
            del type

            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            # Constructing the base for query
            data = Q()

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(tar_wt_date__gte = start_date)

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M","Asia/Kolkata",False)
                data &= Q(tar_wt_date__lte = end_date)
            
            if search_text:
                if search_text.isdigit():
                    data &= Q(rr_no__icontains=search_text)
                else:
                    data &= (Q(mine__icontains=search_text))

            usecase_data = RcrRoadData.objects(data).order_by("-tar_wt_date")
            count = len(usecase_data)
            path = None
            logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
            if usecase_data:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "RCR_road_data_Report_{}.xlsx".format(
                            datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format()
                    cell_format2.set_bold()
                    cell_format2.set_font_size(10)
                    cell_format2.set_align("center")
                    cell_format2.set_align("vcenter")
                    cell_format2.set_text_wrap(True)
                    cell_format2.set_border(1)

                    header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
                    date_format = workbook.add_format({'align': 'center', 'font_size': 12, "bold": True})
                    report_name_format = workbook.add_format({'align': 'center', 'font_size': 15, "bold": True})

                    header_format.set_align("vcenter")
                    date_format.set_align("vcenter")
                    report_name_format.set_align("vcenter")
                    header_format.set_border(1)
                    date_format.set_border(1)
                    report_name_format.set_border(1)

                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)
                    cell_format = workbook.add_format()
                    cell_format.set_font_size(10)
                    cell_format.set_align("center")
                    cell_format.set_align("vcenter")
                    cell_format.set_text_wrap(True)
                    cell_format.set_border(1)

                    worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                    
                    # Merge cells for the main header and place it in the center
                    main_header = "GMR Warora Energy Limited"  # Set your main header text here
                    worksheet.merge_range("A1:AH1", main_header, header_format)  # Merge cells A1 to H1 for the header
                    
                    # Write the current date on the left side (A2)
                    worksheet.write("A2", f"Date: {end_date.strftime('%d-%m-%Y')}", date_format)
                    worksheet.merge_range("C2:AH2", f"RCR Road Coal Journey", report_name_format)

                    headers = [
                        "Sr. No.",
                        "Do Number",
                        "RRS WT Date",
                        "GRS WT Time",
                        "Received Gross Weight",
                        "Tar WT Date",
                        "Tar WT Time",
                        "Received Tare Weight",
                        "Received Net Weight",
                        "Unloading Slip Number",
                        "Vehicle number",
                        "Transporter",
                        "TP Number",
                        "Mine",
                        "Secl Delivery Challan Number",
                        "DC Gross WT",
                        "DC Tare WT",
                        "DC Net WT",
                        "Loading Date",
                        "Out Time",
                        "Lr No",
                        "Lr Date",
                        "Sap PO",
                        "Line Item",
                        "Po Date",
                        "Do Date",
                        "Start Date",
                        "End Date",
                        "SLNO",
                        "Type Consumer",
                        "Grade",
                        "Po Qty",
                        "Po Amount",
                        "Created At"
                    ]
                   
                    for index, header in enumerate(headers):
                        worksheet.write(2, index, header, cell_format2)

                    for row, query in enumerate(usecase_data, start=3):
                        result = query.payload()
                        worksheet.write(row, 0, count, cell_format)     
                        worksheet.write(row, 1, str(result["do_number"]), cell_format)                      
                        worksheet.write(row, 2, str(result["rrs_wt_date"]), cell_format)                      
                        worksheet.write(row, 3, str(result["grs_wt_time"]), cell_format)                      
                        worksheet.write(row, 4, float(result["received_gross_weight"]), cell_format)                      
                        worksheet.write(row, 5, str(result["tar_wt_date"]), cell_format)                      
                        worksheet.write(row, 6, str(result["tar_wt_time"]), cell_format)                      
                        worksheet.write(row, 7, float(result["received_tare_weight"]), cell_format)                      
                        worksheet.write(row, 8, float(result["received_net_weight"]), cell_format)                      
                        worksheet.write(row, 9, str(result["unloading_slip_number"]), cell_format)                      
                        worksheet.write(row, 10, str(result["vehicle_no"]), cell_format)                      
                        worksheet.write(row, 11, str(result["transporter"]), cell_format)                      
                        worksheet.write(row, 12, str(result["tp_number"]), cell_format)                      
                        worksheet.write(row, 13, str(result["mine"]), cell_format)                   
                        worksheet.write(row, 14, str(result["secl_delivery_challan_number"]), cell_format)                   
                        worksheet.write(row, 15, float(result["dc_gross_wt"]), cell_format)                   
                        worksheet.write(row, 16, float(result["dc_tare_wt"]), cell_format)                   
                        worksheet.write(row, 17, float(result["dc_net_wt"]), cell_format)                   
                        worksheet.write(row, 18, str(result["loading_date"]), cell_format)                   
                        worksheet.write(row, 19, str(result["out_time"]), cell_format)                   
                        worksheet.write(row, 20, str(result["lr_no"]), cell_format)                   
                        worksheet.write(row, 21, str(result["lr_date"]), cell_format)                   
                        worksheet.write(row, 22, str(result["sap_po"]), cell_format)                   
                        worksheet.write(row, 23, str(result["line_item"]), cell_format)                   
                        worksheet.write(row, 24, str(result["po_date"]), cell_format)                   
                        worksheet.write(row, 25, str(result["do_date"]), cell_format)                   
                        worksheet.write(row, 26, str(result["start_date"]), cell_format)                   
                        worksheet.write(row, 27, str(result["end_date"]), cell_format)                   
                        worksheet.write(row, 28, str(result["slno"]), cell_format)                   
                        worksheet.write(row, 29, str(result["type_consumer"]), cell_format)                   
                        worksheet.write(row, 30, str(result["grade"]), cell_format)                   
                        worksheet.write(row, 31, float(result["po_qty"]), cell_format)
                        if result.get("po_amount") == "Not found":                
                            worksheet.write(row, 32, str(result["po_amount"]), cell_format)
                        else:                 
                            worksheet.write(row, 32, float(result["po_amount"]), cell_format)                   
                        worksheet.write(row, 33, str(result["created_at"]), cell_format)                   
                        
                        count-=1
                        
                    workbook.close()
                    console_logger.debug("Successfully {} report generated".format(service_id))
                    console_logger.debug("sent data {}".format(path))

                    return {
                            "Type": "gmr_rcr_road_data_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                            }
                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            else:
                console_logger.error("No data found")
                return {
                        "Type": "gmr_rcr_road_data_download_event",
                        "Datatype": "Report",
                        "File_Path": path,
                        }
    except Exception as e:
        console_logger.debug("----- Fetch RCR Report Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


def is_nan(value):
    return value == 'nan' or value != value


# def convert_time_to_hms(time_str):
#     if isinstance(time_str, time):
#         # Convert time object to string with the required format
#         time_str = time_str.strftime("%I:%M %p")

#     # Now parse the time string
#     time_obj = datetime.datetime.strptime(time_str, "%I:%M %p")
#     # time_obj = datetime.datetime.strptime(time_str, "%I:%M %p")
#     return time_obj.strftime("%H:%M:%S")


# def convert_time_to_hms(time_str):
#     if isinstance(time_str, time):
#         # Convert time object to string with the required format
#         time_str = time_str.strftime("%I:%M %p")
    
#     # Ensure there's a space between the time and "AM"/"PM"
#     if time_str[-2:] in ["AM", "PM"]:
#         time_str = time_str[:-2] + " " + time_str[-2:]

#     if "." in time_str:
#         time_str = time_str.replace(".", ":")

#     # Parse the time string
#     time_obj = datetime.datetime.strptime(time_str, "%I:%M %p")
#     return time_obj.strftime("%H:%M:%S")


def convert_time_to_hms(time_str):
    # If time_str is already a datetime object, convert it to a string
    if isinstance(time_str, datetime.datetime):
        time_str = time_str.strftime("%I:%M %p")
    elif isinstance(time_str, time):
        # If it's a time object, convert to string in the required format
        time_str = time_str.strftime("%I:%M %p")
    
    # Ensure time_str is in string format
    if isinstance(time_str, str):
        # Ensure there's a space between the time and "AM"/"PM"
        if time_str[-2:] in ["AM", "PM"]:
            time_str = time_str[:-2] + " " + time_str[-2:]

        # Replace any dot with a colon for parsing
        time_str = time_str.replace(".", ":")

        # Parse the time string
        time_obj = datetime.datetime.strptime(time_str, "%I:%M %p")
        return time_obj.strftime("%H:%M:%S")

    # If the input is not a recognized format, raise a TypeError
    raise TypeError("Input must be a string, time, or datetime object")


@router.post("/rcr_mode_excel", tags=["Rail Map"])                                   
async def rcr_mode_excel(response: Response, file: UploadFile = File(...)):
    
    try:
        if file is None:
            return {"error": "No file Uploaded!"}
        
        contents = await file.read()
        
        if not contents:
            return {"error": "Uploaded file is empty!"}

        if file.filename.endswith(".xlsx"):
            # file saving start
            date = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{date}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)
            file_extension = file.filename.split(".")[-1]
            file_name = f'secl_annexure_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.{file_extension}'
            full_path = os.path.join(os.getcwd(), target_directory, file_name)
            with open(full_path, "wb") as file_object:
                file_object.write(contents)
            excel_data = pd.read_excel(BytesIO(contents), sheet_name="Details", header=1)

            # Drop rows where all values are NaN
            excel_data = excel_data.dropna(how='all')

            excel_data.columns = excel_data.columns.str.strip()

            excel_data = excel_data.reset_index(drop=True)

            if "Grs.Wt. Date" in excel_data.columns and "Grs.Wt. Time" in excel_data.columns:
                excel_data["Grs.Wt. Date"] = pd.to_datetime(excel_data["Grs.Wt. Date"], errors='coerce').dt.date  # Only date part
                excel_data["Grs.Wt. Time"] = pd.to_datetime(excel_data["Grs.Wt. Time"], format='%H:%M', errors='coerce').dt.time  # Only time part
            else:
                print("One or more required columns are missing")

            dataa = excel_data.fillna(0)

            json_data = dataa.to_dict(orient="records")

            for single_data in json_data[1:-1]:  # Skip first and last item in json_data
                # Check if all values are zero (including float zeros)
                if all(value == 0 or pd.isna(value) for value in single_data.values()):
                    continue  # Skip this iteration if all values are zero or NaN

                single_data_value_named5 = single_data.get('Unnamed: 5')

                if isinstance(single_data_value_named5, str):
                    if re.match(r"^\d{2}-\d{2}-\d{4}$", single_data_value_named5):
                        tare_dt_tm = datetime.datetime.strptime(
                            f"{single_data_value_named5}T{convert_time_to_hms(single_data.get('Unnamed: 6'))}", "%d-%m-%YT%H:%M:%S"
                        )
                    elif re.match(r"^\d{2}/\d{2}/\d{4}$", single_data_value_named5):
                        tare_dt_tm = datetime.datetime.strptime(
                            f"{single_data_value_named5}T{convert_time_to_hms(single_data.get('Unnamed: 6'))}", "%d/%m/%YT%H:%M:%S"
                        )
                    elif re.match(r"^\d{4}-\d{2}-\d{2}$", single_data_value_named5):
                        tare_dt_tm = datetime.datetime.strptime(
                            f"{single_data_value_named5}T{convert_time_to_hms(single_data.get('Unnamed: 6'))}", "%Y-%m-%dT%H:%M:%S"
                        )
                elif isinstance(single_data_value_named5, datetime.datetime):
                    tare_dt_tm = datetime.datetime.strptime(
                        f"{single_data_value_named5.strftime('%Y-%m-%d')}T{convert_time_to_hms(single_data.get('Unnamed: 6'))}", "%Y-%m-%dT%H:%M:%S"
                    )

                # Grs.Wt. Date
                single_data_value_named2 = single_data.get('Unnamed: 2')

                if isinstance(single_data_value_named2, str):
                    if re.match(r"^\d{2}-\d{2}-\d{4}$", single_data_value_named2):
                        grs_wt_date = datetime.datetime.strptime(
                            f"{single_data_value_named2}T{convert_time_to_hms(single_data.get('Unnamed: 3'))}", "%d-%m-%YT%H:%M:%S"
                        )
                    elif re.match(r"^\d{2}/\d{2}/\d{4}$", single_data_value_named2):
                        grs_wt_date = datetime.datetime.strptime(
                            f"{single_data_value_named2}T{convert_time_to_hms(single_data.get('Unnamed: 3'))}", "%d/%m/%YT%H:%M:%S"
                        )
                    elif re.match(r"^\d{4}-\d{2}-\d{2}$", single_data_value_named2):
                        grs_wt_date = datetime.datetime.strptime(
                            f"{single_data_value_named2}T{convert_time_to_hms(single_data.get('Unnamed: 3'))}", "%Y-%m-%dT%H:%M:%S"
                        )
                elif isinstance(single_data_value_named2, datetime.datetime):
                    grs_wt_date = datetime.datetime.strptime(
                        f"{single_data_value_named2.strftime('%Y-%m-%d')}T{convert_time_to_hms(single_data.get('Unnamed: 3'))}", "%Y-%m-%dT%H:%M:%S"
                    )

                
                loading_date_var = single_data.get('Unnamed: 19')

                if isinstance(loading_date_var, str):
                    if re.match(r"^\d{2}-\d{2}-\d{4}$", loading_date_var):
                        load_date = datetime.datetime.strptime(
                            f"{loading_date_var}", "%d-%m-%Y"
                        )
                    elif re.match(r"^\d{2}/\d{2}/\d{4}$", loading_date_var):
                        load_date = datetime.datetime.strptime(
                            f"{loading_date_var}", "%d/%m/%Y"
                        )
                    elif re.match(r"^\d{4}-\d{2}-\d{2}$", loading_date_var):
                        load_date = datetime.datetime.strptime(
                            f"{loading_date_var}", "%Y-%m-%d"
                        )
                elif isinstance(loading_date_var, datetime.datetime):
                    load_date = datetime.datetime.strptime(
                        f"{loading_date_var.strftime('%Y-%m-%d')}", "%Y-%m-%d"
                    )


                lr_date_var = single_data.get('Unnamed: 22')

                if isinstance(lr_date_var, str):
                    if re.match(r"^\d{2}-\d{2}-\d{4}$", lr_date_var):
                        lr_date = datetime.datetime.strptime(
                            f"{lr_date_var}", "%d-%m-%Y"
                        )
                    elif re.match(r"^\d{2}/\d{2}/\d{4}$", lr_date_var):
                        lr_date = datetime.datetime.strptime(
                            f"{lr_date_var}", "%d/%m/%Y"
                        )
                    elif re.match(r"^\d{4}-\d{2}-\d{2}$", lr_date_var):
                        lr_date = datetime.datetime.strptime(
                            f"{lr_date_var}", "%Y-%m-%d"
                        )
                elif isinstance(lr_date_var, datetime.datetime):
                    lr_date = datetime.datetime.strptime(
                        f"{lr_date_var.strftime('%Y-%m-%d')}", "%Y-%m-%d"
                    )


                try:
                    console_logger.debug("there")
                    checkData = RcrRoadData.objects.get(
                        secl_delivery_challan_number=single_data.get("Unnamed: 15"), tp_number=single_data.get("Unnamed: 12")
                    )
                    if checkData:
                        checkData.update(
                            # rrs_wt_date=single_data.get("Unnamed: 2"),
                            rrs_wt_date=grs_wt_date,
                            grs_wt_time=str(single_data.get("Unnamed: 3")) if single_data.get("Unnamed: 3") else None,
                            received_gross_weight=single_data.get("Unnamed: 4"),
                            # tar_wt_date=f'{single_data.get("Unnamed: 5").strftime("%Y-%m-%d")}T{single_data.get("Unnamed: 6")}',
                            tar_wt_date = tare_dt_tm,
                            # tar_wt_date = datetime.datetime.strptime(f"{single_data.get('Unnamed: 5').strftime('%Y-%m-%d')}T{single_data.get('Unnamed: 6')}","%Y-%m-%dT%H:%M:%S"),
                            # tar_wt_time=str(single_data.get("Unnamed: 6")) if single_data.get("Unnamed: 6") else None,
                            received_tare_weight=single_data.get("Unnamed: 7"),
                            received_net_weight=single_data.get("Unnamed: 8"),
                            unloading_slip_number=str(single_data.get("Unnamed: 9")) if single_data.get("Unnamed: 9") else None,
                            vehicle_no=single_data.get("Unnamed: 10"),
                            transporter=single_data.get("Unnamed: 11"),
                            tp_number=single_data.get("Unnamed: 12"),
                            do_number=str(single_data.get("Unnamed: 13")),
                            mine=single_data.get("Unnamed: 14").strip() if single_data.get("Unnamed: 14") else None,
                            dc_gross_wt=single_data.get("Unnamed: 16"),
                            dc_tare_wt=single_data.get("Unnamed: 17"),
                            dc_net_wt=single_data.get("Unnamed: 18"),
                            loading_date=load_date,
                            out_time=str(single_data.get("Unnamed: 20")) if single_data.get("Unnamed: 20") else None,
                            lr_no=single_data.get("Unnamed: 21"),
                            lr_date=lr_date
                        )
                except DoesNotExist as e:
                    # Create new record if not found
                    RcrRoadData(
                        # rrs_wt_date=single_data.get("Unnamed: 2") if single_data.get("Unnamed: 2") else None,
                        rrs_wt_date=grs_wt_date,
                        grs_wt_time=str(single_data.get("Unnamed: 3")) if single_data.get("Unnamed: 3") else None,
                        received_gross_weight=single_data.get("Unnamed: 4") if single_data.get("Unnamed: 4") else None,
                        # tar_wt_date=single_data.get("Unnamed: 5") if single_data.get("Unnamed: 5") else None,
                        tar_wt_date=tare_dt_tm,
                        tar_wt_time=str(single_data.get("Unnamed: 6")) if single_data.get("Unnamed: 6") else None,
                        received_tare_weight=single_data.get("Unnamed: 7") if single_data.get("Unnamed: 7") else None,
                        received_net_weight=single_data.get("Unnamed: 8") if single_data.get("Unnamed: 8") else None,
                        unloading_slip_number=str(single_data.get("Unnamed: 9")) if single_data.get("Unnamed: 9") else None,
                        vehicle_no=single_data.get("Unnamed: 10") if single_data.get("Unnamed: 10") else None,
                        transporter=single_data.get("Unnamed: 11") if single_data.get("Unnamed: 11") else None,
                        tp_number=single_data.get("Unnamed: 12") if single_data.get("Unnamed: 12") else None,
                        do_number=str(single_data.get("Unnamed: 13")) if single_data.get("Unnamed: 13") else None,
                        mine=single_data.get("Unnamed: 14").strip() if single_data.get("Unnamed: 14") else None,
                        secl_delivery_challan_number=single_data.get("Unnamed: 15") if single_data.get("Unnamed: 15") else None,
                        dc_gross_wt=single_data.get("Unnamed: 16") if single_data.get("Unnamed: 16") else None,
                        dc_tare_wt=single_data.get("Unnamed: 17") if single_data.get("Unnamed: 17") else None,
                        dc_net_wt=single_data.get("Unnamed: 18") if single_data.get("Unnamed: 18") else None,
                        loading_date=load_date,
                        out_time=str(single_data.get("Unnamed: 20")) if single_data.get("Unnamed: 20") else None,
                        lr_no=single_data.get("Unnamed: 21") if single_data.get("Unnamed: 21") else None,
                        lr_date=lr_date
                    ).save()
            endpoint_to_update_gmrdata_using_saprecords()
            return {"details":"success"}
    except KeyError as e:
        raise HTTPException(status_code=404, detail="Key Error")
    except Exception as e:
        console_logger.debug("----- Secl Annexure Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/sync/saprecordsrcrroad", tags=["Coal Testing"])
def endpoint_to_update_gmrdata_using_saprecords():
    try:
        try:
            sapData = SapRecordsRcrRoad.objects()
            for single_data_sap in sapData:
                if single_data_sap:
                    RcrRoadData.objects(
                        do_number=single_data_sap.do_no,
                    ).update(
                        do_date=single_data_sap.do_date, 
                        start_date=single_data_sap.start_date, 
                        end_date=single_data_sap.end_date, 
                        slno=single_data_sap.slno,
                        type_consumer= single_data_sap.consumer_type,
                        grade= single_data_sap.grade,
                        mine= single_data_sap.mine_name,
                        po_qty= single_data_sap.do_qty,
                        po_amount= single_data_sap.po_amount)
        except DoesNotExist as e:
            raise HTTPException(status_code=404, detail="No data found")
        return {"details": "success"}
    except Exception as e:
        console_logger.debug("----- Road SapRecordsRcr Upload Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/sync/saprecordsrcrrail", tags=["Coal Testing"])
def endpoint_to_update__rcrroad_Rcrrail(response: Response):
    try:
        try:
            sapData = sapRecordsRCR.objects()
            for single_data_sap in sapData:
                if single_data_sap:
                    RcrData.objects(
                        rr_no=single_data_sap.rr_no,
                    ).update(
                        rr_date=single_data_sap.rr_date, 
                        start_date=single_data_sap.start_date, 
                        end_date=single_data_sap.end_date, 
                        slno=single_data_sap.month,
                        type_consumer= single_data_sap.consumer_type,
                        grade= single_data_sap.grade,
                        mine= single_data_sap.mine,
                        po_qty= single_data_sap.rr_qty,
                        po_amount= single_data_sap.po_amount)
        except DoesNotExist as e:
            raise HTTPException(status_code=404, detail="No data found")
        return {"details": "success"}
    except Exception as e:
        console_logger.debug("----- Road SapRecordsRcr Upload Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

    
# @router.get("/update/gmrdata", tags=["Coal Testing"])
# async def endpoint_to_update_gmrdata_using_saprecords(response: Response, do_no: str):
#     try:
#         try:
#             sapData = SapRecords.objects.get(do_no=do_no)
#             if sapData:
#                 Gmrdata.objects(
#                     arv_cum_do_number=do_no,
#                 ).update(
#                     do_date=sapData.do_date, 
#                     start_date=sapData.start_date, 
#                     end_date=sapData.end_date, 
#                     slno=sapData.slno,
#                     type_consumer= sapData.consumer_type,
#                     grade= sapData.grade,
#                     mine= sapData.mine_name,
#                     po_qty= sapData.do_qty,
#                     po_amount= sapData.po_amount)
#         except DoesNotExist as e:
#             raise HTTPException(status_code=404, detail="No data found")
#         return {"details": "success"}
#     except Exception as e:
#         console_logger.debug("----- Road Sap Upload Error -----",e)
#         response.status_code = 400
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#         console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#         return e


# @router.get("/check/saprecords", tags=["Coal Testing"])
# async def endpoint_to_dono_using_saprecords(response: Response, do_no: str):
#     try:
#         try:
#             sapData = SapRecords.objects.get(do_no=do_no)
#             if sapData:
#                 return sapData.payload()
#         except DoesNotExist as e:
#             raise HTTPException(status_code=404, detail="No data found")
#     except Exception as e:
#         console_logger.debug("----- Road Sap Upload Error -----",e)
#         response.status_code = 400
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#         console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#         return e


# @router.get("/fetch/secllinkage", tags=["Rail Map"])
# def endpoint_to_fetch_secl_linkage_matrialization(response: Response, year_data: str):
#     try:
#         railData_pipeline = [
#             {
#                 '$match': {
#                     'placement_date': {
#                         '$ne': None
#                     }, 
#                     'placement_date': {
#                         '$ne': ''
#                     }
#                 }
#             }, {
#                 '$project': {
#                     'year': {
#                         '$substr': [
#                             '$placement_date', 0, 4
#                         ]
#                     }, 
#                     'month': {
#                         '$dateToString': {
#                             'format': '%Y-%m', 
#                             'date': {
#                                 '$cond': {
#                                     'if': { '$regexMatch': { 'input': '$placement_date', 'regex': 'T' } }, # Check if date contains 'T'
#                                     'then': {
#                                         '$dateFromString': {
#                                             'dateString': '$placement_date', 
#                                             'format': '%Y-%m-%dT%H:%M'
#                                         }
#                                     },
#                                     'else': {
#                                         '$dateFromString': {
#                                             'dateString': '$placement_date', 
#                                             'format': '%Y-%m-%d %H:%M'
#                                         }
#                                     }
#                                 }
#                             }
#                         }
#                     }, 
#                     'total_rly_tare_wt': {
#                         '$toDouble': '$total_rly_tare_wt'
#                     }
#                 }
#             }, {
#                 '$match': {
#                     'year': year_data
#                 }
#             }, {
#                 '$group': {
#                     '_id': '$month', 
#                     'total_rly_tare_wt': {
#                         '$sum': '$total_rly_tare_wt'
#                     }
#                 }
#             }, {
#                 '$sort': {
#                     '_id': 1
#                 }
#             }
#         ]

#         sapRecordsRail_pipeline = [
#             {
#                 "$project": {
#                     "year": {
#                         "$substr": ["$month", 7, 4]
#                     },
#                     "month": {
#                         "$dateToString": {
#                             "format": "%Y-%m",
#                             "date": {
#                                 "$dateFromString": {
#                                     "dateString": "$month"
#                                 }
#                             }
#                         }
#                     },
#                     "total_rr_qty": {
#                         "$toDouble": "$rr_qty"
#                     }
#                 }
#             },
#             {
#                 "$match": {
#                     "year": year_data
#                 }
#             },
#             {
#                 "$group": {
#                     "_id": "$month",
#                     "total_rr_qty": {
#                         "$sum": "$total_rr_qty"
#                     }
#                 }
#             },
#             {
#                 "$sort": {
#                     "_id": 1
#                 }
#             }
#         ]

#         # Run aggregations
#         railData_result_cursor = RailData.objects().aggregate(railData_pipeline)
#         sapRecordsRail_result_cursor = sapRecordsRail.objects().aggregate(sapRecordsRail_pipeline)

#         if railData_result_cursor and sapRecordsRail_result_cursor:
#             # Convert cursors to lists
#             railData_result = list(railData_result_cursor)
#             sapRecordsRail_result = list(sapRecordsRail_result_cursor)

#             # Convert results to dictionaries by month
#             railData_dict = {item["_id"]: item["total_rly_tare_wt"] for item in railData_result}
#             sapRecordsRail_dict = {item["_id"]: item["total_rr_qty"] for item in sapRecordsRail_result}

#             # Prepare Chart.js data
#             months = sorted(set(railData_dict.keys()).union(sapRecordsRail_dict.keys()))
#             total_rr_qty_data = [sapRecordsRail_dict.get(month, 0) for month in months]
#             total_rly_tare_wt_data = [railData_dict.get(month, 0) for month in months]
#             percentages = [
#                 (total_rly_tare_wt / rr_qty * 100 if rr_qty != 0 else 0)
#                 for rr_qty, total_rly_tare_wt in zip(total_rr_qty_data, total_rly_tare_wt_data)
#             ]

#             chart_data = {
#                 "labels": months,
#                 "datasets": [
#                     {
#                         "label": "Percentage",
#                         "data": percentages,
#                         "borderWidth": 1,
#                     }
#                 ]
#             }
#             return chart_data

#     except Exception as e:
#         console_logger.debug("----- Secl Linkage Matrialization Error -----",e)
#         response.status_code = 400
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#         console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#         return e


# @router.get("/fetch/secllinkage", tags=["Rail Map"])
# def endpoint_to_fetch_secl_linkage_matrialization(response: Response, year_data: str):
#     try:
#         # railData_pipeline = [
#         #     {
#         #         '$match': {
#         #             'placement_date': {
#         #                 '$ne': None, 
#         #                 '$ne': ''
#         #             }
#         #         }
#         #     }, {
#         #         '$project': {
#         #             'year': {
#         #                 '$substr': [
#         #                     '$placement_date', 0, 4
#         #                 ]
#         #             }, 
#         #             'month': {
#         #                 '$dateToString': {
#         #                     'format': '%Y-%m', 
#         #                     'date': {
#         #                         '$cond': {
#         #                             'if': {
#         #                                 '$regexMatch': {
#         #                                     'input': '$placement_date', 
#         #                                     'regex': 'T'
#         #                                 }
#         #                             }, 
#         #                             'then': {
#         #                                 '$dateFromString': {
#         #                                     'dateString': '$placement_date', 
#         #                                     'format': '%Y-%m-%dT%H:%M'
#         #                                 }
#         #                             }, 
#         #                             'else': {
#         #                                 '$dateFromString': {
#         #                                     'dateString': '$placement_date', 
#         #                                     'format': '%Y-%m-%d %H:%M'
#         #                                 }
#         #                             }
#         #                         }
#         #                     }
#         #                 }
#         #             }, 
#         #             'total_secl_net_wt': {
#         #                 '$cond': {
#         #                     'if': {
#         #                         '$and': [
#         #                             {
#         #                                 '$ne': [
#         #                                     '$total_secl_net_wt', None
#         #                                 ]
#         #                             }, {
#         #                                 '$ne': [
#         #                                     '$total_secl_net_wt', ''
#         #                                 ]
#         #                             }, {
#         #                                 '$regexMatch': {
#         #                                     'input': '$total_secl_net_wt', 
#         #                                     'regex': '^-?[0-9]+(\\.[0-9]+)?$'
#         #                                 }
#         #                             }
#         #                         ]
#         #                     }, 
#         #                     'then': {
#         #                         '$toDouble': '$total_secl_net_wt'
#         #                     }, 
#         #                     'else': 0
#         #                 }
#         #             }
#         #         }
#         #     }, {
#         #         '$match': {
#         #             '$expr': {
#         #                 '$and': [
#         #                     {
#         #                         '$gte': [
#         #                             {
#         #                                 '$dateFromString': {
#         #                                     'dateString': {
#         #                                         '$concat': [
#         #                                             '$month', '-01'
#         #                                         ]
#         #                                     }, 
#         #                                     'format': '%Y-%m-%d'
#         #                                 }
#         #                             }, {
#         #                                 '$dateFromString': {
#         #                                     'dateString': {
#         #                                         '$concat': [
#         #                                             {
#         #                                                 '$toString': year_data
#         #                                             }, '-04-01'
#         #                                         ]
#         #                                     }, 
#         #                                     'format': '%Y-%m-%d'
#         #                                 }
#         #                             }
#         #                         ]
#         #                     }, {
#         #                         '$lte': [
#         #                             {
#         #                                 '$dateFromString': {
#         #                                     'dateString': {
#         #                                         '$concat': [
#         #                                             '$month', '-01'
#         #                                         ]
#         #                                     }, 
#         #                                     'format': '%Y-%m-%d'
#         #                                 }
#         #                             }, {
#         #                                 '$dateFromString': {
#         #                                     'dateString': {
#         #                                         '$concat': [
#         #                                             {
#         #                                                 '$toString': {
#         #                                                     '$add': [
#         #                                                         int(year_data), 1
#         #                                                     ]
#         #                                                 }
#         #                                             }, '-03-31'
#         #                                         ]
#         #                                     }, 
#         #                                     'format': '%Y-%m-%d'
#         #                                 }
#         #                             }
#         #                         ]
#         #                     }
#         #                 ]
#         #             }
#         #         }
#         #     }, {
#         #         '$group': {
#         #             '_id': '$month', 
#         #             'total_secl_net_wt': {
#         #                 '$sum': '$total_secl_net_wt'
#         #             }
#         #         }
#         #     }, {
#         #         '$sort': {
#         #             '_id': 1
#         #         }
#         #     }
#         # ]

#         railData_pipeline = [
#             {
#                 '$match': {
#                     'month': {
#                         '$ne': None, 
#                         '$ne': ''
#                     }
#                 }
#             }, {
#                 '$project': {
#                     'year': {
#                         '$substr': [
#                             '$month', 0, 4
#                         ]
#                     }, 
#                     'month': {
#                         '$dateToString': {
#                             'format': '%Y-%m', 
#                             'date': {
#                                 '$dateFromString': {
#                                     'dateString': {
#                                         '$concat': [
#                                             {
#                                                 '$substr': [
#                                                     '$month', 0, 7
#                                                 ]
#                                             }, '-01'
#                                         ]
#                                     }, 
#                                     'format': '%Y-%m-%d'
#                                 }
#                             }
#                         }
#                     }, 
#                     'total_secl_net_wt': {
#                         '$cond': {
#                             'if': {
#                                 '$and': [
#                                     {
#                                         '$ne': [
#                                             '$total_secl_net_wt', None
#                                         ]
#                                     }, {
#                                         '$ne': [
#                                             '$total_secl_net_wt', ''
#                                         ]
#                                     }, {
#                                         '$regexMatch': {
#                                             'input': '$total_secl_net_wt', 
#                                             'regex': '^-?[0-9]+(\\.[0-9]+)?$'
#                                         }
#                                     }
#                                 ]
#                             }, 
#                             'then': {
#                                 '$toDouble': '$total_secl_net_wt'
#                             }, 
#                             'else': 0
#                         }
#                     }
#                 }
#             }, {
#                 '$match': {
#                     '$expr': {
#                         '$and': [
#                             {
#                                 '$gte': [
#                                     {
#                                         '$dateFromString': {
#                                             'dateString': {
#                                                 '$concat': [
#                                                     '$month', '-01'
#                                                 ]
#                                             }, 
#                                             'format': '%Y-%m-%d'
#                                         }
#                                     }, {
#                                         '$dateFromString': {
#                                             'dateString': {
#                                                 '$concat': [
#                                                     {
#                                                         '$toString': year_data
#                                                     }, '-04-01'
#                                                 ]
#                                             }, 
#                                             'format': '%Y-%m-%d'
#                                         }
#                                     }
#                                 ]
#                             }, {
#                                 '$lte': [
#                                     {
#                                         '$dateFromString': {
#                                             'dateString': {
#                                                 '$concat': [
#                                                     '$month', '-01'
#                                                 ]
#                                             }, 
#                                             'format': '%Y-%m-%d'
#                                         }
#                                     }, {
#                                         '$dateFromString': {
#                                             'dateString': {
#                                                 '$concat': [
#                                                     {
#                                                         '$toString': {
#                                                             '$add': [
#                                                                 int(year_data), 1
#                                                             ]
#                                                         }
#                                                     }, '-03-31'
#                                                 ]
#                                             }, 
#                                             'format': '%Y-%m-%d'
#                                         }
#                                     }
#                                 ]
#                             }
#                         ]
#                     }
#                 }
#             }, {
#                 '$group': {
#                     '_id': '$month', 
#                     'total_secl_net_wt': {
#                         '$sum': '$total_secl_net_wt'
#                     }
#                 }
#             }, {
#                 '$sort': {
#                     '_id': 1
#                 }
#             }
#         ]

#         # sapRecordsRail_pipeline = [
#         #     {
#         #         "$project": {
#         #             "year": {
#         #                 "$substr": ["$month", 0, 4]
#         #             },
#         #             "month": {
#         #                 "$dateToString": {
#         #                     "format": "%Y-%m",
#         #                     "date": {
#         #                         "$dateFromString": {
#         #                             "dateString": "$month",
#         #                             "format": "%Y-%m-%d"
#         #                         }
#         #                     }
#         #                 }
#         #             },
#         #             "total_rr_qty": {
#         #                 "$toDouble": "$rr_qty"
#         #             }
#         #         }
#         #     },
#         #     {
#         #         "$match": {
#         #             "year": year_data
#         #         }
#         #     },
#         #     {
#         #         "$group": {
#         #             "_id": "$month",
#         #             "total_rr_qty": {
#         #                 "$sum": "$total_rr_qty"
#         #             }
#         #         }
#         #     },
#         #     {
#         #         "$sort": {
#         #             "_id": 1
#         #         }
#         #     }
#         # ]

#         sapRecordsRail_pipeline = [
#             {
#                 '$match': {
#                     'month': {
#                         '$ne': None
#                     }
#                 }
#                 }, {
#                 '$project': {
#                     'year': {
#                         '$substr': [
#                             '$month', 0, 4
#                         ]
#                     }, 
#                     'month': {
#                         '$dateToString': {
#                             'format': '%Y-%m', 
#                             'date': {
#                                 '$dateFromString': {
#                                     'dateString': {
#                                         '$concat': [
#                                             {
#                                                 '$substr': [
#                                                     '$month', 0, 7
#                                                 ]
#                                             }, '-01'
#                                         ]
#                                     }, 
#                                     'format': '%Y-%m-%d'
#                                 }
#                             }
#                         }
#                     }, 
#                     'total_rr_qty': {
#                         '$toDouble': '$rr_qty'
#                     }
#                 }
#             }, {
#                 '$match': {
#                     '$expr': {
#                         '$and': [
#                             {
#                                 '$gte': [
#                                     {
#                                         '$dateFromString': {
#                                             'dateString': {
#                                                 '$concat': [
#                                                     '$month', '-01'
#                                                 ]
#                                             }, 
#                                             'format': '%Y-%m-%d'
#                                         }
#                                     }, {
#                                         '$dateFromString': {
#                                             'dateString': {
#                                                 '$concat': [
#                                                     {
#                                                         '$toString': year_data
#                                                     }, '-04-01'
#                                                 ]
#                                             }, 
#                                             'format': '%Y-%m-%d'
#                                         }
#                                     }
#                                 ]
#                             }, {
#                                 '$lte': [
#                                     {
#                                         '$dateFromString': {
#                                             'dateString': {
#                                                 '$concat': [
#                                                     '$month', '-01'
#                                                 ]
#                                             }, 
#                                             'format': '%Y-%m-%d'
#                                         }
#                                     }, {
#                                         '$dateFromString': {
#                                             'dateString': {
#                                                 '$concat': [
#                                                     {
#                                                         '$toString': {
#                                                             '$add': [
#                                                                 int(year_data), 1
#                                                             ]
#                                                         }
#                                                     }, '-03-31'
#                                                 ]
#                                             }, 
#                                             'format': '%Y-%m-%d'
#                                         }
#                                     }
#                                 ]
#                             }
#                         ]
#                     }
#                 }
#             }, {
#                 '$group': {
#                     '_id': '$month', 
#                     'total_rr_qty': {
#                         '$sum': '$total_rr_qty'
#                     }
#                 }
#             }, {
#                 '$sort': {
#                     '_id': 1
#                 }
#             }
#         ]

#         railData_result_cursor = RailData.objects().aggregate(railData_pipeline)
#         sapRecordsRail_result_cursor = sapRecordsRail.objects().aggregate(sapRecordsRail_pipeline)

#         railData_result = list(railData_result_cursor)
#         sapRecordsRail_result = list(sapRecordsRail_result_cursor)

#         console_logger.debug(railData_result)
#         console_logger.debug(sapRecordsRail_result)

#         railData_dict = {item["_id"]: item["total_secl_net_wt"] for item in railData_result}
#         sapRecordsRail_dict = {item["_id"]: item["total_rr_qty"] for item in sapRecordsRail_result}

#         months = sorted(set(railData_dict.keys()).union(sapRecordsRail_dict.keys()))
#         total_rr_qty_data = [sapRecordsRail_dict.get(month, 0) for month in months]
#         total_secl_net_wt_data = [railData_dict.get(month, 0) for month in months]
#         percentages = [
#             (total_secl_net_wt / rr_qty * 100 if rr_qty != 0 else 0)
#             for rr_qty, total_secl_net_wt in zip(total_rr_qty_data, total_secl_net_wt_data)
#         ]

#         chart_data = {
#             "labels": months,
#             "datasets": [
#                 {
#                     "label": "Percentage",
#                     "data": percentages,
#                     "borderWidth": 1,
#                 }
#             ]
#         }
#         return chart_data

#     except Exception as e:
#         console_logger.debug("----- Secl Linkage Matrialization Error -----", e)
#         response.status_code = 400
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#         console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#         return e


# for dashboard secl linkage
@router.get("/fetch/secllinkage", tags=["Rail Map"])
def endpoint_to_fetch_secl_linkage_matrialization(response: Response, year_data: str):
    try:
        queryData = rakeQuota.objects(year=year_data, source_type="SECL Linkage(U1)").order_by("year", "month")
        listData = []
        for log in queryData:
            # console_logger.debug(log)
            rake_year = log.year
            rake_month = log.month

            month_year = f"{rake_year}-{rake_month[:2].upper()}"
            date_obj = datetime.datetime.strptime(month_year, "%Y-%m")
            formatted_date = date_obj.strftime("%Y-%m")
            dictData = {"month": "", "rake_planned_for_month": "", "total_rakes_received_for_month": 0}
            dictData["month"] = datetime.datetime.strptime(log.month, "%m-%Y").strftime("%b-%Y")
            dictData["rake_planned_for_month"] = log.rake_alloted
            if log.cancelled_rakes:
                dictData["cancelled_rakes"] = log.cancelled_rakes
                cancelled_rakes = int(dictData["cancelled_rakes"])
            dictData["total_rakes_received_for_month"] = RailData.objects.filter(
                        Q(month__icontains=formatted_date) & 
                        Q(avery_placement_date__ne=None) &
                        Q(source_type__iexact = log.source_type)
                    ).count()
            planned = int(dictData["rake_planned_for_month"])
            received = dictData['total_rakes_received_for_month']
            
            # Calculate the percentage
            if log.cancelled_rakes:
                percentage = (received + cancelled_rakes / planned) * 100 if planned > 0 else 0
            else:
                percentage = (received / planned) * 100 if planned > 0 else 0
            new_percentage = percentage if percentage < 100 else 100.0 
            dictData["percentage_received"] = new_percentage
            listData.append(dictData)

        # Extract months and percentages
        # months = [entry['month'] for entry in listData]
        months = [datetime.datetime.strptime(entry['month'], "%b-%Y").strftime("%b") for entry in listData]
        percentages = [round(entry['percentage_received'], 2) for entry in listData]
        targets = [100 for n in range(0, len(months))]

        chart_data = {
            "labels": months,
            "datasets": [
                {
                    "label": "Percentage",
                    "data": percentages,
                    "borderWidth": 1,
                },
                {
                    "label": "Target",
                    "data": targets,
                    "order": 0,
                    "type": "line"
                }
            ]
        }
        return chart_data
    except Exception as e:
        console_logger.debug("----- Secl Linkage Matrialization Error -----", e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/fetch/wcllinkage", tags=["Road Map"])
def endpoint_to_fetch_wcl_linkage_matrialization(response: Response, year_data: str):
    try:
        financial_year_start = f"{year_data}-04-01"
        next_year = str(int(year_data) + 1)
        financial_year_end = f"{next_year}-03-31"

        # old without consumer_type
        # sapRecordsPipeline = [
        #     {
        #         '$project': {
        #             'month': {
        #                 '$cond': {
        #                     'if': {
        #                         '$and': [
        #                             {
        #                                 '$gte': [
        #                                     {
        #                                         '$strLenCP': {
        #                                             '$ifNull': [
        #                                                 '$slno', ''
        #                                             ]
        #                                         }
        #                                     }, 6
        #                                 ]
        #                             }, {
        #                                 '$regexMatch': {
        #                                     'input': '$slno', 
        #                                     'regex': '^[0-9]{6}'
        #                                 }
        #                             }
        #                         ]
        #                     }, 
        #                     'then': {
        #                         '$dateToString': {
        #                             'format': '%Y%m', 
        #                             'date': {
        #                                 '$dateFromString': {
        #                                     'dateString': {
        #                                         '$concat': [
        #                                             {
        #                                                 '$substr': [
        #                                                     '$slno', 0, 4
        #                                                 ]
        #                                             }, '-', {
        #                                                 '$substr': [
        #                                                     '$slno', 4, 2
        #                                                 ]
        #                                             }, '-01'
        #                                         ]
        #                                     }, 
        #                                     'format': '%Y-%m-%d'
        #                                 }
        #                             }
        #                         }
        #                     }, 
        #                     'else': None
        #                 }
        #             }, 
        #             'year': {
        #                 '$substr': [
        #                     '$slno', 0, 4
        #                 ]
        #             }, 
        #             'do_qty': {
        #                 '$toDouble': '$do_qty'
        #             }, 
        #             'do_no': 1
        #         }
        #     }, {
        #         '$match': {
        #             'year': {'$in': [str(year_data), str(next_year)]}, 
        #             'month': {
        #                 '$ne': None
        #             }, 
        #             '$expr': {
        #                 '$and': [
        #                     {
        #                         '$gte': [
        #                             {
        #                                 '$dateFromString': {
        #                                     'dateString': {
        #                                         '$concat': [
        #                                             {
        #                                                 '$substr': [
        #                                                     '$month', 0, 4
        #                                                 ]
        #                                             }, '-', {
        #                                                 '$substr': [
        #                                                     '$month', 4, 2
        #                                                 ]
        #                                             }, '-01'
        #                                         ]
        #                                     }, 
        #                                     'format': '%Y-%m-%d'
        #                                 }
        #                             }, {
        #                                 '$dateFromString': {
        #                                     'dateString': financial_year_start
        #                                 }
        #                             }
        #                         ]
        #                     }, {
        #                         '$lte': [
        #                             {
        #                                 '$dateFromString': {
        #                                     'dateString': {
        #                                         '$concat': [
        #                                             {
        #                                                 '$substr': [
        #                                                     '$month', 0, 4
        #                                                 ]
        #                                             }, '-', {
        #                                                 '$substr': [
        #                                                     '$month', 4, 2
        #                                                 ]
        #                                             }, '-01'
        #                                         ]
        #                                     }, 
        #                                     'format': '%Y-%m-%d'
        #                                 }
        #                             }, {
        #                                 '$dateFromString': {
        #                                     'dateString': financial_year_end
        #                                 }
        #                             }
        #                         ]
        #                     }
        #                 ]
        #             }
        #         }
        #     }, {
        #         '$group': {
        #             '_id': '$month', 
        #             'total_do_qty': {
        #                 '$sum': '$do_qty'
        #             }, 
        #             'do_nos': {
        #                 '$addToSet': '$do_no'
        #             }
        #         }
        #     }, {
        #         '$sort': {
        #             '_id': 1
        #         }
        #     }
        # ]

        #new with consumer_type 
        sapRecordsPipeline = [
            {
                '$project': {
                    'month': {
                        '$cond': {
                            'if': {
                                '$and': [
                                    {
                                        '$gte': [
                                            {
                                                '$strLenCP': {
                                                    '$ifNull': [
                                                        '$slno', ''
                                                    ]
                                                }
                                            }, 6
                                        ]
                                    }, {
                                        '$regexMatch': {
                                            'input': '$slno', 
                                            'regex': '^[0-9]{6}'
                                        }
                                    }
                                ]
                            }, 
                            'then': {
                                '$dateToString': {
                                    'format': '%Y%m', 
                                    'date': {
                                        '$dateFromString': {
                                            'dateString': {
                                                '$concat': [
                                                    {
                                                        '$substr': [
                                                            '$slno', 0, 4
                                                        ]
                                                    }, '-', {
                                                        '$substr': [
                                                            '$slno', 4, 2
                                                        ]
                                                    }, '-01'
                                                ]
                                            }, 
                                            'format': '%Y-%m-%d'
                                        }
                                    }
                                }
                            }, 
                            'else': None
                        }
                    }, 
                    'year': {
                        '$substr': [
                            '$slno', 0, 4
                        ]
                    }, 
                    'do_qty': {
                        '$toDouble': '$do_qty'
                    }, 
                    'do_no': 1, 
                    'consumer_type': 1
                }
            }, {
                '$match': {
                    'year': {'$in': [str(year_data), str(next_year)]},  
                    'month': {
                        '$ne': None
                    }, 
                    'consumer_type': 'WCL Shakti B(iii) Round 5 Coal', 
                    '$expr': {
                        '$and': [
                            {
                                '$gte': [
                                    {
                                        '$dateFromString': {
                                            'dateString': {
                                                '$concat': [
                                                    {
                                                        '$substr': [
                                                            '$month', 0, 4
                                                        ]
                                                    }, '-', {
                                                        '$substr': [
                                                            '$month', 4, 2
                                                        ]
                                                    }, '-01'
                                                ]
                                            }, 
                                            'format': '%Y-%m-%d'
                                        }
                                    }, {
                                        '$dateFromString': {
                                            'dateString': financial_year_start
                                        }
                                    }
                                ]
                            }, {
                                '$lte': [
                                    {
                                        '$dateFromString': {
                                            'dateString': {
                                                '$concat': [
                                                    {
                                                        '$substr': [
                                                            '$month', 0, 4
                                                        ]
                                                    }, '-', {
                                                        '$substr': [
                                                            '$month', 4, 2
                                                        ]
                                                    }, '-01'
                                                ]
                                            }, 
                                            'format': '%Y-%m-%d'
                                        }
                                    }, {
                                        '$dateFromString': {
                                            'dateString': financial_year_end
                                        }
                                    }
                                ]
                            }
                        ]
                    }
                }
            }, {
                '$group': {
                    '_id': '$month', 
                    'total_do_qty': {
                        '$sum': '$do_qty'
                    }, 
                    'do_nos': {
                        '$addToSet': '$do_no'
                    }
                }
            }, {
                '$sort': {
                    '_id': 1
                }
            }
        ]
        
        roadDataSap_result_cursor = SapRecords.objects().aggregate(sapRecordsPipeline)
        sapData_result = list(roadDataSap_result_cursor)

        listData = []
        for month_data in sapData_result:
            month = month_data['_id']
            do_nos = month_data['do_nos']

            pipelineData = [
                {
                    '$match': {
                        'arv_cum_do_number': {
                            '$in': do_nos,
                        },
                        'type_consumer': 'WCL Shakti B(iii) Round 5 Coal',
                    }
                }, 
                {
                    '$addFields': {
                        'net_qty': {
                            '$cond': {
                                'if': {
                                    '$isNumber': '$net_qty'
                                }, 
                                'then': '$net_qty', 
                                'else': {
                                    '$toDouble': '$net_qty'
                                }
                            }
                        }
                    }
                }, 
                {
                    '$match': {
                        'net_qty': {
                            '$ne': None
                        }
                    }
                }, 
                {
                    '$group': {
                        '_id': month, 
                        'total_net_qty': {
                            '$sum': '$net_qty'
                        }
                    }
                }
            ]

            # pipelineData = [
            #     {
            #         '$match': {
            #             'arv_cum_do_number': {
            #                 '$in': do_nos,
            #             },
            #         }
            #     },
            #     {
            #         '$addFields': {
            #             'net_qty': {
            #                 '$cond': {
            #                     'if': {
            #                         '$isNumber': '$net_qty'
            #                     },
            #                     'then': '$net_qty',
            #                     'else': {
            #                         '$toDouble': '$net_qty'
            #                     }
            #                 }
            #             }
            #         }
            #     },
            #     {
            #         '$match': {
            #             'net_qty': { '$ne': None }
            #         }
            #     },
            #     {
            #         '$project': {
            #             'year': {
            #                 '$cond': {
            #                     'if': { '$lte': [{ '$substr': ['$month', 5, 2] }, '03'] },
            #                     'then': {
            #                         '$subtract': [
            #                             { '$toInt': { '$substr': ['$month', 0, 4] } }, 1
            #                         ]
            #                     },
            #                     'else': {
            #                         '$toInt': { '$substr': ['$month', 0, 4] }
            #                     }
            #                 }
            #             },
            #             'month': 1,
            #             'net_qty': 1
            #         }
            #     },
            #     {
            #         '$group': {
            #             '_id': '$month',
            #             'total_net_qty': {
            #                 '$sum': '$net_qty'
            #             }
            #         }
            #     },
            #     {
            #         '$sort': {
            #             '_id': 1
            #         }
            #     }
            # ]


            gmrdata_result_cursor = Gmrdata.objects().aggregate(pipelineData)
            gmrdataHist_result_cursor = gmrdataHistoric.objects().aggregate(pipelineData)

            gmrData_result = list(gmrdata_result_cursor)
            gmrDataHist_result = list(gmrdataHist_result_cursor)
            listData.append(gmrData_result)
            listData.append(gmrDataHist_result)

        flat_gmrData_result = [item for sublist in listData for item in sublist]

        clubbed_agg_data = {
            month: sum(round(item['total_net_qty'], 2) for item in flat_gmrData_result if item['_id'] == month)
            for month in {item['_id'] for item in flat_gmrData_result}
        }

        # gmr_data_dict = {item['_id']: item['total_net_qty'] for item in flat_gmrData_result}

        chart_data = {
            'labels': [],
            'datasets': [{
                'label': 'Percentage',
                'data': [],
                # 'backgroundColor': 'rgba(75, 192, 192, 0.2)',
                # 'borderColor': 'rgba(75, 192, 192, 1)',
                'borderWidth': 1
            }]
        }

        for month_data in sapData_result:
            month = month_data['_id']
            total_do_qty = month_data['total_do_qty']
            total_net_qty = clubbed_agg_data.get(month, 0)
            
            percentage = (total_net_qty / total_do_qty) * 100 if total_do_qty > 0 else 0
            new_percentage = percentage if percentage < 100 else 100.0
            # chart_data['labels'].append(total_do_qty)
            chart_data['labels'].append(datetime.datetime.strptime(month, "%Y%m").strftime("%b"))
            chart_data['datasets'][0]['data'].append(new_percentage)

        # added on 24-11-2024 as said by sachin bhai once data is clear and sorted we will remove start
        chart_data["labels"].insert(5, "Sep")

        chart_data["datasets"][0]["data"] = [100.0] * 5 + [100.0] + chart_data["datasets"][0]["data"][5:]
        # added on 24-11-2024 as said by sachin bhai once data is clear and sorted we will remove end
        
        targets = [100 for n in range(0, len(chart_data["datasets"][0]["data"]))]
        
        chart_data["datasets"].append({"label": "Target", "data": targets,"order": 0, "type": "line"})
        return chart_data 
    except Exception as e:
        console_logger.debug("----- Wcl Linkage Matrialization Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/fetch/wcllinkage/new", tags=["Road Map"])
def endpoint_to_fetch_wcl_linkage_matrialization_new(response: Response, year_data: str):
    try:
        financial_year_start = f"{year_data}-04-01"
        next_year = str(int(year_data) + 1)
        financial_year_end = f"{next_year}-03-31"

        sapRecordsPipeline = [
            {
                '$project': {
                    'month': {
                        '$cond': {
                            'if': {
                                '$and': [
                                    {
                                        '$gte': [
                                            {
                                                '$strLenCP': {
                                                    '$ifNull': [
                                                        '$slno', ''
                                                    ]
                                                }
                                            }, 6
                                        ]
                                    }, {
                                        '$regexMatch': {
                                            'input': '$slno', 
                                            'regex': '^[0-9]{6}'
                                        }
                                    }
                                ]
                            }, 
                            'then': {
                                '$dateToString': {
                                    'format': '%Y%m', 
                                    'date': {
                                        '$dateFromString': {
                                            'dateString': {
                                                '$concat': [
                                                    {
                                                        '$substr': [
                                                            '$slno', 0, 4
                                                        ]
                                                    }, '-', {
                                                        '$substr': [
                                                            '$slno', 4, 2
                                                        ]
                                                    }, '-01'
                                                ]
                                            }, 
                                            'format': '%Y-%m-%d'
                                        }
                                    }
                                }
                            }, 
                            'else': None
                        }
                    }, 
                    'year': {
                        '$substr': [
                            '$slno', 0, 4
                        ]
                    }, 
                    'do_qty': {
                        '$toDouble': '$do_qty'
                    }, 
                    'do_no': 1
                }
            }, {
                '$match': {
                    'year': {'$in': [str(year_data), str(next_year)]}, 
                    'month': {
                        '$ne': None
                    }, 
                    '$expr': {
                        '$and': [
                            {
                                '$gte': [
                                    {
                                        '$dateFromString': {
                                            'dateString': {
                                                '$concat': [
                                                    {
                                                        '$substr': [
                                                            '$month', 0, 4
                                                        ]
                                                    }, '-', {
                                                        '$substr': [
                                                            '$month', 4, 2
                                                        ]
                                                    }, '-01'
                                                ]
                                            }, 
                                            'format': '%Y-%m-%d'
                                        }
                                    }, {
                                        '$dateFromString': {
                                            'dateString': financial_year_start
                                        }
                                    }
                                ]
                            }, {
                                '$lte': [
                                    {
                                        '$dateFromString': {
                                            'dateString': {
                                                '$concat': [
                                                    {
                                                        '$substr': [
                                                            '$month', 0, 4
                                                        ]
                                                    }, '-', {
                                                        '$substr': [
                                                            '$month', 4, 2
                                                        ]
                                                    }, '-01'
                                                ]
                                            }, 
                                            'format': '%Y-%m-%d'
                                        }
                                    }, {
                                        '$dateFromString': {
                                            'dateString': financial_year_end
                                        }
                                    }
                                ]
                            }
                        ]
                    }
                }
            }, {
                '$group': {
                    '_id': '$month', 
                    'total_do_qty': {
                        '$sum': '$do_qty'
                    }, 
                    'do_nos': {
                        '$addToSet': '$do_no'
                    }
                }
            }, {
                '$sort': {
                    '_id': 1
                }
            }
        ]

        
        roadDataSap_result_cursor = SapRecords.objects().aggregate(sapRecordsPipeline)
        sapData_result = list(roadDataSap_result_cursor)

        listData = []
        for month_data in sapData_result:
            month = month_data['_id']
            do_nos = month_data['do_nos']

            pipelineData = [
                {
                    '$match': {
                        'arv_cum_do_number': {
                            '$in': do_nos,
                        },
                    }
                }, 
                {
                    '$addFields': {
                        'net_qty': {
                            '$cond': {
                                'if': {
                                    '$isNumber': '$net_qty'
                                }, 
                                'then': '$net_qty', 
                                'else': {
                                    '$toDouble': '$net_qty'
                                }
                            }
                        }
                    }
                }, 
                {
                    '$match': {
                        'net_qty': {
                            '$ne': None
                        }
                    }
                }, 
                {
                    '$group': {
                        '_id': month, 
                        'total_net_qty': {
                            '$sum': '$net_qty'
                        }
                    }
                }
            ]

            # pipelineData = [
            #     {
            #         '$match': {
            #             'arv_cum_do_number': {
            #                 '$in': do_nos,
            #             },
            #         }
            #     },
            #     {
            #         '$addFields': {
            #             'net_qty': {
            #                 '$cond': {
            #                     'if': {
            #                         '$isNumber': '$net_qty'
            #                     },
            #                     'then': '$net_qty',
            #                     'else': {
            #                         '$toDouble': '$net_qty'
            #                     }
            #                 }
            #             }
            #         }
            #     },
            #     {
            #         '$match': {
            #             'net_qty': { '$ne': None }
            #         }
            #     },
            #     {
            #         '$project': {
            #             'year': {
            #                 '$cond': {
            #                     'if': { '$lte': [{ '$substr': ['$month', 5, 2] }, '03'] },
            #                     'then': {
            #                         '$subtract': [
            #                             { '$toInt': { '$substr': ['$month', 0, 4] } }, 1
            #                         ]
            #                     },
            #                     'else': {
            #                         '$toInt': { '$substr': ['$month', 0, 4] }
            #                     }
            #                 }
            #             },
            #             'month': 1,
            #             'net_qty': 1
            #         }
            #     },
            #     {
            #         '$group': {
            #             '_id': '$month',
            #             'total_net_qty': {
            #                 '$sum': '$net_qty'
            #             }
            #         }
            #     },
            #     {
            #         '$sort': {
            #             '_id': 1
            #         }
            #     }
            # ]


            gmrdata_result_cursor = Gmrdatanew.objects().aggregate(pipelineData)

            gmrData_result = list(gmrdata_result_cursor)
            listData.append(gmrData_result)

        flat_gmrData_result = [item for sublist in listData for item in sublist]

        gmr_data_dict = {item['_id']: item['total_net_qty'] for item in flat_gmrData_result}

        chart_data = {
            'labels': [],
            'datasets': [{
                'label': 'Percentage',
                'data': [],
                # 'backgroundColor': 'rgba(75, 192, 192, 0.2)',
                # 'borderColor': 'rgba(75, 192, 192, 1)',
                'borderWidth': 1
            }]
        }

        for month_data in sapData_result:
            month = month_data['_id']
            total_do_qty = month_data['total_do_qty']
            total_net_qty = gmr_data_dict.get(month, 0)
            
            percentage = (total_net_qty / total_do_qty) * 100 if total_do_qty > 0 else 0
            
            # chart_data['labels'].append(total_do_qty)
            chart_data['labels'].append(month)
            chart_data['datasets'][0]['data'].append(percentage)
            
        return chart_data 
    except Exception as e:
        console_logger.debug("----- Wcl Linkage Matrialization Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e



@router.post("/update/rakequota", tags=["Rail Map"])
def endpoint_to_update_rakequota_data(response: Response, data:rakeQuotaUpdate):
    try:
        payload = data.dict()
        try:
            fetchrakeQuota = rakeQuota.objects(month=datetime.datetime.strptime(payload.get("month"), "%b-%Y").strftime("%m-%Y"))
            if fetchrakeQuota:
                fetchrakeQuota.update(rake_alloted=str(payload.get("rakes_planned_for_month")), expected_rakes=payload.get("expected_rakes"), source_type=payload.get("source_type"), cancelled_rakes=payload.get("cancelled_rakes"), remarks=payload.get("remarks"))
        except DoesNotExist as e:
            return {"detail": "No data found"}
        
        return {"details": "success"}
    except Exception as e:
        console_logger.debug("-----Update RakeQuota Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/update/rcrrakequota", tags=["Rail Map"])
def endpoint_to_update_rcr_rakequota_data(response: Response, data:rakeQuotaUpdate):
    try:
        payload = data.dict()
        try:
            fetchrakeQuota = rcrrakeQuota.objects(month=datetime.datetime.strptime(payload.get("month"), "%b-%Y").strftime("%m-%Y"))
            if fetchrakeQuota:
                fetchrakeQuota.update(rake_alloted=str(payload.get("rakes_planned_for_month")), expected_rakes=payload.get("expected_rakes"), source_type=payload.get("source_type"), cancelled_rakes=payload.get("cancelled_rakes"), remarks=payload.get("remarks"))
        except DoesNotExist as e:
            return {"detail": "No data found"}
        
        return {"details": "success"}
    except Exception as e:
        console_logger.debug("-----Update Rcr RakeQuota Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


# Function to make the request
def make_request(endpoint_name, url, avery_id, avery_pass, proxies):
    try:
        response = requests.get(url, auth=HTTPBasicAuth(avery_id, avery_pass), proxies=proxies)
        # Check if the request was successful
        if response.status_code == 200:
            # print(f"{endpoint_name} Response Data:", response.json())
            return response.json()
        else:
            console_logger.debug(f"{endpoint_name} Failed to retrieve data. Status code: {response.status_code} - Message: {response.text}")
    except Exception as e:
        console_logger.debug(f"{endpoint_name} An error occurred:", str(e))


@router.get("/update/placementcompleteiondate", tags=["Rail Map"])
def endpoint_to_update_averydata_placement(response:Response):
    try:
        fetchAveryData = RailData.objects()
        for single_data in fetchAveryData:
            # console_logger.debug(fetchAveryData.rr_no)
            if hasattr(single_data, 'avery_rly_data'): 
                placement_date = []
                completion_date = []
                for avery_single_data in single_data.avery_rly_data:
                    if avery_single_data.tip_startdate and avery_single_data.tip_starttime:
                        try:
                            if len(avery_single_data.tip_startdate) == 19:
                                formatted_date = datetime.datetime.strptime(avery_single_data.tip_startdate, "%m/%d/%Y %H:%M:%S").strftime("%Y-%m-%d")
                                placement_date.append(f'{formatted_date}T{avery_single_data.tip_starttime}')
                            elif len(avery_single_data.tip_startdate) == 10:
                                placement_date.append(f'{avery_single_data.tip_startdate}T{avery_single_data.tip_starttime}')
                        except ValueError as e:
                            console_logger.error(f"Error parsing tip_startdate: {e}")
                            
                    if avery_single_data.tip_enddate and avery_single_data.tip_endtime:
                        try:
                            if len(avery_single_data.tip_enddate) == 19:
                                formatted_date = datetime.datetime.strptime(avery_single_data.tip_enddate, "%m/%d/%Y %H:%M:%S").strftime("%Y-%m-%d")
                                completion_date.append(f'{formatted_date}T{avery_single_data.tip_endtime}')
                            elif len(avery_single_data.tip_enddate) == 10:
                                completion_date.append(f'{avery_single_data.tip_enddate}T{avery_single_data.tip_endtime}')
                        except ValueError as e:
                            console_logger.error(f"Error parsing tip_enddate: {e}")

                if placement_date:
                    console_logger.debug(f"Placement date: {placement_date[0]}")
                    single_data.avery_placement_date = placement_date[0]
                
                if completion_date:
                    console_logger.debug(f"Completion date: {completion_date[-1]}")
                    single_data.avery_completion_date = completion_date[-1]
                
                single_data.save()
        return "success"
    except Exception as e:
        console_logger.debug("----- Road Sap Upload Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

@router.get("/test/averydata", tags=["Rail Map"])
def endpoint_test(rr_number: str):
    try:
        fetchRailDatanew = RailData.objects.get(rr_no=rr_number)
        console_logger.debug(len([entry["gwel_net_wt"] for entry in fetchRailDatanew["avery_rly_data"] if entry["gwel_net_wt"]]))
        console_logger.debug(len([entry["gwel_net_wt"] for entry in fetchRailDatanew["avery_rly_data"] if not entry["gwel_net_wt"]]))
        console_logger.debug(round(sum([float(entry["gwel_gross_wt"]) for entry in fetchRailDatanew["avery_rly_data"] if entry["gwel_gross_wt"]]), 2))

        console_logger.debug(round(sum([float(entry["gwel_tare_wt"]) for entry in fetchRailDatanew["avery_rly_data"] if entry["gwel_tare_wt"]]), 2))
        console_logger.debug(round(sum([float(entry["gwel_net_wt"]) for entry in fetchRailDatanew["avery_rly_data"] if entry["gwel_net_wt"]]), 2))
    except Exception as e:
        console_logger.debug("----- Road Sap Upload Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


# @router.get("/update/averydata", tags=["Rail Map"])
# def endpoint_to_update_averydata(start_date: str, end_date: str):
#     try:
#         emailData = EmailDevelopmentCheck.objects()

#         avery_id = emailData[0].avery_id
#         avery_pass = emailData[0].avery_pass

#         console_logger.debug(emailData[0].avery_id)
#         console_logger.debug(emailData[0].avery_pass)
#         console_logger.debug(emailData[0].wagontrippler1)
#         console_logger.debug(emailData[0].wagontrippler2)
#         console_logger.debug(emailData[0].port)

#         proxies = {
#             "http": None,
#             "https": None
#         }
        
#         # urls = {
#         #     "WagonTrippler_1": f"http://172.21.92.15:8081/API/values/getdata?FromDate={start_date}&ToDate={end_date}",          #available data from 2024/08/13
#         #     "WagonTrippler_2": f"http://172.21.92.24:8081/API/values/getdata?FromDate={start_date}&ToDate={end_date}"           #available data from 2024/08/23
#         # }

#         urls = {
#             "WagonTrippler_1": f"http://{emailData[0].wagontrippler1}:{emailData[0].port}/API/values/getdata?FromDate={start_date}&ToDate={end_date}",          #available data from 2024/08/13
#             "WagonTrippler_2": f"http://{emailData[0].wagontrippler2}:{emailData[0].port}/API/values/getdata?FromDate={start_date}&ToDate={end_date}"           #available data from 2024/08/23
#         }

#         listData = []
#         for name, url in urls.items():
#             fetchAveryData = make_request(name, url, avery_id, avery_pass, proxies)
#             # console_logger.debug(fetchAveryData)
#             if fetchAveryData:
#                 for singleAveryData in fetchAveryData:
#                     if singleAveryData.get("rakeId"):
#                         if "A" in singleAveryData.get("rakeId"):
#                             rr_number = singleAveryData.get("rakeId").split("A")[1]
#                         elif "B" in singleAveryData.get("rakeId"):
#                             rr_number = singleAveryData.get("rakeId").split("B")[1]
#                         try:
#                             fetchRailData = RailData.objects.get(rr_no=rr_number)
#                             fetchRailData.GWEL_received_wagons = str(len([entry["gwel_net_wt"] for entry in fetchRailData["avery_rly_data"] if entry["gwel_net_wt"]]))
#                             fetchRailData.GWEL_pending_wagons = str(len([entry["gwel_net_wt"] for entry in fetchRailData["avery_rly_data"] if not entry["gwel_net_wt"]]))
#                             fetchRailData.Total_gwel_gross = str(round(sum([float(entry["gwel_gross_wt"]) for entry in fetchRailData["avery_rly_data"] if entry["gwel_gross_wt"]]), 2))
#                             fetchRailData.Total_gwel_tare = str(round(sum([float(entry["gwel_tare_wt"]) for entry in fetchRailData["avery_rly_data"] if entry["gwel_tare_wt"]]), 2))
#                             fetchRailData.Total_gwel_net = str(round(sum([float(entry["gwel_net_wt"]) for entry in fetchRailData["avery_rly_data"] if entry["gwel_net_wt"]]), 2))
#                             if fetchRailData:
#                                 start_date_tip = [f'{datetime.datetime.strptime(single_data.tip_enddate, "%m/%d/%Y %H:%M:%S").strftime("%Y-%m-%d")}T{single_data.tip_endtime}' for single_data in fetchRailData.avery_rly_data if single_data.tip_enddate is not None]
#                                 end_date_tip = [f'{datetime.datetime.strptime(single_data.tip_enddate, "%m/%d/%Y %H:%M:%S").strftime("%Y-%m-%d")}T{single_data.tip_endtime}' for single_data in fetchRailData.avery_rly_data if single_data.tip_enddate is not None]
#                                 if start_date_tip:
#                                     fetchRailData.avery_placement_date = [f'{datetime.datetime.strptime(single_data.tip_startdate, "%m/%d/%Y %H:%M:%S").strftime("%Y-%m-%d")}T{single_data.tip_starttime}' for single_data in fetchRailData.avery_rly_data if single_data.tip_startdate is not None][0]
#                                 if end_date_tip:
#                                     fetchRailData.avery_completion_date = [f'{datetime.datetime.strptime(single_data.tip_enddate, "%m/%d/%Y %H:%M:%S").strftime("%Y-%m-%d")}T{single_data.tip_endtime}' for single_data in fetchRailData.avery_rly_data if single_data.tip_enddate is not None][-1]
#                                 for single_rail_data in fetchRailData.avery_rly_data:
#                                     if singleAveryData.get("wagonId")[-5:] == single_rail_data.wagon_no[-5:]:
#                                         # Update fields
#                                         single_rail_data.ser_no = singleAveryData.get("serNo")
#                                         single_rail_data.rake_no = singleAveryData.get("rakeNo")
#                                         single_rail_data.rake_id = singleAveryData.get("rakeId")
#                                         single_rail_data.wagon_no_avery = singleAveryData.get("wagonNo")
#                                         single_rail_data.wagon_id = singleAveryData.get("wagonId")
#                                         single_rail_data.wagon_type_avery = singleAveryData.get("wagonType")
#                                         single_rail_data.wagon_cc = singleAveryData.get("wagonCC")
#                                         single_rail_data.mode = singleAveryData.get("mode")
#                                         single_rail_data.tip_startdate = singleAveryData.get("tipStartDate")
#                                         single_rail_data.tip_starttime = singleAveryData.get("tipStartTime")
#                                         single_rail_data.tip_enddate = singleAveryData.get("tipEndDate")
#                                         single_rail_data.tip_endtime = singleAveryData.get("tipEndTime")
#                                         single_rail_data.tipple_time = singleAveryData.get("tippleTime")
#                                         single_rail_data.status = singleAveryData.get("status")
#                                         single_rail_data.wagon_gross_wt = str(singleAveryData.get("wagonGrossWt"))
#                                         single_rail_data.wagon_tare_wt = str(singleAveryData.get("wagonTareWt"))
#                                         single_rail_data.wagon_net_wt = str(singleAveryData.get("wagonNetWt"))
#                                         single_rail_data.time_in_tipp = singleAveryData.get("timeIn_tipp")
#                                         single_rail_data.po_number = singleAveryData.get("ponumber")
#                                         single_rail_data.coal_grade = singleAveryData.get("coalgrade")
                                        
#                                         # Save the changes
#                                         fetchRailData.save()
#                             # return {"detail": "success"}
#                         except DoesNotExist as e:
#                             console_logger.debug("no data found")
#         return {"detail": "success"}
#     except Exception as e:
#         console_logger.debug("----- Road Sap Upload Error -----",e)
#         # response.status_code = 400
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#         console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#         return e


def determine_date_format(date_str):
    """
    Determine the date format of a given date string.
    :param date_str: The date string to check.
    :return: The matching date format or None if no match.
    """
    formats = ["%m/%d/%Y %H:%M:%S", "%Y-%m-%d"]
    for fmt in formats:
        try:
            datetime.datetime.strptime(date_str, fmt)
            return fmt
        except ValueError:
            continue
    return None


@router.get("/update/averydata", tags=["Rail Map"])
def endpoint_to_update_averydata(start_date: str, end_date: str):
    try:
        emailData = EmailDevelopmentCheck.objects()

        avery_id = emailData[0].avery_id
        avery_pass = emailData[0].avery_pass

        console_logger.debug(emailData[0].avery_id)
        console_logger.debug(emailData[0].avery_pass)
        console_logger.debug(emailData[0].wagontrippler1)
        console_logger.debug(emailData[0].wagontrippler2)
        console_logger.debug(emailData[0].port)

        proxies = {
            "http": None,
            "https": None
        }
        
        # urls = {
        #     "WagonTrippler_1": f"http://172.21.92.15:8081/API/values/getdata?FromDate={start_date}&ToDate={end_date}",          #available data from 2024/08/13
        #     "WagonTrippler_2": f"http://172.21.92.24:8081/API/values/getdata?FromDate={start_date}&ToDate={end_date}"           #available data from 2024/08/23
        # }

        urls = {
            "WagonTrippler_1": f"http://{emailData[0].wagontrippler1}:{emailData[0].port}/API/values/getdata?FromDate={start_date}&ToDate={end_date}",          #available data from 2024/08/13
            "WagonTrippler_2": f"http://{emailData[0].wagontrippler2}:{emailData[0].port}/API/values/getdata?FromDate={start_date}&ToDate={end_date}"           #available data from 2024/08/23
        }

        listData = []
        for name, url in urls.items():
            fetchAveryData = make_request(name, url, avery_id, avery_pass, proxies)
            if fetchAveryData:
                for singleAveryData in fetchAveryData:
                    if singleAveryData.get("rakeId"):
                        if " " in singleAveryData.get("rakeId"):
                            splitted_rakeid = singleAveryData.get("rakeId").split(" ")[0]
                        else:
                            splitted_rakeid = singleAveryData.get("rakeId")
                        # if "A" in singleAveryData.get("rakeId"):
                        #     rr_number = singleAveryData.get("rakeId").split("A")[1]
                        # elif "B" in singleAveryData.get("rakeId"):
                        #     rr_number = singleAveryData.get("rakeId").split("B")[1]
                        if "A" in splitted_rakeid:
                            rr_number = splitted_rakeid.split("A")[1]
                        elif "B" in splitted_rakeid:
                            rr_number = splitted_rakeid.split("B")[1]
                        date_formats = ["%m/%d/%Y %H:%M:%S", "%Y-%m-%d", "%d-%m-%Y", "%m/%d/%Y"]
                        try:
                            fetchRailData = RailData.objects.get(rr_no=rr_number)
                            fetchRailData.GWEL_received_wagons = str(len([entry["gwel_net_wt"] for entry in fetchRailData["avery_rly_data"] if entry["gwel_net_wt"]]))
                            fetchRailData.GWEL_pending_wagons = str(len([entry["gwel_net_wt"] for entry in fetchRailData["avery_rly_data"] if not entry["gwel_net_wt"]]))
                            fetchRailData.Total_gwel_gross = str(round(sum([float(entry["gwel_gross_wt"]) for entry in fetchRailData["avery_rly_data"] if entry["gwel_gross_wt"]]), 2))
                            fetchRailData.Total_gwel_tare = str(round(sum([float(entry["gwel_tare_wt"]) for entry in fetchRailData["avery_rly_data"] if entry["gwel_tare_wt"]]), 2))
                            fetchRailData.Total_gwel_net = str(round(sum([float(entry["gwel_net_wt"]) for entry in fetchRailData["avery_rly_data"] if entry["gwel_net_wt"]]), 2))
                            if fetchRailData:
                                start_date_tip = [
                                    f'{datetime.datetime.strptime(single_data.tip_startdate, determine_date_format(single_data.tip_startdate)).strftime("%Y-%m-%d")}T{single_data.tip_starttime}'
                                    for single_data in fetchRailData.avery_rly_data
                                    if single_data.tip_startdate and determine_date_format(single_data.tip_startdate)
                                ]
                                end_date_tip = [
                                    f'{datetime.datetime.strptime(single_data.tip_enddate, determine_date_format(single_data.tip_enddate)).strftime("%Y-%m-%d")}T{single_data.tip_endtime}'
                                    for single_data in fetchRailData.avery_rly_data
                                    if single_data.tip_enddate and determine_date_format(single_data.tip_enddate)
                                ]
                                if start_date_tip:
                                    # fetchRailData.avery_placement_date = [f'{datetime.datetime.strptime(single_data.tip_startdate, "%m/%d/%Y %H:%M:%S").strftime("%Y-%m-%d")}T{single_data.tip_starttime}' for single_data in fetchRailData.avery_rly_data if single_data.tip_startdate and single_data.tip_startdate.strip() != ""][0]
                                    fetchRailData.avery_placement_date =  [
                                        f'{datetime.datetime.strptime(single_data.tip_startdate, determine_date_format(single_data.tip_startdate)).strftime("%Y-%m-%d")}T{single_data.tip_starttime}'
                                        for single_data in fetchRailData.avery_rly_data
                                        if single_data.tip_startdate and determine_date_format(single_data.tip_startdate)
                                    ][0]
                                if end_date_tip:
                                    # fetchRailData.avery_completion_date = [f'{datetime.datetime.strptime(single_data.tip_enddate, "%m/%d/%Y %H:%M:%S").strftime("%Y-%m-%d")}T{single_data.tip_endtime}' for single_data in fetchRailData.avery_rly_data if single_data.tip_enddate and single_data.tip_enddate.strip() != ""][-1]
                                    fetchRailData.avery_completion_date = [
                                        f'{datetime.datetime.strptime(single_data.tip_enddate, determine_date_format(single_data.tip_enddate)).strftime("%Y-%m-%d")}T{single_data.tip_endtime}'
                                        for single_data in fetchRailData.avery_rly_data
                                        if single_data.tip_enddate and determine_date_format(single_data.tip_enddate)
                                    ][-1]
                                for single_rail_data in fetchRailData.avery_rly_data:
                                    if singleAveryData.get("wagonId")[-5:] == single_rail_data.wagon_no[-5:]:
                                        # Update fields
                                        single_rail_data.ser_no = singleAveryData.get("serNo")
                                        single_rail_data.rake_no = singleAveryData.get("rakeNo")
                                        # single_rail_data.rake_id = singleAveryData.get("rakeId")
                                        single_rail_data.rake_id = rr_number
                                        single_rail_data.wagon_no_avery = singleAveryData.get("wagonNo")
                                        single_rail_data.wagon_id = singleAveryData.get("wagonId")
                                        single_rail_data.wagon_type_avery = singleAveryData.get("wagonType")
                                        single_rail_data.wagon_cc = singleAveryData.get("wagonCC")
                                        single_rail_data.mode = singleAveryData.get("mode")
                                        single_rail_data.tip_startdate = singleAveryData.get("tipStartDate")
                                        single_rail_data.tip_starttime = singleAveryData.get("tipStartTime")
                                        single_rail_data.tip_enddate = singleAveryData.get("tipEndDate")
                                        single_rail_data.tip_endtime = singleAveryData.get("tipEndTime")
                                        single_rail_data.tipple_time = singleAveryData.get("tippleTime")
                                        single_rail_data.status = singleAveryData.get("status")
                                        single_rail_data.gwel_gross_wt = str(singleAveryData.get("wagonGrossWt"))
                                        single_rail_data.gwel_tare_wt = str(singleAveryData.get("wagonTareWt"))
                                        single_rail_data.gwel_net_wt = str(singleAveryData.get("wagonNetWt"))
                                        single_rail_data.time_in_tipp = singleAveryData.get("timeIn_tipp")
                                        single_rail_data.po_number = singleAveryData.get("ponumber")
                                        single_rail_data.coal_grade = singleAveryData.get("coalgrade")
                                        
                                        # Save the changes
                                        fetchRailData.save()
                            # return {"detail": "success"}
                        except DoesNotExist as e:
                            console_logger.debug("no data found")
                        except MultipleObjectsReturned: # added to ignore if muliple entry found on db on 21112024 at 12:39
                            continue 

                        try:
                            fetchRailData = RcrData.objects.get(rr_no=rr_number)
                            fetchRailData.GWEL_received_wagons = str(len([entry["gwel_net_wt"] for entry in fetchRailData["avery_rly_data"] if entry["gwel_net_wt"]]))
                            fetchRailData.GWEL_pending_wagons = str(len([entry["gwel_net_wt"] for entry in fetchRailData["avery_rly_data"] if not entry["gwel_net_wt"]]))
                            fetchRailData.Total_gwel_gross = str(round(sum([float(entry["gwel_gross_wt"]) for entry in fetchRailData["avery_rly_data"] if entry["gwel_gross_wt"]]), 2))
                            fetchRailData.Total_gwel_tare = str(round(sum([float(entry["gwel_tare_wt"]) for entry in fetchRailData["avery_rly_data"] if entry["gwel_tare_wt"]]), 2))
                            fetchRailData.Total_gwel_net = str(round(sum([float(entry["gwel_net_wt"]) for entry in fetchRailData["avery_rly_data"] if entry["gwel_net_wt"]]), 2))
                            if fetchRailData:
                                # start_date_tip = [f'{datetime.datetime.strptime(single_data.tip_enddate, "%m/%d/%Y %H:%M:%S").strftime("%Y-%m-%d")}T{single_data.tip_endtime}' for single_data in fetchRailData.avery_rly_data if single_data.tip_enddate is not None]
                                # end_date_tip = [f'{datetime.datetime.strptime(single_data.tip_enddate, "%m/%d/%Y %H:%M:%S").strftime("%Y-%m-%d")}T{single_data.tip_endtime}' for single_data in fetchRailData.avery_rly_data if single_data.tip_enddate is not None]
                                start_date_tip = [
                                    f'{datetime.datetime.strptime(single_data.tip_startdate, determine_date_format(single_data.tip_startdate)).strftime("%Y-%m-%d")}T{single_data.tip_starttime}'
                                    for single_data in fetchRailData.avery_rly_data
                                    if single_data.tip_startdate and determine_date_format(single_data.tip_startdate)
                                ]
                                end_date_tip = [
                                    f'{datetime.datetime.strptime(single_data.tip_enddate, determine_date_format(single_data.tip_enddate)).strftime("%Y-%m-%d")}T{single_data.tip_endtime}'
                                    for single_data in fetchRailData.avery_rly_data
                                    if single_data.tip_enddate and determine_date_format(single_data.tip_enddate)
                                ]
                                if start_date_tip:
                                    # fetchRailData.avery_placement_date = [f'{datetime.datetime.strptime(single_data.tip_startdate, "%m/%d/%Y %H:%M:%S").strftime("%Y-%m-%d")}T{single_data.tip_starttime}' for single_data in fetchRailData.avery_rly_data if single_data.tip_startdate is not None][0]
                                    fetchRailData.avery_placement_date =  [
                                        f'{datetime.datetime.strptime(single_data.tip_startdate, determine_date_format(single_data.tip_startdate)).strftime("%Y-%m-%d")}T{single_data.tip_starttime}'
                                        for single_data in fetchRailData.avery_rly_data
                                        if single_data.tip_startdate and determine_date_format(single_data.tip_startdate)
                                    ][0]
                                if end_date_tip:
                                    fetchRailData.avery_completion_date = [f'{datetime.datetime.strptime(single_data.tip_enddate, "%m/%d/%Y %H:%M:%S").strftime("%Y-%m-%d")}T{single_data.tip_endtime}' for single_data in fetchRailData.avery_rly_data if single_data.tip_enddate is not None][-1]
                                    fetchRailData.avery_completion_date = [
                                        f'{datetime.datetime.strptime(single_data.tip_enddate, determine_date_format(single_data.tip_enddate)).strftime("%Y-%m-%d")}T{single_data.tip_endtime}'
                                        for single_data in fetchRailData.avery_rly_data
                                        if single_data.tip_enddate and determine_date_format(single_data.tip_enddate)
                                    ][-1]
                                for single_rail_data in fetchRailData.avery_rly_data:
                                    if singleAveryData.get("wagonId")[-5:] == single_rail_data.wagon_no[-5:]:
                                        # Update fields
                                        single_rail_data.ser_no = singleAveryData.get("serNo")
                                        single_rail_data.rake_no = singleAveryData.get("rakeNo")
                                        # single_rail_data.rake_id = singleAveryData.get("rakeId")
                                        single_rail_data.rake_id = rr_number
                                        single_rail_data.wagon_no_avery = singleAveryData.get("wagonNo")
                                        single_rail_data.wagon_id = singleAveryData.get("wagonId")
                                        single_rail_data.wagon_type_avery = singleAveryData.get("wagonType")
                                        single_rail_data.wagon_cc = singleAveryData.get("wagonCC")
                                        single_rail_data.mode = singleAveryData.get("mode")
                                        single_rail_data.tip_startdate = singleAveryData.get("tipStartDate")
                                        single_rail_data.tip_starttime = singleAveryData.get("tipStartTime")
                                        single_rail_data.tip_enddate = singleAveryData.get("tipEndDate")
                                        single_rail_data.tip_endtime = singleAveryData.get("tipEndTime")
                                        single_rail_data.tipple_time = singleAveryData.get("tippleTime")
                                        single_rail_data.status = singleAveryData.get("status")
                                        single_rail_data.gwel_gross_wt = str(singleAveryData.get("wagonGrossWt"))
                                        single_rail_data.gwel_tare_wt = str(singleAveryData.get("wagonTareWt"))
                                        single_rail_data.gwel_net_wt = str(singleAveryData.get("wagonNetWt"))
                                        single_rail_data.time_in_tipp = singleAveryData.get("timeIn_tipp")
                                        single_rail_data.po_number = singleAveryData.get("ponumber")
                                        single_rail_data.coal_grade = singleAveryData.get("coalgrade")
                                        
                                        # Save the changes
                                        fetchRailData.save()
                            # return {"detail": "success"}
                        except DoesNotExist as e:
                            console_logger.debug("no data found")
                        except MultipleObjectsReturned: # added to ignore if muliple entry found on db on 21112024 at 12:39
                            continue 


        return {"detail": "success"}
    except Exception as e:
        console_logger.debug("----- Road Sap Upload Error -----",e)
        # response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/update/averydata/test", tags=["Rail Map"])
def endpoint_to_update_averydata_test(response:Response):
    try:
        fetchAveryData = [{'serNo': '3', 'rakeNo': '2', 'rakeId': 'B162014638', 'wagonNo': '3', 'wagonId': 'SECR    12254', 'wagonType': 'Single', 'wagonCC': '80', 'mode': 'MD MD', 'tipStartDate': '08/25/2024 00:00:00', 'tipStartTime': '00:21:55', 'tipEndDate': '08/25/2024 00:00:00', 'tipEndTime': '00:37:54', 'tippleTime': '16', 'status': None, 'wagonGrossWt': 88.55, 'wagonTareWt': 20.7, 'wagonNetWt': 67.85, 'timeIn_tipp': None, 'ponumber': '4500099650', 'coalgrade': 'G-11'}, {'serNo': '3', 'rakeNo': '2', 'rakeId': 'B162014638', 'wagonNo': '4', 'wagonId': 'WC      10219', 'wagonType': 'Single', 'wagonCC': '80', 'mode': 'MD MD', 'tipStartDate': '08/25/2024 00:00:00', 'tipStartTime': '00:40:53', 'tipEndDate': '08/25/2024 00:00:00', 'tipEndTime': '02:16:50', 'tippleTime': '96', 'status': None, 'wagonGrossWt': 89.5, 'wagonTareWt': 20.95, 'wagonNetWt': 68.55, 'timeIn_tipp': None, 'ponumber': '4500099650', 'coalgrade': 'G-11'}, {'serNo': '3', 'rakeNo': '2', 'rakeId': 'B162014638', 'wagonNo': '5', 'wagonId': 'NR       81575', 'wagonType': 'Single', 'wagonCC': '80', 'mode': 'MD MD', 'tipStartDate': '08/25/2024 00:00:00', 'tipStartTime': '02:19:39', 'tipEndDate': '08/25/2024 00:00:00', 'tipEndTime': '02:23:07', 'tippleTime': '4', 'status': None, 'wagonGrossWt': 87.1, 'wagonTareWt': 22.55, 'wagonNetWt': 64.55, 'timeIn_tipp': None, 'ponumber': '4500099650', 'coalgrade': 'G-11'}, {'serNo': '3', 'rakeNo': '2', 'rakeId': 'B162014638', 'wagonNo': '6', 'wagonId': 'SECR   812070', 'wagonType': 'Single', 'wagonCC': '80', 'mode': 'MD MD', 'tipStartDate': '08/25/2024 00:00:00', 'tipStartTime': '02:25:23', 'tipEndDate': '08/25/2024 00:00:00', 'tipEndTime': '02:46:04', 'tippleTime': '21', 'status': None, 'wagonGrossWt': 86.4, 'wagonTareWt': 24.0, 'wagonNetWt': 62.4, 'timeIn_tipp': None, 'ponumber': '4500099650', 'coalgrade': 'G-11'}, {'serNo': '3', 'rakeNo': '2', 'rakeId': 'B162014638', 'wagonNo': '7', 'wagonId': 'SECR   21836', 'wagonType': 'Single', 'wagonCC': '80', 'mode': 'MD MD', 'tipStartDate': '08/25/2024 00:00:00', 'tipStartTime': '02:48:25', 'tipEndDate': '08/25/2024 00:00:00', 'tipEndTime': '02:59:44', 'tippleTime': '11', 'status': None, 'wagonGrossWt': 85.15, 'wagonTareWt': 21.4, 'wagonNetWt': 63.75, 'timeIn_tipp': None, 'ponumber': '4500099650', 'coalgrade': 'G-11'}, {'serNo': '3', 'rakeNo': '2', 'rakeId': 'B162014638', 'wagonNo': '8', 'wagonId': 'SECR   10714', 'wagonType': 'Single', 'wagonCC': '80', 'mode': 'MD MD', 'tipStartDate': '08/25/2024 00:00:00', 'tipStartTime': '03:01:47', 'tipEndDate': '08/25/2024 00:00:00', 'tipEndTime': '03:58:32', 'tippleTime': '57', 'status': None, 'wagonGrossWt': 93.8, 'wagonTareWt': 21.95, 'wagonNetWt': 71.85, 'timeIn_tipp': None, 'ponumber': '4500099650', 'coalgrade': 'G-11'}, {'serNo': '3', 'rakeNo': '2', 'rakeId': 'B162014638', 'wagonNo': '9', 'wagonId': 'SECR   10752', 'wagonType': 'Single', 'wagonCC': '80', 'mode': 'MD MD', 'tipStartDate': '08/25/2024 00:00:00', 'tipStartTime': '04:00:49', 'tipEndDate': '08/25/2024 00:00:00', 'tipEndTime': '04:05:42', 'tippleTime': '5', 'status': None, 'wagonGrossWt': 83.05, 'wagonTareWt': 22.0, 'wagonNetWt': 61.05, 'timeIn_tipp': None, 'ponumber': '4500099650', 'coalgrade': 'G-11'}]
        listData = []
        if fetchAveryData:
            
            for singleAveryData in fetchAveryData:
                if "A" in singleAveryData.get("rakeId"):
                    rr_number = singleAveryData.get("rakeId").split("A")[1]
                elif "B" in singleAveryData.get("rakeId"):
                    rr_number = singleAveryData.get("rakeId").split("B")[1]
                fetchRailData = RailData.objects.get(rr_no=rr_number)

                if fetchRailData:
                    for single_rail_data in fetchRailData.avery_rly_data:
                        if singleAveryData.get("wagonId")[-5:] == single_rail_data.wagon_no[-5:]:
                            single_rail_data.ser_no = singleAveryData.get("serNo")
                            single_rail_data.rake_no = singleAveryData.get("rakeNo")
                            single_rail_data.rake_id = singleAveryData.get("rakeId")
                            single_rail_data.wagon_no_avery = singleAveryData.get("wagonNo")
                            single_rail_data.wagon_id = singleAveryData.get("wagonId")
                            single_rail_data.wagon_type_avery = singleAveryData.get("wagonType")
                            single_rail_data.wagon_cc = singleAveryData.get("wagonCC")
                            single_rail_data.mode = singleAveryData.get("mode")
                            single_rail_data.tip_startdate = singleAveryData.get("tipStartDate")
                            single_rail_data.tip_starttime = singleAveryData.get("tipStartTime")
                            single_rail_data.tip_enddate = singleAveryData.get("tipEndDate")
                            single_rail_data.tip_endtime = singleAveryData.get("tipEndTime")
                            single_rail_data.tipple_time = singleAveryData.get("tippleTime")
                            single_rail_data.status = singleAveryData.get("status")
                            single_rail_data.wagon_gross_wt = str(singleAveryData.get("wagonGrossWt"))
                            single_rail_data.wagon_tare_wt = str(singleAveryData.get("wagonTareWt"))
                            single_rail_data.wagon_net_wt = str(singleAveryData.get("wagonNetWt"))
                            single_rail_data.time_in_tipp = singleAveryData.get("timeIn_tipp")
                            single_rail_data.po_number = singleAveryData.get("ponumber")
                            single_rail_data.coal_grade = singleAveryData.get("coalgrade")

                            fetchRailData.save()

        return {"detail": "success"}
    except Exception as e:
        console_logger.debug("----- Road Sap Upload Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e       


@router.post("/update/useraverydata", tags=["Rail Map"])
def endpoint_to_update_avery_user_data(response: Response, data: mainAveryData, rr_no: str, placement_date: str, completion_date: str, GWEL_received_wagons: str, GWEL_pending_wagons: str, Total_gwel_gross: str, Total_gwel_tare: str, Total_gwel_net: str, Total_no_of_boxes_supplied: str, Total_no_of_boxes_loaded: str):
    try:
        payload = data.dict()
        fetchRailData = RailData.objects.get(rr_no=rr_no)
        fetchRailData.avery_placement_date = placement_date
        fetchRailData.avery_completion_date = completion_date
        fetchRailData.GWEL_received_wagons = GWEL_received_wagons
        fetchRailData.GWEL_pending_wagons = GWEL_pending_wagons
        fetchRailData.Total_gwel_gross = Total_gwel_gross
        fetchRailData.Total_gwel_tare = Total_gwel_tare
        fetchRailData.Total_gwel_net = Total_gwel_net
        fetchRailData.boxes_loaded = Total_no_of_boxes_loaded
        fetchRailData.boxes_supplied = Total_no_of_boxes_supplied

        if payload.get("data"):
            avery_user_data_instances = [AveryRailData(**item) for item in payload.get("data")]
            fetchRailData.avery_rly_data = avery_user_data_instances
        else:
            fetchRailData.avery_rly_data.clear()
        try:
            fetchRailData.save()
        except ValidationError as e:
            console_logger.error(f"Validation error while saving RailData: {e}")
            return {"details": "error", "message": str(e)}
        return {"details": "success"}
    except Exception as e:
        console_logger.debug("----- Road Sap Upload Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e  


@router.post("/update/useraverydatarcr", tags=["Rail Map"])
def endpoint_to_update_avery_user_data(response: Response, data: mainAveryData, rr_no: str, placement_date: str, completion_date: str, GWEL_received_wagons: str, GWEL_pending_wagons: str, Total_gwel_gross: str, Total_gwel_tare: str, Total_gwel_net: str, Total_no_of_boxes_supplied: str, Total_no_of_boxes_loaded: str):
    try:
        payload = data.dict()
        fetchRailData = RcrData.objects.get(rr_no=rr_no)
        fetchRailData.avery_placement_date = placement_date
        fetchRailData.avery_completion_date = completion_date
        fetchRailData.GWEL_received_wagons = GWEL_received_wagons
        fetchRailData.GWEL_pending_wagons = GWEL_pending_wagons
        fetchRailData.Total_gwel_gross = Total_gwel_gross
        fetchRailData.Total_gwel_tare = Total_gwel_tare
        fetchRailData.Total_gwel_net = Total_gwel_net
        fetchRailData.boxes_loaded = Total_no_of_boxes_loaded
        fetchRailData.boxes_supplied = Total_no_of_boxes_supplied

        if payload.get("data"):
            avery_user_data_instances = [AveryRailData(**item) for item in payload.get("data")]
            fetchRailData.avery_rly_data = avery_user_data_instances
        else:
            fetchRailData.avery_rly_data.clear()
        try:
            fetchRailData.save()
        except ValidationError as e:
            console_logger.error(f"Validation error while saving RcrData: {e}")
            return {"details": "error", "message": str(e)}
        return {"details": "success"}
    except Exception as e:
        console_logger.debug("----- Update Avery Rcr Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e  


@router.get("/fetch/avery/singlerail", tags=["Rail Map"])
def endpoint_to_fetch_avery_railway_data(response: Response, rrno: str):
    try:
        fetchRailData = RailData.objects.get(rr_no=rrno)
        dictData = fetchRailData.averyPayload()
        if fetchRailData.avery_placement_date and fetchRailData.avery_completion_date:
            dictData["avery_placement"] = fetchRailData.avery_placement_date
            dictData["avery_completion"] = fetchRailData.avery_completion_date
        else:
            if fetchRailData.avery_rly_data:
                startlistData = []
                endlistData = []
                for singleRailData in fetchRailData.avery_rly_data:
                    if singleRailData.tip_startdate and singleRailData.tip_starttime:
                        start_dated_date = f'{datetime.datetime.strptime(singleRailData.tip_startdate, "%m/%d/%Y %H:%M:%S").strftime("%Y-%m-%d")} {singleRailData.tip_starttime}'
                        startlistData.append(start_dated_date)
                    if singleRailData.tip_enddate and singleRailData.tip_endtime:
                        end_dated_date = f'{datetime.datetime.strptime(singleRailData.tip_startdate, "%m/%d/%Y %H:%M:%S").strftime("%Y-%m-%d")} {singleRailData.tip_starttime}'
                        endlistData.append(end_dated_date)
                
                if startlistData and endlistData:
                    dictData["avery_placement"] = startlistData[0]
                    dictData["avery_completion"] = endlistData[-1]
                else:
                    dictData["avery_placement"] = ""
                    dictData["avery_completion"] = ""
            else:
                dictData["avery_placement"] = ""
                dictData["avery_completion"] = ""
            
        mongoPipeline = [
            {
                '$match': {
                    'rr_no': rrno,
                }
            }, {
                '$unwind': '$avery_rly_data'
            }, {
                '$match': {
                    'avery_rly_data.po_number': {
                        '$exists': True
                    }
                }
            }, {
                '$group': {
                    '_id': None, 
                    'count': {
                        '$sum': 1
                    }
                }
            }
        ]

        railDataobjects = RailData.objects().aggregate(mongoPipeline)

        railData_result = list(railDataobjects)
        if railData_result:
            dictData["total_avery"] = railData_result[0].get("count")
        else:
            dictData["total_avery"] = 0
        return dictData
    except DoesNotExist as e:
        raise HTTPException(status_code=404, detail="No data found")
    except Exception as e:
        console_logger.debug("----- Fetch Railway Data Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/update/saprecordsroad", tags=["Road Map"])
def endpoint_to_update_averydata_saprecords(response:Response):
    try:
        fetchsapRecords = SapRecords.objects()
        for single_saprecords in fetchsapRecords:
            if single_saprecords:
                gmrDatafetch = Gmrdata.objects(
                        arv_cum_do_number=single_saprecords.do_no,
                    )
                if gmrDatafetch:
                    gmrDatafetch.update(
                            do_date=single_saprecords.do_date, 
                            start_date=single_saprecords.start_date, 
                            end_date=single_saprecords.end_date, 
                            slno=single_saprecords.slno,
                            type_consumer= single_saprecords.consumer_type,
                            grade= single_saprecords.grade,
                            mine= single_saprecords.mine_name,
                            po_qty= single_saprecords.do_qty,
                            po_amount= single_saprecords.po_amount)

                gmrDataHistfetch = gmrdataHistoric.objects(
                        arv_cum_do_number=single_saprecords.do_no,
                    )
                if gmrDataHistfetch:
                    gmrDataHistfetch.update(
                            do_date=single_saprecords.do_date, 
                            start_date=single_saprecords.start_date, 
                            end_date=single_saprecords.end_date, 
                            slno=single_saprecords.slno,
                            type_consumer= single_saprecords.consumer_type,
                            grade= single_saprecords.grade,
                            mine= single_saprecords.mine_name,
                            po_qty= single_saprecords.do_qty,
                            po_amount= single_saprecords.po_amount)
                
        return {"detail": "success"}
    except Exception as e:
        console_logger.debug("----- Road Saprecords Upload Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e
    

@router.get("/update/saprecordsrail", tags=["Rail Map"])
def endpoint_to_update_saprecprdsrail(response:Response):
    try:
        fetchrailsapRecords = sapRecordsRail.objects()
        for single_saprecords in fetchrailsapRecords:
            if single_saprecords:
                railDatafetch = RailData.objects(
                        rr_no=single_saprecords.rr_no, siding=single_saprecords.siding
                    )
                if railDatafetch:
                    if single_saprecords.month is not None:
                        # console_logger.debug(single_saprecords.month)
                        # if len(single_saprecords.month) == 7:
                        #     current_month_str = single_saprecords.month
                        #     console_logger.debug(single_saprecords.rr_no)
                        #     console_logger.debug(current_month_str)
                        # elif len(single_saprecords.month) == 10:
                        #     date_obj = datetime.datetime.strptime(single_saprecords.month, "%Y-%m-%d")

                        #     # Check if the day is less than or equal to 3
                        #     if date_obj.day <= 3:
                        #         # Calculate the previous month by subtracting one day from the 1st of the current month
                        #         first_day_of_current_month = date_obj.replace(day=1)
                        #         previous_month = first_day_of_current_month - timedelta(days=1)

                        #         # Format and display the previous month (YYYY-MM)
                        #         # previous_month_str = previous_month.strftime("%Y-%m")
                        #         current_month_str = previous_month.strftime("%Y-%m")
                        #         console_logger.debug(single_saprecords.rr_no)
                        #         console_logger.debug(current_month_str)
                        #     else:
                        #         # Otherwise, display the current month
                        #         current_month_str = date_obj.strftime("%Y-%m")
                        #         console_logger.debug(single_saprecords.rr_no)
                        #         console_logger.debug(current_month_str)
                        # elif len(single_saprecords.month) == 11:
                        #     date_obj = datetime.datetime.strptime(single_saprecords.month, "%b %d, %Y")

                        #     #  Check if the day is less than or equal to 3
                        #     if date_obj.day <= 3:
                        #         # Calculate the previous month by subtracting one day from the 1st of the current month
                        #         first_day_of_current_month = date_obj.replace(day=1)
                        #         previous_month = first_day_of_current_month - timedelta(days=1)

                        #         # Format and display the previous month (YYYY-MM)
                        #         current_month_str = previous_month.strftime("%Y-%m")
                        #         console_logger.debug(single_saprecords.rr_no)
                        #         console_logger.debug(current_month_str)
                        #     else:
                        #         # Otherwise, display the current month
                        #         current_month_str = date_obj.strftime("%Y-%m")
                        #         console_logger.debug(single_saprecords.rr_no)
                        #         console_logger.debug(current_month_str)
                        
                        # month commented by sachin bhai on 11-11-2024 on 12:42pm 
                        # if len(single_saprecords.month) == 7:
                        #     current_month_str = single_saprecords.month
                        # elif len(single_saprecords.month) == 10:
                        #     current_month_str = datetime.datetime.strptime(single_saprecords.month, "%Y-%m-%d").strftime("%Y-%m")
                        # elif len(single_saprecords.month) == 11:
                        #     current_month_str = datetime.datetime.strptime(single_saprecords.month, "%b %d, %Y").strftime("%Y-%m")

                        railDatafetch.update(
                                # month=datetime.datetime.strptime(single_saprecords.month, '%b %d, %Y').strftime('%Y-%m-%d'), 
                                # month=datetime.datetime.strptime(single_saprecords.month, '%Y-%m-%d').strftime('%Y-%m-%d'),
                                # month=current_month_str,
                                rr_date=single_saprecords.rr_date, 
                                siding=single_saprecords.siding, 
                                mine=single_saprecords.mine,
                                grade= single_saprecords.grade,
                                rr_qty= single_saprecords.rr_qty,
                                po_amount= single_saprecords.po_amount)
        return {"detail": "success"}
    except Exception as e:
        console_logger.debug("----- Road Saprecords Upload Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/extract_secl_annexure", tags=["Rail Map"])                                   
async def extract_secl_annexure(response: Response, file: UploadFile = File(...)):
    try:
        if file is None:
            return {"error": "No file Uploaded!"}
        
        contents = await file.read()
        
        if not contents:
            return {"error": "Uploaded file is empty!"}

        if file.filename.endswith(".xlsx"):
            # file saving start
            date = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{date}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)
            file_extension = file.filename.split(".")[-1]
            file_name = f'secl_annexure_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.{file_extension}'
            full_path = os.path.join(os.getcwd(), target_directory, file_name)
            with open(full_path, "wb") as file_object:
                file_object.write(contents)
            # file saving end

            excel_data = pd.read_excel(BytesIO(contents))
            data_excel_fetch = json.loads(excel_data.to_json(orient="records"))

            listData = []
            for single_data in data_excel_fetch:
                if single_data.get("WAGON TYPE") is not None:
                    single_data["WAGON NO"] = str(single_data.get('WAGON NO'))
                    temp = re.compile("([a-zA-Z]+)([0-9]+)")
                    match = temp.match(str(single_data.get('WAGON NO')))
                    if match:
                        res = match.groups()
                        wagon_no_str = res[1]
                        single_data["WAGON NO"] = str(wagon_no_str)
                    listData.append(single_data)
            return listData
    except KeyError as e:
        raise HTTPException(status_code=404, detail="Key Error")
    except Exception as e:
        console_logger.debug("----- Secl Annexure Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/coal_gcv_comparison_analysis", tags=["Coal Consumption"])                                   
def coal_gdv_comparision_analysis(response: Response):
    try:
        coalTestingPipeline = [
            {
                '$addFields': {
                    'year': {
                        '$year': {
                            '$toDate': '$receive_date'
                        }
                    }, 
                    'month': {
                        '$month': {
                            '$toDate': '$receive_date'
                        }
                    }, 
                    'yearMonth': {
                        '$dateToString': {
                            'format': '%Y-%m', 
                            'date': {
                                '$toDate': '$receive_date'
                            }
                        }
                    }
                }
            }, {
                '$unwind': '$parameters'
            }, {
                '$match': {
                    'parameters.parameter_Name': 'Gross_Calorific_Value_(Arb)'
                }
            }, {
                '$group': {
                    '_id': '$yearMonth', 
                    'total_rR_Qty': {
                        '$sum': {
                            '$toDouble': {
                                '$replaceAll': {
                                    'input': '$rR_Qty', 
                                    'find': ',', 
                                    'replacement': ''
                                }
                            }
                        }
                    }, 
                    'total_arb_GCV': {
                        '$sum': {
                            '$toDouble': {
                                '$replaceAll': {
                                    'input': '$parameters.val1', 
                                    'find': ',', 
                                    'replacement': ''
                                }
                            }
                        }
                    }, 
                    'year': {
                        '$first': '$year'
                    }, 
                    'month': {
                        '$first': '$month'
                    }
                }
            }, {
                '$sort': {
                    '_id': 1
                }
            }, {
                '$project': {
                    '_id': 0, 
                    'yearMonth': '$_id', 
                    'year': 1, 
                    'month': 1, 
                    'total_rR_Qty': 1, 
                    'total_arb_GCV': 1
                }
            }
        ]

        bunkerDataPipeline = [
            {
                '$addFields': {
                    'year': {
                        '$year': {
                            '$toDate': '$sample_received_date'
                        }
                    }, 
                    'month': {
                        '$month': {
                            '$toDate': '$sample_received_date'
                        }
                    }, 
                    'yearMonth': {
                        '$dateToString': {
                            'format': '%Y-%m', 
                            'date': {
                                '$toDate': '$sample_received_date'
                            }
                        }
                    }
                }
            }, {
                '$unwind': '$sample_parameters'
            }, {
                '$match': {
                    'sample_parameters.parameter_type': 'ReceivedBasis_GCV'
                }
            }, {
                '$group': {
                    '_id': '$yearMonth', 
                    'total_rR_Qty': {
                        '$sum': {
                            '$toDouble': '$rR_Qty'
                        }
                    }, 
                    'total_ReceivedBasis_GCV': {
                        '$sum': {
                            '$toDouble': '$sample_parameters.val1'
                        }
                    }, 
                    'year': {
                        '$first': '$year'
                    }, 
                    'month': {
                        '$first': '$month'
                    }
                }
            }, {
                '$sort': {
                    '_id': 1
                }
            }, {
                '$project': {
                    '_id': 0, 
                    'yearMonth': '$_id', 
                    'year': 1, 
                    'month': 1, 
                    'total_rR_Qty': 1, 
                    'total_ReceivedBasis_GCV': 1
                }
            }
        ]

        # coalTestingobjects = CoalTesting.objects().aggregate(coalTestingPipeline)    
        # bunkerDataobjects = BunkerDataExtra.objects().aggregate(bunkerDataPipeline)    
        # # bunkerlistData = []
        # # bunkercoal = {}
        # # coalreceipt = {}
        # grouped_data = defaultdict(list)
        # for coal_single_data in coalTestingobjects:
        #     console_logger.debug(coal_single_data)
        #     coal_single_data["imported_qty"] = 0
        #     # coal_single_data.get("total_rR_Qty")
        #     # coal_single_data.get("total_arb_GCV")
        #     # coal_single_data.get("year")
        #     # coal_single_data.get("yearMonth")
        #     # bunkerlistData.append()
        #     # bunkercoal[coal_single_data.get("year")] = coal_single_data
        #     year = coal_single_data.get("year")
        #     grouped_data[year].append(coal_single_data)


        # Execute the aggregation pipeline
        coalTestingobjects = CoalTesting.objects().aggregate(coalTestingPipeline)
        bunkerDataobjects = BunkerData.objects().aggregate(bunkerDataPipeline) 

        # Initialize the data structure for storing results
        grouped_data = defaultdict(list)
        bunker_grouped_data = defaultdict(list)

        finalDict = {}

        # Process the aggregated data
        for coal_single_data in coalTestingobjects:
            coal_single_data["imported_qty"] = 0
            year = coal_single_data.get("year")
            grouped_data[year].append(coal_single_data)

        
        for bunker_single_data in bunkerDataobjects:
            bunker_single_data["imported_qty"] = 0
            bunker_single_data["imported_gcv"] = 0
            bunker_year = bunker_single_data.get("year")
            bunker_grouped_data[bunker_year].append(bunker_single_data)

        # Calculate annually_gcv
        annually_data = defaultdict(list)
        for year, data_list in grouped_data.items():
            # Sort data by month
            sorted_data = sorted(data_list, key=lambda x: x['yearMonth'])
            cumulative_gcv = 0
            for entry in sorted_data:
                cumulative_gcv += entry['total_arb_GCV']
                entry['annually_gcv'] = cumulative_gcv
                annually_data[year].append(entry)

        # Log the final result
        finalDict["coal_receipt"] = dict(annually_data)

        # Calculate annually_gcv
        bunker_annually_data = defaultdict(list)
        for bunker_year, bunker_data_list in bunker_grouped_data.items():
            # Sort data by month
            bunker_sorted_data = sorted(bunker_data_list, key=lambda x: x['yearMonth'])
            bunker_cumulative_gcv = 0
            for bunker_entry in bunker_sorted_data:
                bunker_cumulative_gcv += bunker_entry['total_ReceivedBasis_GCV']
                bunker_entry['annually_gcv'] = bunker_cumulative_gcv
                bunker_annually_data[bunker_year].append(bunker_entry)

        finalDict["bunker_data"] = dict(bunker_annually_data)

        # Initialize an empty dictionary to store the results
        mtd_ytd_result = {"mtd": {}, "ytd": {}}

        # Get the list of years from the data keys (assuming both datasets have the same years)
        years = annually_data.keys()

        # Process the data for each year
        for year in years:
            # Extract data for the current year
            coal_data = dict(annually_data).get(year, [])
            bunker_data = dict(bunker_annually_data).get(year, [])

            # Create a dictionary to store month-wise results for the current year
            month_wise_result = {}
            ytd_result = {}

            for entry in coal_data:
                month = entry['month']
                total_arb_GCV = entry['total_arb_GCV']
                annually_gcv_coal = entry['annually_gcv']

                # Find the corresponding entry in bunkerTestingData
                bunker_entry = next((item for item in bunker_data if item['month'] == month), None)

                if bunker_entry:
                    total_ReceivedBasis_GCV = bunker_entry['total_ReceivedBasis_GCV']
                    annually_gcv_bunker = bunker_entry['annually_gcv']

                    # Calculate month-wise difference
                    difference = total_arb_GCV - total_ReceivedBasis_GCV
                    month_wise_result[entry['yearMonth']] = difference

                    # Calculate year-to-date (YTD) difference
                    ytd_difference = annually_gcv_coal - annually_gcv_bunker
                    ytd_result[entry['yearMonth']] = ytd_difference

            # Add the results for the current year to the final result dictionary
            mtd_ytd_result['mtd'][year] = month_wise_result
            mtd_ytd_result['ytd'][year] = ytd_result

        finalDict["gcv_difference"] = mtd_ytd_result
        date = str(datetime.datetime.now().strftime("%Y-%m-%d"))
        fetchFinalData = generate_report_comparision(date, finalDict)
        return fetchFinalData
    except Exception as e:
        console_logger.debug("----- Road Saprecords Upload Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/pdf_coal_comparison_analysis", tags=["Road Map"])
def pdf_coal_gcv_comparison_analysis(response:Response):
    try:
        finalDict = coal_gcv_comparision_analysis(response)
        date = str(datetime.datetime.now().strftime("%Y-%m-%d"))
        fetchFinalData = generate_report_comparision(date, finalDict)
        return fetchFinalData
    except Exception as e:
        console_logger.debug("----- Coal Gcv Comparison Analysis Error -----",e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

def sort_key(entry):
    month = entry["coal_receipt_month"]
    return (month if month != 12 else 0)

def get_financial_year_bunker(year, month):
    if month >= 4:  # If month is April or later, use the same year
        return year
    else:  # Otherwise, use the previous year
        return year - 1


@router.get("/coal_consumption_graph_month", tags=["Coal Consumption"])
def coal_consumption_analysis_month(response: Response):
    try:
        data = {}
        UTC_OFFSET_TIMEDELTA = datetime.datetime.utcnow() - datetime.datetime.now()

        basePipeline = [
            {
                '$match': {
                    'tagid': {
                        '$in': [
                            16, 3536
                        ]
                    }
                }
            }, {
                '$addFields': {
                    'sum': {
                        '$toDouble': '$sum'
                    }
                }
            }, {
                '$group': {
                    '_id': {
                        'year': {
                            '$year': '$created_date'
                        }, 
                        'month': {
                            '$month': '$created_date'
                        }, 
                        'tagid': '$tagid'
                    }, 
                    'total_value': {
                        '$sum': '$sum'
                    }
                }
            }, {
                '$project': {
                    '_id': 0, 
                    'year': '$_id.year', 
                    'month': '$_id.month', 
                    'tagid': '$_id.tagid', 
                    'total_value': 1
                }
            }, {
                '$sort': {
                    'year': 1, 
                    'month': 1, 
                    'tagid': 1
                }
            }
        ]

        output = Historian.objects().aggregate(basePipeline)
        outputDict = {}

        grouped_data = defaultdict(list)

        for data in output:
            year = data.get("year")
            grouped_data[year].append(data)

        year_month_wise_sum = {}

        # Loop through the data for each year dynamically
        for year, entries in dict(grouped_data).items():
            # Initialize a dictionary for the current year
            if year not in year_month_wise_sum:
                year_month_wise_sum[year] = {}
            
            # Loop through the entries for each year
            for entry in entries:
                month = entry['month']
                total_value = entry['total_value']
                
                # Sum the total_value for each month of the current year
                if month in year_month_wise_sum[year]:
                    year_month_wise_sum[year][month] += total_value
                else:
                    year_month_wise_sum[year][month] = total_value

        # Base pipeline for both CoalTesting and coaltestingtrain collections
        basePipelineCoal = [
            {
               "$match": {
                    "parameters.parameter_Name": "Gross_Calorific_Value_(Adb)"
                }
            },
            {
                "$unwind": "$parameters"
            },
            {
                "$match": {
                    "parameters.parameter_Name": "Gross_Calorific_Value_(Adb)"
                }
            },
            {
                "$project": {
                    "year": { "$year": "$receive_date" },
                    "month": { "$month": "$receive_date" },
                    "value": { "$toDouble": "$parameters.val1" }
                }
            },
            {
                "$group": {
                    "_id": { "year": "$year", "month": "$month" },
                    "avg_gross_calorific_value_adb": { "$avg": "$value" }
                }
            },
            {
                "$sort": {
                    "_id.year": 1,
                    "_id.month": 1
                }
            }
        ]

        # Apply the basePipeline to both collections (CoalTesting and coaltestingtrain)
        # Aggregate the results from both collections
        # output_coaltesting = db.CoalTesting.aggregate(basePipelineCoal)
        output_coaltesting = CoalTesting.objects().aggregate(basePipelineCoal)
        output_coaltestingtrain = CoalTestingTrain.objects().aggregate(basePipelineCoal)
        # output_coaltestingtrain = db.coaltestingtrain.aggregate(basePipelineCoal)

        # Merge the results from both collections (if needed)
        combined_output = list(output_coaltesting) + list(output_coaltestingtrain)

        # Initialize the result dictionary
        result = {}

        # Loop through the data
        for entry in combined_output:
            year = entry['_id']['year']
            month = entry['_id']['month']
            avg_value = entry['avg_gross_calorific_value_adb']

            # Prepare the month data
            month_data = {'month': month, 'avg_gross_calorific_value_adb': avg_value}

            # Check if the year exists in the result
            if year in result:
                # Check if the month is already present in the list for the year
                existing_months = [m['month'] for m in result[year]]
                if month not in existing_months:
                    result[year].append(month_data)
            else:
                # Create a new key for the year if it doesn't exist
                result[year] = [month_data]

        # Combine the dictionaries
        resultData = {}

        # Loop through each year in data2
        for year, months in result.items():
            # Initialize result for the year
            resultData[year] = []
            
            # Get domestic_qty for the year from data1, defaulting to empty dict if year not present
            domestic_qty = year_month_wise_sum.get(year, {})
            
            # Loop through each month in data2 for the current year
            for month_data in months:
                month = month_data['month']
                
                # Get the domestic_qty for the current month
                domestic_qty_value = domestic_qty.get(month, 0)
                
                # Combine the data
                combined_data = {
                    'coal_receipt_month': month,
                    'coal_receipt_domestic_qty': domestic_qty_value,
                    'coal_receipt_domestic_gcv': round(month_data['avg_gross_calorific_value_adb'],2 ),
                    'coal_receipt_imported_qty': 0,
                    'coal_receipt_imported_gcv': 0
                }
                
                # Append to the result for the year
                resultData[year].append(combined_data)

        # Add the coal_receipt_weighted_gcv field
        for year, entries in resultData.items():
            for entry in entries:
                if entry['coal_receipt_imported_gcv']:
                    entry['coal_receipt_weighted_gcv'] = (
                        (entry['coal_receipt_domestic_gcv'] + entry['coal_receipt_imported_gcv']) / 2
                    )
                else:
                    entry['coal_receipt_weighted_gcv'] = entry['coal_receipt_domestic_gcv']

        # average for coal_receipt_ytd_weighted_gcv
        for year, entries in resultData.items():
            # Sort entries by month
            entries.sort(key=lambda x: x['coal_receipt_month'])
            
            # Initialize a list to store cumulative weighted GCV
            cumulative_weighted_gcv = []
            total_weighted_gcv = 0
            
            for entrydata in entries:
                total_weighted_gcv += entrydata['coal_receipt_weighted_gcv']
                ytd_weighted_gcv = total_weighted_gcv / len(cumulative_weighted_gcv + [entrydata['coal_receipt_weighted_gcv']])
                cumulative_weighted_gcv.append(entrydata['coal_receipt_weighted_gcv'])
                entrydata['coal_receipt_ytd_weighted_gcv'] = round(ytd_weighted_gcv, 2)
        
        resultDatamain = {}

        # Sort the years
        sorted_years = sorted(resultData.keys())

        for year in sorted_years:
            entries = resultData[year]
            financial_year = year

            # Initialize the current year's entries if not already present
            if year not in resultDatamain:
                resultDatamain[year] = []

            # Merge previous year's data if needed
            if entries[0]["coal_receipt_month"] < 4:
                previous_year = financial_year - 1
                if previous_year in resultDatamain:
                    # Add entries from previous year's data
                    resultDatamain[previous_year] += [entry for entry in entries if entry["coal_receipt_month"] < 4]
                    # Add the remaining entries to the current year
                    resultDatamain[year] += [entry for entry in entries if entry["coal_receipt_month"] >= 4]
                else:
                    # If previous year data does not exist, initialize it
                    resultDatamain[previous_year] = [entry for entry in entries if entry["coal_receipt_month"] < 4]
                    resultDatamain[year] = [entry for entry in entries if entry["coal_receipt_month"] >= 4]
            else:
                resultDatamain[year] += entries

        for year in resultDatamain:
            resultDatamain[year].sort(key=sort_key)

        
        bunkerAnalysisPipeline = [
            {
                '$group': {
                    '_id': {
                        'year': {
                            '$year': '$created_date'
                        }, 
                        'month': {
                            '$month': '$created_date'
                        }
                    }, 
                    'totalBunkering': {
                        '$sum': {
                            '$toDouble': '$bunkering'
                        }
                    }
                }
            }, {
                '$project': {
                    '_id': 0, 
                    'year': '$_id.year', 
                    'month': '$_id.month', 
                    'totalBunkering': 1
                }
            }, {
                '$sort': {
                    'year': 1, 
                    'month': 1
                }
            }
        ]


        bunkerDataPipeline = [
            {
                '$match': {
                    'sample_parameters.parameter_type': 'AirDryBasis_GCV'
                }
            }, {
                '$unwind': '$sample_parameters'
            }, {
                '$match': {
                    'sample_parameters.parameter_type': 'AirDryBasis_GCV'
                }
            }, {
                '$project': {
                    'sample_received_date': {
                        '$dateFromString': {
                            'dateString': '$sample_received_date'
                        }
                    }, 
                    'year': {
                        '$year': {
                            '$dateFromString': {
                                'dateString': '$sample_received_date'
                            }
                        }
                    }, 
                    'month': {
                        '$month': {
                            '$dateFromString': {
                                'dateString': '$sample_received_date'
                            }
                        }
                    }, 
                    'value': {
                        '$toDouble': '$sample_parameters.val1'
                    }
                }
            }, {
                '$group': {
                    '_id': {
                        'year': '$year', 
                        'month': '$month'
                    }, 
                    'avg_gross_calorific_value_adb': {
                        '$avg': '$value'
                    }
                }
            }, {
                '$sort': {
                    '_id.year': 1, 
                    '_id.month': 1
                }
            }, {
                '$project': {
                    '_id': 0,
                    'year': '$_id.year', 
                    'month': '$_id.month',
                    'avg_gross_calorific_value_adb': 1
                }
            }
        ]

        output_bunkerAnalysisData = bunkerAnalysis.objects().aggregate(bunkerAnalysisPipeline)
        output_bunkerData = BunkerData.objects().aggregate(bunkerDataPipeline)

        bunker_grouped_data = defaultdict(list)
        bunker_analysis_grouped_data = defaultdict(list)

        for singleBunkerData in output_bunkerData:
            singleBunkerData["bunker_coal_weighted_gcv"] = round(singleBunkerData.get("avg_gross_calorific_value_adb"), 2)
            bunkeryear = singleBunkerData.get("year")
            bunker_grouped_data[bunkeryear].append(singleBunkerData)
            
        for singleBunkerAnalysis in output_bunkerAnalysisData:
            singleBunkerAnalysis["bunker_coal_domestic_qty"] = round(singleBunkerAnalysis.get("totalBunkering"), 2)
            bunkerAnalysisYear = singleBunkerAnalysis.get("year")
            bunker_analysis_grouped_data[bunkerAnalysisYear].append(singleBunkerAnalysis)


        combined_data_bunker = defaultdict(list)

        # Process data1
        for year, entries in dict(bunker_grouped_data).items():
            for entry in entries:
                fy = get_financial_year_bunker(entry['year'], entry['month'])
                combined_data_bunker[fy].append({
                    'year': entry['year'],
                    'month': entry['month'],
                    # 'avg_gross_calorific_value_adb': entry.get('avg_gross_calorific_value_adb'),
                    'bunker_coal_weighted_gcv': entry.get('bunker_coal_weighted_gcv'),
                    'bunker_coal_imported_qty': 0
                })

        # Process data2 and merge with combined_data
        for year, entries in dict(bunker_analysis_grouped_data).items():
            for entry in entries:
                fy = get_financial_year_bunker(entry['year'], entry['month'])
                # Find if the same year and month already exist in combined_data
                for data in combined_data_bunker[fy]:
                    if data['year'] == entry['year'] and data['month'] == entry['month']:
                        # data['totalBunkering'] = entry.get('totalBunkering')
                        data['bunker_coal_domestic_qty'] = entry.get('bunker_coal_domestic_qty')
                        break
                else:
                    # If not found, add a new entry
                    combined_data_bunker[fy].append({
                        'year': entry['year'],
                        'month': entry['month'],
                        # 'totalBunkering': entry.get('totalBunkering'),
                        'bunker_coal_domestic_qty': entry.get('bunker_coal_domestic_qty')
                    })

        # Convert defaultdict back to a regular dict
        combined_data_bunker = dict(combined_data_bunker)

        ytd_sum = 0

        # Process data to add the bunker_coal_weighted_gcv_ytd key
        for yeardata, entriesdata in combined_data_bunker.items():
            for entry in entriesdata:
                # Add the current month's bunker_coal_weighted_gcv to the YTD sum
                ytd_sum += entry['bunker_coal_weighted_gcv']
                # Add the YTD value to the dictionary
                entry['bunker_coal_weighted_gcv_ytd'] = ytd_sum

        # Merge resultDatamain and combined_data_bunker based on year and month
        for year, entries in combined_data_bunker.items():
            if year in resultDatamain:
                for entry in entries:
                    # Find the corresponding entry in resultDatamain by matching the month
                    for d1_entry in resultDatamain[year]:
                        if d1_entry['coal_receipt_month'] == entry['month']:
                            # Add data2 fields into resultDatamain entry
                            d1_entry.update({
                                'bunker_coal_weighted_gcv': entry['bunker_coal_weighted_gcv'],
                                'bunker_coal_imported_qty': entry['bunker_coal_imported_qty'],
                                'bunker_coal_domestic_qty': entry['bunker_coal_domestic_qty'],
                                'bunker_coal_weighted_gcv_ytd':
                                entry['bunker_coal_weighted_gcv_ytd']
                            })
                        
        # Keys to check and default values
        required_keys = {
            'bunker_coal_weighted_gcv': 0,
            'bunker_coal_imported_qty': 0,
            'bunker_coal_domestic_qty': 0,
            'bunker_coal_weighted_gcv_ytd': 0
        }

        # Ensure all dictionaries in resultDatamain have the required keys
        for year, entries in resultDatamain.items():
            for entry in entries:
                # Check if each required key exists, and if not, add it with a value of 0
                for key, default_value in required_keys.items():
                    if key not in entry:
                        entry[key] = default_value
            
                # Calculate mtd and ytd
                entry['difference_mtd'] = entry['coal_receipt_weighted_gcv'] - entry['bunker_coal_weighted_gcv']
                entry['difference_ytd'] = entry['coal_receipt_ytd_weighted_gcv'] - entry['bunker_coal_weighted_gcv_ytd']


        for year, valuedata in resultDatamain.items():
            for singlevaluedata in valuedata:
                # console_logger.debug(singlevaluedata)
                try:
                    checkgcvComparision = gcvComparisionAnalysis.objects.get(coal_receipt_year=year, coal_receipt_month=singlevaluedata.get("coal_receipt_month"))
                except DoesNotExist as e:
                    insertgcvComparision = gcvComparisionAnalysis(coal_receipt_year=year,
                                                                coal_receipt_month=singlevaluedata.get("coal_receipt_month"),
                                                                coal_receipt_domestic_qty=singlevaluedata.get("coal_receipt_domestic_qty"),
                                                                coal_receipt_domestic_gcv=singlevaluedata.get("coal_receipt_domestic_gcv"),
                                                                coal_receipt_imported_qty=singlevaluedata.get("coal_receipt_imported_qty"),
                                                                coal_receipt_imported_gcv=singlevaluedata.get("coal_receipt_imported_gcv"),
                                                                coal_receipt_weighted_gcv=singlevaluedata.get("coal_receipt_weighted_gcv"),
                                                                coal_receipt_ytd_weighted_gcv=singlevaluedata.get("coal_receipt_ytd_weighted_gcv"),
                                                                bunker_coal_weighted_gcv=singlevaluedata.get("bunker_coal_weighted_gcv"),
                                                                bunker_coal_imported_qty=singlevaluedata.get("bunker_coal_imported_qty"),
                                                                bunker_coal_domestic_qty=singlevaluedata.get("bunker_coal_domestic_qty"),
                                                                bunker_coal_weighted_gcv_ytd=singlevaluedata.get("bunker_coal_weighted_gcv_ytd"),
                                                                difference_mtd=singlevaluedata.get("difference_mtd"),
                                                                difference_ytd=singlevaluedata.get("difference_ytd"))
                    insertgcvComparision.save()

        return resultDatamain

    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


from datetime import date, time, timedelta

def daterange(start_date: date, end_date: date):

    days = int((end_date - start_date).days)
    for n in range(days):
        yield start_date + datetime.timedelta(n)

@router.get("/extractlotbunker", tags=["test"])
def endpoint_to_extract_lot_bunker_data(response: Response):
    try:
        shifts = {
            "ShiftA": (datetime.time(7, 0, 0), datetime.time(14, 0, 0)),
            "ShiftB": (datetime.time(14, 0, 0), datetime.time(22, 0, 0)),
            "ShiftC": (datetime.time(22, 0, 0), datetime.time(7, 0, 0)) 
        }

        start_date = datetime.date(2024, 4, 1)
        # print("Start Date:", start_date)

        end_date = datetime.datetime.now().date()
        # print("End Date:", end_date)

        # Iterate over each date in the date range
        for single_date in daterange(start_date, end_date):
            # print(f"\nDate: {single_date.strftime('%Y-%m-%d')}")
            
            for shift_name, (start_time, end_time) in shifts.items():
                if shift_name == "ShiftC":
                    shift_start = f"{single_date}T{start_time}"
                    shift_end_var = single_date + timedelta(days=1)
                    shift_end = f"{shift_end_var}T{end_time}"
                else:
                    shift_start = f"{single_date}T{start_time}"
                    shift_end = f"{single_date}T{end_time}"
                # print(f"  {shift_name}: {shift_start} - {shift_end}")
                save_bunker_data(shift_start, shift_end, shift_name)

    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/coalstatement", tags=["Coal Test"])
def endpoint_to_fetch_coal_statement(response:Response, currentPage: Optional[int] = None, perPage: Optional[int] = None, type: Optional[str]="display"):
    try:
        result = {        
                "labels": [],
                "datasets": [],
                "total" : 0,
                "page_size": 15
        }

        if type and type == "display":
            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage

            # Calculate skip value
            skip_value = (page_no - 1) * page_len

            created_at_date = datetime.datetime(2024, 9, 23, 19, 50, 51, 572000)

            pipeline = [
                {
                    '$match': {
                        'created_at': {
                            '$gt': created_at_date,  # Match records created after this date
                        }
                    }
                },
                {
                    '$group': {
                        '_id': {
                            'do_number': '$arv_cum_do_number', 
                            'mine': '$mine', 
                            'month': {
                                '$dateToString': {
                                    'format': '%Y-%m', 
                                    'date': '$GWEL_Gross_Time'
                                }
                            },
                            'source': '$source', 
                            'line_item': '$line_item', 
                            'consumer_type': '$type_consumer', 
                            'start_date': '$start_date', 
                            'end_date': '$end_date',
                            'po_no': '$po_no',
                            'po_date': '$po_date',
                        }, 
                        'challan_gross_qty': {
                            '$sum': {
                                '$toDouble': '$gross_qty'
                            }
                        }, 
                        'challan_tare_qty': {
                            '$sum': {
                                '$toDouble': '$tare_qty'
                            }
                        }, 
                        'challan_net_qty': {
                            '$sum': {
                                '$toDouble': '$net_qty'
                            }
                        }, 
                        'gwel_actual_gross_qty': {
                            '$sum': {
                                '$toDouble': '$actual_gross_qty'
                            }
                        }, 
                        'gwel_actual_tare_qty': {
                            '$sum': {
                                '$toDouble': '$actual_tare_qty'
                            }
                        }, 
                        'gwel_actual_net_qty': {
                            '$sum': {
                                '$toDouble': '$actual_net_qty'
                            }
                        }, 
                        'do_qty': {
                            '$last': '$po_qty'
                        }
                    }
                },
                {
                    '$project': {
                        '_id': 0, 
                        'do_number': '$_id.do_number', 
                        'mine': '$_id.mine', 
                        'source': '$_id.source', 
                        'month': '$_id.month', 
                        'line_item': '$_id.line_item', 
                        'consumer_type': '$_id.consumer_type', 
                        'start_date': '$_id.start_date', 
                        'end_date': '$_id.end_date',
                        'challan_gross_qty': 1, 
                        'challan_tare_qty': 1, 
                        'challan_net_qty': 1, 
                        'gwel_actual_gross_qty': 1, 
                        'gwel_actual_tare_qty': 1,
                        'gwel_actual_net_qty': 1, 
                        'do_qty': 1,
                        'po_no': '$_id.po_no',
                        'po_date': '$_id.po_date',
                    }
                },
                {
                    "$facet": {
                        # "totalData": [
                        #     { "$skip": skip_value },  # Apply skip based on calculated skip_value
                        #     { "$limit": page_len }    # Apply limit based on page_len
                        # ],
                        "totalData": [],
                        "totalCount": [
                            { "$count": "count" }  # Get total count of documents
                        ]
                    }
                }
            ]


            gmHistoricPipeline = [
                {
                    '$group': {
                        '_id': {
                            'do_number': '$arv_cum_do_number', 
                            'mine': '$mine', 
                            'month': {
                                '$dateToString': {
                                    'format': '%Y-%m', 
                                    'date': '$GWEL_Gross_Time'
                                }
                            },
                            'source': '$source', 
                            'line_item': '$line_item', 
                            'consumer_type': '$type_consumer', 
                            'start_date': '$start_date', 
                            'end_date': '$end_date',
                            'po_no': '$po_no',
                            'po_date': '$po_date',
                        }, 
                        'challan_gross_qty': {
                            '$sum': {
                                '$toDouble': '$gross_qty'
                            }
                        }, 
                        'challan_tare_qty': {
                            '$sum': {
                                '$toDouble': '$tare_qty'
                            }
                        }, 
                        'challan_net_qty': {
                            '$sum': {
                                '$toDouble': '$net_qty'
                            }
                        }, 
                        'gwel_actual_gross_qty': {
                            '$sum': {
                                '$toDouble': '$actual_gross_qty'
                            }
                        }, 
                        'gwel_actual_tare_qty': {
                            '$sum': {
                                '$toDouble': '$actual_tare_qty'
                            }
                        }, 
                        'gwel_actual_net_qty': {
                            '$sum': {
                                '$toDouble': '$actual_net_qty'
                            }
                        }, 
                        'do_qty': {
                            '$last': '$po_qty'
                        }
                    }
                },
                {
                    '$project': {
                        '_id': 0, 
                        'do_number': '$_id.do_number', 
                        'mine': '$_id.mine', 
                        'source': '$_id.source', 
                        'month': '$_id.month', 
                        'line_item': '$_id.line_item', 
                        'consumer_type': '$_id.consumer_type', 
                        'start_date': '$_id.start_date', 
                        'end_date': '$_id.end_date',
                        'challan_gross_qty': 1, 
                        'challan_tare_qty': 1, 
                        'challan_net_qty': 1, 
                        'gwel_actual_gross_qty': 1, 
                        'gwel_actual_tare_qty': 1,
                        'gwel_actual_net_qty': 1, 
                        'do_qty': 1,
                        'po_no': '$_id.po_no',
                        'po_date': '$_id.po_date',
                    }
                },
                {
                    "$facet": {
                        # "totalData": [
                        #     { "$skip": skip_value },  # Apply skip based on calculated skip_value
                        #     { "$limit": page_len }    # Apply limit based on page_len
                        # ],
                        "totalData": [],
                        "totalCount": [
                            { "$count": "count" }  # Get total count of documents
                        ]
                    }
                }
            ]

            # saprecordsPipeline = [
            #     {
            #         '$group': {
            #             '_id': {
            #                 'do_number': '$do_no', 
            #                 'mine': '$mine_name', 
            #                 'source': '$source', 
            #                 'month': '$slno', 
            #                 'line_item': '$line_item', 
            #                 'consumer_type': '$consumer_type', 
            #                 'start_date': '$start_date', 
            #                 'end_date': '$end_date',
            #                 'po_no': '$sap_po',
            #                 'po_date': '$po_date',
            #             }, 
            #             'do_qty': {
            #                 '$last': '$do_qty'
            #             }
            #         }
            #     }, {
            #         '$project': {
            #             '_id': 0, 
            #             'do_number': '$_id.do_number', 
            #             'mine': '$_id.mine', 
            #             'source': '$_id.source', 
            #             'quota': '$_id.month', 
            #             'line_item': '$_id.line_item', 
            #             'consumer_type': '$_id.consumer_type', 
            #             'start_date': '$_id.start_date', 
            #             'end_date': '$_id.end_date', 
            #             'do_qty': 1
            #         }
            #     },
            #     {
            #         "$facet": {
            #             # "totalData": [
            #             #     { "$skip": skip_value },  
            #             #     { "$limit": page_len }  
            #             # ],
            #             "totalData": [],
            #             "totalCount": [
            #                 { "$count": "count" } 
            #             ]
            #         }
            #     }
            # ]

            saprecordsPipeline = [
                {
                    '$group': {
                        '_id': {
                            'do_number': '$do_no', 
                            'mine': '$mine_name', 
                            'source': '$source', 
                            'month': '$slno', 
                            'line_item': '$line_item', 
                            'consumer_type': '$consumer_type', 
                            'start_date': '$start_date', 
                            'end_date': '$end_date', 
                            'po_no': '$sap_po', 
                            'po_date': '$po_date'
                        }, 
                        'do_qty': {
                            '$last': '$do_qty'
                        }, 
                        'material_code': {
                            '$last': '$material_code'
                        }, 
                        'material_description': {
                            '$last': '$material_description'
                        }, 
                        'plant_code': {
                            '$last': '$plant_code'
                        }, 
                        'po_open_quantity': {
                            '$last': '$po_open_quantity'
                        }, 
                        'storage_location': {
                            '$last': '$storage_location'
                        }, 
                        'transport_code': {
                            '$last': '$transport_code'
                        }, 
                        'transport_name': {
                            '$last': '$transport_name'
                        }, 
                        'uom': {
                            '$last': '$uom'
                        }, 
                        'valuation_type': {
                            '$last': '$valuation_type'
                        }, 
                        'basic_price': {
                            '$last': '$basic_price'
                        }, 
                        'cgst': {
                            '$last': '$cgst'
                        }, 
                        'dmf': {
                            '$last': '$dmf'
                        }, 
                        'evac_facility_charges': {
                            '$last': '$evac_facility_charges'
                        }, 
                        'gst_comp_cess': {
                            '$last': '$gst_comp_cess'
                        }, 
                        'nmet_charges': {
                            '$last': '$nmet_charges'
                        }, 
                        'royality_charges': {
                            '$last': '$royality_charges'
                        }, 
                        'sgst': {
                            '$last': '$sgst'
                        }, 
                        'sizing_charges': {
                            '$last': '$sizing_charges'
                        }, 
                        'so_value_grand_total': {
                            '$last': '$so_value_grand_total'
                        }, 
                        'stc_charges': {
                            '$last': '$stc_charges'
                        }
                    }
                }, {
                    '$project': {
                        '_id': 0, 
                        'do_number': '$_id.do_number', 
                        'mine': '$_id.mine', 
                        'source': '$_id.source', 
                        'quota': '$_id.month', 
                        'line_item': '$_id.line_item', 
                        'consumer_type': '$_id.consumer_type', 
                        'start_date': '$_id.start_date', 
                        'end_date': '$_id.end_date', 
                        'do_qty': 1, 
                        'material_code': 1, 
                        'material_description': 1, 
                        'plant_code': 1, 
                        'po_open_quantity': 1, 
                        'storage_location': 1, 
                        'transport_code': 1, 
                        'transport_name': 1, 
                        'uom': 1, 
                        'valuation_type': 1, 
                        'basic_price': 1, 
                        'cgst': 1, 
                        'dmf': 1, 
                        'evac_facility_charges': 1, 
                        'gst_comp_cess': 1, 
                        'nmet_charges': 1, 
                        'royality_charges': 1, 
                        'sgst': 1, 
                        'sizing_charges': 1, 
                        'so_value_grand_total': 1, 
                        'stc_charges': 1
                    }
                }, {
                    '$facet': {
                        'totalData': [], 
                        'totalCount': [
                            {
                                '$count': 'count'
                            }
                        ]
                    }
                }
            ]

            output = Gmrdata.objects().aggregate(pipeline)
            gmrHistoricoutput = gmrdataHistoric.objects().aggregate(gmHistoricPipeline)
            sapRecordsOutput = SapRecords.objects().aggregate(saprecordsPipeline)
            countPipeline = pipeline.copy()
            results = list(Gmrdata.objects.aggregate(countPipeline))

            historiccountPipeline = gmHistoricPipeline.copy()
            historicgmrresults = list(gmrdataHistoric.objects.aggregate(historiccountPipeline))

            sapRecordscountPipeline = saprecordsPipeline.copy()
            saprecordsResults = list(SapRecords.objects.aggregate(sapRecordscountPipeline))

            outputDict = {}
            listData = []
            for singleData in output:
                for dataload in singleData["totalData"]:
                    dataload["do_no"] = dataload.get("do_number")
                    dataload["quota"] = dataload.get("month")
                    dataload["challan_gross_qty"] = round(dataload.get("challan_gross_qty"), 2) if dataload.get("challan_gross_qty") and isfinite(dataload.get("challan_gross_qty")) else 0
                    dataload["challan_tare_qty"] = round(dataload.get("challan_tare_qty"), 2) if dataload.get("challan_tare_qty") and isfinite(dataload.get("challan_tare_qty")) else 0
                    dataload["challan_net_qty"] = round(dataload.get("challan_net_qty"), 2) if dataload.get("challan_net_qty") and isfinite(dataload.get("challan_net_qty")) else 0
                    dataload["gwel_actual_gross_qty"] = round(dataload.get("gwel_actual_gross_qty"), 2) if dataload.get("gwel_actual_gross_qty") and isfinite(dataload.get("gwel_actual_gross_qty")) else 0
                    dataload["gwel_actual_tare_qty"] = round(dataload.get("gwel_actual_tare_qty"), 2) if dataload.get("gwel_actual_tare_qty") and isfinite(dataload.get("gwel_actual_tare_qty")) else 0
                    dataload["gwel_actual_net_qty"] = round(dataload.get("gwel_actual_net_qty"), 2) if dataload.get("gwel_actual_net_qty") and isfinite(dataload.get("gwel_actual_net_qty")) else 0
                    dataload["transist_loss"] = round(dataload.get("challan_net_qty", 0) - dataload.get("gwel_actual_net_qty", 0), 2) if isfinite(dataload.get("challan_net_qty", 0) - dataload.get("gwel_actual_net_qty", 0)) else 0
                    dataload["material_code"] = None
                    dataload["material_description"] = None
                    dataload["plant_code"] = None
                    dataload["po_open_quantity"] = None
                    dataload["storage_location"] = None
                    dataload["transport_code"] = None
                    dataload["transport_name"] = None
                    dataload["uom"] = None
                    dataload["valuation_type"] = None
                    dataload["basic_price"] = None
                    dataload["cgst"] = None
                    dataload["dmf"] = None
                    dataload["evac_facility_charges"] = None
                    dataload["gst_comp_cess"] = None
                    dataload["nmet_charges"] = None
                    dataload["royality_charges"] = None
                    dataload["sgst"] = None
                    dataload["sizing_charges"] = None
                    dataload["so_value_grand_total"] = None
                    dataload["stc_charges"] = None
                    
                    listData.append(dataload)
                
            for singlehistoricdata in gmrHistoricoutput:
                for historicDataload in singlehistoricdata["totalData"]:
                    historicDataload["do_no"] = historicDataload.get("do_number")
                    historicDataload["quota"] = historicDataload.get("month")
                    historicDataload["challan_gross_qty"] = round(historicDataload.get("challan_gross_qty"), 2) if historicDataload.get("challan_gross_qty") and isfinite(historicDataload.get("challan_gross_qty")) else 0
                    historicDataload["challan_tare_qty"] = round(historicDataload.get("challan_tare_qty"), 2) if historicDataload.get("challan_tare_qty") and isfinite(historicDataload.get("challan_tare_qty")) else 0
                    historicDataload["challan_net_qty"] = round(historicDataload.get("challan_net_qty"), 2) if historicDataload.get("challan_net_qty") and isfinite(historicDataload.get("challan_net_qty")) else 0
                    historicDataload["gwel_actual_gross_qty"] = round(historicDataload.get("gwel_actual_gross_qty"), 2) if historicDataload.get("gwel_actual_gross_qty") and isfinite(historicDataload.get("gwel_actual_gross_qty")) else 0
                    historicDataload["gwel_actual_tare_qty"] = round(historicDataload.get("gwel_actual_tare_qty"), 2) if historicDataload.get("gwel_actual_tare_qty") and isfinite(historicDataload.get("gwel_actual_tare_qty")) else 0
                    historicDataload["gwel_actual_net_qty"] = round(historicDataload.get("gwel_actual_net_qty"), 2) if historicDataload.get("gwel_actual_net_qty") and isfinite(historicDataload.get("gwel_actual_net_qty")) else 0
                    historicDataload["transist_loss"] = round(historicDataload.get("challan_net_qty", 0) - historicDataload.get("gwel_actual_net_qty", 0), 2) if isfinite(historicDataload.get("challan_net_qty", 0) - historicDataload.get("gwel_actual_net_qty", 0)) else 0
                    historicDataload["material_code"] = None
                    historicDataload["material_description"] = None
                    historicDataload["plant_code"] = None
                    historicDataload["po_open_quantity"] = None
                    historicDataload["storage_location"] = None
                    historicDataload["transport_code"] = None
                    historicDataload["transport_name"] = None
                    historicDataload["uom"] = None
                    historicDataload["valuation_type"] = None
                    historicDataload["basic_price"] = None
                    historicDataload["cgst"] = None
                    historicDataload["dmf"] = None
                    historicDataload["evac_facility_charges"] = None
                    historicDataload["gst_comp_cess"] = None
                    historicDataload["nmet_charges"] = None
                    historicDataload["royality_charges"] = None
                    historicDataload["sgst"] = None
                    historicDataload["sizing_charges"] = None
                    historicDataload["so_value_grand_total"] = None
                    historicDataload["stc_charges"] = None
                    do_no_exists = any(item['do_no'] == historicDataload.get("do_number") for item in listData)
                    if not do_no_exists:
                        print("DO_No does not exist in final_data.")
                        listData.append(historicDataload)

            for singlesapdata in sapRecordsOutput:
                for sapDataload in singlesapdata["totalData"]:
                    sapDataload["do_no"] = sapDataload.get("do_number")
                    sapDataload["challan_gross_qty"] = 0
                    sapDataload["challan_tare_qty"] = 0
                    sapDataload["challan_net_qty"] = 0
                    sapDataload["gwel_actual_gross_qty"] = 0
                    sapDataload["gwel_actual_tare_qty"] = 0
                    sapDataload["gwel_actual_net_qty"] = 0
                    sapDataload["transist_loss"] = 0
                    sapDataload["source"] = sapDataload["source"]
                    sapDataload["quota"] = sapDataload["quota"]
                    sapDataload["line_item"] = sapDataload["line_item"]
                    sapDataload["consumer_type"] = sapDataload["consumer_type"]
                    sapDataload["start_date"] = sapDataload["start_date"]
                    sapDataload["end_date"] = sapDataload["end_date"]
                    sapDataload["transist_loss"] = 0
                    do_no_exists = any(item['do_no'] == sapDataload.get("do_number") for item in listData)
                    if not do_no_exists:
                        print("DO_No does not exist in final_data.")
                        listData.append(sapDataload)
                    elif do_no_exists:
                        # updating saprecords data on gmr dictionary
                        update_data = [item.update({"material_code": sapDataload.get("material_code"), "material_description": sapDataload.get("material_description"), "plant_code": sapDataload.get("plant_code"), "po_open_quantity": sapDataload.get("po_open_quantity"), "storage_location": sapDataload.get("storage_location"), "transport_code": sapDataload.get("transport_code"), "transport_name": sapDataload.get("transport_name"), "uom": sapDataload.get("uom"), "valuation_type": sapDataload.get("valuation_type"), "basic_price": sapDataload.get("basic_price"), "cgst": sapDataload.get("cgst"), "dmf": sapDataload.get("dmf"),"evac_facility_charges": sapDataload.get("evac_facility_charges"), "gst_comp_cess": sapDataload.get("gst_comp_cess"), "nmet_charges": sapDataload.get("nmet_charges"), "royality_charges": sapDataload.get("royality_charges"), "sgst": sapDataload.get("sgst"), "sizing_charges": sapDataload.get("sizing_charges"), "so_value_grand_total": sapDataload.get("so_value_grand_total"), "stc_charges": sapDataload.get("stc_charges")}) for item in listData if item['do_no'] == sapDataload.get("do_number")]
            start_idx = (page_no - 1) * page_len
            end_idx = start_idx + page_len
            paginated_data = listData[start_idx:end_idx]

            total_count = results[0]["totalCount"][0]["count"] + historicgmrresults[0]["totalCount"][0]["count"] + saprecordsResults[0]["totalCount"][0]["count"]
            result["labels"] = ["do_no", "mine", "quota", "do_qty", "challan_gross_qty", "challan_tare_qty", "challan_net_qty", "gwel_actual_gross_qty", "gwel_actual_tare_qty", "gwel_actual_net_qty", "transist_loss", "line_item", "start_date", "end_date", "po_no", "po_date", "material_code", "material_description", "plant_code", "po_open_quantity", "storage_location", "transport_code", "transport_name", "uom", "valuation_type", "basic_price", "cgst", "dmf", "evac_facility_charges", "gst_comp_cess", "nmet_charges", "royality_charges", "sgst", "sizing_charges", "so_value_grand_total", "stc_charges"]
            result["datasets"] = paginated_data
            # result["total"] = total_count
            result["total"] = len(listData)

            return result
        elif type and type == "download":
            del type
            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            created_at_date = datetime.datetime(2024, 9, 23, 19, 50, 51, 572000)
            pipeline = [
                {
                    '$match': {
                        'created_at': {
                            '$gt': created_at_date,
                        }
                    }
                },
                {
                    '$group': {
                        '_id': {
                            'do_number': '$arv_cum_do_number', 
                            'mine': '$mine', 
                            'month': {
                                '$dateToString': {
                                    'format': '%Y-%m', 
                                    'date': '$GWEL_Gross_Time'
                                }
                            },
                            'source': '$source', 
                            # 'month': '$slno', 
                            'line_item': '$line_item', 
                            'consumer_type': '$type_consumer', 
                            'start_date': '$start_date', 
                            'end_date': '$end_date',
                            'po_no': '$po_no',
                            'po_date': '$po_date',
                        }, 
                        'challan_gross_qty': {
                            '$sum': {
                                '$toDouble': '$gross_qty'
                            }
                        }, 
                        'challan_tare_qty': {
                            '$sum': {
                                '$toDouble': '$tare_qty'
                            }
                        }, 
                        'challan_net_qty': {
                            '$sum': {
                                '$toDouble': '$net_qty'
                            }
                        }, 
                        'gwel_actual_gross_qty': {
                            '$sum': {
                                '$toDouble': '$actual_gross_qty'
                            }
                        }, 
                        'gwel_actual_tare_qty': {
                            '$sum': {
                                '$toDouble': '$actual_tare_qty'
                            }
                        }, 
                        'gwel_actual_net_qty': {
                            '$sum': {
                                '$toDouble': '$actual_net_qty'
                            }
                        }, 
                        'do_qty': {
                            '$last': '$po_qty'
                        }
                    }
                }, {
                    '$project': {
                        '_id': 0, 
                        'do_number': '$_id.do_number', 
                        'mine': '$_id.mine', 
                        'source': '$_id.source', 
                        'month': '$_id.month', 
                        'line_item': '$_id.line_item', 
                        'consumer_type': '$_id.type_consumer', 
                        'start_date': '$_id.start_date', 
                        'end_date': '$_id.end_date',
                        'challan_gross_qty': 1, 
                        'challan_tare_qty': 1, 
                        'challan_net_qty': 1, 
                        'gwel_actual_gross_qty': 1, 
                        'gwel_actual_tare_qty': 1,
                        'gwel_actual_net_qty': 1, 
                        'do_qty': 1,
                        'po_no': '$_id.po_no',
                        'po_date': '$_id.po_date',
                    }
                },
                # { 
                #     "$skip": skip_value  # Skip documents based on the calculated value
                # },
                # { 
                #     "$limit": page_len  # Limit the number of documents to the page length
                # }
                {
                    "$facet": {
                        "totalData": [],
                        "totalCount": [
                            { "$count": "count" } 
                        ]
                    }
                }
            ]

            gmHistoricPipeline = [
                {
                    '$group': {
                        '_id': {
                            'do_number': '$arv_cum_do_number', 
                            'mine': '$mine', 
                            'month': {
                                '$dateToString': {
                                    'format': '%Y-%m', 
                                    'date': '$GWEL_Gross_Time'
                                }
                            },
                            'source': '$source', 
                            # 'month': '$slno', 
                            'line_item': '$line_item', 
                            'consumer_type': '$type_consumer', 
                            'start_date': '$start_date', 
                            'end_date': '$end_date',
                            'po_no': '$po_no',
                            'po_date': '$po_date',
                        }, 
                        'challan_gross_qty': {
                            '$sum': {
                                '$toDouble': '$gross_qty'
                            }
                        }, 
                        'challan_tare_qty': {
                            '$sum': {
                                '$toDouble': '$tare_qty'
                            }
                        }, 
                        'challan_net_qty': {
                            '$sum': {
                                '$toDouble': '$net_qty'
                            }
                        }, 
                        'gwel_actual_gross_qty': {
                            '$sum': {
                                '$toDouble': '$actual_gross_qty'
                            }
                        }, 
                        'gwel_actual_tare_qty': {
                            '$sum': {
                                '$toDouble': '$actual_tare_qty'
                            }
                        }, 
                        'gwel_actual_net_qty': {
                            '$sum': {
                                '$toDouble': '$actual_net_qty'
                            }
                        }, 
                        'do_qty': {
                            '$last': '$po_qty'
                        }
                    }
                }, {
                    '$project': {
                        '_id': 0, 
                        'do_number': '$_id.do_number', 
                        'mine': '$_id.mine', 
                        'source': '$_id.source', 
                        'month': '$_id.month', 
                        'line_item': '$_id.line_item', 
                        'consumer_type': '$_id.type_consumer', 
                        'start_date': '$_id.start_date', 
                        'end_date': '$_id.end_date',
                        'challan_gross_qty': 1, 
                        'challan_tare_qty': 1, 
                        'challan_net_qty': 1, 
                        'gwel_actual_gross_qty': 1, 
                        'gwel_actual_tare_qty': 1,
                        'gwel_actual_net_qty': 1, 
                        'do_qty': 1,
                        'po_no': '$_id.po_no',
                        'po_date': '$_id.po_date',
                    }
                },
                # { 
                #     "$skip": skip_value  # Skip documents based on the calculated value
                # },
                # { 
                #     "$limit": page_len  # Limit the number of documents to the page length
                # }
                {
                    "$facet": {
                        "totalData": [],
                        "totalCount": [
                            { "$count": "count" } 
                        ]
                    }
                }
            ]

            saprecordsPipeline = [
                {
                    '$group': {
                        '_id': {
                            'do_number': '$do_no', 
                            'mine': '$mine_name', 
                            'source': '$source', 
                            'month': '$slno', 
                            'line_item': '$line_item', 
                            'consumer_type': '$consumer_type', 
                            'start_date': '$start_date', 
                            'end_date': '$end_date', 
                            'po_no': '$sap_po', 
                            'po_date': '$po_date'
                        }, 
                        'do_qty': {
                            '$last': '$do_qty'
                        }, 
                        'material_code': {
                            '$last': '$material_code'
                        }, 
                        'material_description': {
                            '$last': '$material_description'
                        }, 
                        'plant_code': {
                            '$last': '$plant_code'
                        }, 
                        'po_open_quantity': {
                            '$last': '$po_open_quantity'
                        }, 
                        'storage_location': {
                            '$last': '$storage_location'
                        }, 
                        'transport_code': {
                            '$last': '$transport_code'
                        }, 
                        'transport_name': {
                            '$last': '$transport_name'
                        }, 
                        'uom': {
                            '$last': '$uom'
                        }, 
                        'valuation_type': {
                            '$last': '$valuation_type'
                        }, 
                        'basic_price': {
                            '$last': '$basic_price'
                        }, 
                        'cgst': {
                            '$last': '$cgst'
                        }, 
                        'dmf': {
                            '$last': '$dmf'
                        }, 
                        'evac_facility_charges': {
                            '$last': '$evac_facility_charges'
                        }, 
                        'gst_comp_cess': {
                            '$last': '$gst_comp_cess'
                        }, 
                        'nmet_charges': {
                            '$last': '$nmet_charges'
                        }, 
                        'royality_charges': {
                            '$last': '$royality_charges'
                        }, 
                        'sgst': {
                            '$last': '$sgst'
                        }, 
                        'sizing_charges': {
                            '$last': '$sizing_charges'
                        }, 
                        'so_value_grand_total': {
                            '$last': '$so_value_grand_total'
                        }, 
                        'stc_charges': {
                            '$last': '$stc_charges'
                        }
                    }
                }, {
                    '$project': {
                        '_id': 0, 
                        'do_number': '$_id.do_number', 
                        'mine': '$_id.mine', 
                        'source': '$_id.source', 
                        'quota': '$_id.month', 
                        'line_item': '$_id.line_item', 
                        'consumer_type': '$_id.consumer_type', 
                        'start_date': '$_id.start_date', 
                        'end_date': '$_id.end_date', 
                        'do_qty': 1, 
                        'material_code': 1, 
                        'material_description': 1, 
                        'plant_code': 1, 
                        'po_open_quantity': 1, 
                        'storage_location': 1, 
                        'transport_code': 1, 
                        'transport_name': 1, 
                        'uom': 1, 
                        'valuation_type': 1, 
                        'basic_price': 1, 
                        'cgst': 1, 
                        'dmf': 1, 
                        'evac_facility_charges': 1, 
                        'gst_comp_cess': 1, 
                        'nmet_charges': 1, 
                        'royality_charges': 1, 
                        'sgst': 1, 
                        'sizing_charges': 1, 
                        'so_value_grand_total': 1, 
                        'stc_charges': 1
                    }
                }, {
                    '$facet': {
                        'totalData': [], 
                        'totalCount': [
                            {
                                '$count': 'count'
                            }
                        ]
                    }
                }
            ]

            output = Gmrdata.objects().aggregate(pipeline)
            gmrHistoricoutput = gmrdataHistoric.objects().aggregate(gmHistoricPipeline)
            sapRecordsOutput = SapRecords.objects().aggregate(saprecordsPipeline)
            countPipeline = pipeline.copy()
            results = list(Gmrdata.objects.aggregate(countPipeline))

            historiccountPipeline = gmHistoricPipeline.copy()
            historicgmrresults = list(gmrdataHistoric.objects.aggregate(historiccountPipeline))

            sapRecordscountPipeline = saprecordsPipeline.copy()
            saprecordsResults = list(SapRecords.objects.aggregate(sapRecordscountPipeline))

            outputDict = {}
            listData = []
            for singleData in output:
                for dataload in singleData["totalData"]:
                    dataload["do_no"] = dataload.get("do_number")
                    dataload["quota"] = dataload.get("month")
                    dataload["challan_gross_qty"] = round(dataload.get("challan_gross_qty"), 2)
                    dataload["challan_tare_qty"] = round(dataload.get("challan_tare_qty"), 2)
                    dataload["challan_net_qty"] = round(dataload.get("challan_net_qty"), 2)
                    dataload["gwel_actual_gross_qty"] = round(dataload.get("gwel_actual_gross_qty"), 2)
                    dataload["gwel_actual_tare_qty"] = round(dataload.get("gwel_actual_tare_qty"), 2)
                    dataload["gwel_actual_net_qty"] = round(dataload.get("gwel_actual_net_qty"), 2)
                    dataload["transist_loss"] = round(dataload.get("challan_net_qty") - dataload.get("gwel_actual_net_qty"), 2)

                    dataload["material_code"] = None
                    dataload["material_description"] = None
                    dataload["plant_code"] = None
                    dataload["po_open_quantity"] = None
                    dataload["storage_location"] = None
                    dataload["transport_code"] = None
                    dataload["transport_name"] = None
                    dataload["uom"] = None
                    dataload["valuation_type"] = None
                    dataload["basic_price"] = None
                    dataload["cgst"] = None
                    dataload["dmf"] = None
                    dataload["evac_facility_charges"] = None
                    dataload["gst_comp_cess"] = None
                    dataload["nmet_charges"] = None
                    dataload["royality_charges"] = None
                    dataload["sgst"] = None
                    dataload["sizing_charges"] = None
                    dataload["so_value_grand_total"] = None
                    dataload["stc_charges"] = None

                    listData.append(dataload)
                
            for singlehistoricdata in gmrHistoricoutput:
                for historicDataload in singlehistoricdata["totalData"]:
                    historicDataload["do_no"] = historicDataload.get("do_number")
                    historicDataload["quota"] = historicDataload.get("month")
                    historicDataload["challan_gross_qty"] = round(historicDataload.get("challan_gross_qty"), 2)
                    historicDataload["challan_tare_qty"] = round(historicDataload.get("challan_tare_qty"), 2)
                    historicDataload["challan_net_qty"] = round(historicDataload.get("challan_net_qty"), 2)
                    historicDataload["gwel_actual_gross_qty"] = round(historicDataload.get("gwel_actual_gross_qty"), 2)
                    historicDataload["gwel_actual_tare_qty"] = round(historicDataload.get("gwel_actual_tare_qty"), 2)
                    historicDataload["gwel_actual_net_qty"] = round(historicDataload.get("gwel_actual_net_qty"), 2)
                    historicDataload["transist_loss"] = round(historicDataload.get("challan_net_qty") - historicDataload.get("gwel_actual_net_qty"), 2)
                    # historicDataload.get("gwel_actual_net_qty", 0)) else 0
                    historicDataload["material_code"] = None
                    historicDataload["material_description"] = None
                    historicDataload["plant_code"] = None
                    historicDataload["po_open_quantity"] = None
                    historicDataload["storage_location"] = None
                    historicDataload["transport_code"] = None
                    historicDataload["transport_name"] = None
                    historicDataload["uom"] = None
                    historicDataload["valuation_type"] = None
                    historicDataload["basic_price"] = None
                    historicDataload["cgst"] = None
                    historicDataload["dmf"] = None
                    historicDataload["evac_facility_charges"] = None
                    historicDataload["gst_comp_cess"] = None
                    historicDataload["nmet_charges"] = None
                    historicDataload["royality_charges"] = None
                    historicDataload["sgst"] = None
                    historicDataload["sizing_charges"] = None
                    historicDataload["so_value_grand_total"] = None
                    historicDataload["stc_charges"] = None
                    do_no_exists = any(item['do_no'] == historicDataload.get("do_number") for item in listData)
                    if not do_no_exists:
                        print("DO_No does not exist in final_data.")
                        listData.append(historicDataload)

            for singlesapdata in sapRecordsOutput:
                for sapDataload in singlesapdata["totalData"]:
                    sapDataload["do_no"] = sapDataload.get("do_number")
                    sapDataload["challan_gross_qty"] = 0
                    sapDataload["challan_tare_qty"] = 0
                    sapDataload["challan_net_qty"] = 0
                    sapDataload["gwel_actual_gross_qty"] = 0
                    sapDataload["gwel_actual_tare_qty"] = 0
                    sapDataload["gwel_actual_net_qty"] = 0
                    sapDataload["transist_loss"] = 0
                    sapDataload["source"] = sapDataload["source"]
                    sapDataload["quota"] = sapDataload["quota"]
                    sapDataload["line_item"] = sapDataload["line_item"]
                    sapDataload["consumer_type"] = sapDataload["consumer_type"]
                    sapDataload["start_date"] = sapDataload["start_date"]
                    sapDataload["end_date"] = sapDataload["end_date"]
                    sapDataload["transist_loss"] = 0
                    do_no_exists = any(item['do_no'] == sapDataload.get("do_number") for item in listData)
                    if not do_no_exists:
                        print("DO_No does not exist in final_data.")
                        listData.append(sapDataload)
                    elif do_no_exists:
                        # updating saprecords data on gmr dictionary
                        update_data = [item.update({"material_code": sapDataload.get("material_code"), "material_description": sapDataload.get("material_description"), "plant_code": sapDataload.get("plant_code"), "po_open_quantity": sapDataload.get("po_open_quantity"), "storage_location": sapDataload.get("storage_location"), "transport_code": sapDataload.get("transport_code"), "transport_name": sapDataload.get("transport_name"), "uom": sapDataload.get("uom"), "valuation_type": sapDataload.get("valuation_type"), "basic_price": sapDataload.get("basic_price"), "cgst": sapDataload.get("cgst"), "dmf": sapDataload.get("dmf"),"evac_facility_charges": sapDataload.get("evac_facility_charges"), "gst_comp_cess": sapDataload.get("gst_comp_cess"), "nmet_charges": sapDataload.get("nmet_charges"), "royality_charges": sapDataload.get("royality_charges"), "sgst": sapDataload.get("sgst"), "sizing_charges": sapDataload.get("sizing_charges"), "so_value_grand_total": sapDataload.get("so_value_grand_total"), "stc_charges": sapDataload.get("stc_charges")}) for item in listData if item['do_no'] == sapDataload.get("do_number")]
            
            total_count = results[0]["totalCount"][0]["count"] + historicgmrresults[0]["totalCount"][0]["count"] + saprecordsResults[0]["totalCount"][0]["count"]
            # result["labels"] = ["do_no", "mine", "quota", "do_qty", "challan_gross_qty", "challan_tare_qty", "challan_net_qty", "gwel_actual_gross_qty", "gwel_actual_tare_qty", "gwel_actual_net_qty", "transist_loss", "line_item", "start_date", "end_date", "po_no", "po_date"]
            # result["datasets"] = listData
            # result["total"] = total_count

            count = len(results)
            path = os.path.join(
                "static_server",
                "gmr_ai",
                file,
                "coal_statement_{}.xlsx".format(
                    datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                ),
            )
            logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
            filename = os.path.join(os.getcwd(), path)
            workbook = xlsxwriter.Workbook(filename)
            workbook.use_zip64()
            cell_format2 = workbook.add_format()
            cell_format2.set_bold()
            cell_format2.set_font_size(10)
            cell_format2.set_align("center")
            cell_format2.set_align("vcenter")
            cell_format2.set_text_wrap(True)
            cell_format2.set_border(1)

            header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
            date_format = workbook.add_format({'align': 'center', 'font_size': 12, "bold": True})
            report_name_format = workbook.add_format({'align': 'center', 'font_size': 15, "bold": True})

            header_format.set_align("vcenter")
            date_format.set_align("vcenter")
            report_name_format.set_align("vcenter")
            header_format.set_border(1)
            date_format.set_border(1)
            report_name_format.set_border(1)

            worksheet = workbook.add_worksheet()
            worksheet.set_column("A:AZ", 20)
            worksheet.set_default_row(50)
            cell_format = workbook.add_format()
            cell_format.set_font_size(10)
            cell_format.set_align("center")
            cell_format.set_align("vcenter")
            cell_format.set_text_wrap(True)
            cell_format.set_border(1)

            worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                    
            # Merge cells for the main header and place it in the center
            main_header = "GMR Warora Energy Limited"  # Set your main header text here
            worksheet.merge_range("A1:AJ1", main_header, header_format)  # Merge cells A1 to H1 for the header
            
            # Write the current date on the left side (A2)
            worksheet.write("A2", f"Date: {datetime.datetime.now().strftime('%d-%m-%Y')}", date_format)
            worksheet.merge_range("C2:AJ2", f"Road Coal Statement", report_name_format)

            # headers = ["Do no", "Mine", "Quota", "DO Qty", "Challan Gross Qty", "Challan Tare Qty", "Challan Net Qty", "Gwel Actual Gross Qty", "Gwel Actual Tare Qty", "Gwel Actual Net Qty", "Transist Loss"]
            headers = ["Do no", "Mine", "Quota", "DO Qty", "Challan Gross Qty", "Challan Tare Qty", "Challan Net Qty", "Gwel Actual Gross Qty", "Gwel Actual Tare Qty", "Gwel Actual net qty", "Transist Loss", "Line Item", "Start Date", "End Date", "Po No", "Po Date", "Material Code", "Material Description", "Plant Code", "Po Open Quantity", "Storage Location", "Transport Code", "Transport Name", "UOM", "Valuation Type", "Basic Price", "CGST", "DMF", "Evac Facility Charges", "Gst Comp Cess", "Nmet Charges", "Royality Charges", "SGST", "Sizing Charges", "So Value Grand Total", "Stc Charges"]
            finalData = []

            for index, header in enumerate(headers):
                worksheet.write(2, index, header, cell_format2)

            for singleData in output:
                for dataload in singleData["totalData"]:
                    dataload["do_no"] = dataload.get("do_number")
                    dataload["quota"] = dataload.get("month")
                    dataload["challan_gross_qty"] = round(dataload.get("challan_gross_qty"), 2)
                    dataload["challan_tare_qty"] = round(dataload.get("challan_tare_qty"), 2)
                    dataload["challan_net_qty"] = round(dataload.get("challan_net_qty"), 2)
                    dataload["gwel_actual_gross_qty"] = round(dataload.get("gwel_actual_gross_qty"), 2)
                    dataload["gwel_actual_tare_qty"] = round(dataload.get("gwel_actual_tare_qty"), 2)
                    dataload["gwel_actual_net_qty"] = round(dataload.get("gwel_actual_net_qty"), 2)
                    dataload["transist_loss"] = round(dataload.get("challan_net_qty") - dataload.get("gwel_actual_net_qty"), 2)
                    listData.append(dataload)
                
            for singlehistoricdata in gmrHistoricoutput:
                for historicDataload in singlehistoricdata["totalData"]:
                    historicDataload["do_no"] = historicDataload.get("do_number")
                    historicDataload["quota"] = historicDataload.get("month")
                    historicDataload["challan_gross_qty"] = round(historicDataload.get("challan_gross_qty"), 2)
                    historicDataload["challan_tare_qty"] = round(historicDataload.get("challan_tare_qty"), 2)
                    historicDataload["challan_net_qty"] = round(historicDataload.get("challan_net_qty"), 2)
                    historicDataload["gwel_actual_gross_qty"] = round(historicDataload.get("gwel_actual_gross_qty"), 2)
                    historicDataload["gwel_actual_tare_qty"] = round(historicDataload.get("gwel_actual_tare_qty"), 2)
                    historicDataload["gwel_actual_net_qty"] = round(historicDataload.get("gwel_actual_net_qty"), 2)
                    historicDataload["transist_loss"] = round(historicDataload.get("challan_net_qty") - historicDataload.get("gwel_actual_net_qty"), 2)
                    do_no_exists = any(item['do_no'] == historicDataload.get("do_number") for item in listData)
                    if not do_no_exists:
                        print("DO_No does not exist in final_data.")
                        listData.append(historicDataload)

            for singlesapdata in sapRecordsOutput:
                for sapDataload in singlesapdata["totalData"]:
                    sapDataload["do_no"] = sapDataload.get("do_number")
                    sapDataload["challan_gross_qty"] = 0
                    sapDataload["challan_tare_qty"] = 0
                    sapDataload["challan_net_qty"] = 0
                    sapDataload["gwel_actual_gross_qty"] = 0
                    sapDataload["gwel_actual_tare_qty"] = 0
                    sapDataload["gwel_actual_net_qty"] = 0
                    sapDataload["transist_loss"] = 0
                    sapDataload["source"] = sapDataload["source"]
                    sapDataload["quota"] = sapDataload["quota"]
                    sapDataload["line_item"] = sapDataload["line_item"]
                    sapDataload["consumer_type"] = sapDataload["consumer_type"]
                    sapDataload["start_date"] = sapDataload["start_date"]
                    sapDataload["end_date"] = sapDataload["end_date"]
                    sapDataload["transist_loss"] = 0
                    do_no_exists = any(item['do_no'] == sapDataload.get("do_number") for item in listData)
                    if not do_no_exists:
                        print("DO_No does not exist in final_data.")
                        listData.append(sapDataload)
            row = 3
            for singlelistData in listData:
                if singlelistData.get("do_number"):
                    worksheet.write(row, 0, singlelistData["do_number"], cell_format)
                else:    
                    worksheet.write(row, 0, "None", cell_format)
                if singlelistData.get("mine"):
                    worksheet.write(row, 1, singlelistData["mine"], cell_format)
                else:
                    worksheet.write(row, 1, "Unknown", cell_format)
                if singlelistData.get("quota"):
                    worksheet.write(row, 2, singlelistData["quota"], cell_format)
                else:
                    worksheet.write(row, 2, "N/A", cell_format)
                if singlelistData.get("do_qty"):
                    worksheet.write(row, 3, singlelistData["do_qty"], cell_format)
                else:
                    worksheet.write(row, 3, singlelistData["do_qty"], cell_format)
                try:
                    if singlelistData.get("challan_gross_qty"):
                        worksheet.write(row, 4, round(singlelistData["challan_gross_qty"], 2), cell_format)
                    else:
                        worksheet.write(row, 4, 0, cell_format)
                except:
                    worksheet.write(row, 4, 0, cell_format)
                if singlelistData.get("challan_lr_qty"):
                    worksheet.write(row, 5, round(singlelistData["challan_tare_qty"], 2), cell_format)
                else:
                    worksheet.write(row, 5, 0, cell_format)
                if singlelistData.get("challan_net_qty"):
                    worksheet.write(row, 6, round(singlelistData["challan_net_qty"], 2), cell_format)
                else:
                    worksheet.write(row, 6, 0, cell_format)
                if singlelistData.get("gwel_actual_gross_qty"):
                    worksheet.write(row, 7, round(singlelistData["gwel_actual_gross_qty"], 2), cell_format)
                else:    
                    worksheet.write(row, 7, 0, cell_format)
                if singlelistData.get("gwel_actual_tare_qty"):
                    worksheet.write(row, 8, round(singlelistData["gwel_actual_tare_qty"], 2), cell_format)
                else:
                    worksheet.write(row, 8, 0, cell_format)
                if singlelistData.get("gwel_actual_net_qty"):
                    worksheet.write(row, 9, round(singlelistData["gwel_actual_net_qty"], 2), cell_format)
                else:
                    worksheet.write(row, 9, 0, cell_format)
                worksheet.write(row, 10, round(singlelistData.get("challan_net_qty") - singlelistData.get("gwel_actual_net_qty"), 2), cell_format)
                if singlelistData.get("line_item"):
                    worksheet.write(row, 11, singlelistData.get("line_item"), cell_format)
                else:
                    worksheet.write(row, 11, "N/A", cell_format)
                if singlelistData.get("start_date"):
                    worksheet.write(row, 12, singlelistData.get("start_date"), cell_format)
                else:    
                    worksheet.write(row, 12, "N/A", cell_format)
                if singlelistData.get("end_date"):
                    worksheet.write(row, 13, singlelistData.get("end_date"), cell_format)
                else:    
                    worksheet.write(row, 13, "N/A", cell_format)
                if singlelistData.get("po_no"):
                    worksheet.write(row, 14, singlelistData.get("po_no"), cell_format)
                else:    
                    worksheet.write(row, 14, "N/A", cell_format)
                if singlelistData.get("po_date"):
                    worksheet.write(row, 15, singlelistData.get("po_date"), cell_format)
                else:    
                    worksheet.write(row, 15, "N/A", cell_format)
                if singlelistData.get("material_code"):
                    worksheet.write(row, 16, singlelistData.get("material_code"), cell_format)
                else:    
                    worksheet.write(row, 16, "N/A", cell_format)
                
                if singlelistData.get("material_description"):
                    worksheet.write(row, 17, singlelistData.get("material_description"), cell_format)
                else:    
                    worksheet.write(row, 17, "N/A", cell_format)

                if singlelistData.get("plant_code"):
                    worksheet.write(row, 18, singlelistData.get("plant_code"), cell_format)
                else:    
                    worksheet.write(row, 18, "N/A", cell_format)

                if singlelistData.get("po_open_quantity"):
                    worksheet.write(row, 19, singlelistData.get("po_open_quantity"), cell_format)
                else:    
                    worksheet.write(row, 19, "N/A", cell_format)

                if singlelistData.get("storage_location"):
                    worksheet.write(row, 20, singlelistData.get("storage_location"), cell_format)
                else:    
                    worksheet.write(row, 20, "N/A", cell_format)

                if singlelistData.get("transport_code"):
                    worksheet.write(row, 21, singlelistData.get("transport_code"), cell_format)
                else:    
                    worksheet.write(row, 21, "N/A", cell_format)

                if singlelistData.get("transport_name"):
                    worksheet.write(row, 22, singlelistData.get("transport_name"), cell_format)
                else:    
                    worksheet.write(row, 22, "N/A", cell_format)

                if singlelistData.get("uom"):
                    worksheet.write(row, 23, singlelistData.get("uom"), cell_format)
                else:    
                    worksheet.write(row, 23, "N/A", cell_format)

                if singlelistData.get("valuation_type"):
                    worksheet.write(row, 24, singlelistData.get("valuation_type"), cell_format)
                else:    
                    worksheet.write(row, 24, "N/A", cell_format)

                if singlelistData.get("basic_price"):
                    worksheet.write(row, 25, singlelistData.get("basic_price"), cell_format)
                else:    
                    worksheet.write(row, 25, "N/A", cell_format)

                if singlelistData.get("cgst"):
                    worksheet.write(row, 26, singlelistData.get("cgst"), cell_format)
                else:    
                    worksheet.write(row, 26, "N/A", cell_format)

                if singlelistData.get("dmf"):
                    worksheet.write(row, 27, singlelistData.get("dmf"), cell_format)
                else:    
                    worksheet.write(row, 27, "N/A", cell_format)

                if singlelistData.get("evac_facility_charges"):
                    worksheet.write(row, 28, singlelistData.get("evac_facility_charges"), cell_format)
                else:    
                    worksheet.write(row, 28, "N/A", cell_format)

                if singlelistData.get("gst_comp_cess"):
                    worksheet.write(row, 29, singlelistData.get("gst_comp_cess"), cell_format)
                else:    
                    worksheet.write(row, 29, "N/A", cell_format)

                if singlelistData.get("nmet_charges"):
                    worksheet.write(row, 30, singlelistData.get("nmet_charges"), cell_format)
                else:    
                    worksheet.write(row, 30, "N/A", cell_format)

                if singlelistData.get("royality_charges"):
                    worksheet.write(row, 31, singlelistData.get("royality_charges"), cell_format)
                else:    
                    worksheet.write(row, 31, "N/A", cell_format)

                if singlelistData.get("sgst"):
                    worksheet.write(row, 32, singlelistData.get("sgst"), cell_format)
                else:    
                    worksheet.write(row, 32, "N/A", cell_format)

                if singlelistData.get("sizing_charges"):
                    worksheet.write(row, 33, singlelistData.get("sizing_charges"), cell_format)
                else:    
                    worksheet.write(row, 33, "N/A", cell_format)

                if singlelistData.get("so_value_grand_total"):
                    worksheet.write(row, 34, singlelistData.get("so_value_grand_total"), cell_format)
                else:    
                    worksheet.write(row, 34, "N/A", cell_format)

                if singlelistData.get("stc_charges"):
                    worksheet.write(row, 35, singlelistData.get("stc_charges"), cell_format)
                else:    
                    worksheet.write(row, 35, "N/A", cell_format)
                # count -= 1
                row += 1
            workbook.close()
            return {
                    "Type": "coal_statement",
                    "Datatype": "Report",
                    "File_Path": path,
                }

    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


def get_sap_data(arv_cum_do_numbers):
    sap_query = {"do_no": {"$in": arv_cum_do_numbers}}
    sap_pipeline = [
        {"$match": sap_query},
        # {
        #     "$project": {
        #         "do_no": 1,
        #         "po_amount": {"$toDouble": "$po_amount"}
        #     }
        # }
        {
            "$project": {
            "do_no": 1,
            "po_amount": {
                "$convert": {
                "input": {
                    "$replaceAll": {
                    "input": "$po_amount",
                    "find": ",",
                    "replacement": ""
                    }
                },
                "to": "double",
                "onError": None, 
                "onNull": None
                }
            }
            }
        }
    ]
    
    sap_results = list(SapRecords.objects().aggregate(sap_pipeline))
    
    sap_data = {item['do_no']: item['po_amount'] for item in sap_results}
    return sap_data

#coalReceiptSummary start

def normalize_sample_qty(qty):
    
    if type(qty) == str:
        qty.strip()
        qty = qty.replace(",","")
    # print(qty)
    return float(qty)

def update_data_type():
    result = RCA.find()
    for d in result:
        if type(d.get("sample_qty")) == str:
            try:
                # RCA.update_one(filter={"_id":d.get("_id")}, update={"$set":{"plant_analysis_date":datetime.strptime(d.get("plant_analysis_date"), '%d/%m/%Y')}})
                RCA.update_one(filter={"_id":d.get("_id")}, update={"$set":{"sample_qty":normalize_sample_qty(d.get("sample_qty"))}})
            except:
                RCA.update_one(filter={"_id":d.get("_id")}, update={"$set":{"sample_qty":normalize_sample_qty(d.get("sample_qty"))}})
                RCA.update_one(filter={"_id":d.get("_id")}, update={"$set":{"plant_analysis_date":datetime.strptime(d.get("plant_analysis_date"), '%d.%m.%Y')}})
    return


def get_month_ends(Month):
    current_date = datetime.datetime.now(datetime.timezone.utc)
    start_month = current_date.replace(month=Month, day=1, hour=0, minute=0, second=0, microsecond=0)
    end_month = start_month.replace(month=Month + 1) - datetime.timedelta(days=1)
    
    return start_month.date()

def get_monthends():
    monthends = []
    for month in range(1, 12):
        monthends.append(get_month_ends(month))
    return monthends

def fetch_bunker_data_gcv():
    bunker_dates = []
    bunker_values = []
    for data in BQS.find(sort={"date":1}):
        bunker_dates.append(data.get("date"))
        bunker_values.append(data.get("domestic_qty"))
    return bunker_dates, bunker_values

#coalReceiptSummary end



@router.get("/fetchgcvsummarydata", tags=["test"])
def endpoint_to_fetch_coal_recipt_summary():
    try:
        monthends = get_monthends()
        bunker_dates, bunker_values = fetch_bunker_data_gcv()
        project={
            'plant_preperation_date': 0,
            'plant_certificate_id':0,
            'plant_ulr_id':0,
            'plant_sample_date':0,
            "sample_no":0,
            "mine":0,
            "mode":0,
            "plant_lab_temp":0,
            "_id":0
        }
        filter = {
        'plant_analysis_date': {
            '$gte': datetime.datetime.now(datetime.timezone.utc).replace(day=3,month=4,hour=0, minute=0,second=0,microsecond=0)
        }, }

        df = pl.DataFrame(list(RCA.find(filter=filter, sort={"plant_analysis_date":1},skip=3)))

        df = df.with_columns([
            pl.lit(0.0).alias("cum_wt"),
            pl.lit(0.0).alias("weighted_gcv"),
            pl.lit(0.0).alias("cum_weighted_gcv"),
            pl.lit(0.0).alias("gcv")
        ])

        df = df.with_columns([
            pl.lit(float(df["sample_qty"][0])).alias('cum_wt')
        ])
        
        # initialize data to previous values    
        cum_wt_list = [73471.5+float(df['sample_qty'][0])]
        weighted_gcv_list = [float(df['sample_qty'][0]) * float(df['plant_arb_gcv'][0])]
        cum_weighted_gcv_list = [weighted_gcv_list[0]+265714358.5]
        gcv_list = [cum_weighted_gcv_list[0]/cum_wt_list[0]]
        
        dates = {}

        fdata = {
            "date": [],
            "sample_qty": [],
            "cum_wt": [],
            "weighted_gcv_arb": [],
            "cum_weighted_gcv": [],
            "weighted_gcv": [],
            "cum_domestic_qty_ytd":[],
            "wt_domestic_gcv_before_final": [],
            "cum_wt_domestic_gcv":[],
            "domestic_gcv":[],
        }
        
        for i in range(1, len(df)):      
            current_month = df['plant_analysis_date'][i].month

            cum_wt_total = float(df['sample_qty'][i]) + cum_wt_list[i-1]
            weighted_gcv_total = float(df['sample_qty'][i]) * float(df['plant_arb_gcv'][i])
            cum_weighted_gcv_total = float(weighted_gcv_total) + cum_weighted_gcv_list[i-1]
            gcv_total = cum_weighted_gcv_total/cum_wt_total
            if len(bunker_values) > 0 and cum_wt_total >= bunker_values[0]:
                # month = df['plant_analysis_date'][i].replace(day=1, month=(df['plant_analysis_date'][i].month) + 1) - timedelta(days=1)               #For Monthend
                
                to_sub = bunker_values.pop(0)
                date = bunker_dates.pop(0)
                month = date
                wt = cum_wt_total - to_sub
                wgcv = wt*float(df['plant_arb_gcv'][i])
                cwgcv = wgcv+cum_weighted_gcv_list[i-1]
                
                cum_wt_total = wt
                weighted_gcv_total = wt * float(df['plant_arb_gcv'][i])
                cum_weighted_gcv_total = weighted_gcv_total
                gcv_total = cum_weighted_gcv_total / cum_wt_total
                if not dates.get(month):
                    if not month:
                        continue

                    # fdata['date'].append(month.strftime("%m/%d/%Y"))
                    if month.strftime("%m/%d/%Y") in fdata['date']:
                        fdata['date'].append(date.strftime("%m/%d/%Y"))
                    else:
                        fdata['date'].append(month.strftime("%m/%d/%Y"))
                    fdata['sample_qty'].append(df['sample_qty'][i] - wt)
                    fdata['cum_wt'].append(to_sub)
                    fdata['weighted_gcv_arb'].append(wgcv)
                    fdata['cum_weighted_gcv'].append(cwgcv)
                    fdata['weighted_gcv'].append(cwgcv/to_sub)
                    fdata['wt_domestic_gcv_before_final'].append((df['sample_qty'][i] - wt)*(cwgcv/to_sub))
                    if len(fdata["cum_domestic_qty_ytd"]) != 0 :
                        fdata["cum_domestic_qty_ytd"].append((df['sample_qty'][i] - wt) + fdata["cum_domestic_qty_ytd"][len(fdata["cum_domestic_qty_ytd"])-1])
                    else:
                        fdata["cum_domestic_qty_ytd"].append(df['sample_qty'][i] - wt)
                    if len(fdata["cum_wt_domestic_gcv"]) != 0 :
                        fdata["cum_wt_domestic_gcv"].append(((df['sample_qty'][i] - wt)*(cwgcv/to_sub)) + fdata["cum_wt_domestic_gcv"][len(fdata["cum_wt_domestic_gcv"])-1])
                    else:
                        fdata["cum_wt_domestic_gcv"].append((df['sample_qty'][i] - wt)*(cwgcv/to_sub))
                    fdata["domestic_gcv"].append(fdata["cum_wt_domestic_gcv"][-1]/fdata["cum_domestic_qty_ytd"][-1])
            elif len(bunker_values) == 1:
                to_sub = bunker_values.pop(0)
                date = bunker_dates.pop(0)
                month = date
                wt = cum_wt_total - to_sub
                cum_wt_total = wt
                weighted_gcv_total = wt * float(df['plant_arb_gcv'][i])
                cum_weighted_gcv_total = weighted_gcv_total
                gcv_total = cum_weighted_gcv_total / cum_wt_total

                if not dates.get(month):
                    if not month:
                        continue

                    # fdata['date'].append(month.strftime("%m/%d/%Y"))
                    if month.strftime("%m/%d/%Y") in fdata['date']:
                        fdata['date'].append(date.strftime("%m/%d/%Y"))
                    else:
                        fdata['date'].append(month.strftime("%m/%d/%Y"))
                    fdata['sample_qty'].append(df['sample_qty'][i] - wt)
                    fdata['cum_wt'].append(to_sub)
                    fdata['weighted_gcv_arb'].append(wgcv)
                    fdata['cum_weighted_gcv'].append(cwgcv)
                    fdata['weighted_gcv'].append(cwgcv/to_sub)
                    fdata['wt_domestic_gcv_before_final'].append((df['sample_qty'][i] - wt)*(cwgcv/to_sub))
                    if len(fdata["cum_domestic_qty_ytd"]) != 0 :
                        fdata["cum_domestic_qty_ytd"].append((df['sample_qty'][i] - wt) + fdata["cum_domestic_qty_ytd"][len(fdata["cum_domestic_qty_ytd"])-1])
                    else:
                        fdata["cum_domestic_qty_ytd"].append(df['sample_qty'][i] - wt)
                    if len(fdata["cum_wt_domestic_gcv"]) != 0 :
                        fdata["cum_wt_domestic_gcv"].append(((df['sample_qty'][i] - wt)*(cwgcv/to_sub)) + fdata["cum_wt_domestic_gcv"][len(fdata["cum_wt_domestic_gcv"])-1])
                    else:
                        fdata["cum_wt_domestic_gcv"].append((df['sample_qty'][i] - wt)*(cwgcv/to_sub))
                    fdata["domestic_gcv"].append(fdata["cum_wt_domestic_gcv"][-1]/fdata["cum_domestic_qty_ytd"][-1])


            
            cum_wt_list.append(cum_wt_total)
            weighted_gcv_list.append(weighted_gcv_total)
            cum_weighted_gcv_list.append(cum_weighted_gcv_total)
            gcv_list.append(gcv_total)

        
        #     if len(bunker_values) == 0:
        #         last_bunker_analysis_date = df['plant_analysis_date'].max()
        #         console_logger.debug(last_bunker_analysis_date)
        #         console_logger.debug(last_bunker_analysis_date)
                
        #         wt = cum_wt_total - to_sub
        #         wgcv = wt*float(df['plant_arb_gcv'][i])
        #         cwgcv = wgcv+cum_weighted_gcv_list[i-1]
                
        #         cum_wt_total = wt
        #         weighted_gcv_total = wt * float(df['plant_arb_gcv'][i])
        #         cum_weighted_gcv_total = weighted_gcv_total
        #         gcv_total = cum_weighted_gcv_total / cum_wt_total
        #         if not dates.get(last_bunker_analysis_date):
        #             if not last_bunker_analysis_date:
        #                 continue

        #             fdata['date'].append(last_bunker_analysis_date.strftime("%m/%d/%Y"))
        #             fdata['sample_qty'].append(df['sample_qty'][i] - wt)
        #             fdata['cum_wt'].append(to_sub)
        #             fdata['weighted_gcv_arb'].append(wgcv)
        #             fdata['cum_weighted_gcv'].append(cwgcv)
        #             fdata['weighted_gcv'].append(cwgcv/to_sub)
        #             fdata['wt_domestic_gcv_before_final'].append((df['sample_qty'][i] - wt)*(cwgcv/to_sub))
        #             if len(fdata["cum_domestic_qty_ytd"]) != 0 :
        #                 fdata["cum_domestic_qty_ytd"].append((df['sample_qty'][i] - wt) + fdata["cum_domestic_qty_ytd"][len(fdata["cum_domestic_qty_ytd"])-1])
        #             else:
        #                 fdata["cum_domestic_qty_ytd"].append(df['sample_qty'][i] - wt)
        #             if len(fdata["cum_wt_domestic_gcv"]) != 0 :
        #                 fdata["cum_wt_domestic_gcv"].append(((df['sample_qty'][i] - wt)*(cwgcv/to_sub)) + fdata["cum_wt_domestic_gcv"][len(fdata["cum_wt_domestic_gcv"])-1])
        #             else:
        #                 fdata["cum_wt_domestic_gcv"].append((df['sample_qty'][i] - wt)*(cwgcv/to_sub))
        #             fdata["domestic_gcv"].append(fdata["cum_wt_domestic_gcv"][-1]/fdata["cum_domestic_qty_ytd"][-1])

            
        #     cum_wt_list.append(cum_wt_total)
        #     weighted_gcv_list.append(weighted_gcv_total)
        #     cum_weighted_gcv_list.append(cum_weighted_gcv_total)
        #     gcv_list.append(gcv_total)     
        
        df = df.with_columns(
            pl.Series('cum_wt', cum_wt_list),
            pl.Series('weighted_gcv', weighted_gcv_list),
            pl.Series('cum_weighted_gcv',cum_weighted_gcv_list),
            pl.Series('gcv', gcv_list)
        )
        fdf = pl.DataFrame(fdata)
        # fdf_dict = fdf.to_dict()
        # fdf_rows = fdf.to_dict(as_series=False).items()
        # console_logger.debug(fdf_rows)
        # csv_str = fdf.write_csv()
        fdf_dict = fdf.to_dict(as_series=False)
        # path_bunker = os.path.join(
        #                 "static_server",
        #                 "test_summary_{}.xlsx".format(
        #                     datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
        #                 ),)

        # fdf.write_excel(path_bunker)
        # console_logger.debug(fdf_dict)
        return fdf_dict
    except Exception as e:
        console_logger.debug("----- Vehicle Scanned Count Error -----",e)
        # response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

def get_week_ends(month: int, year: int):
    start_month = datetime.datetime(year, month, 1)
    if month == 12:
        end_month = start_month.replace(year=year + 1, month=1) - datetime.timedelta(days=1)
    else:
        end_month = start_month.replace(month=month + 1) - datetime.timedelta(days=1)
    weekends = []
    current_day = start_month
    while current_day <= end_month:
        # Check if the current day is a weekend (Saturday or Sunday)
        if current_day.weekday() == 5 or current_day.weekday() == 6:  # 5 = Saturday, 6 = Sunday
            weekends.append(current_day)
        current_day += datetime.timedelta(days=1)    
    return weekends


def get_current_financial_year_week(Month):
    current_date = datetime.datetime.now(datetime.timezone.utc)    
    start_month = current_date.replace(month=Month if Month < 12 else Month, day=1, year=current_date.year+1 if Month in [1,2,3] else current_date.year, hour=0,minute=0,second=0,microsecond=0)
    if Month == 12:
        end_month = start_month.replace(day=31)
    else:
        end_month = start_month.replace(month=Month+1 ) - datetime.timedelta(days=1)
    
    return start_month, end_month



@router.get("/summary/week", tags=["test"])
def summarybyWeek():
    try:
        if "BunkerQualityAnalysis" not in collectionList or "BunkerQualitySummary" not in collectionList:
            BQA = gmrDB.create_collection("BunkerQualityAnalysis")
            BQS = gmrDB.create_collection("BunkerQualitySummary")
        else:
            BQA = gmrDB.get_collection("BunkerQualityAnalysis")
            BQS = gmrDB.get_collection("BunkerQualitySummary")
            index_name = "po_index"
            BQA.create_index(index_name)
        monthends = []
        weekends = []
        for month in range(1, 13):
            start_year, end_year = get_current_financial_year_week(month)
            weekends.extend(get_week_ends(start_year.month, end_year.year))
        weekends.sort()
        
        result = BQA.aggregate([ 
            { 
                "$facet": {
                    "WeightedGCV": [
                        {
                            '$match': {
                                'sample_date': {
                                '$gte': datetime.datetime.now().replace(day=1),  # Date range from 1st of the month
                                '$lte': datetime.datetime.now()         # To the current date
                        }
                            }
                        },
                        {
                            '$sort': {
                                'bunker_wt_gcv': -1
                            }
                        },
                        {
                            '$group': {
                                '_id': {
                                    'sample_date': '$sample_date'
                                },
                                'bunker_wt_gcv': {
                                    '$max': '$bunker_wt_gcv'
                                }
                            }
                        },
                    ],
                    "DomesticQty": [
                        {
                            '$project': {
                                'sample_date': 1,  
                                'bunkered_qty': 1  
                            }
                        },
                        {
                            '$group': {
                                '_id': {
                                    '$dateToString': {
                                        'date': '$sample_date',
                                        'format': '%Y-%V'
                                    }
                                },
                                'domestic_qty': {
                                    '$sum': '$bunkered_qty' 
                                }
                            }
                        }
                    ]
                }
            }
        ])

        console_logger.debug([ 
            { 
                "$facet": {
                    "WeightedGCV": [
                        {
                            '$match': {
                                'sample_date': {
                                '$gte': datetime.datetime.now().replace(day=1),  # Date range from 1st of the month
                                '$lte': datetime.datetime.now()         # To the current date
                        }
                            }
                        },
                        {
                            '$sort': {
                                'bunker_wt_gcv': -1
                            }
                        },
                        {
                            '$group': {
                                '_id': {
                                    'sample_date': '$sample_date'
                                },
                                'bunker_wt_gcv': {
                                    '$max': '$bunker_wt_gcv'
                                }
                            }
                        },
                    ],
                    "DomesticQty": [
                        {
                            '$project': {
                                'sample_date': 1,  
                                'bunkered_qty': 1  
                            }
                        },
                        {
                            '$group': {
                                '_id': {
                                    '$dateToString': {
                                        'date': '$sample_date',
                                        'format': '%Y-%V'
                                    }
                                },
                                'domestic_qty': {
                                    '$sum': '$bunkered_qty' 
                                }
                            }
                        }
                    ]
                }
            }
        ])
        
        wgcv = []
        dqty = []
        
        for col in result:
            wgcv.extend(col.get("WeightedGCV", []))
            dqty.extend(col.get("DomesticQty", []))
        
        data = []
        # for i in wgcv:
        #     for s in dqty:
        #         if s.get("_id") == i.get("_id").get("sample_date").strftime("%Y-%m"):
        #             data.append({
        #                 "date": i.get("_id").get("sample_date"), 
        #                 "domestic_qty": s.get("domestic_qty"), 
        #                 "wt_gcv": i.get("bunker_wt_gcv"),
        #                 "imported_qty": 0, 
        #                 "total_qty": s.get("domestic_qty") + 0, 
        #                 "cum_total_qty": s.get("domestic_qty") + 0
        #             })
        
        for i in wgcv:
            sample_date = i.get("_id").get("sample_date")
            week_id = sample_date.strftime("%Y-%V")  # Convert to year-week format
            
            for s in dqty:
                if s.get("_id") == week_id:  # Match using the weekly ID
                    data.append({
                        "date": sample_date, 
                        "domestic_qty": s.get("domestic_qty"), 
                        "wt_gcv": i.get("bunker_wt_gcv"),
                        "imported_qty": 0, 
                        "total_qty": s.get("domestic_qty") + 0, 
                        "cum_total_qty": s.get("domestic_qty") + 0
                    })
        console_logger.debug(data)
        df = pl.DataFrame(data).sort(by='date', descending=False)

        df = df.with_columns([
            pl.lit(0.0).alias("cum_total_qty"),
            pl.lit(0.0).alias("weighted_domestic_gcv"),
            pl.lit(0.0).alias("cum_weighted_domestic_gcv"),
            pl.lit(0.0).alias("weighted_gcv")
            
        ])
        
        
        df = df.with_columns([
            pl.lit(df['total_qty'][0]).alias("cum_total_qty"),
            pl.lit(df['wt_gcv'][0] * df['total_qty'][0]).alias("weighted_domestic_gcv"),
            pl.lit(df['wt_gcv'][0] * df['total_qty'][0]).alias("cum_weighted_domestic_gcv"),
            pl.lit((df['wt_gcv'][0] * df['total_qty'][0]) / df['total_qty'][0]).alias("weighted_gcv")
        ])
        
        # Perform calculations for cumulative columns
        cum_total_qty_list = [df['cum_total_qty'][0]]
        weighted_domestic_gcv_list = [df['weighted_domestic_gcv'][0]]
        cum_weighted_domestic_gcv_list = [df['cum_weighted_domestic_gcv'][0]]
        weighted_gcv_list = [df['weighted_gcv'][0]]

        for i in range(1, len(df)):
            cum_total_qty = df['total_qty'][i] + cum_total_qty_list[i-1]
            
            if df['wt_gcv'][i] is not None:
                weighted_domestic_gcv = df['wt_gcv'][i] * df['total_qty'][i]
            else:
                weighted_domestic_gcv = 0
            cum_weighted_domestic_gcv = weighted_domestic_gcv + cum_weighted_domestic_gcv_list[i-1]
            weighted_gcv = cum_weighted_domestic_gcv / cum_total_qty

            cum_total_qty_list.append(cum_total_qty)
            weighted_domestic_gcv_list.append(weighted_domestic_gcv)
            cum_weighted_domestic_gcv_list.append(cum_weighted_domestic_gcv)
            weighted_gcv_list.append(weighted_gcv)

        # Update the DataFrame with the calculated lists
        df = df.with_columns([
            pl.Series("cum_total_qty", cum_total_qty_list),
            pl.Series("weighted_domestic_gcv", weighted_domestic_gcv_list),
            pl.Series("cum_weighted_domestic_gcv", cum_weighted_domestic_gcv_list),
            pl.Series("weighted_gcv", weighted_gcv_list)
        ])

        bulk = []
        
        today = datetime.datetime.today()

        # Extract the latest date from your dataframe
        latest_date = df['date'].max()

        if today.month == latest_date.month and today.year == latest_date.year:
            last_row = {key: value[0] for key, value in df.tail(1).to_dict(as_series=False).items()}
            filter_query = {"date": last_row["date"]}
            update_query = {
                "$set": {
                    "domestic_qty": last_row['domestic_qty'],
                    "wt_gcv": last_row['wt_gcv'],
                    "imported_qty": last_row['imported_qty'],
                    "total_qty": last_row['total_qty'],
                    "cum_total_qty": last_row['cum_total_qty'],
                    "weighted_domestic_gcv": last_row['weighted_domestic_gcv'],
                    "cum_weighted_domestic_gcv": last_row['cum_weighted_domestic_gcv'],
                    "weighted_gcv": last_row['weighted_gcv'],
                    # "cr_domestic_gcv_mtd": last_row['domestic_qty'],
                    
                }
            }
            bulk.append(UpdateOne(filter_query, update_query, upsert=True))
        else:
            console_logger.info("The current month is completed; no data will be inserted.")
            
        if bulk:
            BQS.bulk_write(bulk)
    except Exception as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

def last_day_of_month(date):
    
    if date.month == 12:
        return date.replace(day=31)
    return date.replace(month=date.month+1, day=1) - datetime.timedelta(days=1)
    
@router.get("/test_coalgcv", tags=["Road Coal"])
def endpoint_to_insert_gcv_analysis():
    try:
        fetchreciptData = endpoint_to_fetch_coal_recipt_summary()
        console_logger.debug(fetchreciptData['date'])
        for i, fetch_date in enumerate(fetchreciptData['date']):
            try:
                fetchBunkerSummary = BunkerQualitySummary.objects.get(date=fetch_date)
                cr_domestic_gcv_mtd = round(fetchreciptData['weighted_gcv'][i], 2)
                cr_weighted_gcv_ytd = round(fetchreciptData['domestic_gcv'][i], 2)
                cr_domestic_qty_mtd = round(fetchBunkerSummary.domestic_qty, 2)
                cr_imported_qty_mtd = 0 
                cr_imported_gcv_mtd = 0 
                cr_weighted_gcv_mtd = round(((fetchBunkerSummary.domestic_qty*cr_domestic_gcv_mtd) + (cr_imported_qty_mtd*cr_imported_gcv_mtd)) / (cr_domestic_qty_mtd+cr_imported_qty_mtd), 2) #((sample_qty*weighted_gcv) + (imported_qty_mtd*imported_gcv_mtd))/ sample_qty+imported_qty_mtd
                if fetchBunkerSummary.wt_gcv is None:
                    fetchBunkerSummary.wt_gcv = 0
                difference_in_gcv_mtd = round(cr_weighted_gcv_mtd - fetchBunkerSummary.wt_gcv, 3)
                # difference_in_gcv_mtd = round(cr_weighted_gcv_mtd - fetchBunkerSummary.wt_gcv, 3)
                difference_in_gcv_ytd = round(cr_weighted_gcv_ytd - fetchBunkerSummary.weighted_gcv, 3)
                
                fetchBunkerSummary.cr_domestic_gcv_mtd = cr_domestic_gcv_mtd
                fetchBunkerSummary.cr_weighted_gcv_ytd = cr_weighted_gcv_ytd
                fetchBunkerSummary.cr_domestic_qty_mtd = cr_domestic_qty_mtd
                fetchBunkerSummary.cr_imported_qty_mtd = cr_imported_qty_mtd
                fetchBunkerSummary.cr_imported_gcv_mtd = cr_imported_gcv_mtd
                fetchBunkerSummary.cr_weighted_gcv_mtd = cr_weighted_gcv_mtd
                fetchBunkerSummary.difference_in_gcv_mtd = difference_in_gcv_mtd
                fetchBunkerSummary.difference_in_gcv_ytd = difference_in_gcv_ytd
                fetchBunkerSummary.save()

            except DoesNotExist as e:
                continue
        return {"detail": "success"}
    except Exception as e:
        console_logger.debug("----- Coal GCV update Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

@router.get("/coal_gcv_analysis", tags=["Road Coal"])
def endpoint_to_fetch_gcv_analysis(response: Response, currentPage: Optional[int] = None, perPage: Optional[int] = None, search_text:  Optional[str] = None, type: Optional[str]="display"):
    try:
        result = {        
                "labels": [],
                "datasets": [],
                "total" : 0,
                "page_size": 15
        }
        if type and type == "display":
            data = Q()
            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage
                
            offset = (page_no - 1) * page_len

            listData = []
            logs = BunkerQualitySummary.objects().skip(offset).limit(page_len).order_by("date")
            if any(logs):
                for log in logs:
                    # result["labels"] = ["year", "month", "cb_domestic_qty_mtd", "cb_imported_qty_mtd", "cb_weighted_gcv_mtd", "cb_weighted_gcv_ytd", "cr_domestic_qty_mtd", "cr_domestic_gcv_mtd", "cr_imported_qty_mtd",  "cr_imported_gcv_mtd", "cr_weighted_gcv_mtd", "cr_weighted_gcv_ytd", "difference_in_gcv_mtd", "difference_in_gcv_ytd"]
                    # result["labels"] = list(log.simplepayload().keys())
                    # result["datasets"].append(log.simplepayload())
                    payload = log.simplepayload()
                    
                    if search_text:
                        if search_text.lower() in str(payload).lower():
                            result["labels"] = ["year", "month", "cb_domestic_qty_mtd", "cb_imported_qty_mtd", "cb_weighted_gcv_mtd", "cb_weighted_gcv_ytd", "cr_domestic_qty_mtd", "cr_domestic_gcv_mtd", "cr_imported_qty_mtd",  "cr_imported_gcv_mtd", "cr_weighted_gcv_mtd", "cr_weighted_gcv_ytd", "difference_in_gcv_mtd", "difference_in_gcv_ytd"]
                            
                            result["datasets"].append(payload)
                    else:
                        # If no search_text provided, add all the data
                        result["labels"] = ["year", "month", "cb_domestic_qty_mtd", "cb_imported_qty_mtd", "cb_weighted_gcv_mtd", "cb_weighted_gcv_ytd", "cr_domestic_qty_mtd", "cr_domestic_gcv_mtd", "cr_imported_qty_mtd",  "cr_imported_gcv_mtd", "cr_weighted_gcv_mtd", "cr_weighted_gcv_ytd", "difference_in_gcv_mtd", "difference_in_gcv_ytd"]

                        result["datasets"].append(payload)

            result["total"]= len(BunkerQualitySummary.objects())
            return result
        elif type and type =="download":
            del type

            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            usecase_data = BunkerQualitySummary.objects()
            count = len(usecase_data)
            path = None
            logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
            if usecase_data:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "coal_gcv_analysis_Report_{}.xlsx".format(
                            datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format()
                    cell_format2.set_bold()
                    cell_format2.set_font_size(10)
                    cell_format2.set_align("center")
                    cell_format2.set_align("vcenter")
                    cell_format2.set_text_wrap(True)
                    cell_format2.set_border(1)

                    header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
                    date_format = workbook.add_format({'align': 'center', 'font_size': 12, "bold": True})
                    report_name_format = workbook.add_format({'align': 'center', 'font_size': 15, "bold": True})
                    # report_name_format.set_text_wrap(True)

                    header_format.set_align("vcenter")
                    date_format.set_align("vcenter")
                    report_name_format.set_align("vcenter")
                    header_format.set_border(1)
                    date_format.set_border(1)
                    report_name_format.set_border(1)

                    worksheet = workbook.add_worksheet()
                    # worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)
                    # cell_format = workbook.add_format()
                    cell_format = workbook.add_format({'num_format': '0'}) 
                    cell_format.set_font_size(10)
                    cell_format.set_align("center")
                    cell_format.set_align("vcenter")
                    cell_format.set_text_wrap(True)
                    cell_format.set_border(1)

                    worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                    
                    # Merge cells for the main header and place it in the center
                    main_header = "GMR Warora Energy Limited"  # Set your main header text here
                    worksheet.merge_range("A1:O1", main_header, header_format)  # Merge cells A1 to H1 for the header
                    
                    # Write the current date on the left side (A2)
                    # worksheet.write("A2", f"Date: {datetime.datetime.now().strftime('%d-%m-%Y')}", date_format)
                    worksheet.merge_range("A2:B2", f"Date: {datetime.datetime.now().strftime('%d-%m-%Y')}", date_format)
                    worksheet.merge_range("C2:O2", f"Coal GCV Analysis", report_name_format)

                    diff_gcv_color = workbook.add_format({'bg_color': '#C4BD97'})

                    headers= [
                        "Srno",
                        "Year",
                        "Month",
                        "Coal Bunker Domestic Qty MTD",
                        "Coal Bunker Imported Qty MTD",
                        "Coal Bunker Weighted GCV MTD",
                        "Coal Bunker Weighted GCVYTD",
                        "Coal Receipt Domestic Qty MTD",
                        "Coal Receipt Domestic GCV MTD",
                        "Coal Receipt Imported Qty MTD",
                        "Coal Receipt Imported GCV MTD",
                        "Coal Receipt Weighted GCV MTD",
                        "Coal Receipt Weighted GCV YTD",
                        "Difference in GCV MTD",
                        "Difference in GCV YTD"
                    ]

                    for index, header in enumerate(headers):
                        worksheet.write(2, index, header, cell_format2)
                    finalData = None
                    color_count = 3+len(usecase_data)
                    worksheet.conditional_format(f'N3:N{color_count}', {'type': 'no_blanks', 'format': diff_gcv_color})
                    worksheet.conditional_format(f'O3:O{color_count}', {'type': 'no_blanks', 'format': diff_gcv_color})

                    for row, query in enumerate(usecase_data, start=3):
                        dataField = query.simplepayload()
                        # Add search filtering here
                        if search_text:
                            if search_text.lower() in str(dataField).lower():
                                finalData = dataField
                        else:
                            finalData = dataField
                        if finalData is not None:
                            worksheet.write(row, 0, count, cell_format)
                            worksheet.write(row, 1, finalData["year"], cell_format)
                            worksheet.write(row, 2, finalData["month"], cell_format)
                            worksheet.write(row, 3, finalData["cb_domestic_qty_mtd"], cell_format)
                            worksheet.write(row, 4, finalData["cb_imported_qty_mtd"], cell_format)
                            worksheet.write(row, 5, finalData["cb_weighted_gcv_mtd"], cell_format)
                            worksheet.write(row, 6, finalData["cb_weighted_gcv_ytd"], cell_format)
                            worksheet.write(row, 7, finalData["cr_domestic_qty_mtd"], cell_format)
                            worksheet.write(row, 8, finalData["cr_domestic_gcv_mtd"], cell_format)
                            worksheet.write(row, 9, finalData["cr_imported_qty_mtd"], cell_format)
                            worksheet.write(row, 10, finalData["cr_imported_gcv_mtd"], cell_format)
                            worksheet.write(row, 11, finalData["cr_weighted_gcv_mtd"], cell_format)
                            worksheet.write(row, 12, finalData["cr_weighted_gcv_ytd"], cell_format)
                            worksheet.write(row, 13, finalData["difference_in_gcv_mtd"], cell_format)
                            worksheet.write(row, 14, finalData["difference_in_gcv_ytd"], cell_format)

                            count-=1
                        else: 
                            return {"Type":"coal_gcv_analysis_download_event","Datatype":"Report","File_Path":None}

                    workbook.close()
                    console_logger.debug("Successfully {} report generated".format(service_id))
                    console_logger.debug("sent data {}".format(path))

                    return {
                            "Type": "coal_gcv_analysis_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                            }
                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            else:
                console_logger.error("No data found")
                return {
                        "Type": "coal_gcv_analysis_download_event",
                        "Datatype": "Report",
                        "File_Path": path,
                        }
    except Exception as e:
        console_logger.debug(e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug((exc_type, fname, exc_tb.tb_lineno))
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


# old calculation for cr part
# @router.get("/coal_gcv_analysis", tags=["Road Coal"])
# def endpoint_to_fetch_gcv_analysis(response: Response, currentPage: Optional[int] = None, perPage: Optional[int] = None, search_text:  Optional[str] = None, type: Optional[str]="display"):
#     try:
#         result = {        
#                 "labels": [],
#                 "datasets": [],
#                 "total" : 0,
#                 "page_size": 15
#         }
#         if type and type == "display":
#             page_no = 1
#             page_len = result["page_size"]

#             if currentPage:
#                 page_no = currentPage

#             if perPage:
#                 page_len = perPage
#                 result["page_size"] = perPage
                
#             offset = (page_no - 1) * page_len
            
#             listData = []
#             fetchBunkerQualitySummary = BunkerQualitySummary.objects().skip(offset).limit(page_len)
#             for singleBunkerQuality in fetchBunkerQualitySummary:
#                 # console_logger.debug(singleBunkerQuality.date)
#                 dataDict = {}
#                 # dataDict["year"] = "FY-2024"
#                 date_obj = singleBunkerQuality.date
#                 year = date_obj.year
#                 month = date_obj.month
#                 fetchreciptData = endpoint_to_fetch_coal_recipt_summary()
                
#                 db_date = singleBunkerQuality.date.strftime("%m/%d/%Y")
#                 if month < 4:  # Months: Jan(1), Feb(2), Mar(3)
#                     financial_year = f"FY {year - 1}-{str(year+1)[2:]}"
#                 else:  # Months: Apr(4), May(5), ..., Dec(12)
#                     financial_year = f"FY {year}-{str(year+1)[2:]}"
#                 # console_logger.debug(financial_year)
#                 dataDict["year"] = financial_year
#                 dataDict["month"] = singleBunkerQuality.date.strftime("%Y-%m-%d")
#                 dataDict["cb_domestic_qty_mtd"] = round(singleBunkerQuality.domestic_qty, 2)
#                 dataDict["cb_imported_qty_mtd"] = round(singleBunkerQuality.imported_qty, 2)
#                 dataDict["cb_weighted_gcv_mtd"] = round(singleBunkerQuality.wt_gcv, 2)
#                 dataDict["cb_weighted_gcv_ytd"] = round(singleBunkerQuality.weighted_gcv, 2)
#                 console_logger.debug(fetchreciptData)

#                 dataDict["cr_domestic_gcv_mtd"] = 0  
#                 dataDict["cr_weighted_gcv_ytd"] = 0 

#                 for i, fetch_date in enumerate(fetchreciptData['date']):
#                     console_logger.debug(fetch_date)
#                     console_logger.debug(db_date)
#                     if fetch_date == db_date:
#                         # dataDict["cr_domestic_qty_mtd"] = round(fetchreciptData['sample_qty'][i], 2)
#                         dataDict["cr_domestic_gcv_mtd"] = round(fetchreciptData['weighted_gcv'][i], 2)
#                         dataDict["cr_weighted_gcv_ytd"] = round(fetchreciptData['domestic_gcv'][i], 2)
#                 # dataDict["cr_domestic_qty_mtd"] = 0 #sample_qty
#                 # dataDict["cr_domestic_gcv_mtd"] = 0 #weighted_gcv
#                 dataDict["cr_domestic_qty_mtd"] = round(singleBunkerQuality.domestic_qty, 2)
#                 dataDict["cr_imported_qty_mtd"] = 0 
#                 dataDict["cr_imported_gcv_mtd"] = 0 
#                 console_logger.debug(dataDict["cr_domestic_gcv_mtd"])
#                 # dataDict["cr_weighted_gcv_mtd"] = round(((dataDict["cr_domestic_qty_mtd"]*dataDict["cr_domestic_gcv_mtd"]) + (dataDict["cr_imported_qty_mtd"]*dataDict["cr_imported_gcv_mtd"]) / dataDict["cr_domestic_qty_mtd"]+dataDict["cr_imported_qty_mtd"]), 2) #((sample_qty*weighted_gcv) + (imported_qty_mtd*imported_gcv_mtd))/ sample_qty+imported_qty_mtd
#                 dataDict["cr_weighted_gcv_mtd"] = round(((singleBunkerQuality.domestic_qty*dataDict["cr_domestic_gcv_mtd"]) + (dataDict["cr_imported_qty_mtd"]*dataDict["cr_imported_gcv_mtd"])) / (dataDict["cr_domestic_qty_mtd"]+dataDict["cr_imported_qty_mtd"]), 2) #((sample_qty*weighted_gcv) + (imported_qty_mtd*imported_gcv_mtd))/ sample_qty+imported_qty_mtd
#                 # dataDict["cr_weighted_gcv_ytd"] = 0
#                 dataDict["difference_in_gcv_mtd"] = round(dataDict["cr_weighted_gcv_mtd"] - dataDict["cb_weighted_gcv_mtd"], 3)
#                 dataDict["difference_in_gcv_ytd"] = round(dataDict["cr_weighted_gcv_ytd"] - dataDict["cb_weighted_gcv_ytd"], 3)
#                 listData.append(dataDict)
#                 # console_logger.debug(dataDict)
#             if search_text and search_text != "all":
#                 filtered_data = [entry for entry in listData if entry['year'] == search_text]
#             elif search_text == "all":
#                 filtered_data = listData
#             else:
#                 filtered_data = listData
#             # result["labels"] = list(dataDict.keys())
#             result["labels"] = ["year", "month", "cb_domestic_qty_mtd", "cb_imported_qty_mtd", "cb_weighted_gcv_mtd", "cb_weighted_gcv_ytd", "cr_domestic_qty_mtd", "cr_domestic_gcv_mtd", "cr_imported_qty_mtd",  "cr_imported_gcv_mtd", "cr_weighted_gcv_mtd", "cr_weighted_gcv_ytd", "difference_in_gcv_mtd", "difference_in_gcv_ytd"]
#             result["datasets"] = filtered_data
#             result["total"]= len(BunkerQualitySummary.objects())
#             return result
#         elif type and type =="download":
#             del type

#             file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
#             target_directory = f"static_server/gmr_ai/{file}"
#             os.umask(0)
#             os.makedirs(target_directory, exist_ok=True, mode=0o777)

#             usecase_data = BunkerQualitySummary.objects()
#             count = len(usecase_data)
#             path = None
#             if usecase_data:
#                 try:
#                     path = os.path.join(
#                         "static_server",
#                         "gmr_ai",
#                         file,
#                         "coal_gcv_analysis_Report_{}.xlsx".format(
#                             datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
#                         ),
#                     )
#                     filename = os.path.join(os.getcwd(), path)
#                     workbook = xlsxwriter.Workbook(filename)
#                     workbook.use_zip64()
#                     cell_format2 = workbook.add_format()
#                     cell_format2.set_bold()
#                     cell_format2.set_font_size(10)
#                     cell_format2.set_align("center")
#                     cell_format2.set_align("vjustify")

#                     worksheet = workbook.add_worksheet()
#                     worksheet.set_column("A:AZ", 20)
#                     worksheet.set_default_row(50)
#                     # cell_format = workbook.add_format()
#                     cell_format = workbook.add_format({'num_format': '0'}) 
#                     cell_format.set_font_size(10)
#                     cell_format.set_align("center")
#                     cell_format.set_align("vcenter")
#                     headers = [
#                         "Srno",
#                         "Year",
#                         "Month",
#                         "Coal Bunker Domestic Qty MTD",
#                         "Coal Bunker Imported Qty MTD",
#                         "Coal Bunker Weighted GCV MTD",
#                         "Coal Bunker Weighted GCV YTD",
#                         "Coal Receipt Domestic Qty MTD",
#                         "Coal Receipt Domestic GCV MTD",
#                         "Coal Receipt Imported Qty MTD",
#                         "Coal Receipt Imported GCV MTD",
#                         "Coal Receipt Weighted GCV MTD",
#                         "Coal Receipt Weighted GCV YTD",
#                         "Difference in GCV MTD",
#                         "Difference in GCV YTD"
#                     ]

#                     for index, header in enumerate(headers):
#                         worksheet.write(0, index, header, cell_format2)

#                     for row, result in enumerate(usecase_data, start=1):
#                         # Writing data to the worksheet
#                         worksheet.write(row, 0, count, cell_format)  # Ensure cell_format is properly defined

#                         # Extracting date and formatting it according to financial year
#                         console_logger.debug(result['date'])    
#                         date_obj = result['date']
#                         year = date_obj.year
#                         month = date_obj.month

#                         fetchreciptData = endpoint_to_fetch_coal_recipt_summary()
                        
#                         db_date = result['date'].strftime("%m/%d/%Y")

#                         # Handling financial year based on month
#                         if month < 4:  # Jan(1), Feb(2), Mar(3)
#                             financial_year = f"FY {year - 1}-{str(year + 1)[2:]}"
#                         else:  # Apr(4) to Dec(12)
#                             financial_year = f"FY {year}-{str(year + 1)[2:]}"
                        
#                         # Writing data into the worksheet
#                         worksheet.write(row, 1, financial_year)                      
#                         worksheet.write(row, 2, date_obj.strftime("%Y-%m-%d"))                      
#                         worksheet.write(row, 3, round(result['domestic_qty'], 2))                      
#                         worksheet.write(row, 4, round(result['imported_qty'], 2))                      
#                         worksheet.write(row, 5, round(result['wt_gcv'], 2))                      
#                         worksheet.write(row, 6, round(result['weighted_gcv'], 2))  # cb_weighted_gcv_ytd
#                         cr_domestic_gcv_mtd = 0
#                         cr_weighted_gcv_ytd = 0
#                         for i, fetch_date in enumerate(fetchreciptData['date']):
#                             if fetch_date == db_date:
#                                 cr_domestic_gcv_mtd = round(fetchreciptData['weighted_gcv'][i], 2)
#                                 worksheet.write(row, 8, round(fetchreciptData['weighted_gcv'][i], 2)) # cr_domestic_gcv_mtd
#                                 worksheet.write(row, 12, round(fetchreciptData['domestic_gcv'][i], 2)) # cr_weighted_gcv_ytd
#                                 cr_weighted_gcv_ytd = round(fetchreciptData['domestic_gcv'][i], 2)
#                         worksheet.write(row, 7, round(result["domestic_qty"], 2)) # cr_domestic_qty_mtd
#                         cr_domestic_qty_mtd = round(result["domestic_qty"], 2)
#                         worksheet.write(row, 9, 0)  # cr_domestic_qty_mtd
#                         worksheet.write(row, 10, 0)  # cr_imported_qty_mtd  
#                         cr_imported_qty_mtd = 0
#                         cr_imported_gcv_mtd = 0
#                         # worksheet.write(row, 11, round(((cr_domestic_qty_mtd*cr_domestic_gcv_mtd) + (cr_imported_qty_mtd*cr_imported_gcv_mtd) / cr_domestic_qty_mtd+cr_imported_qty_mtd), 2))  # cr_weighted_gcv_mtd 
#                         worksheet.write(row, 11, round(((cr_domestic_qty_mtd*cr_domestic_gcv_mtd) + (cr_imported_qty_mtd*cr_imported_gcv_mtd)) / (cr_domestic_qty_mtd+cr_imported_qty_mtd), 2))
#                         cr_weighted_gcv_mtd = round(((cr_domestic_qty_mtd*cr_domestic_gcv_mtd) + (cr_imported_qty_mtd*cr_imported_gcv_mtd)) / (cr_domestic_qty_mtd+cr_imported_qty_mtd), 2)
                        
#                         worksheet.write(row, 13, round(cr_weighted_gcv_mtd - result['wt_gcv'], 2))   # cr_weighted_gcv_mtd
#                         worksheet.write(row, 14, round(cr_weighted_gcv_ytd - result['weighted_gcv'], 2))  # cr_weighted_gcv_ytd

#                         count-=1
                        
#                     workbook.close()
#                     console_logger.debug("Successfully {} report generated".format(service_id))
#                     console_logger.debug("sent data {}".format(path))

#                     return {
#                             "Type": "coal_gcv_analysis_download_event",
#                             "Datatype": "Report",
#                             "File_Path": path,
#                             }
#                 except Exception as e:
#                     console_logger.debug(e)
#                     exc_type, exc_obj, exc_tb = sys.exc_info()
#                     fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#                     console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
#                     console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#             else:
#                 console_logger.error("No data found")
#                 return {
#                         "Type": "coal_gcv_analysis_download_event",
#                         "Datatype": "Report",
#                         "File_Path": path,
#                         }
#     except Exception as e:
#         console_logger.debug(e)
#         response.status_code = 400
#         exc_type, exc_obj, exc_tb = sys.exc_info()
#         fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
#         console_logger.debug((exc_type, fname, exc_tb.tb_lineno))
#         console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
#         return e


def get_sap_data(arv_cum_do_numbers):
    sap_query = {"do_no": {"$in": arv_cum_do_numbers}}
    sap_pipeline = [
        {"$match": sap_query},
        {
            "$project": {
                "do_no": 1,
                "po_amount": {
                    "$toDouble": {
                        "$replaceAll": {
                            "input": "$po_amount",
                            "find": ",",
                            "replacement": ""
                        }
                    }
                }
            }
        }
    ]
    sap_results = list(sapdb.aggregate(sap_pipeline))
    sap_data = {item['do_no']: item['po_amount'] for item in sap_results}
    return sap_data


@router.get("/cmpl_recovery", tags=["Recovery"])
def get_cmpl_recovery(response: Response,
                   search_text: Optional[str]=None,
                   date: Optional[str] = None,
                   start_timestamp:Optional[str]=None, 
                   end_timestamp:Optional[str]=None,
                   currentPage: Optional[int] = 1, perPage: Optional[int] = 10,
                   type: Optional[str] = "display"):
    try:
        query = {}
        alldata = []   

        results = {        
                "labels": [],
                "datasets": [],
                "total" : 0,
                "page_size": 15
        }

        if type and type == "display":
            page_no = 1
            page_len = results["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                results["page_size"] = perPage

            skip_value = (page_no - 1) * page_len

            if date:
                end_date = f'{date} 23:59:59'
                start_date = f'{date} 00:00:00'
                format_data = "%Y-%m-%d %H:%M:%S"

                endd_date = datetime.datetime.strptime(end_date, format_data)
                startd_date = datetime.datetime.strptime(start_date, format_data)

                startd_date = startd_date.replace(tzinfo=pytz.timezone('Asia/Kolkata')).astimezone(pytz.utc)
                endd_date = endd_date.replace(tzinfo=pytz.timezone('Asia/Kolkata')).astimezone(pytz.utc)

                startd_date = datetime.datetime.strptime("{}.000Z".format(startd_date.isoformat().split("+")[0]),
                                                        "%Y-%m-%dT%H:%M:%S.%fZ")
                endd_date = datetime.datetime.strptime("{}.000Z".format(endd_date.isoformat().split("+")[0]),
                                                    "%Y-%m-%dT%H:%M:%S.%fZ")

                query["GWEL_Tare_Time"] = {"$gte": startd_date, "$lte": endd_date}
            
            if search_text:
                query["$or"] = [
                    {"type_consumer": {"$regex": f"{search_text}", "$options": "i"}},
                    {"arv_cum_do_number": {"$regex": f"{search_text}", "$options": "i"}}]

            total_count_pipeline = [
                {"$match": query},
                {"$count": "total_documents"}
            ]

            # pipeline = [
            #     {"$match": query},
            #     {"$sort": {"type_consumer": -1, "created_at": -1}},
            #     {
            #         "$group": {
            #             "type_consumer": {"$first": "$type_consumer"},                    
            #             "mine_name": {"$first": "$mine"},
            #             "_id": "$arv_cum_do_number",
            #             "po_qty": {"$first": "$po_qty"},
            #             # "cc_net_qty": {"$sum": {"$toDouble": "$actual_net_qty"}}
            #             "cc_net_qty": {"$sum": {
            #                 "$toDouble": {
            #                     "$replaceAll": {
            #                         "input": "$actual_net_qty",
            #                         "find": ",",
            #                         "replacement": ""
            #                     }
            #                 }
            #             }}
                        
            #         }
            #     },
            #     {
            #         "$addFields": {
            #             "TL": {
            #                 "$subtract": [
            #                     {
            #                         "$toDouble": {
            #                             "$replaceAll": {
            #                                 "input": "$po_qty",
            #                                 "find": ",",
            #                                 "replacement": ""
            #                             }
            #                         }
            #                     },
            #                     "$cc_net_qty"
            #                 ]
            #             }
            #         }
            #     },
            #     {
            #         "$project": {
                        
            #             "_id": 0,
            #             "type_consumer": 1,
            #             "mine_name": 1,
            #             "arv_cum_do_number": "$_id",
            #             "po_qty": {"$toDouble": "$po_qty"},
            #             "cc_net_qty": 1,
            #             "TL": 1,
                        
            #         }
            #     }
            # ]

            pipeline = [
                {"$match": query},
                {"$sort": {"type_consumer": -1, "created_at": -1}},
                {
                    "$group": {
                        "type_consumer": {"$first": "$type_consumer"},
                        "mine_name": {"$first": "$mine"},
                        "_id": "$arv_cum_do_number",
                        "po_qty": {"$first": "$po_qty"},
                        "cc_net_qty": {"$sum": {
                            "$convert": {
                                "input": {
                                    "$replaceAll": {
                                        "input": "$actual_net_qty",
                                        "find": ",",
                                        "replacement": ""
                                    }
                                },
                                "to": "double",
                                "onError": 0
                            }
                        }}
                    }
                },
                {
                    "$addFields": {
                        "TL": {
                            "$subtract": [
                                {
                                    "$convert": {
                                        "input": {
                                            "$replaceAll": {
                                                "input": "$po_qty",
                                                "find": ",",
                                                "replacement": ""
                                            }
                                        },
                                        "to": "double",
                                        "onError": 0
                                    }
                                },
                                "$cc_net_qty"
                            ]
                        }
                    }
                },
                {
                    "$project": {
                        "_id": 0,
                        "type_consumer": 1,
                        "mine_name": 1,
                        "arv_cum_do_number": "$_id",
                        "po_qty": {
                            "$convert": {
                                "input": {
                                    "$replaceAll": {
                                        "input": "$po_qty",
                                        "find": ",",
                                        "replacement": ""
                                    }
                                },
                                "to": "double",
                                "onError": 0
                            }
                        },
                        "cc_net_qty": 1,
                        "TL": 1
                    }
                },
                { 
                    "$skip": skip_value
                },
                { 
                    "$limit": page_len
                }
            ]

            countAggregation = [
                {"$match": query},
                {
                    '$sort': {
                        'type_consumer': -1, 
                        'created_at': -1
                    }
                }, {
                    '$group': {
                        'type_consumer': {
                            '$first': '$type_consumer'
                        }, 
                        'mine_name': {
                            '$first': '$mine'
                        }, 
                        '_id': '$arv_cum_do_number', 
                        'po_qty': {
                            '$first': '$po_qty'
                        }, 
                        'cc_net_qty': {
                            '$sum': {
                                '$convert': {
                                    'input': {
                                        '$replaceAll': {
                                            'input': '$actual_net_qty', 
                                            'find': ',', 
                                            'replacement': ''
                                        }
                                    }, 
                                    'to': 'double', 
                                    'onError': 0
                                }
                            }
                        }
                    }
                }, {
                    '$count': 'total_count'
                }
            ]

            # result = list(db.aggregate(pipeline))
            result = list(Gmrdata.objects().aggregate(pipeline))
            countResult = list(Gmrdata.objects().aggregate(countAggregation))
            arv_cum_do_numbers = [item['arv_cum_do_number'] for item in result]
            sap_data = get_sap_data(arv_cum_do_numbers)

            for item in result:
                if item['TL'] is not None:
                    item['TL'] = round(item['TL'], 2)
                else:
                    item['TL'] = 0

                arv_cum_do_number = item['arv_cum_do_number']
                po_amount = sap_data.get(arv_cum_do_number, None)
                item['po_amount'] = po_amount

                po_qty = float(item['po_qty']) if item['po_qty'] is not None else None

                if (item['TL'] > 0.005 and
                    item['cc_net_qty'] is not None and item['cc_net_qty'] > 0 and
                    po_amount is not None):
                    
                    if po_qty is not None:
                        item['penalty'] = round(po_qty * 0.005, 2)
                    else:
                        item['penalty'] = 0.0
                else:
                    item['penalty'] = 0.0
            results["labels"] = ["type_consumer", "mine_name", "cc_net_qty", "TL", "arv_cum_do_number", "po_qty", "po_amount", "penalty"]
            results["datasets"] = result
            results["total"] = countResult[0].get("total_count") if countResult else 0
            return results

        elif type and type == "download":
            del type

            file = str(datetime.datetime.utcnow().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            if start_timestamp and end_timestamp:
                start_date = datetime.datetime.strptime(start_timestamp, "%Y-%m-%dT%H:%M")
                end_date = datetime.datetime.strptime(end_timestamp, "%Y-%m-%dT%H:%M")

                start_date = start_date.replace(tzinfo=pytz.timezone('Asia/Kolkata')).astimezone(pytz.utc)
                end_date = end_date.replace(tzinfo=pytz.timezone('Asia/Kolkata')).astimezone(pytz.utc)

                start_date = datetime.datetime.strptime("{}.000Z".format(start_date.isoformat().split("+")[0]), "%Y-%m-%dT%H:%M:%S.%fZ")
                end_date = datetime.datetime.strptime("{}.000Z".format(end_date.isoformat().split("+")[0]), "%Y-%m-%dT%H:%M:%S.%fZ")

                query["GWEL_Tare_Time"] = {"$gte": start_date,"$lte": end_date}

            if search_text:
                query["$or"] = [
                    {"type_consumer": {"$regex": f"{search_text}", "$options": "i"}},
                    {"arv_cum_do_number": {"$regex": f"{search_text}", "$options": "i"}}]
                    
            pipeline = [
                {"$match": query},
                {"$sort": {"type_consumer": -1, "created_at": -1}},
                {
                    "$group": {
                        "type_consumer": {"$first": "$type_consumer"},                    
                        "mine_name": {"$first": "$mine"},
                        "_id": "$arv_cum_do_number",
                        "po_qty": {"$first": "$po_qty"},
                        "cc_net_qty": {"$sum": {"$toDouble": "$actual_net_qty"}}
                        
                    }
                },
                {
                    "$addFields": {
                        "TL": {"$subtract": [{"$toDouble": "$po_qty"}, "$cc_net_qty"]}
                    }
                },
                {
                    "$project": {
                        
                        "_id": 0,
                        "type_consumer": 1,
                        "mine_name": 1,
                        "arv_cum_do_number": "$_id",
                        "po_qty": {"$toDouble": "$po_qty"},
                        "cc_net_qty": 1,

                        "TL": 1,
                        
                    }
                }
            ]

            alldata = list(db.aggregate(pipeline))
            arv_cum_do_numbers = [item['arv_cum_do_number'] for item in alldata]
            sap_data = get_sap_data(arv_cum_do_numbers)

            for item in alldata:
                if item['TL'] is not None:
                    item['TL'] = round(item['TL'], 2)
                else:
                    item['TL'] = 0

                arv_cum_do_number = item['arv_cum_do_number']
                po_amount = sap_data.get(arv_cum_do_number, None)
                item['po_amount'] = po_amount

                po_qty = float(item['po_qty']) if item['po_qty'] is not None else None

                if (item['TL'] > 0.005 and
                    item['cc_net_qty'] is not None and item['cc_net_qty'] > 0 and
                    po_amount is not None):
                    
                    if po_qty is not None:
                        item['penalty'] = round(po_qty * 0.005, 2)
                    else:
                        item['penalty'] = 0.0
                else:
                    item['penalty'] = 0.0

            path = None
            logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
            if alldata:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "CMPL_Recovery_Report_{}.xlsx".format(
                            datetime.datetime.utcnow().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format()
                    cell_format2.set_bold()
                    cell_format2.set_font_size(10)
                    cell_format2.set_align("center")
                    cell_format2.set_align("vcenter")
                    cell_format2.set_text_wrap(True)
                    cell_format2.set_border(1)

                    header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
                    date_format = workbook.add_format({'align': 'center', 'font_size': 12, "bold": True})
                    report_name_format = workbook.add_format({'align': 'center', 'font_size': 15, "bold": True})
                    # report_name_format.set_text_wrap(True)

                    header_format.set_align("vcenter")
                    date_format.set_align("vcenter")
                    report_name_format.set_align("vcenter")
                    header_format.set_border(1)
                    date_format.set_border(1)
                    report_name_format.set_border(1)

                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)
                    cell_format = workbook.add_format()
                    cell_format.set_font_size(10)
                    cell_format.set_align("center")
                    cell_format.set_align("vcenter")
                    cell_format.set_text_wrap(True)
                    cell_format.set_border(1)

                    worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                    
                    # Merge cells for the main header and place it in the center
                    main_header = "GMR Warora Energy Limited"  # Set your main header text here
                    worksheet.merge_range("A1:H1", main_header, header_format)
                    
                    # Write the current date on the left side (A2)
                    worksheet.write("A2", f"Date: {end_date.strftime('%d-%m-%Y')}", date_format)
                    worksheet.merge_range("C2:H2", f"RCF Road Losses", report_name_format)

                    headers = [
                        "Sr.No.",
                        "Mode",
                        "Mine",
                        "Do Number",
                        "Mines WT",
                        "GWEL Net Weight",
                        "TL",
                        "Penalty"
                    ]
                    for index, header in enumerate(headers):
                        worksheet.write(2, index, header, cell_format2)
                    sr_no = 1
                    for row, item in enumerate(alldata, start=3):
                        worksheet.write(row, 0, sr_no, cell_format)
                        worksheet.write(row, 1, str(item["type_consumer"]), cell_format)
                        worksheet.write(row, 2, str(item["mine_name"]), cell_format)     
                        worksheet.write(row, 3, str(item["arv_cum_do_number"]), cell_format)
                        if item.get("po_qty"):
                            worksheet.write(row, 4, float(item["po_qty"]), cell_format)         
                        else:
                            worksheet.write(row, 4, 0, cell_format)
                        if item.get("cc_net_qty"):      
                            worksheet.write(row, 5, float(item["cc_net_qty"]), cell_format)     
                        else:
                            worksheet.write(row, 5, 0, cell_format)
                        if item.get("TL"):
                            worksheet.write(row, 6, float(item["TL"]), cell_format)
                        else:            
                            worksheet.write(row, 6, 0, cell_format)
                        if item.get("penalty"):      
                            worksheet.write(row, 7, float(item["penalty"]), cell_format)        
                        else:
                            worksheet.write(row, 7, 0, cell_format)        
                        sr_no += 1
   
                    workbook.close()
                    console_logger.debug("Successfully report generated")
                    console_logger.debug("sent data {}".format(path))

                    return {
                            "Type": "CMPL_Recovery_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                            }
                
                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug((exc_type, fname, exc_tb.tb_lineno))
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            else:
                console_logger.debug("No data found")
                return {
                        "Type": "CMPL_Recovery_download_event",
                        "Datatype": "Report",
                        "File_Path": path,
                        }

    except Exception as e:
        console_logger.debug(f"----- CMPL Recovery Error ----- {e}")
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(f"{exc_type} in {fname} on line {exc_tb.tb_lineno}")
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return {"detail": str(e)}


@router.get("/rcf_losses", tags=["Recovery"])
def get_rcf_losses(response: Response,
                   month: Optional[str] = None,
                   currentPage: Optional[int] = None,
                   perPage: Optional[int] = None,                   
                   type: Optional[str] = "display"):
    try:
        query = {}

        result = {        
                "labels": [],
                "datasets": [],
                "footerdatasets": [],
                "total" : 0,
                "page_size": 15
        }
        
        if type and type == "display":
            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage

            skip_value = (page_no - 1) * page_len

            if month:
                query["month"] = {"$regex": f"{month}", "$options": "i"}

            pipeline = [
                {'$match': query}, 
                {
                    "$group": {
                        "_id": {
                            "$substr": ["$month", 0, 7]
                        },
                        "total_secl_net_wt_sum": {"$sum": {"$toDouble": "$total_secl_net_wt"}},
                        "total_gwel_net_wt_sum": {"$sum": {"$toDouble": "$total_gwel_net_wt"}},
                        "tl_sum": {
                            "$sum": {
                                "$subtract": [
                                    {"$toDouble": "$total_secl_net_wt"},
                                    {"$toDouble": "$total_gwel_net_wt"}
                                ]
                            }
                        },
                        "tl_tolerance_sum": {
                            "$sum": {
                                "$multiply": [
                                    {"$toDouble": "$total_secl_net_wt"},
                                    0.008
                                ]
                            }
                        }
                    }
                },
                {
                    "$project": {
                        "month": "$_id",
                        "total_secl_net_wt_sum": 1,
                        "total_gwel_net_wt_sum": 1,
                        "tl_sum": 1,
                        "tl_tolerance_sum": 1,
                        "_id": 0
                    }
                },
                {
                    "$sort": {"month": 1}
                },
                # { 
                #     "$skip": skip_value 
                # },
                # { 
                #     "$limit": page_len 
                # }
            ]

            # pipeline = [
            #     {'$match': query}, 
            #     {
            #         "$group": {
            #             "_id": {
            #                 "$substr": ["$month", 0, 7]
            #             },
            #             "total_secl_net_wt_sum": {"$sum": {"$toDouble": "$total_secl_net_wt"}},
            #             "total_gwel_net_wt_sum": {"$sum": {"$toDouble": "$total_gwel_net_wt"}},
            #             "tl_sum": {
            #                 "$sum": {
            #                     "$subtract": [
            #                         {"$toDouble": "$total_secl_net_wt"},
            #                         {"$toDouble": "$total_gwel_net_wt"}
            #                     ]
            #                 }
            #             },
            #             "tl_tolerance_sum": {
            #                 "$sum": {
            #                     "$multiply": [
            #                         {"$toDouble": "$total_secl_net_wt"},
            #                         0.008
            #                     ]
            #                 }
            #             }
            #         }
            #     },
            #     {
            #         '$facet': {
            #             'data': [
            #                 {
            #                     '$project': {
            #                         'month': '$_id', 
            #                         'total_secl_net_wt_sum': 1, 
            #                         'total_gwel_net_wt_sum': 1, 
            #                         'tl_sum': 1, 
            #                         'tl_tolerance_sum': 1, 
            #                         '_id': 0
            #                     }
            #                 }, {
            #                     '$sort': {
            #                         'month': 1
            #                     }
            #                 }, 
            #                 # {
            #                 #     '$skip': 0
            #                 # }, {
            #                 #     '$limit': 200
            #                 # }
            #             ], 
            #             'totalCount': [
            #                 {
            #                     '$count': 'total'
            #                 }
            #             ]
            #         }
            #     }
            # ]

            alldata = list(RailData.objects().aggregate(pipeline))
            listData = []
            totalDict = {}
            if alldata:
                result["labels"] = [
                    "month",
                    "secl_qty",
                    "gwel_qty",
                    "transit_loss",
                    "tl_tolerance",
                    "50_percent_of_ol_recovery",                        
                    "ul_recovery"                                 
                ]
                grand_total_secl = 0
                grand_total_gwel = 0
                grand_total_tl = 0
                grand_total_tolerance = 0
                sr_no = 1
                for row, item in enumerate(alldata, start=1):
                    secl_qty = round(item["total_secl_net_wt_sum"], 2)
                    gwel_qty = round(item["total_gwel_net_wt_sum"], 2)
                    tl = item["tl_sum"]
                    tl_tolerance = round(item["tl_tolerance_sum"], 2)  # Round to 2 decimal places
                    dictData = {} 
                    if item.get("month"):
                        dictData["month"] = str(item["month"])
                    else:
                        dictData["month"] = "N/A"
                    dictData["secl_qty"] = secl_qty
                    dictData["gwel_qty"] = gwel_qty
                    dictData["transit_loss"] = tl
                    dictData["tl_tolerance"] = tl_tolerance
                    dictData["50_percent_of_ol_recovery"] = 0
                    dictData["ul_recovery"] = 0

                    listData.append(dictData)

                    grand_total_secl += secl_qty
                    grand_total_gwel += gwel_qty
                    grand_total_tl += tl
                    grand_total_tolerance += tl_tolerance
                    
                grand_total_row = len(alldata) + 1
                totalDict["month"] = ""
                # totalDict["grand_total"] = grand_total_row
                totalDict["secl_qty"] = round(grand_total_secl, 2)
                totalDict["gwel_qty"] = round(grand_total_gwel, 2)
                totalDict["transit_loss"] = round(grand_total_tl, 2)
                totalDict["tl_tolerance"] = round(grand_total_tolerance, 2)
                totalDict["50_percent_of_ol_recovery"] = ""
                totalDict["ul_recovery"] = ""
                # footerlistData.append(totalDict)

            result["total"] = len(listData)
            # apgination added
            start_idx = (page_no - 1) * page_len
            end_idx = start_idx + page_len
            paginated_data = listData[start_idx:end_idx]
            
            if paginated_data:
                result["datasets"] = paginated_data
                result["footerdatasets"] = totalDict
            return result        

        elif type and type == "download":
            del type

            file = str(datetime.datetime.utcnow().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            if month:
                query["month"] = {"$regex": f"{month}", "$options": "i"}

            pipeline = [
                {'$match': query}, 
                {
                    "$group": {
                        "_id": {
                            "$substr": ["$month", 0, 7]
                        },
                        "total_secl_net_wt_sum": {"$sum": {"$toDouble": "$total_secl_net_wt"}},
                        "total_gwel_net_wt_sum": {"$sum": {"$toDouble": "$total_gwel_net_wt"}},
                        "tl_sum": {
                            "$sum": {
                                "$subtract": [
                                    {"$toDouble": "$total_secl_net_wt"},
                                    {"$toDouble": "$total_gwel_net_wt"}
                                ]
                            }
                        },
                        "tl_tolerance_sum": {
                            "$sum": {
                                "$multiply": [
                                    {"$toDouble": "$total_secl_net_wt"},
                                    0.008
                                ]
                            }
                        }
                    }
                },
                {
                    "$project": {
                        "month": "$_id",
                        "total_secl_net_wt_sum": 1,
                        "total_gwel_net_wt_sum": 1,
                        "tl_sum": 1,
                        "tl_tolerance_sum": 1,
                        "_id": 0
                    }
                },
                {
                    "$sort": {"month": 1}
                }
            ]


            alldata = list(raildb.aggregate(pipeline))

            path = None
            logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
            if alldata:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "RCF_Loss_Report_{}.xlsx".format(
                            datetime.datetime.utcnow().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format()
                    cell_format2.set_bold()
                    cell_format2.set_font_size(10)
                    cell_format2.set_align("center")
                    cell_format2.set_align("vcenter")
                    cell_format2.set_text_wrap(True)
                    cell_format2.set_border(1)

                    header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
                    date_format = workbook.add_format({'align': 'center', 'font_size': 12, "bold": True})
                    report_name_format = workbook.add_format({'align': 'center', 'font_size': 15, "bold": True})
                    # report_name_format.set_text_wrap(True)

                    header_format.set_align("vcenter")
                    date_format.set_align("vcenter")
                    report_name_format.set_align("vcenter")
                    header_format.set_border(1)
                    date_format.set_border(1)
                    report_name_format.set_border(1)
                    
                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)
                    cell_format = workbook.add_format()
                    cell_format.set_font_size(10)
                    cell_format.set_align("center")
                    cell_format.set_align("vcenter")
                    cell_format.set_text_wrap(True)
                    cell_format.set_border(1)

                    worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                    
                    # Merge cells for the main header and place it in the center
                    main_header = "GMR Warora Energy Limited"  # Set your main header text here
                    worksheet.merge_range("A1:H1", main_header, header_format)  # Merge cells A1 to H1 for the header
                    
                    # Write the current date on the left side (A2)
                    worksheet.write("A2", f"Date: {datetime.datetime.now().strftime('%d-%m-%Y')}", date_format)
                    worksheet.merge_range("C2:H2", f"RCF Rail Losses", report_name_format)

                    headers = [
                        "Sr.No.",
                        "Month",
                        "SECL Qty",
                        "GWEL Qty",
                        "Transit Loss",
                        "TL Tolerance",
                        "50% of O/L recovery",                         # penalty_ol * 0.5
                        "U/L recovery"                                 # penal_ul * freight_pmt
                    ]
                    for index, header in enumerate(headers):
                        worksheet.write(2, index, header, cell_format2)

                    grand_total_secl = 0
                    grand_total_gwel = 0
                    grand_total_tl = 0
                    grand_total_tolerance = 0
                    sr_no = 1

                    for row, item in enumerate(alldata, start=3):
                        secl_qty = item["total_secl_net_wt_sum"]
                        gwel_qty = item["total_gwel_net_wt_sum"]
                        tl = item["tl_sum"]
                        tl_tolerance = round(item["tl_tolerance_sum"], 2)  # Round to 2 decimal places

                        worksheet.write(row, 0, sr_no, cell_format)
                        worksheet.write(row, 1, str(item["month"]), cell_format)
                        if secl_qty:
                            worksheet.write(row, 2, float(secl_qty), cell_format)
                        else:
                            worksheet.write(row, 2, 0, cell_format)
                        if gwel_qty:
                            worksheet.write(row, 3, float(gwel_qty), cell_format)
                        else:
                            worksheet.write(row, 3, 0, cell_format)
                        if tl:
                            worksheet.write(row, 4, float(tl), cell_format)
                        else:
                            worksheet.write(row, 4, 0, cell_format)
                        if tl_tolerance:
                            worksheet.write(row, 5, float(tl_tolerance), cell_format)
                        else:
                            worksheet.write(row, 5, 0, cell_format)
                        worksheet.write(row, 6, "", cell_format) #50% of O/L recovery
                        worksheet.write(row, 7, "", cell_format) #U/L recovery
                        

                        grand_total_secl += secl_qty
                        grand_total_gwel += gwel_qty
                        grand_total_tl += tl
                        grand_total_tolerance += tl_tolerance

                        sr_no += 1
                    grand_total_row = len(alldata) + 3
                    worksheet.write(grand_total_row, 0, "", cell_format2)
                    worksheet.write(grand_total_row, 1, "Grand Total", cell_format2)
                    worksheet.write(grand_total_row, 2, grand_total_secl, cell_format2)
                    worksheet.write(grand_total_row, 3, grand_total_gwel, cell_format2)
                    worksheet.write(grand_total_row, 4, grand_total_tl, cell_format2)
                    worksheet.write(grand_total_row, 5, round(grand_total_tolerance, 2), cell_format2)
                    worksheet.write(grand_total_row, 6, "", cell_format2)
                    worksheet.write(grand_total_row, 7, "", cell_format2)

                    workbook.close()
                    console_logger.debug("Successfully report generated")
                    console_logger.debug("sent data {}".format(path))

                    return {
                        "Type": "RCF_Loss_download_event",
                        "Datatype": "Report",
                        "File_Path": path,
                    }
                
                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug((exc_type, fname, exc_tb.tb_lineno))
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            else:
                console_logger.debug("No data found")
                return {
                    "Type": "RCF_Loss_download_event",
                    "Datatype": "Report",
                    "File_Path": path,
                }

    except Exception as e:
        console_logger.debug(e)
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug((exc_type, fname, exc_tb.tb_lineno))
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e
    

@router.get("/secl_analysis", tags=["Recovery"])
def get_secl_analysis(response: Response,
                      page_no:Optional[int]=1, page_size:Optional[int]=10,
                      search_text: Optional[str] = None,
                      start_timestamp: Optional[str] = None,
                      end_timestamp: Optional[str] = None,
                      month: Optional[str] = None,
                      type: Optional[str] = "display"):
    try:
        query = {}
        response_data = {
                "labels": [],
                "datasets": [],
                "total" : 0,
                "page_size": 10}

        if type and type == "display":    
            skip = page_size * (page_no - 1)
            limit = page_size

            query["mode"] = {"$eq": "Rail"}

            if search_text:
                query["$or"] = [
                    {"sample_id": {"$regex": f"{search_text}", "$options": "i"}}
                ]
            
            if month:
                start_date = f'{month}-01'
                startd_date = datetime.datetime.strptime(f"{start_date}T00:00", "%Y-%m-%dT%H:%M")
                end_date = (startd_date + relativedelta(months=+1, days=-1))
                query["plant_analysis_date"] = {
                    "$gte": startd_date,
                    "$lte": end_date.replace(hour=23, minute=59, second=59)
                }

            Pipeline = [
                {"$match": query},
                {'$skip': skip}, 
                {'$limit': limit},
                {
                    "$group": {
                        "_id": {
                            "sample_id": "$sample_id",
                            "sample_no": "$sample_no",
                            "mine": "$mine",
                            "plant_certificate_id": "$plant_certificate_id",
                            "plant_sample_date": "$plant_sample_date",
                            "plant_analysis_date": "$plant_analysis_date",
                            "plant_preperation_date": "$plant_preperation_date",
                            "mode": "$mode"
                        },
                        "last_sample_qty": {"$last": "$sample_qty"},
                        "plant_arb_tm": {"$last": "$plant_arb_tm"},
                        "plant_arb_vm": {"$last": "$plant_arb_vm"},
                        "plant_arb_ash": {"$last": "$plant_arb_ash"},
                        "plant_arb_fc": {"$last": "$plant_arb_fc"},
                        "plant_arb_gcv": {"$last": "$plant_arb_gcv"},
                        "plant_adb_im": {"$last": "$plant_adb_im"},
                        "plant_adb_vm": {"$last": "$plant_adb_vm"},
                        "plant_adb_ash": {"$last": "$plant_adb_ash"},
                        "plant_adb_fc": {"$last": "$plant_adb_fc"},
                        "plant_adb_gcv": {"$last": "$plant_adb_gcv"}
                    }
                },
                {
                    "$project": {
                        "_id": 0,
                        "sample_id": "$_id.sample_id",
                        "sample_no": "$_id.sample_no",
                        "mine": "$_id.mine",
                        "plant_certificate_id": "$_id.plant_certificate_id",
                        "plant_sample_date": "$_id.plant_sample_date",
                        "plant_analysis_date": "$_id.plant_analysis_date",
                        "plant_preperation_date": "$_id.plant_preperation_date",
                        "mode": "$_id.mode",
                        "mine_qty": {
                            "$toDouble": {
                                "$replaceAll": {
                                    "input": {
                                        "$toString": "$last_sample_qty"
                                    },
                                    "find": ",",
                                    "replacement": ""
                                }
                            }
                        },
                        "parameters": {
                            "plant_arb_tm": "$plant_arb_tm",
                            "plant_arb_vm": "$plant_arb_vm",
                            "plant_arb_ash": "$plant_arb_ash",
                            "plant_arb_fc": "$plant_arb_fc",
                            "plant_arb_gcv": "$plant_arb_gcv",
                            "plant_adb_im": "$plant_adb_im",
                            "plant_adb_vm": "$plant_adb_vm",
                            "plant_adb_ash": "$plant_adb_ash",
                            "plant_adb_fc": "$plant_adb_fc",
                            "plant_adb_gcv": "$plant_adb_gcv"
                        }
                    }
                },
                {
                    "$addFields": {
                        "data": {
                            "$mergeObjects": [
                                {
                                    "sample_id": "$sample_id",
                                    "sample_no": "$sample_no",
                                    "mine": "$mine",
                                    "plant_certificate_id": "$plant_certificate_id",
                                    "plant_sample_date": "$plant_sample_date",
                                    "plant_analysis_date": "$plant_analysis_date",
                                    "plant_preperation_date": "$plant_preperation_date",
                                    "mode": "$mode",
                                    "mine_qty": "$mine_qty"
                                },
                                "$parameters"
                            ]
                        }
                    }
                },
                {"$replaceRoot": {"newRoot": "$data"}},
                {"$sort": {"plant_analysis_date": -1}}
            ]

            headers = [
                "Rake_No", 
                "Mine_Name",
                "Month",
                "RR_No",
                "RR_Date",
                "SECL_Quantity", 
                "WT_Quantity", 
                "TL", 
                "Placement_Date",
                "Placement_Time",
                "Analysis_Date",         # plant_analysis_date
                "GCV_(ADB)",
                "GCV_(ARB)",
                "TM",
                "IM",
                "VM_(ADB)",
                "ASH_(ADB)",
                "VM_(ARB)",
                "ASH_(ARB)",
                "FC_(ARB)"
            ]

            alldata = list(receiptCoalQualityAnalysisdb.aggregate(Pipeline))
            total_count = receiptCoalQualityAnalysisdb.count_documents(query)
            secl_data = sapraildb.find({})
            raildata = raildb.find({})
            secl_dict = {entry.get("rr_no", ""): entry for entry in secl_data}
            rail_dict = {entry.get("rr_no", ""): entry for entry in raildata}

            for item in alldata:
                do_no = item.get("sample_id")

                transit_loss = 0
                rr_date = None
                rr_qty = 0
                mine_qty = 0
                mine_name = None
                month = None

                if do_no in secl_dict:
                    rr_date = f"{secl_dict.get(do_no, {}).get('rr_date', '')}"
                    rr_qty = float(secl_dict.get(do_no, {}).get("rr_qty", 0) or 0)
                    mine_name = f"{secl_dict.get(do_no, {}).get('mine', '')}"
                    month = f"{secl_dict.get(do_no, {}).get('month', '')}"
                    mine_qty = float(item.get("mine_qty", 0) or 0)

                if rr_qty != 0 or mine_qty != 0:
                    transit_loss = round(rr_qty - mine_qty, 2)

                item["Rake_No"] = f"Rake-{item.pop('sample_no', '')}"
                item["RR_No"] = item.pop("sample_id", '')
                item["WT_Quantity"] = mine_qty
                item["GCV_(ADB)"] = item.pop("plant_adb_gcv", '')
                item["GCV_(ARB)"] = item.pop("plant_arb_gcv", '')
                item["TM"] = item.pop("plant_arb_tm", '')
                item["IM"] = item.pop("plant_adb_im", '')
                item["VM_(ADB)"] = item.pop("plant_adb_vm", '')
                item["ASH_(ADB)"] = item.pop("plant_adb_ash", '')
                item["VM_(ARB)"] = item.pop("plant_arb_vm", '')
                item["ASH_(ARB)"] = item.pop("plant_arb_ash", '')
                item["FC_(ARB)"] = item.pop("plant_arb_fc", '')
                item["Analysis_Date"] = str(item.pop("plant_analysis_date").replace(tzinfo=pytz.utc).astimezone(
                                    pytz.timezone('Asia/Kolkata')).strftime("%Y-%m-%d"))

                del item['mine']
                del item['mine_qty']
                del item["plant_certificate_id"]
                del item['plant_preperation_date']
                del item['plant_sample_date']
                del item['mode']
                del item['plant_adb_fc']

                rail_entry = rail_dict.get(do_no)
                if rail_entry:
                    avery_placement_date = rail_entry.get("avery_placement_date")
                    
                    if avery_placement_date:
                        receive_date, receive_time = avery_placement_date.split('T')
                    else:
                        receive_date, receive_time = None, None
                else:
                    receive_date, receive_time = None, None
                
                item.update({
                    "SECL_Quantity":rr_qty,
                    "TL":transit_loss,
                    "RR_Date":rr_date,
                    "Mine_Name": mine_name,
                    "Month": month,
                    "Placement_Date": receive_date,
                    "Placement_Time": receive_time
                })
                        
                response_data = {
                    "labels": headers,
                    "datasets": alldata,
                    "total": total_count,
                    "page_size": page_size
                    }

            return response_data


        elif type == "download":
            del type

            file = str(datetime.datetime.utcnow().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            query["mode"] = {"$eq": "Rail"}

            if start_timestamp and end_timestamp:
                start_date = datetime.datetime.strptime(start_timestamp, "%Y-%m-%dT%H:%M")
                end_date = datetime.datetime.strptime(end_timestamp, "%Y-%m-%dT%H:%M")

                start_date = start_date.replace(tzinfo=pytz.timezone('Asia/Kolkata')).astimezone(pytz.utc)
                end_date = end_date.replace(tzinfo=pytz.timezone('Asia/Kolkata')).astimezone(pytz.utc)

                query["plant_analysis_date"] = {"$gte": start_date, "$lte": end_date}
        
            if search_text:
                query["$or"] = [
                    {"sample_id": {"$regex": f"{search_text}", "$options": "i"}}
                ]

            pipeline = [
                {"$match": query},
                {
                    "$group": {
                        "_id": {
                            "sample_id": "$sample_id",
                            "sample_no": "$sample_no",
                            "mine": "$mine",
                            "plant_certificate_id": "$plant_certificate_id",
                            "plant_sample_date": "$plant_sample_date",
                            "plant_analysis_date": "$plant_analysis_date",
                            "plant_preperation_date": "$plant_preperation_date",
                            "mode": "$mode"
                        },
                        "last_sample_qty": {"$last": "$sample_qty"},
                        "plant_arb_tm": {"$last": "$plant_arb_tm"},
                        "plant_arb_vm": {"$last": "$plant_arb_vm"},
                        "plant_arb_ash": {"$last": "$plant_arb_ash"},
                        "plant_arb_fc": {"$last": "$plant_arb_fc"},
                        "plant_arb_gcv": {"$last": "$plant_arb_gcv"},
                        "plant_adb_im": {"$last": "$plant_adb_im"},
                        "plant_adb_vm": {"$last": "$plant_adb_vm"},
                        "plant_adb_ash": {"$last": "$plant_adb_ash"},
                        "plant_adb_fc": {"$last": "$plant_adb_fc"},
                        "plant_adb_gcv": {"$last": "$plant_adb_gcv"}
                    }
                },
                {
                    "$project": {
                        "_id": 0,
                        "sample_id": "$_id.sample_id",
                        "sample_no": "$_id.sample_no",
                        "mine": "$_id.mine",
                        "plant_certificate_id": "$_id.plant_certificate_id",
                        "plant_sample_date": "$_id.plant_sample_date",
                        "plant_analysis_date": "$_id.plant_analysis_date",
                        "plant_preperation_date": "$_id.plant_preperation_date",
                        "mode": "$_id.mode",
                        "mine_qty": {
                            "$toDouble": {
                                "$replaceAll": {
                                    "input": {
                                        "$toString": "$last_sample_qty"  # Convert to string first
                                    },
                                    "find": ",",
                                    "replacement": ""
                                }
                            }
                        },
                        "parameters": {
                            "plant_arb_tm": "$plant_arb_tm",
                            "plant_arb_vm": "$plant_arb_vm",
                            "plant_arb_ash": "$plant_arb_ash",
                            "plant_arb_fc": "$plant_arb_fc",
                            "plant_arb_gcv": "$plant_arb_gcv",
                            "plant_adb_im": "$plant_adb_im",
                            "plant_adb_vm": "$plant_adb_vm",
                            "plant_adb_ash": "$plant_adb_ash",
                            "plant_adb_fc": "$plant_adb_fc",
                            "plant_adb_gcv": "$plant_adb_gcv"
                        }
                    }
                },
                {
                    "$addFields": {
                        "data": {
                            "$mergeObjects": [
                                {
                                    "sample_id": "$sample_id",
                                    "sample_no": "$sample_no",
                                    "mine": "$mine",
                                    "plant_certificate_id": "$plant_certificate_id",
                                    "plant_sample_date": "$plant_sample_date",
                                    "plant_analysis_date": "$plant_analysis_date",
                                    "plant_preperation_date": "$plant_preperation_date",
                                    "mode": "$mode",
                                    "mine_qty": "$mine_qty"
                                },
                                "$parameters"
                            ]
                        }
                    }
                },
                {"$replaceRoot": {"newRoot": "$data"}},
                {"$sort": {"sample_id": 1, "plant_sample_date": 1}}
            ]

            alldata = list(receiptCoalQualityAnalysisdb.aggregate(pipeline))
            secl_data = sapraildb.find({})
            raildata = raildb.find({})
            secl_dict = {entry.get("rr_no", ""): entry for entry in secl_data}
            rail_dict = {entry.get("rr_no", ""): entry for entry in raildata}

            grouped_data = {}
            for item in alldata:
                rr_no = item.get("sample_id")
                secl_entry = secl_dict.get(rr_no)

                if secl_entry:
                    if "SECL Linkage" not in grouped_data:
                        grouped_data["SECL Linkage"] = {}
                    if rr_no not in grouped_data["SECL Linkage"]:
                        grouped_data["SECL Linkage"][rr_no] = []
                    grouped_data["SECL Linkage"][rr_no].append(item)
                # else:
                #     console_logger.debug(f"No matching entry in secl_dict for rr_no: {rr_no}")

            path = None
            logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
            if os.path.exists(target_directory):
                path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "SECL_Analysis_Report_{}.xlsx".format(
                            datetime.datetime.utcnow().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                filename = os.path.join(os.getcwd(), path)

            workbook = xlsxwriter.Workbook(filename)
            workbook.use_zip64()
            
            cell_format = workbook.add_format({'font_size': 10, 'align': 'center', 'valign': 'vcenter',
                                               'border': 1, 'border_color': 'black'})

            normal_row_format = workbook.add_format({'bold': True, 'font_size': 10, 'align': 'center', 
                                                     'valign': 'vcenter', 'border': 1, 'border_color': 'black'})

            cell_format.set_align("vcenter")
            normal_row_format.set_align("vcenter")

            cell_format.set_border(1)
            normal_row_format.set_border(1)

            header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
            date_format = workbook.add_format({'align': 'center', 'font_size': 12, "bold": True})
            report_name_format = workbook.add_format({'align': 'center', 'font_size': 15, "bold": True})

            cell_format.set_text_wrap(True)
            header_format.set_text_wrap(True)
            normal_row_format.set_text_wrap(True)

            header_format.set_align("vcenter")
            date_format.set_align("vcenter")
            report_name_format.set_align("vcenter")

            cell_format.set_border(1)
            header_format.set_border(1)
            normal_row_format.set_border(1)

            for consumer_type, do_grouped_data in grouped_data.items():
                try:
                    # Create a new worksheet for the current consumer type
                    worksheet = workbook.add_worksheet(consumer_type)
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)

                    worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                    
                    main_header = "GMR Warora Energy Limited"
                    worksheet.merge_range("A1:T1", main_header, header_format)
                    
                    # Write the current date on the left side (A2)
                    worksheet.write("A2", f"Date: {end_date.strftime('%d-%m-%Y')}", date_format)
                    worksheet.merge_range("C2:T2", f"Coal Analysis", report_name_format)

                    # Write headers only once for the consumer type
                    headers = [
                        "Rake No",          #0             
                        "Mine Name",        #1       
                        "Month",            #2  
                        "RR No",            #3  
                        "RR Date",          #4      
                        "SECL Quantity",    #5             
                        "WT Quantity",      #6           
                        "TL",               #7  
                        "Placement Date",   #8               
                        "Placement Time",   #9               
                        "Analysis Date",    #10                       # plant_analysis_date
                        "GCV (ADB)",        #11          
                        "GCV (ARB)",        #12          
                        "TM",               #13   
                        "IM",               #14   
                        "VM (ADB)",         #15         
                        "ASH (ADB)",        #16          
                        "VM (ARB)",         #17         
                        "ASH (ARB)",        #18          
                        "FC (ARB)"          #19  
                    ]

                    for index, header in enumerate(headers):
                        worksheet.write(2, index, header, normal_row_format)

                    current_row = 3  # Start writing data from the second row after headers

                    for do_no, entries in do_grouped_data.items():
                        sorted_entries = []
                        for item in entries:
                            lot_no = item.get("sample_no", "")
                            sorted_entries.append((lot_no, item))

                        # Sort by lot_no
                        sorted_entries.sort(key=lambda x: (int(x[0]) if x[0].isdigit() else float('inf'), x[0]))
                        mine_name = f"{secl_dict.get(do_no, {}).get('mine', 'Unknown')}"
                        month = f"{secl_dict.get(do_no, {}).get('month', 'Unknown')}"

                        rail_entry = rail_dict.get(do_no)
                        if rail_entry:
                            avery_placement_date = rail_entry.get("avery_placement_date")
                            
                            if avery_placement_date:
                                receive_date, receive_time = avery_placement_date.split('T')
                            else:
                                receive_date, receive_time = None, None
                        else:
                            console_logger.debug(f"No matching entry in rail_dict for rr_no: {do_no}")
                            receive_date, receive_time = None, None 

                        # Write data to the worksheet
                        for lot_no, item in sorted_entries:
                            worksheet.write(current_row, 0, f"Rake-{lot_no}", cell_format)
                            worksheet.write(current_row, 1, mine_name, cell_format)
                            try:
                                worksheet.write(current_row, 2, datetime.datetime.strptime(month, "%Y-%m").strftime("%b %Y"), cell_format)
                            except ValueError as e:
                                worksheet.write(current_row, 2, "", cell_format)

                            worksheet.write(current_row, 3, do_no, cell_format)
                            worksheet.write(current_row, 4, secl_dict.get(do_no, {}).get("rr_date", 0), cell_format)
                            worksheet.write(current_row, 5, secl_dict.get(do_no, {}).get("rr_qty", 0), cell_format)
                            worksheet.write(current_row, 6, item.get("mine_qty", 0), cell_format)
                            rr_qty = float(secl_dict.get(do_no, {}).get("rr_qty", 0) or 0)
                            mine_qty = float(item.get("mine_qty", 0) or 0)

                            worksheet.write(current_row, 7, round(rr_qty - mine_qty, 2), cell_format)
                            worksheet.write(current_row, 8, receive_date, cell_format)
                            worksheet.write(current_row, 9, receive_time, cell_format)
                            worksheet.write(current_row, 10, str(item.get("plant_analysis_date").replace(tzinfo=pytz.utc).astimezone(
                                    pytz.timezone('Asia/Kolkata')).strftime("%Y-%m-%d")
                                    if item.get("plant_analysis_date") is not None else None), cell_format)
                            worksheet.write(current_row, 11, item.get("plant_adb_gcv", ""), cell_format)
                            worksheet.write(current_row, 12, item.get("plant_arb_gcv", ""), cell_format)
                            worksheet.write(current_row, 13, item.get("plant_arb_tm", ""), cell_format)
                            worksheet.write(current_row, 14, item.get("plant_adb_im", ""), cell_format)
                            worksheet.write(current_row, 15, item.get("plant_adb_vm", ""), cell_format)
                            worksheet.write(current_row, 16, item.get("plant_adb_ash", ""), cell_format)
                            worksheet.write(current_row, 17, item.get("plant_arb_vm", ""), cell_format)
                            worksheet.write(current_row, 18, item.get("plant_arb_ash", ""), cell_format)
                            worksheet.write(current_row, 19, item.get("plant_arb_fc", ""), cell_format)
                        
                        current_row += 1

                except Exception as e:
                    console_logger.error(f"Error while creating the Excel sheet for {consumer_type}: {e}")
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(f"{exc_type} in {fname} on line {exc_tb.tb_lineno}")
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))

            workbook.close()
            if path:
                return {
                    "Type": "SECL_Analysis_download_event",
                    "Datatype": "Report",
                    "File_Path": path,
                }
            else:
                console_logger.debug("No files were generated.")
                return {
                    "Type": "SECL_Analysis_download_event",
                    "Datatype": "Report",
                    "File_Path": None,
                }

    except Exception as e:
        console_logger.debug(f"----- SECL Analysis Error ----- {e}")
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(f"{exc_type} in {fname} on line {exc_tb.tb_lineno}")
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return {"detail": str(e)}


@router.get("/consumer_types", tags=["Recovery"])
def get_consumer_types(response: Response,):
    try:
        consumer_types = sapdb.distinct("consumer_type", {"consumer_type": {"$ne": None}})
        return consumer_types
    except Exception as e:
        console_logger.debug(f"----- Consumer Type Get Error ----- {e}")
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(f"{exc_type} in {fname} on line {exc_tb.tb_lineno}")
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return {"detail": str(e)}


@router.get("/wcl_analysis", tags=["Recovery"])
def get_wcl_analysis(response: Response,
                     page_no:Optional[int]=1, 
                     page_size:Optional[int]=10,
                     search_text: Optional[str] = None,
                     search_mine: Optional[str] = "All",
                     start_timestamp: Optional[str] = None,
                     end_timestamp: Optional[str] = None,
                     month: Optional[str] = None,
                     consumer: Optional[str] = None,
                     type: Optional[str] = "display",
                     display_type : Optional[str] ="do_wise",
                     sort_type : Optional[str] ="sample_id"):
    try:
        query = {}

        if type == "display":
            header = []
            query["mode"] = {"$eq": "Road"}

            if search_text:
                query["$or"] = [
                    {"sample_id": {"$regex": f"{search_text}", "$options": "i"}},
                ]

            if search_mine and search_mine != "All":
                query["$or"] = [
                    {"mine": {"$regex": f"{search_mine}", "$options": "i"}}
                ]

            if month:
                start_date = f'{month}-01'
                startd_date = datetime.datetime.strptime(f"{start_date}T00:00", "%Y-%m-%dT%H:%M")
                end_date = (startd_date + relativedelta(months=+1, days=-1))
                query["plant_analysis_date"] = {
                    "$gte": startd_date,
                    "$lte": end_date.replace(hour=23, minute=59, second=59)
                }

            pipeline = [
                {"$match": query},
                # {'$skip': skip},
                # {'$limit': limit},
                {
                    "$group": {
                        "_id": {
                            "sample_id": "$sample_id",
                            "sample_no": "$sample_no",
                            "mine": "$mine",
                            "plant_certificate_id": "$plant_certificate_id",
                            "plant_sample_date": "$plant_sample_date",
                            "plant_analysis_date": "$plant_analysis_date",
                            "plant_preperation_date": "$plant_preperation_date",
                            "mode": "$mode"
                        },
                        "last_sample_qty": {"$last": "$sample_qty"},
                        "plant_arb_tm": {"$last": "$plant_arb_tm"},
                        "plant_arb_vm": {"$last": "$plant_arb_vm"},
                        "plant_arb_ash": {"$last": "$plant_arb_ash"},
                        "plant_arb_fc": {"$last": "$plant_arb_fc"},
                        "plant_arb_gcv": {"$last": "$plant_arb_gcv"},
                        "plant_adb_im": {"$last": "$plant_adb_im"},
                        "plant_adb_vm": {"$last": "$plant_adb_vm"},
                        "plant_adb_ash": {"$last": "$plant_adb_ash"},
                        "plant_adb_fc": {"$last": "$plant_adb_fc"},
                        "plant_adb_gcv": {"$last": "$plant_adb_gcv"}
                    }
                },
                {
                    "$project": {
                        "_id": 0,
                        "sample_id": "$_id.sample_id",
                        "sample_no": "$_id.sample_no",
                        "mine": "$_id.mine",
                        "plant_certificate_id": "$_id.plant_certificate_id",
                        "plant_sample_date": "$_id.plant_sample_date",
                        "plant_analysis_date": "$_id.plant_analysis_date",
                        "plant_preperation_date": "$_id.plant_preperation_date",
                        "mode": "$_id.mode",
                        "mine_qty": {
                            "$toDouble": {
                                "$replaceAll": {
                                    "input": {
                                        "$toString": "$last_sample_qty"
                                    },
                                    "find": ",",
                                    "replacement": ""
                                }
                            }
                        },
                        "parameters": {
                            "plant_arb_tm": "$plant_arb_tm",
                            "plant_arb_vm": "$plant_arb_vm",
                            "plant_arb_ash": "$plant_arb_ash",
                            "plant_arb_fc": "$plant_arb_fc",
                            "plant_arb_gcv": "$plant_arb_gcv",
                            "plant_adb_im": "$plant_adb_im",
                            "plant_adb_vm": "$plant_adb_vm",
                            "plant_adb_ash": "$plant_adb_ash",
                            "plant_adb_fc": "$plant_adb_fc",
                            "plant_adb_gcv": "$plant_adb_gcv"
                        }
                    }
                },
                {
                    "$addFields": {
                        "data": {
                            "$mergeObjects": [
                                {
                                    "sample_id": "$sample_id",
                                    "sample_no": "$sample_no",
                                    "mine": "$mine",
                                    "plant_certificate_id": "$plant_certificate_id",
                                    "plant_sample_date": "$plant_sample_date",
                                    "plant_analysis_date": "$plant_analysis_date",
                                    "plant_preperation_date": "$plant_preperation_date",
                                    "mode": "$mode",
                                    "mine_qty": "$mine_qty"
                                },
                                "$parameters"
                            ]
                        }
                    }
                },
                {"$replaceRoot": {"newRoot": "$data"}},
                {"$sort": {sort_type: 1}}
            ]
            alldata = list(receiptCoalQualityAnalysisdb.aggregate(pipeline))
            datasets = []
            grouped_data = {}

            sapdb_data = sapdb.find({})
            sapdb_dict = {entry.get("do_no", ""): entry for entry in sapdb_data}
            
            for item in alldata:
                do_no = item.get("sample_id")
                mine = item.get("mine")

                sapdb_entry = sapdb_dict.get(do_no)
                if sapdb_entry is not None:
                    consumer_type = sapdb_entry.get("consumer_type")
                    do_qty = sapdb_dict.get(do_no, {}).get("do_qty", 0)
                    if display_type == "do_wise":
                        header = [
                            "LOT_No",
                            "Sample_Quantity",  # mine qty is sample qty
                            "Received_Date",
                            "Total_Moisture",
                            "Inherent_Moisture_(ADB)",
                            "Ash_(ADB)",
                            "Volatile_Matter_(ADB)",
                            "Gross_Calorific_Value_(ADB)",
                            "Ash_(ARB)",
                            "Volatile_Matter_(ARB)",
                            "Fixed_Carbon_(ARB)",
                            "Gross_Calorific_Value_(ARB)"]
                        if consumer == consumer_type:
                            if do_no not in grouped_data:
                                grouped_data[do_no] = {
                                    "do_no": do_no,
                                    "consumer_type" : consumer_type if consumer_type not in ["", None] else None,
                                    "mine": mine,
                                    "mine_quantity": do_qty,
                                    "body_arr": [],
                                    "footer_arr": {
                                        "Sample_Quantity": 0
                                    }
                                }

                            grouped_data[do_no]["body_arr"].append({
                                "LOT_No": item.get("sample_no", ""),
                                "Sample_Quantity": item.get("mine_qty", 0),
                                "Received_Date":item.get("plant_analysis_date", "").replace(tzinfo=pytz.utc).astimezone(
                                                pytz.timezone('Asia/Kolkata')).strftime("%Y-%m-%d"),
                                "Total_Moisture": item.get("plant_arb_tm", ""),
                                "Inherent_Moisture_(ADB)": item.get("plant_adb_im", ""),
                                "Ash_(ADB)": item.get("plant_adb_ash", ""),
                                "Volatile_Matter_(ADB)": item.get("plant_adb_vm", ""),
                                "Gross_Calorific_Value_(ADB)": item.get("plant_adb_gcv", ""),
                                "Ash_(ARB)": item.get("plant_arb_ash", ""),
                                "Volatile_Matter_(ARB)": item.get("plant_arb_vm", ""),
                                "Fixed_Carbon_(ARB)": item.get("plant_arb_fc", ""),
                                "Gross_Calorific_Value_(ARB)": item.get("plant_arb_gcv", "")
                            })

                            total_mine_qty=0
                            mine_qty = item.get("mine_qty", 0)
                            
                            total_mine_qty += round(float(mine_qty or 0), 1)
                            grouped_data[do_no]["footer_arr"]["Sample_Quantity"] = round(total_mine_qty,2)

                            C = np.array([float(item.get("mine_qty", 0) or 0)])
                            C42 = total_mine_qty
                            if C42 == 0:
                                arb_tm_avg = 0
                                adb_im_avg = 0
                                adb_ash_avg = 0
                                adb_vm_avg = 0
                                adb_gcv_avg = 0
                                arb_ash_avg = 0
                                arb_vm_avg = 0
                                arb_fc_avg = 0
                                arb_gcv_avg = 0
                            else:
                                arb_tm_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_arb_tm", 0) or 0)]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                                adb_im_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_adb_im", 0) or 0)]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                                adb_ash_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_adb_ash", 0) or 0)]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                                adb_vm_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_adb_vm", 0) or 0)]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                                adb_gcv_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_adb_gcv", 0) or 0)]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                                arb_ash_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_arb_ash", 0) or 0)]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                                arb_vm_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_arb_vm", 0) or 0)]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                                arb_fc_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_arb_fc", 0) or 0)]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                                arb_gcv_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_arb_gcv", 0) or 0)]) * C) / C42, nan=0, posinf=0, neginf=0), 2)

                            grouped_data[do_no]["footer_arr"]["Total_Moisture"] = arb_tm_avg
                            grouped_data[do_no]["footer_arr"]["Inherent_Moisture_(ADB)"] = adb_im_avg
                            grouped_data[do_no]["footer_arr"]["Ash_(ADB)"] = adb_ash_avg
                            grouped_data[do_no]["footer_arr"]["Volatile_Matter_(ADB)"] = adb_vm_avg
                            grouped_data[do_no]["footer_arr"]["Gross_Calorific_Value_(ADB)"] = adb_gcv_avg
                            grouped_data[do_no]["footer_arr"]["Ash_(ARB)"] = arb_ash_avg
                            grouped_data[do_no]["footer_arr"]["Volatile_Matter_(ARB)"] = arb_vm_avg
                            grouped_data[do_no]["footer_arr"]["Fixed_Carbon_(ARB)"] = arb_fc_avg
                            grouped_data[do_no]["footer_arr"]["Gross_Calorific_Value_(ARB)"] = arb_gcv_avg
                    else:
                        header = [
                            "do_no",
                            "mine",
                            "consumer_type",
                            "Sample_Quantity",  # mine qty is sample qty
                            "Mine_Quantity",    # do_qty from saprecords
                            "Total_Moisture",
                            "Inherent_Moisture_(ADB)",
                            "Ash_(ADB)",
                            "Volatile_Matter_(ADB)",
                            "Gross_Calorific_Value_(ADB)",
                            "Ash_(ARB)",
                            "Volatile_Matter_(ARB)",
                            "Fixed_Carbon_(ARB)",
                            "Gross_Calorific_Value_(ARB)"]
                        if consumer == consumer_type:
                            if do_no not in grouped_data:
                                grouped_data[do_no] = {
                                    "do_no": do_no,
                                    "mine": mine,
                                    "consumer_type" : consumer_type if consumer_type not in ["", None] else None,
                                    "Sample_Quantity": 0,
                                    "Mine_Quantity" : do_qty,
                                    "Total_Moisture": 0,
                                    "Inherent_Moisture_(ADB)": 0,
                                    "Ash_(ADB)": 0,
                                    "Volatile_Matter_(ADB)": 0,
                                    "Gross_Calorific_Value_(ADB)": 0,
                                    "Ash_(ARB)": 0,
                                    "Volatile_Matter_(ARB)": 0,
                                    "Fixed_Carbon_(ARB)": 0,
                                    "Gross_Calorific_Value_(ARB)": 0
                                    }

                                total_mine_qty=0
                                mine_qty = item.get("mine_qty", 0)
                                
                                total_mine_qty += round(float(mine_qty or 0), 1)
                                grouped_data[do_no]["Sample_Quantity"] = round(total_mine_qty,2)

                                C = np.array([float(item.get("mine_qty", 0) or 0)])
                                C42 = total_mine_qty
                                if C42 == 0:
                                    arb_tm_avg = 0
                                    adb_im_avg = 0
                                    adb_ash_avg = 0
                                    adb_vm_avg = 0
                                    adb_gcv_avg = 0
                                    arb_ash_avg = 0
                                    arb_vm_avg = 0
                                    arb_fc_avg = 0
                                    arb_gcv_avg = 0
                                else:
                                    arb_tm_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_arb_tm", 0) or 0)]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                                    adb_im_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_adb_im", 0) or 0)]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                                    adb_ash_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_adb_ash", 0) or 0)]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                                    adb_vm_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_adb_vm", 0) or 0)]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                                    adb_gcv_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_adb_gcv", 0) or 0)]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                                    arb_ash_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_arb_ash", 0) or 0)]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                                    arb_vm_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_arb_vm", 0) or 0)]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                                    arb_fc_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_arb_fc", 0) or 0)]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                                    arb_gcv_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_arb_gcv", 0) or 0)]) * C) / C42, nan=0, posinf=0, neginf=0), 2)

                                grouped_data[do_no]["Total_Moisture"] = arb_tm_avg
                                grouped_data[do_no]["Inherent_Moisture_(ADB)"] = adb_im_avg
                                grouped_data[do_no]["Ash_(ADB)"] = adb_ash_avg
                                grouped_data[do_no]["Volatile_Matter_(ADB)"] = adb_vm_avg
                                grouped_data[do_no]["Gross_Calorific_Value_(ADB)"] = adb_gcv_avg
                                grouped_data[do_no]["Ash_(ARB)"] = arb_ash_avg
                                grouped_data[do_no]["Volatile_Matter_(ARB)"] = arb_vm_avg
                                grouped_data[do_no]["Fixed_Carbon_(ARB)"] = arb_fc_avg
                                grouped_data[do_no]["Gross_Calorific_Value_(ARB)"] = arb_gcv_avg

                datasets = list(grouped_data.values())

            return {
                "headers":header,
                "datasets": datasets
            }

        elif type == "download":
            del type

            file = str(datetime.datetime.utcnow().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            query["mode"] = {"$eq": "Road"}

            if start_timestamp and end_timestamp:
                start_date = datetime.datetime.strptime(start_timestamp, "%Y-%m-%dT%H:%M")
                end_date = datetime.datetime.strptime(end_timestamp, "%Y-%m-%dT%H:%M")

                start_date = start_date.replace(tzinfo=pytz.timezone('Asia/Kolkata')).astimezone(pytz.utc)
                end_date = end_date.replace(tzinfo=pytz.timezone('Asia/Kolkata')).astimezone(pytz.utc)

                query["plant_analysis_date"] = {"$gte": start_date, "$lte": end_date}
        
            if search_text:
                query["$or"] = [
                    {"sample_id": {"$regex": f"{search_text}", "$options": "i"}}
                ]

            pipeline = [
                {"$match": query},
                {
                    "$group": {
                        "_id": {
                            "sample_id": "$sample_id",
                            "sample_no": "$sample_no",
                            "mine": "$mine",
                            "plant_certificate_id": "$plant_certificate_id",
                            "plant_sample_date": "$plant_sample_date",
                            "plant_analysis_date": "$plant_analysis_date",
                            "plant_preperation_date": "$plant_preperation_date",
                            "mode": "$mode"
                        },
                        "last_sample_qty": {"$last": "$sample_qty"},
                        "plant_arb_tm": {"$last": "$plant_arb_tm"},
                        "plant_arb_vm": {"$last": "$plant_arb_vm"},
                        "plant_arb_ash": {"$last": "$plant_arb_ash"},
                        "plant_arb_fc": {"$last": "$plant_arb_fc"},
                        "plant_arb_gcv": {"$last": "$plant_arb_gcv"},
                        "plant_adb_im": {"$last": "$plant_adb_im"},
                        "plant_adb_vm": {"$last": "$plant_adb_vm"},
                        "plant_adb_ash": {"$last": "$plant_adb_ash"},
                        "plant_adb_fc": {"$last": "$plant_adb_fc"},
                        "plant_adb_gcv": {"$last": "$plant_adb_gcv"}
                    }
                },
                {
                    "$project": {
                        "_id": 0,
                        "sample_id": "$_id.sample_id",
                        "sample_no": "$_id.sample_no",
                        "mine": "$_id.mine",
                        "plant_certificate_id": "$_id.plant_certificate_id",
                        "plant_sample_date": "$_id.plant_sample_date",
                        "plant_analysis_date": "$_id.plant_analysis_date",
                        "plant_preperation_date": "$_id.plant_preperation_date",
                        "mode": "$_id.mode",
                        "mine_qty": {
                            "$toDouble": {
                                "$replaceAll": {
                                    "input": {
                                        "$toString": "$last_sample_qty"  # Convert to string first
                                    },
                                    "find": ",",
                                    "replacement": ""
                                }
                            }
                        },
                        "parameters": {
                            "plant_arb_tm": "$plant_arb_tm",
                            "plant_arb_vm": "$plant_arb_vm",
                            "plant_arb_ash": "$plant_arb_ash",
                            "plant_arb_fc": "$plant_arb_fc",
                            "plant_arb_gcv": "$plant_arb_gcv",
                            "plant_adb_im": "$plant_adb_im",
                            "plant_adb_vm": "$plant_adb_vm",
                            "plant_adb_ash": "$plant_adb_ash",
                            "plant_adb_fc": "$plant_adb_fc",
                            "plant_adb_gcv": "$plant_adb_gcv"
                        }
                    }
                },
                {
                    "$addFields": {
                        "data": {
                            "$mergeObjects": [
                                {
                                    "sample_id": "$sample_id",
                                    "sample_no": "$sample_no",
                                    "mine": "$mine",
                                    "plant_certificate_id": "$plant_certificate_id",
                                    "plant_sample_date": "$plant_sample_date",
                                    "plant_analysis_date": "$plant_analysis_date",
                                    "plant_preperation_date": "$plant_preperation_date",
                                    "mode": "$mode",
                                    "mine_qty": "$mine_qty"
                                },
                                "$parameters"
                            ]
                        }
                    }
                },
                {"$replaceRoot": {"newRoot": "$data"}},
                {"$sort": {"sample_id": 1, "plant_sample_date": 1}}
            ]

            alldata = list(receiptCoalQualityAnalysisdb.aggregate(pipeline))
            sapdb_data = sapdb.find({})
            sapdb_dict = {entry.get("do_no", ""): entry for entry in sapdb_data}

            grouped_data = {}
            for item in alldata:
                rrNo = item.get("sample_id")
                sapdb_entry = sapdb_dict.get(rrNo)

                if sapdb_entry:
                    consumer_type = sapdb_entry.get("consumer_type", "Unknown")
                    if consumer_type not in grouped_data:
                        grouped_data[consumer_type] = {}
                    if rrNo not in grouped_data[consumer_type]:
                        grouped_data[consumer_type][rrNo] = []
                    grouped_data[consumer_type][rrNo].append(item)
                else:
                    console_logger.debug(f"No matching entry in sapdb_dict for rrNo: {rrNo}")
            path = None
            logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
            if os.path.exists(target_directory):
                path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "WCL_Analysis_Report_{}.xlsx".format(
                            datetime.datetime.utcnow().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                filename = os.path.join(os.getcwd(), path)

            workbook = xlsxwriter.Workbook(filename)
            workbook.use_zip64()

            total_row_format = workbook.add_format({'bold': True, 'font_size': 10, 'align': 'center',
                                                    'valign': 'vcenter', 'border': 1, 'border_color': 'black'}) 
            
            cell_format = workbook.add_format({'font_size': 10, 'align': 'center', 'valign': 'vcenter',
                                               'border': 1, 'border_color': 'black'})

            normal_row_format = workbook.add_format({'bold': True, 'font_size': 10, 'align': 'center', 
                                                     'valign': 'vcenter', 'border': 1, 'border_color': 'black'})

            normal_row_format.set_text_wrap(True)
            cell_format.set_text_wrap(True)
            total_row_format.set_text_wrap(True)
            
            normal_row_format.set_border(1)
            cell_format.set_border(1)
            total_row_format.set_border(1)

            for consumer_type, do_grouped_data in grouped_data.items():
                try:
                    # Create a new worksheet for the current consumer type
                    worksheet = workbook.add_worksheet(consumer_type)
                    # worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(60)

                    header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
                    date_format = workbook.add_format({'align': 'center', 'font_size': 12, "bold": True})
                    report_name_format = workbook.add_format({'align': 'center', 'font_size': 15, "bold": True})

                    header_format.set_align("vcenter")
                    date_format.set_align("vcenter")
                    report_name_format.set_align("vcenter")

                    header_format.set_text_wrap(True)
                    date_format.set_text_wrap(True)
                    report_name_format.set_text_wrap(True)
                    
                    header_format.set_border(1)
                    date_format.set_border(1)
                    report_name_format.set_border(1)


                    worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                    
                    # Merge cells for the main header and place it in the center
                    main_header = "GMR Warora Energy Limited"  # Set your main header text here
                    worksheet.merge_range("A1:Q1", main_header, header_format)  # Merge cells A1 to H1 for the header
                    
                    # Write the current date on the left side (A2)
                    worksheet.write("A2", f"Date: {end_date.strftime('%d-%m-%Y')}", date_format)
                    worksheet.merge_range("C2:Q2", f"Coal Analysis", report_name_format)

                    # Write headers only once for the consumer type
                    headers = [
                        "LOT No", 
                        "Mine Name", 
                        "Quota", 
                        "DO/RR Quantity", 
                        "Mine Quantity", 
                        "Received Quantity", 
                        "Received Date",
                        "Analysis Date",
                        "Total Moisture", 
                        "Inherent Moisture (ADB)", 
                        "Ash (ADB)", 
                        "Volatile Matter (ADB)",
                        "Gross Calorific Value (ADB)", 
                        "Ash (ARB)", 
                        "Volatile Matter (ARB)", 
                        "Fixed Carbon (ARB)",
                        "Gross Calorific Value (ARB)"
                    ]

                    # for index, header in enumerate(headers):
                        # worksheet.write(0, index, header, normal_row_format)

                    current_row = 2  # Start writing data from the second row after headers

                    for do_no, entries in do_grouped_data.items():
                        # Write the headers for each DO No
                        for index, header in enumerate(headers):
                            worksheet.write(current_row, index, header, normal_row_format)

                        current_row += 1  # Move to the next row after headers for this DO No

                        # Extract and sort entries by lot_no
                        sorted_entries = []
                        for item in entries:
                            lot_no = item.get("sample_no", "")
                            # console_logger.debug(lot_no)
                            # lot_no_str = item.get("sample_no", "")
                            # if re.match(r'^\d+$', lot_no_str):
                            #     lot_no = lot_no_str
                            # else:
                            #     lot_no_match = re.search(r'LOT-(\d+)', lot_no_str)
                            #     if lot_no_match:
                            #         lot_no = lot_no_match.group(1)
                            #         lot_no = str(int(lot_no))
                            #     else:
                            #         lot_no = "Unknown" 

                            sorted_entries.append((lot_no, item))

                        # Sort by lot_no
                        sorted_entries.sort(key=lambda x: (int(x[0]) if x[0].isdigit() else float('inf'), x[0]))
                        mine_name = f"{sapdb_dict.get(do_no, {}).get('mine_name', 'Unknown')}-{do_no}"
                        month = f"{sapdb_dict.get(do_no, {}).get('slno', 'Unknown')}"
                        # Write data to the worksheet
                        for lot_no, item in sorted_entries:
                            worksheet.write(current_row, 0, lot_no, cell_format)
                            worksheet.write(current_row, 1, mine_name, cell_format)
                            worksheet.write(current_row, 2, datetime.datetime.strptime(month, "%Y%m").strftime("%b %Y"), cell_format)
                            worksheet.write(current_row, 3, sapdb_dict.get(do_no, {}).get("do_qty", 0), cell_format)
                            worksheet.write(current_row, 4, item.get("mine_qty", 0), cell_format)
                            worksheet.write(current_row, 5, item.get("mine_qty", 0), cell_format)
                            
                            receive_date = item.get("plant_sample_date")
                            # if receive_date:
                            #     if isinstance(receive_date, str):
                            #         receive_date = datetime.datetime.fromisoformat(receive_date)
                            #     worksheet.write(current_row, 5, receive_date.strftime("%Y-%m-%d"), cell_format)
                            worksheet.write(current_row, 6, receive_date, cell_format)
                            worksheet.write(current_row, 7, str(item.get("plant_analysis_date").replace(tzinfo=pytz.utc).astimezone(
                                    pytz.timezone('Asia/Kolkata')).strftime("%Y-%m-%d")
                                    if item.get("plant_analysis_date") is not None else None), cell_format)
                            worksheet.write(current_row, 8, item.get("plant_arb_tm", ""), cell_format)
                            worksheet.write(current_row, 9, item.get("plant_adb_im", ""), cell_format)
                            worksheet.write(current_row, 10, item.get("plant_adb_ash", ""), cell_format)
                            worksheet.write(current_row, 11, item.get("plant_adb_vm", ""), cell_format)
                            worksheet.write(current_row, 12, item.get("plant_adb_gcv", ""), cell_format)
                            worksheet.write(current_row, 13, item.get("plant_arb_ash", ""), cell_format)
                            worksheet.write(current_row, 14, item.get("plant_arb_vm", ""), cell_format)
                            worksheet.write(current_row, 15, item.get("plant_arb_fc", ""), cell_format)
                            worksheet.write(current_row, 16, item.get("plant_arb_gcv", ""), cell_format)

                            current_row += 1

                        total_row = current_row
                        worksheet.write(total_row, 0, "Total", total_row_format)
                        worksheet.write(total_row, 1, "", total_row_format)
                        worksheet.write(total_row, 2, "", total_row_format)
                        # worksheet.write(total_row, 2, sum(float(sapdb_dict.get(do_no, {}).get("do_qty", 0)) for _, item in sorted_entries), total_row_format)
                        worksheet.write(current_row, 3, sapdb_dict.get(do_no, {}).get("do_qty", 0), total_row_format)
                        # total_mine_qty = sum(float(item.get("mine_qty", 0)) for _, item in sorted_entries)
                        total_mine_qty = sum(float(item.get("mine_qty", 0) or 0) for _, item in sorted_entries)

                        worksheet.write(total_row, 4, total_mine_qty, total_row_format)
                        worksheet.write(total_row, 5, total_mine_qty, total_row_format)
                        worksheet.write(total_row, 6, "", total_row_format)
                        worksheet.write(total_row, 7, "", total_row_format)
                        # I = np.array([float(item.get("Total_Moisture", 0)) for _, item in sorted_entries])
                        # C = np.array([float(item.get("mine_qty", 0)) for _, item in sorted_entries])
                        C = np.array([float(item.get("mine_qty", 0) or 0) for _, item in sorted_entries])
                        C42 = total_mine_qty
                        if C42 == 0:
                            arb_tm_avg = 0
                            adb_im_avg = 0
                            adb_ash_avg = 0
                            adb_vm_avg = 0
                            adb_gcv_avg = 0
                            arb_ash_avg = 0
                            arb_vm_avg = 0
                            arb_fc_avg = 0
                            arb_gcv_avg = 0
                        else:
                            arb_tm_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_arb_tm", 0) or 0) for _, item in sorted_entries]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                            adb_im_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_adb_im", 0) or 0) for _, item in sorted_entries]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                            adb_ash_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_adb_ash", 0) or 0) for _, item in sorted_entries]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                            adb_vm_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_adb_vm", 0) or 0) for _, item in sorted_entries]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                            adb_gcv_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_adb_gcv", 0) or 0) for _, item in sorted_entries]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                            arb_ash_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_arb_ash", 0) or 0) for _, item in sorted_entries]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                            arb_vm_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_arb_vm", 0) or 0) for _, item in sorted_entries]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                            arb_fc_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_arb_fc", 0) or 0) for _, item in sorted_entries]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                            arb_gcv_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_arb_gcv", 0) or 0) for _, item in sorted_entries]) * C) / C42, nan=0, posinf=0, neginf=0), 2)

                        worksheet.write(total_row, 8, arb_tm_avg, total_row_format)
                        worksheet.write(total_row, 9, adb_im_avg, total_row_format)
                        worksheet.write(total_row, 10, adb_ash_avg, total_row_format)
                        worksheet.write(total_row, 11, adb_vm_avg, total_row_format)
                        worksheet.write(total_row, 12, adb_gcv_avg, total_row_format)
                        worksheet.write(total_row, 13, arb_ash_avg, total_row_format)
                        worksheet.write(total_row, 14, arb_vm_avg, total_row_format)
                        worksheet.write(total_row, 15, arb_fc_avg, total_row_format)
                        worksheet.write(total_row, 16, arb_gcv_avg, total_row_format)

                                                
                        current_row += 3  # Move to the next row for the next DO number

                except Exception as e:
                    console_logger.error(f"Error while creating the Excel sheet for consumer type {consumer_type}: {e}")

            workbook.close()
            if path:
                return {
                    "Type": "WCL_Analysis_download_event",
                    "Datatype": "Report",
                    "File_Path": path,
                }
            else:
                console_logger.debug("No files were generated.")
                return {
                    "Type": "WCL_Analysis_download_event",
                    "Datatype": "Report",
                    "File_Path": None,
                }

    except Exception as e:
        console_logger.debug(f"----- WCL Analysis Error ----- {e}")
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(f"{exc_type} in {fname} on line {exc_tb.tb_lineno}")
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return {"detail": str(e)}


@router.get("/overall_analysis", tags=["Recovery"])
def get_overall_analysis(response: Response,
                     search_text: Optional[str] = None,
                     start_timestamp: Optional[str] = None,
                     end_timestamp: Optional[str] = None,
                     type: Optional[str] = "download"):
    try:
        query = {}

        if type == "display":
            return {"detail": "In progress"}

        elif type == "download":
            del type

            file = str(datetime.datetime.utcnow().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            if start_timestamp and end_timestamp:
                start_date = datetime.datetime.strptime(start_timestamp, "%Y-%m-%dT%H:%M")
                end_date = datetime.datetime.strptime(end_timestamp, "%Y-%m-%dT%H:%M")

                start_date = start_date.replace(tzinfo=pytz.timezone('Asia/Kolkata')).astimezone(pytz.utc)
                end_date = end_date.replace(tzinfo=pytz.timezone('Asia/Kolkata')).astimezone(pytz.utc)

                query["plant_analysis_date"] = {"$gte": start_date, "$lte": end_date}
        
            if search_text:
                query["$or"] = [
                    {"sample_id": {"$regex": f"{search_text}", "$options": "i"}}
                ]

            pipeline = [
                {"$match": query},
                {
                    "$group": {
                        "_id": {
                            "sample_id": "$sample_id",
                            "sample_no": "$sample_no",
                            "mine": "$mine",
                            "plant_certificate_id": "$plant_certificate_id",
                            "plant_sample_date": "$plant_sample_date",
                            "plant_analysis_date": "$plant_analysis_date",
                            "plant_preperation_date": "$plant_preperation_date",
                            "mode": "$mode"
                        },
                        "last_sample_qty": {"$last": "$sample_qty"},
                        "plant_arb_tm": {"$last": "$plant_arb_tm"},
                        "plant_arb_vm": {"$last": "$plant_arb_vm"},
                        "plant_arb_ash": {"$last": "$plant_arb_ash"},
                        "plant_arb_fc": {"$last": "$plant_arb_fc"},
                        "plant_arb_gcv": {"$last": "$plant_arb_gcv"},
                        "plant_adb_im": {"$last": "$plant_adb_im"},
                        "plant_adb_vm": {"$last": "$plant_adb_vm"},
                        "plant_adb_ash": {"$last": "$plant_adb_ash"},
                        "plant_adb_fc": {"$last": "$plant_adb_fc"},
                        "plant_adb_gcv": {"$last": "$plant_adb_gcv"}
                    }
                },
                {
                    "$project": {
                        "_id": 0,
                        "sample_id": "$_id.sample_id",
                        "sample_no": "$_id.sample_no",
                        "mine": "$_id.mine",
                        "plant_certificate_id": "$_id.plant_certificate_id",
                        "plant_sample_date": "$_id.plant_sample_date",
                        "plant_analysis_date": "$_id.plant_analysis_date",
                        "plant_preperation_date": "$_id.plant_preperation_date",
                        "mode": "$_id.mode",
                        "mine_qty": {
                            "$toDouble": {
                                "$replaceAll": {
                                    "input": {
                                        "$toString": "$last_sample_qty"  # Convert to string first
                                    },
                                    "find": ",",
                                    "replacement": ""
                                }
                            }
                        },
                        "parameters": {
                            "plant_arb_tm": "$plant_arb_tm",
                            "plant_arb_vm": "$plant_arb_vm",
                            "plant_arb_ash": "$plant_arb_ash",
                            "plant_arb_fc": "$plant_arb_fc",
                            "plant_arb_gcv": "$plant_arb_gcv",
                            "plant_adb_im": "$plant_adb_im",
                            "plant_adb_vm": "$plant_adb_vm",
                            "plant_adb_ash": "$plant_adb_ash",
                            "plant_adb_fc": "$plant_adb_fc",
                            "plant_adb_gcv": "$plant_adb_gcv"
                        }
                    }
                },
                {
                    "$addFields": {
                        "data": {
                            "$mergeObjects": [
                                {
                                    "sample_id": "$sample_id",
                                    "sample_no": "$sample_no",
                                    "mine": "$mine",
                                    "plant_certificate_id": "$plant_certificate_id",
                                    "plant_sample_date": "$plant_sample_date",
                                    "plant_analysis_date": "$plant_analysis_date",
                                    "plant_preperation_date": "$plant_preperation_date",
                                    "mode": "$mode",
                                    "mine_qty": "$mine_qty"
                                },
                                "$parameters"
                            ]
                        }
                    }
                },
                {"$replaceRoot": {"newRoot": "$data"}},
                {"$sort": {"sample_id": 1, "plant_sample_date": 1}}
            ]

            alldata = list(receiptCoalQualityAnalysisdb.aggregate(pipeline))
            sapdb_data = sapdb.find({})
            secl_data = sapraildb.find({})
            raildata = raildb.find({})

            sapdb_dict = {entry.get("do_no", ""): entry for entry in sapdb_data}
            secl_dict = {entry.get("rr_no", ""): entry for entry in secl_data}
            rail_dict = {entry.get("rr_no", ""): entry for entry in raildata}

            grouped_data = {}
            for item in alldata:
                # console_logger.debug(item)
                rr_no = item.get("sample_id")
                sapdb_entry = sapdb_dict.get(rr_no)
                secl_entry = secl_dict.get(rr_no)

                if sapdb_entry:
                    consumer_type = sapdb_entry.get("consumer_type", "Unknown")
                    if consumer_type not in grouped_data:
                        grouped_data[consumer_type] = {}
                    if rr_no not in grouped_data[consumer_type]:
                        grouped_data[consumer_type][rr_no] = []
                    grouped_data[consumer_type][rr_no].append(item)

                if secl_entry:
                    if "SECL Linkage" not in grouped_data:
                        grouped_data["SECL Linkage"] = {}
                    if rr_no not in grouped_data["SECL Linkage"]:
                        grouped_data["SECL Linkage"][rr_no] = []
                    grouped_data["SECL Linkage"][rr_no].append(item)

            path = None
            logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
            if os.path.exists(target_directory):
                path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "Coal_Analysis_{}.xlsx".format(
                            datetime.datetime.utcnow().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                filename = os.path.join(os.getcwd(), path)

            workbook = xlsxwriter.Workbook(filename)
            workbook.use_zip64()
            # worksheet = workbook.add_worksheet()

            total_row_format = workbook.add_format({'bold': True, 'font_size': 10, 'align': 'center',
                                                    'valign': 'vcenter', 'border': 1, 'border_color': 'black'}) 
            
            cell_format = workbook.add_format({'font_size': 10, 'align': 'center', 'valign': 'vcenter',
                                               'border': 1, 'border_color': 'black'})

            normal_row_format = workbook.add_format({'bold': True, 'font_size': 10, 'align': 'center', 
                                                     'valign': 'vcenter', 'border': 1, 'border_color': 'black'})

            total_row_format.set_text_wrap(True)
            cell_format.set_text_wrap(True)
            normal_row_format.set_text_wrap(True)

            total_row_format.set_border(1)
            cell_format.set_border(1)
            normal_row_format.set_border(1)

            header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
            date_format = workbook.add_format({'align': 'center', 'font_size': 12, "bold": True})
            report_name_format = workbook.add_format({'align': 'center', 'font_size': 15, "bold": True})

            header_format.set_align("vcenter")
            date_format.set_align("vcenter")
            report_name_format.set_align("vcenter")
            header_format.set_border(1)
            date_format.set_border(1)
            report_name_format.set_border(1)

            # Write headers only once for the consumer type
            headers = [
                "LOT No", 
                "Mine Name", 
                "Quota", 
                "DO/RR Quantity", 
                "Mine Quantity", 
                "Received Quantity", 
                "Received Date",
                "Analysis Date",
                "Total Moisture", 
                "Inherent Moisture (ADB)", 
                "Ash (ADB)", 
                "Volatile Matter (ADB)",
                "Gross Calorific Value (ADB)", 
                "Ash (ARB)", 
                "Volatile Matter (ARB)", 
                "Fixed Carbon (ARB)",
                "Gross Calorific Value (ARB)"
            ]

            secl_headers = [
                "Rake No", 
                "Mine Name", 
                "Month",
                "RR No",
                "RR Date",
                "SECL Quantity", 
                "WT Quantity", 
                "TL", 
                "Placement Date",
                "Placement Time",
                "Analysis Date",         # plant_analysis_date
                "GCV (ADB)",
                "GCV (ARB)",
                "TM",
                "IM",
                "VM (ADB)",
                "ASH (ADB)",
                "VM (ARB)",
                "ASH (ARB)",
                "FC (ARB)"
            ]

            for consumer_type, do_grouped_data in grouped_data.items():
                try:
                    worksheet = workbook.add_worksheet(consumer_type)
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)
                    # Merge cells for the main header and place it in the center
                    main_header = "GMR Warora Energy Limited"  # Set your main header text here
                    worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                    
                    worksheet.write("A2", f"Date: {end_date.strftime('%d-%m-%Y')}", date_format)

                    if consumer_type == "SECL Linkage":
                        headers = secl_headers
                        worksheet.merge_range("A1:T1", main_header, header_format) 
                        worksheet.merge_range("C2:T2", f"Coal Analysis", report_name_format)

                    else:
                        headers = headers
                        worksheet.merge_range("A1:Q1", main_header, header_format) 
                        worksheet.merge_range("C2:Q2", f"Coal Analysis", report_name_format)

                    current_row = 2
                    secl_linkage_headers_written = False
                    for do_no, entries in do_grouped_data.items():
                        if consumer_type != "SECL Linkage" or not secl_linkage_headers_written:
                            for index, header in enumerate(headers):
                                worksheet.write(current_row, index, header, normal_row_format)
                            current_row += 1
                            if consumer_type == "SECL Linkage":
                                secl_linkage_headers_written = True 
                        
                        sorted_entries = []
                        for item in entries:
                            lot_no = item.get("sample_no", "")
                            sorted_entries.append((lot_no, item))

                        sorted_entries.sort(key=lambda x: (int(x[0]) if x[0].isdigit() else float('inf'), x[0]))
                        mine_name = f"{sapdb_dict.get(do_no, {}).get('mine_name', 'Unknown')}-{do_no}"
                        month = f"{sapdb_dict.get(do_no, {}).get('slno', 'Unknown')}"

                        secl_mine = f"{secl_dict.get(do_no, {}).get('mine', 'Unknown')}"
                        secl_month = f"{secl_dict.get(do_no, {}).get('month', 'Unknown')}"

                        rail_entry = rail_dict.get(do_no)
                        if rail_entry:
                            avery_placement_date = rail_entry.get("avery_placement_date")
                            
                            if avery_placement_date:
                                receive_date, receive_time = avery_placement_date.split('T')
                            else:
                                receive_date, receive_time = None, None
                        else:
                            receive_date, receive_time = None, None 

                        for lot_no, item in sorted_entries:
                            if consumer_type == "SECL Linkage":
                                worksheet.write(current_row, 0, f"Rake-{lot_no}", cell_format)
                                worksheet.write(current_row, 1, secl_mine, cell_format)
                                try:
                                    worksheet.write(current_row, 2, datetime.datetime.strptime(secl_month, "%Y-%m").strftime("%b %Y"), cell_format)
                                except ValueError as e:
                                    worksheet.write(current_row, 2, "N/A", cell_format)

                                worksheet.write(current_row, 3, do_no, cell_format)
                                worksheet.write(current_row, 4, secl_dict.get(do_no, {}).get("rr_date", 0), cell_format)
                                worksheet.write(current_row, 5, secl_dict.get(do_no, {}).get("rr_qty", 0), cell_format)
                                worksheet.write(current_row, 6, item.get("mine_qty", 0), cell_format)
                                rr_qty = float(secl_dict.get(do_no, {}).get("rr_qty", 0) or 0)
                                mine_qty = float(item.get("mine_qty", 0) or 0)
                                worksheet.write(current_row, 7, round(rr_qty - mine_qty, 2), cell_format)
                                worksheet.write(current_row, 8, receive_date, cell_format)
                                worksheet.write(current_row, 9, receive_time, cell_format)
                                worksheet.write(current_row, 10, str(item.get("plant_analysis_date").replace(tzinfo=pytz.utc).astimezone(
                                    pytz.timezone('Asia/Kolkata')).strftime("%Y-%m-%d")
                                    if item.get("plant_analysis_date") is not None else None), cell_format)
                                worksheet.write(current_row, 11, item.get("plant_adb_gcv", ""), cell_format)
                                worksheet.write(current_row, 12, item.get("plant_arb_gcv", ""), cell_format)
                                worksheet.write(current_row, 13, item.get("plant_arb_tm", ""), cell_format)
                                worksheet.write(current_row, 14, item.get("plant_adb_im", ""), cell_format)
                                worksheet.write(current_row, 15, item.get("plant_adb_vm", ""), cell_format)
                                worksheet.write(current_row, 16, item.get("plant_adb_ash", ""), cell_format)
                                worksheet.write(current_row, 17, item.get("plant_arb_vm", ""), cell_format)
                                worksheet.write(current_row, 18, item.get("plant_arb_ash", ""), cell_format)
                                worksheet.write(current_row, 19, item.get("plant_arb_fc", ""), cell_format)
                                current_row += 1

                            else:
                                worksheet.write(current_row, 0, lot_no, cell_format)
                                worksheet.write(current_row, 1, mine_name, cell_format)
                                if month is not None:
                                    worksheet.write(current_row, 2, datetime.datetime.strptime(month, "%Y%m").strftime("%b %Y"), cell_format)
                                else:
                                    worksheet.write(current_row, 2, "N/A", cell_format)
                                worksheet.write(current_row, 3, sapdb_dict.get(do_no, {}).get("do_qty", 0), cell_format)
                                worksheet.write(current_row, 4, item.get("mine_qty", 0), cell_format)
                                worksheet.write(current_row, 5, item.get("mine_qty", 0), cell_format)
                                receive_date = item.get("plant_sample_date")
                                worksheet.write(current_row, 6, receive_date, cell_format)
                                worksheet.write(current_row, 7, str(item.get("plant_analysis_date").replace(tzinfo=pytz.utc).astimezone(
                                    pytz.timezone('Asia/Kolkata')).strftime("%Y-%m-%d")
                                    if item.get("plant_analysis_date") is not None else None), cell_format)
                                worksheet.write(current_row, 8, item.get("plant_arb_tm", ""), cell_format)
                                worksheet.write(current_row, 9, item.get("plant_adb_im", ""), cell_format)
                                worksheet.write(current_row, 10, item.get("plant_adb_ash", ""), cell_format)
                                worksheet.write(current_row, 11, item.get("plant_adb_vm", ""), cell_format)
                                worksheet.write(current_row, 12, item.get("plant_adb_gcv", ""), cell_format)
                                worksheet.write(current_row, 13, item.get("plant_arb_ash", ""), cell_format)
                                worksheet.write(current_row, 14, item.get("plant_arb_vm", ""), cell_format)
                                worksheet.write(current_row, 15, item.get("plant_arb_fc", ""), cell_format)
                                worksheet.write(current_row, 16, item.get("plant_arb_gcv", ""), cell_format)

                                current_row += 1

                        if consumer_type != "SECL Linkage":
                            total_row = current_row
                            worksheet.write(total_row, 0, "Total", total_row_format)
                            worksheet.write(total_row, 1, "", total_row_format)
                            worksheet.write(total_row, 2, "", total_row_format)
                            # worksheet.write(total_row, 2, sum(float(sapdb_dict.get(do_no, {}).get("do_qty", 0)) for _, item in sorted_entries), total_row_format)
                            worksheet.write(current_row, 3, sapdb_dict.get(do_no, {}).get("do_qty", 0), total_row_format)
                            total_mine_qty = sum(float(item.get("mine_qty", 0) or 0) for _, item in sorted_entries)
                            worksheet.write(total_row, 4, total_mine_qty, total_row_format)
                            worksheet.write(total_row, 5, total_mine_qty, total_row_format)
                            worksheet.write(total_row, 6, "", total_row_format)
                            worksheet.write(total_row, 7, "", total_row_format)
                            C = np.array([float(item.get("mine_qty", 0) or 0) for _, item in sorted_entries])
                            C42 = total_mine_qty
                            if C42 == 0:
                                arb_tm_avg = 0
                                adb_im_avg = 0
                                adb_ash_avg = 0
                                adb_vm_avg = 0
                                adb_gcv_avg = 0
                                arb_ash_avg = 0
                                arb_vm_avg = 0
                                arb_fc_avg = 0
                                arb_gcv_avg = 0
                            else:
                                arb_tm_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_arb_tm", 0) or 0) for _, item in sorted_entries]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                                adb_im_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_adb_im", 0) or 0) for _, item in sorted_entries]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                                adb_ash_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_adb_ash", 0) or 0) for _, item in sorted_entries]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                                adb_vm_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_adb_vm", 0) or 0) for _, item in sorted_entries]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                                adb_gcv_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_adb_gcv", 0) or 0) for _, item in sorted_entries]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                                arb_ash_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_arb_ash", 0) or 0) for _, item in sorted_entries]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                                arb_vm_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_arb_vm", 0) or 0) for _, item in sorted_entries]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                                arb_fc_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_arb_fc", 0) or 0) for _, item in sorted_entries]) * C) / C42, nan=0, posinf=0, neginf=0), 2)
                                arb_gcv_avg = round(np.nan_to_num(np.sum(np.array([float(item.get("plant_arb_gcv", 0) or 0) for _, item in sorted_entries]) * C) / C42, nan=0, posinf=0, neginf=0), 2)

                            worksheet.write(total_row, 8, arb_tm_avg, total_row_format)
                            worksheet.write(total_row, 9, adb_im_avg, total_row_format)
                            worksheet.write(total_row, 10, adb_ash_avg, total_row_format)
                            worksheet.write(total_row, 11, adb_vm_avg, total_row_format)
                            worksheet.write(total_row, 12, adb_gcv_avg, total_row_format)
                            worksheet.write(total_row, 13, arb_ash_avg, total_row_format)
                            worksheet.write(total_row, 14, arb_vm_avg, total_row_format)
                            worksheet.write(total_row, 15, arb_fc_avg, total_row_format)
                            worksheet.write(total_row, 16, arb_gcv_avg, total_row_format)
                            
                            current_row += 3

                except Exception as e:
                    console_logger.error(f"Error while creating the Excel sheet for consumer type {consumer_type}: {e}")
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(f"{exc_type} in {fname} on line {exc_tb.tb_lineno}")
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))

            workbook.close()
            if path:
                return {
                    "Type": "Coal_Analysis_download_event",
                    "Datatype": "Report",
                    "File_Path": path,
                }
            else:
                console_logger.debug("No files were generated.")
                return {
                    "Type": "Coal_Analysis_download_event",
                    "Datatype": "Report",
                    "File_Path": None,
                }

    except Exception as e:
        console_logger.debug(f"----- Coal Analysis Error ----- {e}")
        response.status_code = 400
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(f"{exc_type} in {fname} on line {exc_tb.tb_lineno}")
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return {"detail": str(e)}


@router.post("/add/igi/data", tags=["Coal Testing"])
async def endpoint_to_add_igi_data(response: Response, excelUpload: List[UploadFile] = File(...)):
    try:
        for UploadedFile in excelUpload:
            f_name = UploadedFile.filename
            
            contents = await UploadedFile.read()

            if not contents:
                return {"error": f"Uploaded file '{f_name}' is empty"}

            if not f_name.endswith(".xlsx"):
                response.status_code = 400
                return {"error": f"Uploaded file '{f_name}' is not a .xlsx"}

            file_date = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file_date}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            file_extension = f_name.split(".")[-1]
            file_name = f'igi_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.{file_extension}'
            full_path = os.path.join(os.getcwd(), target_directory, file_name)

            with open(full_path, "wb") as file_object:
                file_object.write(contents)
            
            excel_data = pd.read_excel(BytesIO(contents))
            data_excel_fetch = json.loads(excel_data.to_json(orient="index"))
            mainDict = {}
            mainDict['reference_no'] = data_excel_fetch['0']["INSPECTORATE GRIFFITH INDIA PVT. LTD."].split("NO-")[1]
            mainDict['date'] = data_excel_fetch['1']["INSPECTORATE GRIFFITH INDIA PVT. LTD."].split(": ")[1]
            mainDict['receiver_client'] = data_excel_fetch['2']["INSPECTORATE GRIFFITH INDIA PVT. LTD."].split(": ")[1]
            mainDict['comodity'] = data_excel_fetch['3']["INSPECTORATE GRIFFITH INDIA PVT. LTD."].split(": ")[1]
            mainDict['source_secl'] = data_excel_fetch['4']["INSPECTORATE GRIFFITH INDIA PVT. LTD."].split("-")[2]


            listData = []
            for key in range(7, 15): 
                dictData = {}
                key_str = str(key)
                if key_str in data_excel_fetch and data_excel_fetch[key_str]["Unnamed: 3"] is not None:
                    dictData["srno"] = data_excel_fetch[key_str]["INSPECTORATE GRIFFITH INDIA PVT. LTD."]
                    dictData["sample_id"] = data_excel_fetch[key_str]["Unnamed: 1"]
                    dictData["ref_id"] = data_excel_fetch[key_str]["Unnamed: 2"]
                    dictData["cil_sap_no"] = data_excel_fetch[key_str]["Unnamed: 3"]
                    dictData["sample_date_collection"] = data_excel_fetch[key_str]["Unnamed: 4"]
                    dictData["sample_date_preparation"] = data_excel_fetch[key_str]["Unnamed: 5"]
                    dictData["sample_date_received_igi_lab"] = data_excel_fetch[key_str]["Unnamed: 6"]
                    dictData["quantity"] = data_excel_fetch[key_str]["Unnamed: 7"]
                    dictData["rr_no"] = data_excel_fetch[key_str]["Unnamed: 8"]
                    dictData["rr_date"] = data_excel_fetch[key_str]["Unnamed: 9"]
                    dictData["declared_grade"] = data_excel_fetch[key_str]["Unnamed: 10"]
                    dictData["igi_lab_code"] = data_excel_fetch[key_str]["Unnamed: 11"]
                    dictData["total_moisture_arb"] = data_excel_fetch[key_str]["Unnamed: 12"]
                    dictData["humidity"] = data_excel_fetch[key_str]["Unnamed: 13"]
                    dictData["temperature"] = data_excel_fetch[key_str]["Unnamed: 14"]
                    dictData["moisture_adb"] = data_excel_fetch[key_str]["Unnamed: 15"]
                    dictData["ash_adb"] = data_excel_fetch[key_str]["Unnamed: 16"]
                    dictData["gcv_adb"] = data_excel_fetch[key_str]["Unnamed: 17"]
                    dictData["rh_moisture"] = data_excel_fetch[key_str]["Unnamed: 18"]
                    dictData["rh_ash"] = data_excel_fetch[key_str]["Unnamed: 19"]
                    dictData["rh_gcv"] = data_excel_fetch[key_str]["Unnamed: 20"]
                    dictData["analysis_grade"] = data_excel_fetch[key_str]["Unnamed: 21"]
                    dictData["grade_coking_coal"] = data_excel_fetch[key_str]["Unnamed: 22"]
                    listData.append(dictData)
            mainDict['table_data'] = listData

            for single_data_mine in mainDict["table_data"]:
                try:
                    fetchminesampleQuantity = minesamplequalityanalysis.objects.get(sample_id=single_data_mine.get("sample_id"))
                    fetchminesampleQuantity.update(
                        mine_thirdparty_sample_reference_no=mainDict.get("reference_no"),
                        source=mainDict.get("source_secl"),
                        sample_analysis_date=datetime.datetime.strptime(mainDict.get("date"), "%d.%m.%Y"),
                        sample_collection_date=datetime.datetime.fromtimestamp(single_data_mine.get("sample_date_collection") / 1000),
                        sample_preparation_date=datetime.datetime.fromtimestamp(single_data_mine.get("sample_date_preparation") / 1000),
                        sample_received_date=datetime.datetime.fromtimestamp(single_data_mine.get("sample_date_received_igi_lab") / 1000),
                        rr_qty=single_data_mine.get("quantity"),
                        rr_no=single_data_mine.get("rr_no"),
                        rr_date=datetime.datetime.fromtimestamp(single_data_mine.get("rr_date") / 1000),
                        declared_grade=single_data_mine.get("declared_grade"),
                        mine_thirdparty_tm_arb=single_data_mine.get("total_moisture_arb"),
                        mine_thirdparty_humidity=single_data_mine.get("humidity"),
                        mine_thirdparty_temperature=single_data_mine.get("temperature"),
                        mine_thirdparty_adb_moisture=single_data_mine.get("moisture_adb"),
                        mine_thirdparty_adb_ash=single_data_mine.get("ash_adb"),
                        mine_thirdparty_adb_gcv=single_data_mine.get("gcv_adb"),
                        mine_thirdparty_arb_moisture=single_data_mine.get("rh_moisture"),
                        mine_thirdparty_arb_ash=single_data_mine.get("rh_ash"),
                        mine_thirdparty_arb_gcv=single_data_mine.get("rh_gcv"),
                        analysed_grade=single_data_mine.get("analysis_grade"),
                    )
                except DoesNotExist as e:
                    minesamplequalityanalysis(
                        mine_thirdparty_sample_reference_no=mainDict.get("reference_no"),
                        source=mainDict.get("source_secl"),
                        sample_analysis_date=datetime.datetime.strptime(mainDict.get("date"), "%d.%m.%Y"),
                        sample_id=single_data_mine.get("sample_id"),
                        sample_collection_date=datetime.datetime.fromtimestamp(int(single_data_mine.get("sample_date_collection")) / 1000),
                        sample_preparation_date=datetime.datetime.fromtimestamp(int(single_data_mine.get("sample_date_preparation")) / 1000),
                        sample_received_date=datetime.datetime.fromtimestamp(int(single_data_mine.get("sample_date_received_igi_lab")) / 1000),
                        rr_qty=single_data_mine.get("quantity"),
                        rr_no=single_data_mine.get("rr_no"),
                        rr_date=datetime.datetime.fromtimestamp(int(single_data_mine.get("rr_date")) / 1000),
                        declared_grade=single_data_mine.get("declared_grade"),
                        mine_thirdparty_tm_arb=single_data_mine.get("total_moisture_arb"),
                        mine_thirdparty_humidity=single_data_mine.get("humidity"),
                        mine_thirdparty_temperature=single_data_mine.get("temperature"),
                        mine_thirdparty_adb_moisture=single_data_mine.get("moisture_adb"),
                        mine_thirdparty_adb_ash=single_data_mine.get("ash_adb"),
                        mine_thirdparty_adb_gcv=single_data_mine.get("gcv_adb"),
                        mine_thirdparty_arb_moisture=single_data_mine.get("rh_moisture"),
                        mine_thirdparty_arb_ash=single_data_mine.get("rh_ash"),
                        mine_thirdparty_arb_gcv=single_data_mine.get("rh_gcv"),
                        analysed_grade=single_data_mine.get("analysis_grade"),
                    ).save()
        return {"detail": "success"}
    except KeyError as e:
        raise HTTPException(status_code=404, detail="Key Error")
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- IGI Excel Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return {"error": str(e)}


@router.get("/fetch/minesamplequalityanalysis", tags=["Coal Quality"])
def endpoint_to_fetch_minesamplequalityanalysis(response: Response, currentPage: Optional[int] = None,
                perPage: Optional[int] = None,
                search_text: Optional[str] = None,
                month_date: Optional[str] = None,
                start_timestamp: Optional[str] = None,
                end_timestamp: Optional[str] = None,
                type: Optional[str] = "display"):
    try:
        data = {}
        result = {        
                "labels": [],
                "datasets": [],
                "total" : 0,
                "page_size": 15
        }

        if type and type == "display":

            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage
            
            data = Q()

            # if date:
            #     end =f'{date} 23:59:59'
            #     start = f'{date} 00:00:00'
                
            #     data["created_at__gte"] = convert_to_utc_format(start, "%Y-%m-%d %H:%M:%S")
            #     data["created_at__lte"] = convert_to_utc_format(end, "%Y-%m-%d %H:%M:%S")

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(plant_analysis_date__gte = f"{start_date}")
            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M","Asia/Kolkata",False)
                data &= Q(plant_analysis_date__lte = f"{start_date}")
            
            if month_date:
                start_date = f'{month_date}-01'
                startd_date=datetime.datetime.strptime(f"{start_date}T00:00","%Y-%m-%dT%H:%M")
                end_date = (datetime.datetime.strptime(start_date, "%Y-%m-%d") + relativedelta(day=31)).strftime("%Y-%m-%d")
                data &= Q(plant_analysis_date__gte = startd_date.strftime("%Y-%m-%dT%H:%M"))
                data &= Q(plant_analysis_date__lte = f"{end_date}T23:59")

            if search_text:
                if search_text.isdigit():
                    data &= (Q(rr_no__icontains=search_text))
                else:
                    data &= (Q(source__icontains=search_text))
                # data &= Q(rr_no__icontains = search_text) | Q(source__icontains = search_text)
            
            offset = (page_no - 1) * page_len

            logs = (
                minesamplequalityanalysis.objects(data)
                .order_by("rr_no", "-plant_analysis_date")
                .skip(offset)
                .limit(page_len)
            )

            if any(logs):
                for log in logs:
                    payload = log.payload()
                    result["labels"] = list(payload.keys())
                    result["datasets"].append(payload)

            result["total"] = minesamplequalityanalysis.objects(data).count()
            return result

        elif type and type == "download":
            del type

            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            if start_timestamp:
                data["plant_analysis_date__gte"] = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")

            if end_timestamp:
                data["plant_analysis_date__lte"] = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")

            usecase_data = minesamplequalityanalysis.objects(**data).order_by("-created_at")
            count = len(usecase_data)
            path = None
            logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
            if usecase_data:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "mine_sample_qyality_analysis_Report_{}.xlsx".format(
                            datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format()
                    cell_format2.set_bold()
                    cell_format2.set_font_size(10)
                    cell_format2.set_align("center")
                    cell_format2.set_align("vcenter")
                    cell_format2.set_text_wrap(True)
                    cell_format2.set_border(1)

                    header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
                    date_format = workbook.add_format({'align': 'center', 'font_size': 12, "bold": True})
                    report_name_format = workbook.add_format({'align': 'center', 'font_size': 15, "bold": True})

                    header_format.set_align("vcenter")
                    date_format.set_align("vcenter")
                    report_name_format.set_align("vcenter")
                    header_format.set_border(1)
                    date_format.set_border(1)
                    report_name_format.set_border(1)

                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)
                    cell_format = workbook.add_format()
                    cell_format.set_font_size(10)
                    cell_format.set_align("center")
                    cell_format.set_align("vcenter")
                    cell_format.set_text_wrap(True)
                    cell_format.set_border(1)


                    worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                    
                    # Merge cells for the main header and place it in the center
                    main_header = "GMR Warora Energy Limited"  # Set your main header text here
                    worksheet.merge_range("A1:AO1", main_header, header_format)  # Merge cells A1 to H1 for the header
                    
                    # Write the current date on the left side (A2)
                    worksheet.write("A2", f"Date: {datetime.datetime.now().strftime('%d-%m-%Y')}", date_format)
                    worksheet.merge_range("C2:AO2", f"Mine Sample Quality Analysis Table", report_name_format)


                    headers = [
                        "Sr.no.",
                        "Mine Thirdparty Sample Reference no",
                        "Source",
                        "Sample id",
                        "Sample Collection Date",
                        "Sample Preparation Date",
                        "Sample Received Date",
                        "Sample Analysis Date",
                        "RR Qty",
                        "RR No",
                        "RR Date",
                        "Declared Grade",
                        "Mine Thirdparty Grade",
                        "Plant Grade",
                        "Plant Certificate Id",
                        "Plant Sample Date",
                        "Plant Preparation Date",
                        "Plant Analysis Sate",
                        "Plant Lab Temp",
                        "Plant ARB TM",
                        "Plant ARB VM",
                        "Plant ARB ASH",
                        "Plant ARB FC",
                        "Plant ARB GCV",
                        "Plant ADB IM",
                        "Plant ADB VM",
                        "Plant ADB ASH",
                        "Plant ADB FC",
                        "Plant ADB GCV",
                        "Plant ULR ID",
                        "Mine Thirdparty TM ARB",
                        "Mine Thirdparty Humidity",
                        "Mine Thirdparty Temperature",
                        "Mine Thirdparty ADB Moisture",
                        "Mine Thirdparty ADB Ash",
                        "Mine Thirdparty ADB GCV",
                        "Mine Thirdparty ARB Moisture",
                        "Mine Thirdparty ARB ASH",
                        "Mine Thirdparty ARB GCV",
                        "Analysed Grade",
                        "Created At"
                    ]

                    for index, header in enumerate(headers):
                        worksheet.write(2, index, header, cell_format2)
                    for row, query in enumerate(usecase_data,start=3):
                        result = query.payload()
                        worksheet.write(row, 0, count, cell_format)
                        worksheet.write(row, 1, str(result["mine_thirdparty_sample_reference_no"]), cell_format)
                        worksheet.write(row, 2, str(result["source"]), cell_format)
                        worksheet.write(row, 3, str(result["sample_id"]), cell_format)
                        worksheet.write(row, 4, str(result["sample_collection_date"]), cell_format)
                        worksheet.write(row, 5, str(result["sample_preparation_date"]), cell_format)
                        
                        worksheet.write(row, 6, str(result["sample_received_date"]), cell_format)
                        worksheet.write(row, 7, str(result["sample_analysis_date"]), cell_format)
                        worksheet.write(row, 8, float(result["rr_qty"]), cell_format)
                        worksheet.write(row, 9, str(result["rr_no"]), cell_format)
                        worksheet.write(row, 10, str(result["rr_date"]), cell_format)
                        worksheet.write(row, 11, str(result["declared_grade"]), cell_format)
                        worksheet.write(row, 12, str(result["mine_thirdparty_grade"]), cell_format)
                        worksheet.write(row, 13, str(result["plant_grade"]), cell_format)
                        worksheet.write(row, 14, str(result["plant_certificate_id"]), cell_format)
                        worksheet.write(row, 15, str(result["plant_sample_date"]), cell_format)
                        worksheet.write(row, 16, str(result["plant_preparation_date"]), cell_format)
                        worksheet.write(row, 17, str(result["plant_analysis_date"]), cell_format)
                        if result.get("plant_lab_temp"):
                            worksheet.write(row, 18, float(result["plant_lab_temp"]), cell_format)
                        else:
                            worksheet.write(row, 18, "N/A", cell_format)

                        if result.get("plant_arb_tm"):
                            worksheet.write(row, 19, float(result["plant_arb_tm"]), cell_format)
                        else:
                            worksheet.write(row, 19, "N/A", cell_format)

                        if result.get("plant_arb_vm"):
                            worksheet.write(row, 20, float(result["plant_arb_vm"]), cell_format)
                        else:
                            worksheet.write(row, 20, "N/A", cell_format)

                        if result.get("plant_arb_ash"):
                            worksheet.write(row, 21, float(result["plant_arb_ash"]), cell_format)
                        else:
                            worksheet.write(row, 21, "N/A", cell_format)

                        if result.get("plant_arb_fc"):
                            worksheet.write(row, 22, float(result["plant_arb_fc"]), cell_format)
                        else:
                            worksheet.write(row, 22, "N/A", cell_format)

                        if result.get("plant_arb_gcv"):
                            worksheet.write(row, 23, float(result["plant_arb_gcv"]), cell_format)
                        else:
                            worksheet.write(row, 23, "N/A", cell_format)

                        if result.get("plant_adb_im"):
                            worksheet.write(row, 24, float(result["plant_adb_im"]), cell_format)
                        else:
                            worksheet.write(row, 24, "N/A", cell_format)

                        if result.get("plant_adb_vm"):
                            worksheet.write(row, 25, float(result["plant_adb_vm"]), cell_format)
                        else:
                            worksheet.write(row, 25, "N/A", cell_format)

                        if result.get("plant_adb_ash"):
                            worksheet.write(row, 26, float(result["plant_adb_ash"]), cell_format)
                        else:
                            worksheet.write(row, 26, "N/A", cell_format)

                        if result.get("plant_adb_fc"):
                            worksheet.write(row, 27, float(result["plant_adb_fc"]), cell_format)
                        else:
                            worksheet.write(row, 27, "N/A", cell_format)

                        if result.get("plant_adb_gcv"):
                            worksheet.write(row, 28, float(result["plant_adb_gcv"]), cell_format)
                        else:
                            worksheet.write(row, 28, "N/A", cell_format)

                        if result.get("plant_ulr_id"):
                            worksheet.write(row, 29, float(result["plant_ulr_id"]), cell_format)
                        else:
                            worksheet.write(row, 29, "N/A", cell_format)

                        if result.get("mine_thirdparty_tm_arb"):
                            worksheet.write(row, 30, float(result["mine_thirdparty_tm_arb"]), cell_format)
                        else:
                            worksheet.write(row, 30, "N/A", cell_format)

                        if result.get("mine_thirdparty_humidity"):
                            worksheet.write(row, 31, float(result["mine_thirdparty_humidity"]), cell_format)
                        else:
                            worksheet.write(row, 31, "N/A", cell_format)

                        if result.get("mine_thirdparty_temperature"):
                            worksheet.write(row, 32, float(result["mine_thirdparty_temperature"]), cell_format)
                        else:
                            worksheet.write(row, 32, "N/A", cell_format)

                        if result.get("mine_thirdparty_adb_moisture"):
                            worksheet.write(row, 33, float(result["mine_thirdparty_adb_moisture"]), cell_format)
                        else:
                            worksheet.write(row, 33, "N/A", cell_format)

                        if result.get("mine_thirdparty_adb_ash"):
                            worksheet.write(row, 34, float(result["mine_thirdparty_adb_ash"]), cell_format)
                        else:
                            worksheet.write(row, 34, "N/A", cell_format)

                        if result.get("mine_thirdparty_adb_gcv"):
                            worksheet.write(row, 35, float(result["mine_thirdparty_adb_gcv"]), cell_format)
                        else:
                            worksheet.write(row, 35, "N/A", cell_format)

                        if result.get("mine_thirdparty_arb_moisture"):
                            worksheet.write(row, 36, float(result["mine_thirdparty_arb_moisture"]), cell_format)
                        else:
                            worksheet.write(row, 36, "N/A", cell_format)

                        if result.get("mine_thirdparty_arb_ash"):
                            worksheet.write(row, 37, float(result["mine_thirdparty_arb_ash"]), cell_format)
                        else:
                            worksheet.write(row, 37, "N/A", cell_format)

                        if result.get("mine_thirdparty_arb_gcv"):
                            worksheet.write(row, 38, float(result["mine_thirdparty_arb_gcv"]), cell_format)
                        else:
                            worksheet.write(row, 38, "N/A", cell_format)

                        if result.get("analysed_grade"):
                            worksheet.write(row, 39, float(result["analysed_grade"]), cell_format)
                        else:
                            worksheet.write(row, 39, "N/A", cell_format)

                        if result.get("created_at"):
                            worksheet.write(row, 40, float(result["created_at"]), cell_format)
                        else:
                            worksheet.write(row, 40, "N/A", cell_format)
                        count-=1
                        
                    workbook.close()
                    console_logger.debug("Successfully {} report generated".format(service_id))
                    console_logger.debug("sent data {}".format(path))

                    return {
                            "Type": "mine_sample_quality_analysis_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                            }
                
                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            else:
                console_logger.error("No data found")
                return {
                        "Type": "mine_sample_quality_analysis_download_event",
                        "Datatype": "Report",
                        "File_Path": path,
                        }

    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/insert/thirdpartyminesamplequality", tags=["Coal Quality"])
async def endpoint_to_add_sap_road_excel_data(response: Response, file: UploadFile = File(...)):
    try:
        if file is None:
            return {"error": "No file Uploaded!"}
        
        contents = await file.read()
        if not contents:
            response.status_code = 400
            return {"error": "Uploaded file is empty!"}

        if file.filename.endswith(".xlsx"):
            # file saving start
            date = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{date}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)
            file_extension = file.filename.split(".")[-1]
            file_name = f'thirdparty_coal_quality_analysis_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.{file_extension}'
            full_path = os.path.join(os.getcwd(), target_directory, file_name)
            with open(full_path, "wb") as file_object:
                file_object.write(contents)
            # file saving end

            excel_data = pd.read_excel(BytesIO(contents))
            data_excel_fetch = json.loads(excel_data.to_json(orient="records"))
            for single_data in data_excel_fetch:
                try:
                    fetchMineSampleQuality = minesamplequalityanalysis.objects.get(sample_id=str(single_data["sample_id"]))
                    fetchMineSampleQuality.update(
                        plant_certificate_id=single_data.get("plant_certificate_id"),
                        # sample_id=single_data.get("sample_id"),
                        rr_no=single_data.get("rr_no"),
                        rr_date=str(single_data.get("rr_date")),	
                        rr_qty=single_data.get("rr_qty"),
                        sample_collection_date=single_data.get("sample_collection_date"),
                        sample_preparation_date=single_data.get("sample_preparation_date"),
                        sample_received_date=single_data.get("sample_received_date"),
                        sample_analysis_date=single_data.get("sample_analysis_date"),
                        plant_sample_date=single_data.get("plant_sample_date"),
                        plant_preparation_date=single_data.get("plant_preparation_date"),	
                        plant_analysis_date=single_data.get("plant_analysis_date"),
                        source=single_data.get("source"),
                        plant_lab_temp=single_data.get("plant_lab_temp"),
                        plant_arb_tm=single_data.get("plant_arb_tm"),
                        plant_arb_vm=single_data.get("plant_arb_vm"),
                        plant_arb_ash=single_data.get("plant_arb_ash"),
                        plant_arb_fc=single_data.get("plant_arb_fc"),
                        plant_arb_gcv=single_data.get("plant_arb_gcv"),
                        plant_adb_im=single_data.get("plant_adb_im"),
                        plant_adb_vm=single_data.get("plant_adb_vm"),
                        plant_adb_ash=single_data.get("plant_adb_ash"),
                        plant_adb_fc=single_data.get("plant_adb_fc"),
                        plant_adb_gcv=single_data.get("plant_adb_gcv"),
                        plant_ulr_id=single_data.get("plant_ulr_id"),
                        mine_thirdparty_sample_reference_no=single_data.get("mine_thirdparty_sample_reference_no"),	
                        declared_grade=single_data.get("declared_grade"),
                        mine_thirdparty_grade=single_data.get("mine_thirdparty_grade"),
                        plant_grade=single_data.get("plant_grade"),
                        mine_thirdparty_tm_arb=single_data.get("mine_thirdparty_tm_arb"),
                        mine_thirdparty_humidity=single_data.get("mine_thirdparty_humidity"),
                        mine_thirdparty_temperature=single_data.get("mine_thirdparty_temperature"),
                        mine_thirdparty_adb_moisture=single_data.get("mine_thirdparty_adb_moisture"),
                        mine_thirdparty_adb_ash=single_data.get("mine_thirdparty_adb_ash"),	
                        mine_thirdparty_adb_gcv=single_data.get("mine_thirdparty_adb_gcv"),
                        mine_thirdparty_arb_moisture=single_data.get("mine_thirdparty_arb_moisture"),
                        mine_thirdparty_arb_ash=single_data.get("mine_thirdparty_arb_ash"),
                        mine_thirdparty_arb_gcv=single_data.get("mine_thirdparty_arb_gcv")
                    )
                except DoesNotExist as e:
                    insertMineSampleQuality = minesamplequalityanalysis(
                        plant_certificate_id=single_data.get("plant_certificate_id"),
                        sample_id=str(single_data.get("sample_id")),
                        rr_no=single_data.get("rr_no"),
                        rr_date=str(single_data.get("rr_date")),	
                        rr_qty=single_data.get("rr_qty"),
                        sample_collection_date=single_data.get("sample_collection_date"),
                        sample_preparation_date=single_data.get("sample_preparation_date"),
                        sample_received_date=single_data.get("sample_received_date"),
                        sample_analysis_date=single_data.get("sample_analysis_date"),
                        plant_sample_date=single_data.get("plant_sample_date"),
                        plant_preparation_date=single_data.get("plant_preparation_date"),	
                        plant_analysis_date=single_data.get("plant_analysis_date"),
                        source=single_data.get("source"),
                        plant_lab_temp=single_data.get("plant_lab_temp"),
                        plant_arb_tm=single_data.get("plant_arb_tm"),
                        plant_arb_vm=single_data.get("plant_arb_vm"),
                        plant_arb_ash=single_data.get("plant_arb_ash"),
                        plant_arb_fc=single_data.get("plant_arb_fc"),
                        plant_arb_gcv=single_data.get("plant_arb_gcv"),
                        plant_adb_im=single_data.get("plant_adb_im"),
                        plant_adb_vm=single_data.get("plant_adb_vm"),
                        plant_adb_ash=single_data.get("plant_adb_ash"),
                        plant_adb_fc=single_data.get("plant_adb_fc"),
                        plant_adb_gcv=single_data.get("plant_adb_gcv"),
                        plant_ulr_id=single_data.get("plant_ulr_id"),
                        mine_thirdparty_sample_reference_no=single_data.get("mine_thirdparty_sample_reference_no"),	
                        declared_grade=single_data.get("declared_grade"),
                        mine_thirdparty_grade=single_data.get("mine_thirdparty_grade"),
                        plant_grade=single_data.get("plant_grade"),
                        mine_thirdparty_tm_arb=single_data.get("mine_thirdparty_tm_arb"),
                        mine_thirdparty_humidity=single_data.get("mine_thirdparty_humidity"),
                        mine_thirdparty_temperature=single_data.get("mine_thirdparty_temperature"),
                        mine_thirdparty_adb_moisture=single_data.get("mine_thirdparty_adb_moisture"),
                        mine_thirdparty_adb_ash=single_data.get("mine_thirdparty_adb_ash"),	
                        mine_thirdparty_adb_gcv=single_data.get("mine_thirdparty_adb_gcv"),
                        mine_thirdparty_arb_moisture=single_data.get("mine_thirdparty_arb_moisture"),
                        mine_thirdparty_arb_ash=single_data.get("mine_thirdparty_arb_ash"),
                        mine_thirdparty_arb_gcv=single_data.get("mine_thirdparty_arb_gcv")
                    )
                    insertMineSampleQuality.save()

        return {"detail": "success"}
    except KeyError as e:
        raise HTTPException(status_code=404, detail="Key Error")
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- thirdparty_coal_quality_analysis Excel Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


def extract_invoice_data_tax_invoice(pdf_path):
    try:
        with pdfplumber.open(pdf_path) as pdf:
            content = ""
            for page in pdf.pages:
                content += page.extract_text() + "\n"
        # Extract required information
        invoice_date = re.search(r'Invoice Date\s*:([^\n]+)', content)
        invoice_no = re.search(r'I:nvoice No\.\s*(\d+)', content)
        sale_date = re.search(r'Sale Order Date\s*:([^\n]+)', content)
        grade = re.search(r'Grade\s*:([^\n]+)', content)
        gcv = re.search(r'GCV\s*:([^\n]+)', content)
        size = re.search(r'Size\s*:([^\n]+)', content)
        dispatch_date = re.search(r'Dispatch date\s*:([^\n]+)', content)
        plant_quantity_pattern =  r"(\d{10})\s+([\w\s&]+)\s+(\d{10})\s+(\w+)\s+Coal\s+\d+\s+TE\s+(\d+\.\d{2})"
        plant_quantity_match = re.search(plant_quantity_pattern, content)
        if plant_quantity_match:
            delivery_plant_name = plant_quantity_match.group(2).strip()
            delivery_quantity = plant_quantity_match.group(5)
        else:
            delivery_plant_name = 'Not found'
            delivery_quantity = 'Not found'

        # Extract particulars
        particulars_start = content.find('PARTICULARS')
        particulars_end = content.find('Remarks/Note/ Declaration')
        particulars_section = content[particulars_start:particulars_end]
        particulars_matches = re.findall(r'(.+?)\s+([\d.]+)\s+([\d.]+)', particulars_section)    
        particulars = {
            "particulars": {"description":{match[0].strip():
                {
                    "rate": match[1],
                    "amount": match[2]
                } 
            for match in particulars_matches},
            "total": {
                "amount": sum(float(match[2]) for match in particulars_matches)
            }
        }}
        # Extract dispatch summary
        plant_pattern = r"Plant\s*:\s*(\d+)"
        plant_name_pattern = r"Plant Name:\s*(.*)"
        data_pattern = r"(\d+)\s+(.*?)\s+(\d+)\s+(\d{2}-\d{2}-\d{4})\s+(\d+)\s+(.*?)\s+(.*?)\s+([\d.]+)\s+([\d.]+)\s+([\d.]+)"
        total_pattern = r"Total\s+([\d.]+)\s+([\d.]+)\s+([\d.]+)"

        plant = re.search(plant_pattern, content)
        plant_name = re.search(plant_name_pattern, content)
        data_rows = re.findall(data_pattern, content)
        total = re.search(total_pattern, content)
        dispatch_summary = {
            "plant": plant.group(1) if plant else None,
            "plant_name": plant_name.group(1) if plant_name else None,
            "data": [
                {
                    "delivery_doc_no.": row[0],
                    "ship_to_party": row[1],
                    "sales_doc_no": row[2],
                    "dispatch_date_time": row[3],
                    "challan_number": row[4],
                    # "grade_size": row[5],
                    "grade_size": f"{row[5].split('/')[0]}{row[5].split('/')[1]} MM",
                    "truck_number": row[6].split("MM ")[1],
                    "tare_weight": endpoint_to_add_zero_after_dot(row[7]),
                    "gross_weight": endpoint_to_add_zero_after_dot(row[8]),
                    "net_weight": endpoint_to_add_zero_after_dot(row[9])
                } for row in data_rows
            ],
            "total": {
                "tare": total.group(1) if total else None,
                "gross_weight": total.group(2) if total else None,
                "net_weight": total.group(3) if total else None
            }
        }

        tax_invoice={
            'Invoice Date': invoice_date.group(1).strip() if invoice_date else 'Not found',
            'Invoice No': invoice_no.group(1).strip() if invoice_no else 'Not found',
            'Sale Date': sale_date.group(1).strip() if sale_date else 'Not found',
            'Grade': grade.group(1).strip() if grade else 'Not found',
            'GCV': gcv.group(1).strip() if gcv else 'Not found',
            'Size': size.group(1).strip() if size else 'Not found',
            'Dispatch Date': dispatch_date.group(1).strip() if dispatch_date else 'Not found',
            'Plant Name': delivery_plant_name,
            'Quantity': delivery_quantity,
        }
        
        results = {
            "tax_invoice":tax_invoice,
            'Particulars': particulars,
            'Dispatch Summary': dispatch_summary
        }
        return results
    except Exception as e:
        console_logger.debug("----- Sap Excel Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return {"error": str(e)}

# add zero at last after decimal
def endpoint_to_add_zero_after_dot(value):
    try:
        if "." in value:
            # integer_part, fractional_part = value.split(".")
            # formatted_frac = fractional_part.zfill(2)
            # formatted_number = f"{integer_part}.{formatted_frac}"
            formatted_number = f"{float(value):.2f}"
            return formatted_number
        else:
            return value
    except Exception as e:
        console_logger.debug("----- DOt Float Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return {"error": str(e)}

@router.post("/extract/tax/invoice", tags=["Road Map"])
async def endpoint_to_extract_tax_invoice_data(response: Response, file: UploadFile = File(...)):
    try:
        # pdfname starts eg: 14-08-2024 10201.PDF
        if file is None:
            return {"error": "No file Uploaded!"}
        
        contents = await file.read()
        
        if not contents:
            return {"error": "Uploaded file is empty!"}

        if not file.filename.endswith(('.pdf','.PDF')):
            response.status_code = 400
            return {"error": "Uploaded file is not a PDF"}

        date = str(datetime.datetime.now().strftime("%d-%m-%Y"))
        target_directory = f"static_server/gmr_ai/{date}"
        os.umask(0)
        os.makedirs(target_directory, exist_ok=True, mode=0o777)
        file_extension = file.filename.split(".")[-1]
        file_name = f'secl_annexure_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.{file_extension}'
        full_path = os.path.join(os.getcwd(), target_directory, file_name)
        with open(full_path, "wb") as file_object:
            file_object.write(contents)

        fetchInvoiceData = extract_invoice_data_tax_invoice(full_path)

        mainDict = {"api_data": None, "db_data": None}
        list_data = []
        
        for key, value in fetchInvoiceData.items():
            if value.get("data") is not None:
                for single_data in value.get("data"):
                    db_data = {}
                    try:
                        # fetchGmrData = Gmrdata.objects.get(arv_cum_do_number=single_data.get("sales_doc_no"), delivery_challan_number=single_data.get("challan_number"), vehicle_number=single_data.get("truck_number"))
                        fetchGmrData = Gmrdata.objects.get(delivery_challan_number=single_data.get("challan_number"))
                        db_data["object_id"] = str(fetchGmrData.id)
                        db_data["sales_doc_no"] = fetchGmrData.arv_cum_do_number
                        datesplit = fetchGmrData.delivery_challan_date.split("-")
                        date_form = f"{datesplit[0].zfill(2)}-{datesplit[1].zfill(2)}-{datesplit[2]}"
                        db_data["dispatch_date_time"] = date_form
                        db_data["challan_number"] = fetchGmrData.delivery_challan_number
                        db_data["grade_size"] = fetchGmrData.grade
                        db_data["truck_number"] = fetchGmrData.vehicle_number
                        db_data["tare_weight"] = endpoint_to_add_zero_after_dot(fetchGmrData.tare_qty)
                        db_data["gross_weight"] = endpoint_to_add_zero_after_dot(fetchGmrData.gross_qty)
                        db_data["net_weight"] = endpoint_to_add_zero_after_dot(fetchGmrData.net_qty)
                        db_data["actual_gross_qty"] = fetchGmrData.actual_gross_qty
                        db_data["actual_tare_qty"] = fetchGmrData.actual_tare_qty
                        db_data["actual_net_qty"] = fetchGmrData.actual_net_qty
                        db_data["type_consumer"] = fetchGmrData.type_consumer
                        list_data.append(db_data)
                    except DoesNotExist as e:
                        continue
        headerData = {
            "invoice_date": datetime.datetime.strptime(fetchInvoiceData.get("tax_invoice").get("Invoice Date"), "%b %d, %Y").date(),
            "invoice_no": fetchInvoiceData.get("tax_invoice").get("Invoice No"),
            "sale_date": datetime.datetime.strptime(fetchInvoiceData.get("tax_invoice").get("Sale Date"), "%b %d, %Y").date(),
            "grade": f'{fetchInvoiceData.get("tax_invoice").get("Grade")}{fetchInvoiceData.get("tax_invoice").get("Size")}',
            "dispatch_date": datetime.datetime.strptime(fetchInvoiceData.get("tax_invoice").get("Dispatch Date"), "%b %d, %Y").date(),
            "mine": fetchInvoiceData.get("tax_invoice").get("Plant Name"),
            "do_qty": fetchInvoiceData.get("tax_invoice").get("Quantity"),
        }

        particulars = {
            "basic_price": fetchInvoiceData.get("Particulars").get("particulars").get("description").get("Basic Price").get("amount"),
            "sizing_charges": fetchInvoiceData.get("Particulars").get("particulars").get("description").get("Sizing Charges").get("amount"),
            "stc_charges": fetchInvoiceData.get("Particulars").get("particulars").get("description").get("STC Charges").get("amount"),
            "evac_facility_charge": fetchInvoiceData.get("Particulars").get("particulars").get("description").get("Evac Facility Charge").get("amount"),
            "royalty_charges": fetchInvoiceData.get("Particulars").get("particulars").get("description").get("Royalty Charges ( 14% of Basic Price)").get("amount"),
            "nmet_charges": fetchInvoiceData.get("Particulars").get("particulars").get("description").get("NMET Charges( 2% of Royalty)").get("amount"),
            "imf": fetchInvoiceData.get("Particulars").get("particulars").get("description").get("DMF( 30% of Royalty)").get("amount"),
            "cgst": fetchInvoiceData.get("Particulars").get("particulars").get("description").get("CGST( 2.5% )").get("amount"),
            "sgst": fetchInvoiceData.get("Particulars").get("particulars").get("description").get("SGST( 2.5% )").get("amount"),
            "gst_comp_cess": fetchInvoiceData.get("Particulars").get("particulars").get("description").get("GST Comp Cess").get("amount"),
            "gross_bill_value": fetchInvoiceData.get("Particulars").get("particulars").get("description").get("Gross Bill Value").get("amount"),
            "net_value": fetchInvoiceData.get("Particulars").get("particulars").get("description").get("Net Value").get("amount"),
            "total_amount": fetchInvoiceData.get("Particulars").get("particulars").get("total").get("amount"),
        }

        mainDict["top_header"] = headerData
        mainDict["api_data"] = fetchInvoiceData.get("Dispatch Summary").get("data")
        mainDict["db_data"] = list_data
        mainDict["particulars"] = particulars
        mainDict["footer_total"] = fetchInvoiceData.get("Dispatch Summary").get("total")

        return mainDict
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- GRN Tax Invoice Excel Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return {"error": str(e)} 


@router.get("/fetch/gmrdata/dcdate", tags=["Road Map"])
async def endpoint_to_fetch_gmrdata_dcdate(response: Response, dc_date: str):
    try:
        listData = []
        dc_date = datetime.datetime.strptime(dc_date, "%d-%m-%Y").strftime("%-d-%-m-%Y")
        fetchGmrData = Gmrdata.objects(delivery_challan_date=dc_date)
        if fetchGmrData:
            for single_data in fetchGmrData:
                dictData = single_data.payload()
                if single_data.delivery_challan_date:
                    datesplit = single_data.delivery_challan_date.split("-")
                    date_form = f"{datesplit[0].zfill(2)}-{datesplit[1].zfill(2)}-{datesplit[2]}"
                    dictData.update({'DC_Date': date_form})
                listData.append(dictData)
            return listData
        else:
            raise HTTPException(status_code=404, detail="No data found")
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- GmrData Dc Date Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return {"error": str(e)}


@router.post("/insertGrn", tags=["Grn Status"])
def endpoint_to_insert_grn(response: Response, data: grnStatus):
    try:
        payloadData = data.dict()
        grnData(
            invoice_data=payloadData.get("invoice_date"),
            invoice_no = payloadData.get("invoice_no"),
            sale_date = payloadData.get("sale_date"),
            grade= payloadData.get("grade"),
            dispatch_date = payloadData.get("dispatch_date"),
            mine = payloadData.get("mine"),
            do_qty = payloadData.get("do_qty"),
        ).save()
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- Sap Excel Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return {"error": str(e)}

    
@router.post("/creategrnfile", tags=["Grn Status"])
def endpoint_to_create_grnfile(data: GrnFileData, invoice_no: Optional[str] = None):
    try:
        console_logger.debug("grn file creation")
        payloadData = data.dict()
        truck_count = len(payloadData.get("table_data"))
        fetchsapRecordsData = SapRecords.objects.get(do_no=payloadData.get("do_no"))
        lot_no = []
        accepted_qty = []
        for single_data_fetch in payloadData.get("table_data"):
            try:
                fetchGmrData = Gmrdata.objects.get(delivery_challan_number=single_data_fetch.get("challan_number"))
                lot_no.append(fetchGmrData.lot)
                if fetchGmrData.actual_net_qty:
                    # accepted_qty.append(round(float(fetchGmrData.actual_net_qty), 2))
                    accepted_qty.append(round(float(fetchGmrData.actual_net_qty), 2))
            except DoesNotExist as e:
                pass
        getCountNetweight = sum(float(single_data.get("net_weight")) for single_data in payloadData.get("table_data"))
        file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
        # target_directory = f"static_server/gmr_ai/{file}"
        target_directory = f"sap_ftp/GRN"
        file_name = f'Challan Invoice No_{invoice_no}.txt'
        os.umask(0)
        os.makedirs(target_directory, exist_ok=True, mode=0o777)
        full_path = os.path.join(os.getcwd(), target_directory, file_name)

        with open(full_path, 'a') as f:
            f.write(f"PO number|PO Item no|Invoice Date|GRN Posting Date|Challan/ Invoice No.|Mine Name|Header Text|Material code|Material Description|Valuation Type|quantity  as per Challan|Accepted Qty.|Plant code|storage location|Transporter name\n")
            f.write(f"""{fetchsapRecordsData.sap_po}|{"{:05d}".format(int(fetchsapRecordsData.line_item.strip()))}|{datetime.datetime.strptime(payloadData.get('invoice_date'), "%Y-%m-%d").strftime("%d.%m.%Y")}|{datetime.datetime.strptime(payloadData.get('dc_date'), "%Y-%m-%d").strftime("%d.%m.%Y")}|{payloadData.get('invoice_no')}|{fetchsapRecordsData.mine_name}|DO{payloadData.get('do_no')}{fetchsapRecordsData.mine_name}{truck_count}TLOT-01|{fetchsapRecordsData.material_code if fetchsapRecordsData.material_code else ' '}|{fetchsapRecordsData.material_description if fetchsapRecordsData.material_description else ' '}|{fetchsapRecordsData.valuation_type if fetchsapRecordsData.valuation_type else ' '}|{round(getCountNetweight, 2)}|{round(sum(accepted_qty), 2)}|{fetchsapRecordsData.plant_code if fetchsapRecordsData.plant_code else ' '}|{fetchsapRecordsData.storage_location if fetchsapRecordsData.storage_location else ' '}|{fetchsapRecordsData.transport_name if fetchsapRecordsData.transport_name else ' '}\n""")
        
        for single_data_inside in payloadData.get("table_data"):
            Gmrdata.objects(arv_cum_do_number=single_data_inside.get("sales_doc_no"), delivery_challan_number=single_data_inside.get("challan_number")).update(grn_status=True, mine_invoice=payloadData.get("invoice_no"))

        return full_path
    except Exception as e:
        console_logger.debug("----- Sap Excel Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return {"error": str(e)}

@router.post("/creategrnfilerail", tags=["Grn Status"])
def endpoint_to_create_grnfile(data: GrnFileDataRail, invoice_no: Optional[str] = None):
    try:
        payloadData = data.dict()
        truck_count = len(payloadData.get("table_data"))
        fetchsapRecordsData = sapRecordsRail.objects.get(rr_no=payloadData.get("do_no"))
        # lot_no = []
        accepted_qty = []
        fetchRailData = RailData.objects.get(rr_no=payloadData.get("do_no"))
        if fetchRailData.rake_no:
            if "Rev" in fetchRailData.rake_no:
                split_rakeno = fetchRailData.rake_no.split("-")
                split_rake = int(split_rakeno[1])
            else:
                split_rake = int(fetchRailData.rake_no)
        else:
            split_rake = 0
        for single_data_fetch in payloadData.get("table_data"):
            if single_data_fetch.get("gwel_net_wt"):
                accepted_qty.append(round(float(single_data_fetch.get("gwel_net_wt")), 2))
        rake_no = split_rake
        getCountNetweight = sum(float(single_data.secl_net_wt) for single_data in fetchRailData.secl_rly_data)
        file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
        target_directory = f"sap_ftp/GRN"
        file_name = f'Challan Invoice No_{invoice_no}.txt'
        os.umask(0)
        os.makedirs(target_directory, exist_ok=True, mode=0o777)
        full_path = os.path.join(os.getcwd(), target_directory, file_name)

        # filename = f'{target_directory}/challan_invoice_no_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S")}.txt'
        with open(full_path, 'a') as f:
            f.write(f"PO number|PO Item no|Invoice Date|GRN Posting Date|Challan/ Invoice No.|Mine Name|Header Text|Material code|Material Description|Valuation Type|quantity  as per Challan|Accepted Qty.|Plant code|storage location|Transporter name\n")
            f.write(f"""{fetchsapRecordsData.sap_po}|{"{:05d}".format(int(fetchsapRecordsData.line_item.strip())) if fetchsapRecordsData.line_item else " "}|{datetime.datetime.strptime(payloadData.get('invoice_date'), "%Y-%m-%d").strftime("%d.%m.%Y")}|{datetime.datetime.strptime(payloadData.get('dc_date'), "%Y-%m-%d").strftime("%d.%m.%Y")}|{payloadData.get('invoice_no')}|{fetchsapRecordsData.mine}|RR{payloadData.get('do_no')}{fetchsapRecordsData.mine}{truck_count}TWGRACK-{rake_no}|{fetchsapRecordsData.material_code if fetchsapRecordsData.material_code else ' '}|{fetchsapRecordsData.material_description if fetchsapRecordsData.material_description else ' '}|{fetchsapRecordsData.valuation_type if fetchsapRecordsData.valuation_type else ' '}|{round(getCountNetweight, 2)}|{round(sum(accepted_qty), 2)}|{fetchsapRecordsData.plant_code if fetchsapRecordsData.plant_code else ' '}|{fetchsapRecordsData.storage_location if fetchsapRecordsData.storage_location else ' '}|{fetchsapRecordsData.transport_name if fetchsapRecordsData.transport_name else ' '}\n""")
        
        RailData.objects.get(rr_no=payloadData.get('do_no')).update(grn_status=True, mine_invoice=payloadData.get("invoice_no"))

        return full_path
    except Exception as e:
        console_logger.debug("----- Sap Excel Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return {"error": str(e)}


@router.post("/update/gmrdata/taxinvoice", tags=["Road Map"])
def endpoint_to_update_gmrdata_taxinvoice(response: Response, data: taxInvoiceGmr):
    try:
        invoiceSerializer = data.dict()
        if invoiceSerializer.get("id"):
            dc_date = datetime.datetime.strptime(invoiceSerializer.get("dc_date"), "%d-%m-%Y").strftime("%-d-%-m-%Y")
            if "MM " in invoiceSerializer.get("truck_no"):
                truck_no = invoiceSerializer.get("truck_no").split("MM ")[1]
            else:
                truck_no = invoiceSerializer.get("truck_no")
            Gmrdata.objects(record_id=invoiceSerializer.get("id")).update(arv_cum_do_number=invoiceSerializer.get("do_no"), 
                                                  delivery_challan_date=dc_date,
                                                  delivery_challan_number=invoiceSerializer.get("challan_no"),
                                                  grade=invoiceSerializer.get("grade").replace(r'/',''),
                                                  vehicle_number=truck_no,
                                                  tare_qty=invoiceSerializer.get("tare"),
                                                  gross_qty=invoiceSerializer.get("gross"),
                                                  net_qty=invoiceSerializer.get("net"), 
                                                  grn_status=True, 
                                                  mine_invoice=invoiceSerializer.get("invoice_no"))
        return {"detail": "success"}
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- Sap Excel Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return {"error": str(e)}

  
@router.post("/saprecordsrcr", tags=["Rail Map"])
async def endpoint_to_upload_rail_data(response: Response, pdf_upload: List[UploadFile] = File(...)):
    try:
        for UploadedFile in pdf_upload:

            f_name = UploadedFile.filename
            contents = UploadedFile.file
            # if pdf_upload is None:
            #     return {"error": "No file uploaded"}
            # contents = await pdf_upload.read()

            if not contents:
                return {"error": "Uploaded file is empty"}

            if not f_name.endswith(('.pdf','.PDF')):
                return {"error": "Uploaded file is not a PDF"}
            
            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            file_extension = f_name.split(".")[-1]
            file_name = f'sap_rail_upload_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.{file_extension}'
            full_path = os.path.join(os.getcwd(), target_directory, file_name)
            with open(full_path, "wb") as file_object:
                # file_object.write(contents)
                shutil.copyfileobj(contents, file_object)

            fetchRailData = extract_fields_rail(full_path)

    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- Sap Excel Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return {"error": str(e)}


@router.get("/logistics/road/cards", tags=["Road Map"])
def enpoint_to_fetch_logistics_road_cards(
    response: Response,
    show_type: Optional[str] = "Daily",
    Month: Optional[str] = None,
    Daily: Optional[str] = None,
    Year: Optional[str] = None
):
    """
    Function that fetches counts from logistics road for multiple cards  
    Parameters:
        show_type: Daily, Week, Month, Year |
        Year: Single Year, for eg: 2024, 2023 |
        Month: Single Month, for eg: 2024-11, 2024-10 |
        Week: No Input |
        Daily: Single Date, for eg: 2024-11-11

    Returns:
        List of dictionaries for multiple cards containing counts
    """
    try:
        finalList = []
        current_time = datetime.datetime.now(IST)
        
        # Set from_ts based on show_type
        if show_type == "Daily":
            date=Daily
            # today = current_time.date()
            from_ts = convert_to_utc_format(f"{date} 00:00:00", "%Y-%m-%d %H:%M:%S")
            to_ts = convert_to_utc_format(f"{date} 23:59:59", "%Y-%m-%d %H:%M:%S")
        elif show_type == "Week":
            start_date = current_time - timedelta(days=7)
            today = current_time.date()
            from_ts = convert_to_utc_format(start_date.strftime("%Y-%m-%d 00:00:00"), "%Y-%m-%d %H:%M:%S")
            to_ts = convert_to_utc_format(today.strftime("%Y-%m-%d 00:00:00"), "%Y-%m-%d %H:%M:%S")
        elif show_type == "Month":
            date=Month
            start_date = f'{date}-01'
            format_data = "%Y-%m-%d"
            startd_date=datetime.datetime.strptime(start_date, format_data)
            end_date = startd_date + relativedelta(day=31)
            end_date.replace(hour=23, minute=59, second=0, microsecond=0)
            endd_date = end_date.replace(hour=23, minute=59, second=0, microsecond=0)
            # end_date = (start_date + timedelta(days=32)).replace(day=1) - timedelta(days=1)
            from_ts = convert_to_utc_format(f"{startd_date}", "%Y-%m-%d %H:%M:%S")
            to_ts = convert_to_utc_format(f"{endd_date}", "%Y-%m-%d %H:%M:%S")
        elif show_type == "Year":
            date=Year
            # start_date = datetime(Year, 1, 1)
            # end_date = datetime(Year, 12, 31)
            end_date =f'{date}-12-31 23:59:59'
            start_date = f'{date}-01-01 00:00:00'
            from_ts = convert_to_utc_format(start_date, "%Y-%m-%d %H:%M:%S")
            to_ts = convert_to_utc_format(end_date, "%Y-%m-%d %H:%M:%S")
        else:
            response.status_code = 400
            return {"error": "Invalid parameters"}

        # Fetch vehicle counts and append to finalList
        vehicle_count = Gmrdata.objects(GWEL_Tare_Time__ne=None, GWEL_Tare_Time__gte=from_ts, GWEL_Tare_Time__lte=to_ts).count()
        finalList.append({
            "title": "Today's Mine Vehicle Scanned",
            "icon": "vehicle",
            "data": vehicle_count,
            "last_updated": current_time.date()
        })

        # Fetch vehicle in/out counts
        vehicle_in_count = Gmrdata.objects(GWEL_Tare_Time__ne=None, GWEL_Tare_Time__gte=from_ts, GWEL_Tare_Time__lte=to_ts).count()
        vehicle_out_count = Gmrdata.objects(GWEL_Tare_Time__ne=None, GWEL_Tare_Time__gte=from_ts, GWEL_Tare_Time__lte=to_ts).count()
        finalList.append({
            "title": "Gate Vehicle",
            "icon": "vehicle",
            "data": f"In: {vehicle_in_count} | Out: {vehicle_out_count}",
            "last_updated": current_time.date()
        })

        # GRN coal count
        pipeline_grn = [
            {"$match": {"GWEL_Tare_Time": {"$gte": from_ts, "$lte": to_ts}, "net_qty": {"$ne": None}}},
            {"$group": {"_id": None, "total_net_qty": {"$sum": {"$toDouble": "$net_qty"}}}}
        ]
        result = Gmrdata.objects.aggregate(pipeline_grn)
        total_coal = sum(doc["total_net_qty"] for doc in result)
        finalList.append({
            "title": "Total GRN Coal(MT)",
            "icon": "coal",
            "data": round(total_coal, 2),
            "last_updated": current_time.date()
        })

        # GWEL coal count
        pipeline_gwel = [
            {"$match": {"GWEL_Tare_Time": {"$gte": from_ts, "$lte": to_ts}, "actual_net_qty": {"$ne": None}}},
            {"$group": {"_id": None, "total_actual_net_qty": {"$sum": {"$toDouble": "$actual_net_qty"}}}}
        ]
        result = Gmrdata.objects.aggregate(pipeline_gwel)
        total_actual_coal = sum(doc["total_actual_net_qty"] for doc in result)
        finalList.append({
            "title": "Total GWEL Coal(MT)",
            "icon": "coal",
            "data": round(total_actual_coal, 2),
            "last_updated": current_time.date()
        })

        # Transit loss count
        pipeline_loss = [
            {"$match": {"GWEL_Tare_Time": {"$gte": from_ts, "$lte": to_ts}}},
            {"$group": {
                "_id": None,
                "net_qty": {"$sum": {"$toDouble": "$net_qty"}},
                "actual_net_qty": {"$sum": {"$toDouble": "$actual_net_qty"}}
            }},
            {"$project": {
                "net_qty": 1,
                "actual_net_qty": 1,
                "transit_loss": {"$subtract": ["$actual_net_qty", "$net_qty"]}
            }}
        ]
        result = Gmrdata.objects.aggregate(pipeline_loss)
        transit_loss = sum(doc["transit_loss"] for doc in result)
        finalList.append({
            "title": "Total Transit Loss (MT)",
            "icon": "coal",
            "data": round(transit_loss, 2),
            "last_updated": current_time.date()
        })

        return finalList

    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug(f"Error {e} on line {sys.exc_info()[-1].tb_lineno}")
        return {"error": str(e)}


@router.delete("/delete/aoptarget/name", tags=["Aop Target"])
def endpoint_to_delete_aoptarget(response: Response, consumer_type:str):
    try:
        checkAopTarget= AopTarget.objects.get(source_name=consumer_type)
        if checkAopTarget:
            checkAopTarget.delete()
        return {"detail": "success"}
    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug(f"Error {e} on line {sys.exc_info()[-1].tb_lineno}")
        return {"error": str(e)}


@router.post("/insert/roadjourneyconsumertype", tags=["Road Map"])
def endpoint_to_insert_roadjourneyconsumertype(response: Response, consumer_type: roadConsumertype):
    try:
        data = consumer_type.dict()
        if not data.get("roadConsumertype"):
            fetchroadTypeconsumer = roadjourneyconsumertype.objects.first()
            list_struct = []
            fetchroadTypeconsumer.consumer_type = list_struct
            fetchroadTypeconsumer.save()
        listData = []
        if consumer_type:
            fetchroadTypeconsumer = roadjourneyconsumertype.objects.first()  # Use .first() for a single document

            if fetchroadTypeconsumer:
                # fetchroadTypeconsumer.consumer_type.append(consumer_type)
                # fetchroadTypeconsumer.save()
                fetchroadTypeconsumer.consumer_type.clear()
            # else:
                fetchroadTypeconsumer.consumer_type=data.get("roadConsumertype")
                fetchroadTypeconsumer.save()
            else:
                addroadTypeconsumer = roadjourneyconsumertype(consumer_type=data.get("roadConsumertype"))
                addroadTypeconsumer.save()
        return {"detail": "success"}
    except Exception as e:
        console_logger.debug("----- Add Consumer Type Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/fetch/consumertype", tags=["Road Map"])
def endpoint_to_fetch_consumer_type_data(response: Response):
    try:
        fetchConsumerType = roadjourneyconsumertype.objects.first()
        return fetchConsumerType.consumer_type
    except Exception as e:
        console_logger.debug("----- consumer type Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/v2/coal_analysis", tags=["Con/Gen V2"])
def coal_analysis(
    response: Response,
    type: Optional[str] = "Daily",
    Month: Optional[str] = None,
    Daily: Optional[str] = None,
    Year: Optional[str] = None
):
    try:
        graph = global_coal_analysis(
                        type = type,
                        Month = Month,
                        Daily = Daily,
                        Year = Year)

        return graph
        
    except Exception as e:
        response.status_code = 400
        console_logger.debug(f"-----  Global Coal Analysis Error ----- {e}")
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/logistics/summary", tags=["Summary"])
def summary_analysis(
    response: Response,
    type: Optional[str] = "Daily",
    Month: Optional[str] = None,
    Daily: Optional[str] = None,
    Year: Optional[str] = None
):
    try:
        card_data = summary_info_card(
                        type = type,
                        Month = Month,
                        Daily = Daily,
                        Year = Year)

        coal_graph = summary_caol_receipt_graph(
                        type = type,
                        Month = Month,
                        Daily = Daily,
                        Year = Year)
        
        road_graph = summary_road_detail_graph(
                        type = type,
                        Month = Month,
                        Daily = Daily,
                        Year = Year)

        return {"cards": card_data,
                "coal_receipt_graph": coal_graph,
                "road_detail_graph": road_graph
                }
        
    except Exception as e:
        response.status_code = 400
        console_logger.debug(f"-----  Summary Error ----- {e}")
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/add/sap/rcr/road/excel", tags=["Coal Testing"])
async def endpoint_to_add_sap_road_excel_data(response: Response, file: UploadFile = File(...)):
    try:
        if file is None:
            return {"error": "No file Uploaded!"}
        
        contents = await file.read()
        if not contents:
            response.status_code = 400
            return {"error": "Uploaded file is empty!"}

        if file.filename.endswith(".xlsx"):
            # file saving start
            date = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{date}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)
            file_extension = file.filename.split(".")[-1]
            file_name = f'coallab_sap_rcr_road_manual_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.{file_extension}'
            full_path = os.path.join(os.getcwd(), target_directory, file_name)
            with open(full_path, "wb") as file_object:
                file_object.write(contents)
            # file saving end

            excel_data = pd.read_excel(BytesIO(contents))
            data_excel_fetch = json.loads(excel_data.to_json(orient="records"))
            for single_data in data_excel_fetch:
                console_logger.debug(single_data)
                try:
                    fetchSapRecords = SapRecordsRcr.objects.get(do_no=str(single_data["DO No"]))
                except DoesNotExist as e:
                    add_data_excel = SapRecordsRcr(
                        # slno=single_data["Slno"] if single_data["Slno"] else None,
                        # source=single_data["source"],
                        # mine_name=single_data["Mines Name"],
                        sap_po=str(single_data["SAP PO"]) if single_data["SAP PO"] else None,
                        do_date=str(single_data["SAP PO Date"]) if single_data["SAP PO Date"] else None,
                        line_item=str(single_data["Line Item"]) if single_data["Line Item"] else None,
                        do_no=str(single_data["DO No"]) if single_data["DO No"] else None,
                        # do_qty=str(single_data["DO QTY"]),
                        # rake_no=single_data["DO/RR Qty"],
                        # start_date=single_data["DO Start Date"],
                        # end_date=single_data["DO End Date"],
                        # grade=single_data["Grade"]
                    )
                    add_data_excel.save()

                # # take it here
                fetchGmrData = Gmrdata.objects(arv_cum_do_number = str(single_data["DO No"]))
                for single_gmr_data in fetchGmrData:
                    single_gmr_data.po_no = str(single_data["SAP PO"]) if single_data["SAP PO"] else None
                    single_gmr_data.line_item = str(single_data["Line Item"]) if single_data["Line Item"] else None
                    single_gmr_data.po_date = str(single_data["SAP PO Date"]) if single_data["SAP PO Date"] else None
                    # single_gmr_data.slno = str(single_data["Slno"]) if single_data["Slno"] else None
                    single_gmr_data.save()

        return {"detail": "success"}
    except KeyError as e:
        raise HTTPException(status_code=404, detail="Key Error")
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- Sap Excel Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/getcompletedcountroad", tags=["Road Map"])
def endpoint_to_get_completed_list(response: Response, currentPage: Optional[int] = None, perPage: Optional[int] = None, start_date: Optional[str] = None, end_date: Optional[str] = None):
    try:
        listData = []
        dictData = {}
        result = {        
                "labels": [],
                "datasets": [],
                "total" : 0,
                "page_size": 15
        }
        page_no = 1
        page_len = result["page_size"]

        if currentPage:
            page_no = currentPage

        if perPage:
            page_len = perPage
            result["page_size"] = perPage
        fetchpendingDataGmr = Gmrdata.objects(actual_net_qty__ne=None, created_at__gte=start_date, created_at__lte=end_date)
        for single_data in fetchpendingDataGmr:
        # if fetchpendingDataGmr:
            # listData.append(single_data.payload())
            result["labels"] = list(single_data.payload().keys())
            result["datasets"].append(single_data.payload())
        result["total"]= len(Gmrdata.objects(actual_net_qty__ne=None, created_at__gte=start_date, created_at__lte=end_date))
        return result
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- getpendingcountroad Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/getpendingcountroad", tags=["Road Map"])
def endpoint_to_get_pending_completed_list(response: Response, currentPage: Optional[int] = None, perPage: Optional[int] = None, start_date: Optional[str] = None, end_date: Optional[str] = None):
    try:
        listData = []
        dictData = {}
        result = {        
                "labels": [],
                "datasets": [],
                "total" : 0,
                "page_size": 15
        }
        page_no = 1
        page_len = result["page_size"]

        if currentPage:
            page_no = currentPage

        if perPage:
            page_len = perPage
            result["page_size"] = perPage
        offset = (page_no - 1) * page_len
        fetchpendingDataGmr = Gmrdata.objects(actual_net_qty=None, created_at__gte=start_date, created_at__lte=end_date).skip(offset).limit(page_len)
        for single_data in fetchpendingDataGmr:
        # if fetchpendingDataGmr:
            # listData.append(single_data.payload())
            result["labels"] = list(single_data.payload().keys())
            result["datasets"].append(single_data.payload())
        result["total"]= len(Gmrdata.objects(actual_net_qty=None, created_at__gte=start_date, created_at__lte=end_date))
        return result
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- Sap Excel Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/coaldata", tags=["Form 15"])
def create_coal_data(response: Response,
                     data: CoalDataModel):
    try:
        data.osd_month.particular = "Opening stock of coal as on 1st Day of the Month"
        data.vos_month.particular = "Value of opening stock as on 1st Day of the Month"
        data.qty_supplied.particular = "Quantity of Coal/Lignite supplied by Coal/Lignite Company"
        data.adj_qty.particular = "Adjustment (+/-) in quantity supplied made by Coal/Lignite Company"
        data.coal_supplied.particular = "Coal Supplied by Coal Lignite company (3+4)"
        data.norm_transit_loss.particular = "Normative Transit & Handling Losses"
        data.net_supplied.particular = "Net Coal/Lignite Supplied (5-6)"
        data.amt_charged.particular = "Amount charged by the Coal/Lignite Company"
        data.adj_amt.particular = "Adjustments (+/-) in amount charged by Coal/Lignite Company"
        data.unloading_charges.particular = "Unloading, Sampling Charges, AMM etc"
        data.total_amt_charged.particular = "Total amount Charged (8+9+10)"
        data.trans_charges.particular = "Transportation charges by Rail/Ship/Road"
        data.adj_trans_charges.particular = "Adjustment (+/-) in amount charged by railway transport"
        data.demurrage.particular = "Demurrage Charge, if any"
        data.diesel_cost.particular = "Cost of diesel in transporting coal"
        data.total_trans_charges.particular = "Total transportation charges (12+/-13+14+15)"

        data.total_amt_incl_trans.particular = "Total amount charged for Coal/lignite including transportation (11+16)"
        data.qty_at_station.particular = "Quantity of coal at station for the month (1+7)"
        data.total_amt_for_coal.particular = "Total amount charged for coal (2+17)"
        data.landed_cost.particular = "Landed cost of coal (19/18)"
        data.qty_consumed.particular = "Coal Quantity consumed"
        data.value_consumed.particular = "Value of coal Consumed (20*21)"
        data.wtd_avg_gcv_prev.particular = "Weighted average GCV with previous month's coal"
        data.wtd_avg_gcv_recv.particular = "Wtd. Average as received GCV"
        data.wtd_avg_gcv_less_85.particular = "Weighted Average GCV of caol as received"
        data.closing_coal_stock.particular = "Closing stock of coal as on last Day of the Month"
        data.closing_coal_stock_value.particular = "Value of Closing stock as on  last Day of the Month"

        try:
            split_date = str(data.month).split(" ")
            split_part_date = split_date[0].split("-")
            start_date = f"{split_part_date[0]}-{split_part_date[1]}-01"
            end_date = (datetime.datetime.strptime(start_date, "%Y-%m-%d") + relativedelta(day=31)).strftime("%Y-%m-%d")
            fetchform15data = Form15Data.objects.get(month__gte=start_date, month__lte=end_date)
            fetchform15data.update(
                osd_month = data.osd_month.dict(exclude_unset=True),
                vos_month = data.vos_month.dict(exclude_unset=True),
                qty_supplied = data.qty_supplied.dict(exclude_unset=True),
                adj_qty = data.adj_qty.dict(exclude_unset=True),
                coal_supplied = data.coal_supplied.dict(exclude_unset=True),
                norm_transit_loss = data.norm_transit_loss.dict(exclude_unset=True),
                net_supplied = data.net_supplied.dict(exclude_unset=True),
                amt_charged = data.amt_charged.dict(exclude_unset=True),
                adj_amt = data.adj_amt.dict(exclude_unset=True),
                unloading_charges = data.unloading_charges.dict(exclude_unset=True),
                total_amt_charged = data.total_amt_charged.dict(exclude_unset=True),
                trans_charges = data.trans_charges.dict(exclude_unset=True),
                adj_trans_charges = data.adj_trans_charges.dict(exclude_unset=True),
                demurrage = data.demurrage.dict(exclude_unset=True),
                diesel_cost = data.diesel_cost.dict(exclude_unset=True),
                total_trans_charges = data.total_trans_charges.dict(exclude_unset=True),
                total_amt_incl_trans = data.total_amt_incl_trans.dict(exclude_unset=True),
                qty_at_station = data.qty_at_station.dict(exclude_unset=True),
                total_amt_for_coal = data.total_amt_for_coal.dict(exclude_unset=True),
                landed_cost = data.landed_cost.dict(exclude_unset=True),
                qty_consumed = data.qty_consumed.dict(exclude_unset=True),
                value_consumed = data.value_consumed.dict(exclude_unset=True),
                wtd_avg_gcv_prev = data.wtd_avg_gcv_prev.dict(exclude_unset=True),
                wtd_avg_gcv_recv = data.wtd_avg_gcv_recv.dict(exclude_unset=True),
                wtd_avg_gcv_less_85 = data.wtd_avg_gcv_less_85.dict(exclude_unset=True),
                closing_coal_stock = data.closing_coal_stock.dict(exclude_unset=True),
                closing_coal_stock_value = data.closing_coal_stock_value.dict(exclude_unset=True), 
            )
        except DoesNotExist as e:
            coal_data = Form15Data(
                osd_month = data.osd_month.dict(exclude_unset=True),
                vos_month = data.vos_month.dict(exclude_unset=True),
                qty_supplied = data.qty_supplied.dict(exclude_unset=True),
                adj_qty = data.adj_qty.dict(exclude_unset=True),
                coal_supplied = data.coal_supplied.dict(exclude_unset=True),
                norm_transit_loss = data.norm_transit_loss.dict(exclude_unset=True),
                net_supplied = data.net_supplied.dict(exclude_unset=True),
                amt_charged = data.amt_charged.dict(exclude_unset=True),
                adj_amt = data.adj_amt.dict(exclude_unset=True),
                unloading_charges = data.unloading_charges.dict(exclude_unset=True),
                total_amt_charged = data.total_amt_charged.dict(exclude_unset=True),
                trans_charges = data.trans_charges.dict(exclude_unset=True),
                adj_trans_charges = data.adj_trans_charges.dict(exclude_unset=True),
                demurrage = data.demurrage.dict(exclude_unset=True),
                diesel_cost = data.diesel_cost.dict(exclude_unset=True),
                total_trans_charges = data.total_trans_charges.dict(exclude_unset=True),
                total_amt_incl_trans = data.total_amt_incl_trans.dict(exclude_unset=True),
                qty_at_station = data.qty_at_station.dict(exclude_unset=True),
                total_amt_for_coal = data.total_amt_for_coal.dict(exclude_unset=True),
                landed_cost = data.landed_cost.dict(exclude_unset=True),
                qty_consumed = data.qty_consumed.dict(exclude_unset=True),
                value_consumed = data.value_consumed.dict(exclude_unset=True),
                wtd_avg_gcv_prev = data.wtd_avg_gcv_prev.dict(exclude_unset=True),
                wtd_avg_gcv_recv = data.wtd_avg_gcv_recv.dict(exclude_unset=True),
                wtd_avg_gcv_less_85 = data.wtd_avg_gcv_less_85.dict(exclude_unset=True),
                closing_coal_stock = data.closing_coal_stock.dict(exclude_unset=True),
                closing_coal_stock_value = data.closing_coal_stock_value.dict(exclude_unset=True),  
                month = data.month
            )

            coal_data.save()
        
        return {"Details": "success"}
    
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- Form 15 -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/manual/coaldata", tags=["Form 15"])
def create_manual_coal_data(response: Response,
                     data: CoalDataModelManual):
    try:
        data.osd_month.particular = "Opening stock of coal as on 1st Day of the Month" #1
        data.adj_qty.particular = "Adjustment (+/-) in quantity supplied made by Coal/Lignite Company" #4
        data.norm_transit_loss.particular = "Normative Transit & Handling Losses" #6
        data.amt_charged.particular = "Amount charged by the Coal/Lignite Company" #8
        data.adj_amt.particular = "Adjustments (+/-) in amount charged by Coal/Lignite Company" #9
        data.unloading_charges.particular = "Unloading, Sampling Charges, AMM etc" #10
        data.trans_charges.particular = "Transportation charges by Rail/Ship/Road transportation" #12
        data.adj_trans_charges.particular = "Adjustment (+/-) in amount charged by railway transport" #13
        data.demurrage.particular = "Demurrage Charge, if any" #14
        data.diesel_cost.particular = "Cost of diesel in transporting coal" #15

        data.qty_consumed.particular = "Coal Quantity consumed" #21
        data.wtd_avg_gcv_prev.particular = "Weighted average GCV with previous month's coal" #23
        data.wtd_avg_gcv_recv.particular = "Wtd. Average as received GCV" #24
        data.wtd_avg_gcv_less_85.particular = "Weighted Average GCV of caol as received" #25
        
        try:
            split_date = str(data.month).split(" ")
            split_part_date = split_date[0].split("-")
            start_date = f"{split_part_date[0]}-{split_part_date[1]}-01"
            end_date = (datetime.datetime.strptime(start_date, "%Y-%m-%d") + relativedelta(day=31)).strftime("%Y-%m-%d")
            fetchform15data = Form15Data.objects.get(month__gte=start_date, month__lte=end_date)
            fetchform15data.update(
                osd_month = data.osd_month.dict(exclude_unset=True), #1
                adj_qty = data.adj_qty.dict(exclude_unset=True), #4
                norm_transit_loss = data.norm_transit_loss.dict(exclude_unset=True), #6
                amt_charged = data.amt_charged.dict(exclude_unset=True), #6
                adj_amt = data.adj_amt.dict(exclude_unset=True), #9
                unloading_charges = data.unloading_charges.dict(exclude_unset=True), #10
                trans_charges = data.trans_charges.dict(exclude_unset=True), #10
                adj_trans_charges = data.adj_trans_charges.dict(exclude_unset=True), #13
                demurrage = data.demurrage.dict(exclude_unset=True), #14
                diesel_cost = data.diesel_cost.dict(exclude_unset=True), #15 
                qty_consumed = data.qty_consumed.dict(exclude_unset=True), #21
                wtd_avg_gcv_prev = data.wtd_avg_gcv_prev.dict(exclude_unset=True), #23 
                wtd_avg_gcv_recv = data.wtd_avg_gcv_recv.dict(exclude_unset=True), #24 
                wtd_avg_gcv_less_85 = data.wtd_avg_gcv_less_85.dict(exclude_unset=True) #25 
            )
        except DoesNotExist as e:
            coal_data = Form15Data(
                osd_month = data.osd_month.dict(exclude_unset=True), #1
                adj_qty = data.adj_qty.dict(exclude_unset=True), #4
                norm_transit_loss = data.norm_transit_loss.dict(exclude_unset=True), #6
                amt_charged = data.amt_charged.dict(exclude_unset=True), #6
                adj_amt = data.adj_amt.dict(exclude_unset=True), #9
                unloading_charges = data.unloading_charges.dict(exclude_unset=True), #10
                trans_charges = data.trans_charges.dict(exclude_unset=True), #10
                adj_trans_charges = data.adj_trans_charges.dict(exclude_unset=True), #13
                demurrage = data.demurrage.dict(exclude_unset=True), #14
                diesel_cost = data.diesel_cost.dict(exclude_unset=True), #15 
                qty_consumed = data.qty_consumed.dict(exclude_unset=True), #21
                wtd_avg_gcv_prev = data.wtd_avg_gcv_prev.dict(exclude_unset=True), #23 
                wtd_avg_gcv_recv = data.wtd_avg_gcv_recv.dict(exclude_unset=True), #24 
                wtd_avg_gcv_less_85 = data.wtd_avg_gcv_less_85.dict(exclude_unset=True), #25
                month = data.month
            )
            coal_data.save()
        return {"details": "success"}
    
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- Form 15 Manual -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


def safe_divide(numerator, denominator):
    try:
        return numerator / denominator if denominator else 0
    except (TypeError, ZeroDivisionError):
        return 0

def round_numeric_values(d):
    try:
        for key, value in d.items():
            if isinstance(value, (int, float)):
                d[key] = round(value)
            elif isinstance(value, dict):
                round_numeric_values(value)
    except Exception as e:
        # response.status_code = 400
        console_logger.debug("----- Form 15 Manual -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

@router.get("/form15data", tags=["Form 15"])
def get_form15data(month:Optional[str] = None, display_type: Optional[str] = "display", download_type: Optional[str] = "excel"):
    try:
        start_date = f'{month}-01'
        format_data = "%Y-%m-%d"
        startd_date=datetime.datetime.strptime(start_date, format_data)
        end_date = startd_date + relativedelta(day=31)
        end_date.replace(hour=23, minute=59, second=0, microsecond=0)
        endd_date = end_date.replace(hour=23, minute=59, second=0, microsecond=0)
        from_ts = convert_to_utc_format(f"{startd_date}", "%Y-%m-%d %H:%M:%S")
        to_ts = convert_to_utc_format(f"{endd_date}", "%Y-%m-%d %H:%M:%S")
        if display_type and display_type=="display":
            data = Form15Data.objects(month__gte=from_ts, month__lte=to_ts)
            response = []
            dictData = {}
            for item in data:
                item_dict = json.loads(json_util.dumps(item.to_mongo()))
                item_dict.pop('_id', None)  # Remove _id field
                item_dict.pop('created_at', None)  # Remove created_at field
                dictData["month"] = item_dict.get("month")
                item_dict.pop('month', None)  # Remove created_at field
                round_numeric_values(item_dict)
                response.append(item_dict)
                response.append(dictData)

            return response
        elif display_type and display_type == "download":
            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)
            usecase_data = Form15Data.objects(month__gte=from_ts, month__lte=to_ts)
            logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
            if download_type and download_type == "excel":
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "Form15_Report_{}.xlsx".format(
                            datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format()
                    cell_format2.set_bold()
                    cell_format2.set_font_size(10)
                    cell_format2.set_align("center")
                    cell_format2.set_align("vcenter")
                    cell_format2.set_text_wrap(True)
                    cell_format2.set_border(1)

                    cell_format3 = workbook.add_format()
                    cell_format3.set_bold()
                    cell_format3.set_font_size(10)
                    cell_format3.set_align("center")
                    cell_format3.set_align("vcenter")
                    cell_format3.set_text_wrap(True)
                    cell_format3.set_border(1)

                    header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
                    date_format = workbook.add_format({'align': 'center', 'font_size': 12, "bold": True})
                    date_format1 = workbook.add_format({'align': 'center', 'font_size': 14, "bold": True})
                    report_name_format = workbook.add_format({'align': 'center', 'font_size': 15, "bold": True})
                    footer_name_format1 = workbook.add_format({'align': 'left', 'font_size': 20, "bold": True, 'font_name':'Calibri', 'text_wrap':'true'})
                    footer_name_format2 = workbook.add_format({'align': 'right', 'font_size': 20, "bold": True, 'font_name':'Calibri', 'text_wrap':'true'})
                    footer_name_format3 = workbook.add_format({'align': 'left', 'font_size': 20, "bold": True, 'font_name':'Calibri', 'text_wrap':'true'})
                    footer_name_format4 = workbook.add_format({'align': 'right', 'font_size': 20, "bold": True, 'font_name':'Calibri', 'text_wrap':'true'})

                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)
                    cell_format = workbook.add_format()
                    cell_format.set_font_size(10)
                    cell_format.set_align("center")
                    cell_format.set_align("vcenter")
                    cell_format.set_text_wrap(True)
                    cell_format.set_border(1)

                    cell_format4 = workbook.add_format()
                    cell_format4.set_font_size(10)
                    cell_format4.set_align("center")
                    cell_format4.set_align("vcenter")
                    cell_format4.set_text_wrap(True)
                    cell_format4.set_border(1)

                    worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                    
                    # Merge cells for the main header and place it in the center
                    main_header = "GMR Warora Energy Limited"  # Set your main header text here
                    worksheet.merge_range("A1:O1", main_header, header_format)  # Merge cells A1 to H1 for the header

                    main_header = "Name of Project: Warora Thermal Power Project"  # Set your main header text here
                    worksheet.merge_range("A2:O2", main_header, header_format)  # Merge cells A1 to H1 for the header
                    
                    # Write the current date on the left side (A2)
                    worksheet.merge_range("A3:B3", f"Month: {datetime.datetime.strptime(month, '%Y-%m').strftime('%B, %Y')}", date_format1)
                    worksheet.merge_range("C3:O3", f"Form - 15", report_name_format)

                    headers = [
                        "Sr.No",
                        "Particulars",
                        "Remarks",
                        "UOM",
                        "MoU Coal",
                        "Linkage",
                        "AIWIB Washery",
                        "Open Mkt",
                        "Spot E-auction",
                        "Spl For-E-auction",
                        "Imported",
                        "Total",
                        # "ShaktiB(viii)*",
                        # "Shakti B3"
                    ]

                    headers_shakti = [
                        "ShaktiB(viii)*",
                        "Shakti B3"
                    ]
                    
                    for index, header in enumerate(headers):
                        worksheet.write(3, index, header, cell_format2)

                    worksheet.write(3, 13, "ShaktiB(viii)*", cell_format2)
                    worksheet.write(3, 14, "Shakti B3", cell_format3)
                    
                    for row, query in enumerate(usecase_data, start=4):
                        result = query.payload()
                        row_number = 4
                        for index, (key, data) in enumerate(result.items(), start=1):
                            if type(data) == dict:
                                worksheet.write(row_number, 0, index, cell_format)  # Sr. No
                                worksheet.write(row_number, 1, data.get("particular", ""), cell_format)  # Particulars
                                if data.get("remark") == "string":
                                    worksheet.write(row_number, 2, "-", cell_format)  # Remarks
                                else:
                                    worksheet.write(row_number, 2, data.get("remark", ""), cell_format)  # Remarks
                                if data.get("uom") == "string":
                                    worksheet.write(row_number, 3, "-", cell_format)  # UOM
                                else:
                                    worksheet.write(row_number, 3, data.get("uom", ""), cell_format)  # UOM
                                worksheet.write(row_number, 4, round(data.get("mou_coal", 0.0), 2), cell_format)  # MoU Coal
                                worksheet.write(row_number, 5, round(data.get("linkage", 0.0), 2), cell_format)  # Linkage
                                worksheet.write(row_number, 6, round(data.get("aiwib_washery", 0.0), 2), cell_format)  # AIWIB Washery
                                worksheet.write(row_number, 7, round(data.get("open_mkt", 0.0), 2), cell_format)  # Open Mkt
                                worksheet.write(row_number, 8, round(data.get("spot_eauction", 0.0), 2), cell_format)  # Spot E-auction
                                worksheet.write(row_number, 9, round(data.get("spl_for_eauction", 0.0), 2), cell_format)  # Spl For-E-auction
                                worksheet.write(row_number, 10, round(data.get("imported", 0.0), 2), cell_format)  # Imported
                                worksheet.write(row_number, 11, round(data.get("total", 0.0), 2), cell_format)  # Total
                                worksheet.write(row_number, 13, round(data.get("shakti_b", 0.0), 2), cell_format4)  # ShaktiB(viii)*
                                worksheet.write(row_number, 14, round(data.get("shakti_b3", 0.0), 2), cell_format4)  # Shakti B3                  

                                row_number += 1 
                        # count-=1
                    #footer details start
                    worksheet.merge_range("A35:F35", f"Date: {datetime.datetime.now().date().strftime('%d.%m.%Y')}\nFor GMR Warora Energy Limited", footer_name_format1)
                    worksheet.merge_range("G35:L35", f"For Kuralkar Shastri & Co\nChartered Accountants", footer_name_format2)
                    worksheet.merge_range("A36:F38", f"\n\n\n(Dhananjay Deshpande)                                                             (Ashish Deshpande)\nChief Operating Officer                                                                              Head - F&A", footer_name_format3)
                    worksheet.merge_range("G36:L38", f"\n\n\n(Nitin R. Kuralkar - Partner)\nM. No. 106430\nFRN : 110010W", footer_name_format4)
                    #footer details end
                    workbook.close()
                    console_logger.debug("Successfully {} report generated".format(service_id))
                    console_logger.debug("sent data {}".format(path))

                    return {
                            "Type": "gmr_from15_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                            }
                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            elif download_type and download_type == "pdf":
                fetchOutputData = single_generate_report_form15(usecase_data, month)
                return fetchOutputData
    except Exception as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug("----- Form 15 ----- Error:", e)
        console_logger.debug(f"Exception type: {exc_type}, Filename: {fname}, Line number: {exc_tb.tb_lineno}")
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")


@router.get("/fetch/multiapproval", tags=["Approval list"])
def endpoint_to_fetch_approval_list_based_on_name(response: Response, approval_name: str):
    try:
        try:
            fetchMultiapproval = MultiApproval.objects.get(approval_name=approval_name)
            if fetchMultiapproval:
                return fetchMultiapproval.payload()
        except DoesNotExist as e:
            response.status_code = 404
            return  {"status":"failed"}
    except Exception as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug("----- Form 15 ----- Error:", e)
        console_logger.debug(f"Exception type: {exc_type}, Filename: {fname}, Line number: {exc_tb.tb_lineno}")
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")

@router.post("/approval_list", tags=["Approval list"])
def endpoint_to_add_approval_list(response: Response, userdata: UserListName):
    # {
    # "user_name": [
    #     [
    #     "vipin", "nitin"
    #     ],
    #     [
    #     "sachin", "faisal"
    #     ]
    # ],
    # "approval_name": "grn_approval"
    # }
    try:
        data = userdata.dict()
        count = 1 
        for user_group in data.get("email", []):
            # listData = []
            
            # for single_email in user_group:
            #     headers = {'accept': 'application/json'}
            #     params = {'username': single_email}
            #     response = requests.get(
            #         f'http://{ip}/api/v1/user/fetch/single/userdetails',
            #         params=params,
            #         headers=headers
            #     )
            #     response_data = response.json()
            #     console_logger.debug(response_data)

            #     email = response_data.get("Email")
            #     if email:
            #         listData.append(email)

            approval_name = data.get("approval_name")
            if approval_name:
                try:
                    approval_list = MultiApproval.objects.get(approval_name=approval_name)
                    if count == 1:
                        approval_list.levels.clear()  
                except MultiApproval.DoesNotExist:
                    approval_list = MultiApproval(approval_name=approval_name)
                count += 1

            user_data_permission = UserDataPermission(user=user_group)
            approval_list.levels.append(user_data_permission)
            approval_list.bypass_level = data.get("bypass_level")
            approval_list.disabled = data.get("disabled")
            approval_list.save()

        return {"detail": "success"}

    except Exception as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(f"Exception type: {exc_type}, Filename: {fname}, Line number: {exc_tb.tb_lineno}")
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")


# settings approval insert
@router.post("/insertapprovalname", tags=["Approval list"])
def endpoint_to_insert_approval_name(response: Response, approval_name: str):
    try:
        fetchApprovalName = ApprovalTableList.objects().first()
        if not fetchApprovalName:
            fetchApprovalName = ApprovalTableList(approval_list=[])
        fetchApprovalName.approval_list.append(approval_name)
        fetchApprovalName.save()
        return {"detail": "success"}
    except Exception as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(f"Exception type: {exc_type}, Filename: {fname}, Line number: {exc_tb.tb_lineno}")
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")


# settings approval fetch
@router.get("/fetchapprovalname", tags=["Approval list"])
def endpoint_to_fetch_approval_name(response: Response):
    try:
        fetchApprovalName = ApprovalTableList.objects().first()
        return fetchApprovalName.approval_list
    except Exception as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(f"Exception type: {exc_type}, Filename: {fname}, Line number: {exc_tb.tb_lineno}")
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")


@router.get("/fetch/usernamelist", tags=["Approval list"])
def endpoint_to_fetch_username_list(response: Response):
    try:
        try:
            headers = {
                'accept': 'application/json',
            }
            response = requests.get(f'http://{ip}/api/v1/user/usernamelist', headers=headers)
            data = json.loads(response.text)
            return data
        except requests.exceptions.Timeout:
            console_logger.debug("Request Timed Out!")
        except requests.exceptions.ConnectionError:
            console_logger.debug("Connection Error")

    except Exception as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(f"Exception type: {exc_type}, Filename: {fname}, Line number: {exc_tb.tb_lineno}")
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")


@router.get("/fetch/emaillist", tags=["Approval list"])
def endpoint_to_fetch_username_list(response: Response):
    try:
        try:
            headers = {
                'accept': 'application/json',
            }
            response = requests.get(f'http://{ip}/api/v1/user/emaillist', headers=headers)
            data = json.loads(response.text)
            return data
        except requests.exceptions.Timeout:
            console_logger.debug("Request Timed Out!")
        except requests.exceptions.ConnectionError:
            console_logger.debug("Connection Error")

    except Exception as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(f"Exception type: {exc_type}, Filename: {fname}, Line number: {exc_tb.tb_lineno}")
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")


def endpoint_to_fetch_single_user_data_using_username(username):
    try:
        headers = {
            'accept': 'application/json',
        }

        params = {
            'username': username,
        }

        response = requests.get(f'http://{ip}/api/v1/user/fetch/single/userdetails', params=params, headers=headers)
        data = json.loads(response.text)
        return data
    except Exception as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(f"Exception type: {exc_type}, Filename: {fname}, Line number: {exc_tb.tb_lineno}")
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")


def endpoint_to_fetch_single_user_data_using_email(email):
    try:
        headers = {
            'accept': 'application/json',
        }

        params = {
            'email': email,
        }
        
        response = requests.get(f'http://{ip}/api/v1/user/fetch/single/useremail', params=params, headers=headers)
        data = json.loads(response.text)
        console_logger.debug(data)
        return data
    except Exception as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(f"Exception type: {exc_type}, Filename: {fname}, Line number: {exc_tb.tb_lineno}")
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")
        

def generate_header_rail(header_text):
    return f'<th style="border: 1px solid #ddd; padding: 8px; background-color: #003974; color: #fff;">{header_text}</th>'


def endpoint_to_make_data_on_table(new_data, original_data, mode, do_no):
    try:
        # per_data = ""
        if mode=="road":
            per_data = '<table style="width: 100%; border-collapse: collapse; margin: 20px 0;">'
            per_data += '<thead><tr>'
            per_data += '<th style="border: 1px solid #ddd; padding: 8px; background-color: #003974; color: #fff;">Ship To Party</th>'
            per_data += '<th style="border: 1px solid #ddd; padding: 8px; background-color: #003974; color: #fff;">Sales Doc No</th>'
            per_data += '<th style="border: 1px solid #ddd; padding: 8px; background-color: #003974; color: #fff;">Dispatch Date Time</th>'
            per_data += '<th style="border: 1px solid #ddd; padding: 8px; background-color: #003974; color: #fff;">Challan Number</th>'
            per_data += '<th style="border: 1px solid #ddd; padding: 8px; background-color: #003974; color: #fff;">Grade Size</th>'
            per_data += '<th style="border: 1px solid #ddd; padding: 8px; background-color: #003974; color: #fff;">Truck Number</th>'
            per_data += '<th style="border: 1px solid #ddd; padding: 8px; background-color: #003974; color: #fff;">Challan Gross Weight</th>'
            per_data += '<th style="border: 1px solid #ddd; padding: 8px; background-color: #003974; color: #fff;">Challan Tare Weight</th>'
            per_data += '<th style="border: 1px solid #ddd; padding: 8px; background-color: #003974; color: #fff;">Challan Net Weight</th>'
            per_data += '<th style="border: 1px solid #ddd; padding: 8px; background-color: #003974; color: #fff;">GWEL Gross Wt</th>'
            per_data += '<th style="border: 1px solid #ddd; padding: 8px; background-color: #003974; color: #fff;">GWEL Tare WT</th>'
            per_data += '<th style="border: 1px solid #ddd; padding: 8px; background-color: #003974; color: #fff;">GWEL Net Wt</th>'
            per_data += '<th style="border: 1px solid #ddd; padding: 8px; background-color: #003974; color: #fff;">Transist Loss</th>'
            per_data += '</tr></thead><tbody>'
            for send_new_data in new_data:
                found = False
                for og_data in original_data:
                    if send_new_data.get("object_id") == og_data.get("object_id"):
                        per_data += '<tr>'
                        per_data += f'<td style="border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">{send_new_data.get("ship_to_party")}</div></td>'
                        if "sales_doc_no_isEdited" in send_new_data:
                            if send_new_data.get("sales_doc_no_isEdited"):
                                per_data += f'<td style="background-color: #fef7ec; border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">{send_new_data.get("sales_doc_no")}</div> <div style="display: flex; flex-direction: column;">{og_data.get("sales_doc_no")}</div></td>'
                        else:
                            per_data += f'<td style="border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">{send_new_data.get("sales_doc_no")}</div></td>'
                        if "dispatch_date_time_isEdited" in send_new_data:
                            if send_new_data.get("dispatch_date_time_isEdited"):
                                per_data += f'<td style="background-color: #fef7ec; border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">{send_new_data.get("dispatch_date_time")}</div> <div style="display: flex; flex-direction: column;">{og_data.get("dispatch_date_time")}</div></td>'
                        else:
                            per_data += f'<td style="border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">{send_new_data.get("dispatch_date_time")}</div></td>'
                        
                        if "challan_number_isEdited" in send_new_data:
                            if send_new_data.get("challan_number_isEdited"):
                                per_data += f'<td style="background-color: #fef7ec; border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">{send_new_data.get("challan_number")}</div> <div style="display: flex; flex-direction: column;">{og_data.get("challan_number")}</td>'
                        else:
                            per_data += f'<td style="border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">{send_new_data.get("challan_number")}</td>'
                        
                        if "grade_size_isEdited" in send_new_data:
                            if send_new_data.get("grade_size_isEdited"):
                                per_data += f'<td style="background-color: #fef7ec; border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">{send_new_data.get("grade_size")}</div> <div style="display: flex; flex-direction: column;">{og_data.get("grade_size")}</div></td>'
                        else:
                            per_data += f'<td atyle="border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">{send_new_data.get("grade_size")}</div></td>'

                        if "truck_number_isEdited" in send_new_data:
                            if send_new_data.get("truck_number_isEdited"):
                                per_data += f'<td style="background-color: #fef7ec; border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">{send_new_data.get("truck_number")}</div> <div style="display: flex; flex-direction: column;">{og_data.get("truck_number")}</div></td>'
                        else:
                            per_data += f'<td style="border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">{send_new_data.get("truck_number")}</div></td>'
                        
                        if "gross_weight_isEdited" in send_new_data:
                            if send_new_data.get("gross_weight_isEdited"):
                                per_data += f'<td style="background-color: #fef7ec; border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">{send_new_data.get("gross_weight")}</div> <div style="display: flex; flex-direction: column;">{og_data.get("gross_weight")}</div></td>'
                        else:
                            per_data += f'<td style="border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">{send_new_data.get("gross_weight")}</div></td>'

                        if "tare_weight_isEdited" in send_new_data:
                            if send_new_data.get("tare_weight_isEdited"):
                                per_data += f'<td style="background-color: #fef7ec; border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">{send_new_data.get("tare_weight")}</div> <div style="display: flex; flex-direction: column;">{og_data.get("tare_weight")}</div></td>'
                        else:
                            per_data += f'<td style="border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">{send_new_data.get("tare_weight")}</div></td>'
                        
                        if "net_weight_isEdited" in send_new_data:
                            if send_new_data.get("net_weight_isEdited"):
                                per_data += f'<td style="background-color: #fef7ec; border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">{send_new_data.get("net_weight")}</div> <div style="display: flex; flex-direction: column;">{og_data.get("net_weight")}</div></td>'
                        else:
                            per_data += f'<td style="border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">{send_new_data.get("net_weight")}</div></td>'
                        
                        if og_data.get("actual_gross_qty"):
                            per_data += f'<td style="border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">{round(float(og_data.get("actual_gross_qty")), 2)}</div></td>'
                        else:
                            per_data += f'<td style="border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">0</div></td>'

                        if og_data.get("actual_tare_qty"):
                            per_data += f'<td style="border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">{round(float(og_data.get("actual_tare_qty")), 2)}</div></td>'
                        else:
                            per_data += f'<td style="border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">0</div></td>'
                        
                        if og_data.get("actual_net_qty"):
                            per_data += f'<td style="border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">{round(float(og_data.get("actual_net_qty")), 2)}</div></td>'
                        else:
                            per_data += f'<td style="border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">0</div></td>'
                        if og_data.get("actual_net_qty"):
                            transist_loss = float(og_data.get("net_weight")) - float(og_data.get("actual_net_qty"))
                        else:
                            transist_loss = float(og_data.get("net_weight")) - 0
                        per_data += f'<td style="border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">{round(transist_loss, 2)}</div></td>'
                        per_data += "</tr>"
            per_data += '</tbody></table>'
        elif mode=="rail":
            fetchRailData = RailData.objects.get(rr_no=do_no)
            per_data = '<table style="width: 100%; border-collapse: collapse; margin: 20px 0;">'
            per_data += '<thead><tr>'
            headers = [
                "Sr. No.", "Wagon Owner", "Wagon Type", "Wagon No", "Ser No", "Rake No", "Secl Gross WT", "Secl Tare WT", "Secl Net WT", "Gwel Gross WT", "Gwel Tare WT", "Gwel Net WT", "Transist Loss"
            ]
            per_data += ''.join(generate_header_rail(header) for header in headers)
            per_data += '</tr></thead><tbody>'

            # Create a mapping for easy lookup of SECL data by wagon_no
            secl_data_mapping = {data.wagon_no: data for data in fetchRailData.secl_rly_data}

            for send_new_data in new_data:
                per_data += '<tr>'

                columns = [
                    send_new_data.get("indexing", "N/A"),
                    send_new_data.get("wagon_owner", "N/A"),
                    send_new_data.get("wagon_type", "N/A"),
                    send_new_data.get("wagon_no", "N/A"),
                    send_new_data.get("ser_no", "N/A"),
                    send_new_data.get("rake_no", "N/A")
                ]

                for col in columns:
                    per_data += f'<td style="border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">{col}</div></td>'

                wagon_no = send_new_data.get("wagon_no")
                secl_data = secl_data_mapping.get(wagon_no)

                if secl_data:
                    secl_columns = [
                        secl_data.secl_gross_wt or 0,
                        secl_data.secl_tare_wt or 0,
                        secl_data.secl_net_wt or 0
                    ]
                else:
                    # if no match then set "N/A"
                    secl_columns = ["N/A", "N/A", "N/A"]

                for col in secl_columns:
                    per_data += f'<td style="border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">{col}</div></td>'

                if send_new_data.get("gwel_net_wt") is not None and secl_data.secl_net_wt is not None:
                    secl_net_wt = secl_data.secl_net_wt.encode('ascii', 'ignore') if secl_data.secl_net_wt else b"0"
                    gwel_net_wt = send_new_data.get("gwel_net_wt").encode('ascii', 'ignore') if send_new_data.get("gwel_net_wt") else b"0"
                    weight_diff = round(float(secl_net_wt) - float(gwel_net_wt), 2)
                else:
                    secl_net_wt = secl_data.secl_net_wt.encode('ascii', 'ignore') if secl_data.secl_net_wt else b"0"
                    weight_diff = float(secl_net_wt)

                additional_columns = [
                    send_new_data.get("gwel_gross_wt") or 0,
                    send_new_data.get("gwel_tare_wt") or 0,
                    send_new_data.get("gwel_net_wt") or 0,
                    # send_new_data.get("coal_grade", "N/A"),
                    weight_diff
                ]

                for col in additional_columns:
                    per_data += f'<td style="border: 1px solid #ddd; padding: 8px;"><div style="display: flex; flex-direction: column;">{col}</div></td>'

                per_data += '</tr>'

            per_data += '</tbody></table>'

        return per_data
    except Exception as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(f"Exception type: {exc_type}, Filename: {fname}, Line number: {exc_tb.tb_lineno}")
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")


@router.get("/update/level/status", tags=['Approval list'])
def endpoint_to_update_level_status_multilevel(response: Response, do_no: Optional[str] = None, username: Optional[str] = None, status: Optional[str] = None, comment:Optional[str] = None, level_name:Optional[str]=None, level_no: Optional[str]=None, invoice_no: Optional[str]=None, edited_by: Optional[str]=None, mode: Optional[str]=None):
    try:
        console_logger.debug(do_no)
        console_logger.debug(invoice_no)
        grn_instance = Grn.objects.get(do_no=do_no, invoice_no=invoice_no)
        dataCheck = grn_instance.approvals
        add_level_no = int(level_no) + 1
        header_data = {
            "do_no":grn_instance.do_no,
            "invoice_date":grn_instance.invoice_date,
            "invoice_no":grn_instance.invoice_no,
            "sale_date":grn_instance.sale_date,
            "grade":grn_instance.grade,
            "dispatch_date":grn_instance.dispatch_date,
            "mine":grn_instance.mine,
            "do_qty":grn_instance.do_qty,
        }
        if not dataCheck.get(str(level_no)):
            if status == "approve":
                if str(level_no) in dataCheck:
                    console_logger.debug("data there")
                    return {"details": "This level is already accepted"}
                else:
                    console_logger.debug("data not there")
                    finalData = [{"username": username, "comment": comment, "created_at": datetime.datetime.now(), "status": status}]
                    grn_instance.approvals[str(level_no)] = finalData
                    grn_instance.save()
                    fetchMultilevelApproval = MultiApproval.objects.get(approval_name="grn_approval")
                    # if level_no == fetchMultilevelApproval.levels[-1]:
                    console_logger.debug(level_no)
                    console_logger.debug(len(fetchMultilevelApproval.levels))
                    if len(fetchMultilevelApproval.levels) == int(level_no)+1:
                        console_logger.debug("inside last data email")
                        params = {
                            'invoice_no': grn_instance.invoice_no,
                        }
                        
                        # console_logger.debug(response.text)
                        if mode == "road":
                            listDataFinal = []
                            for single_new_data in grn_instance.new_data:
                                dictDataa = {
                                    "sales_doc_no": single_new_data.get("sales_doc_no"),
                                    "dispatch_date_time": single_new_data.get("dispatch_date_time"),
                                    "challan_number": single_new_data.get("challan_number"),
                                    "grade_size": single_new_data.get("grade_size"),
                                    "truck_number": single_new_data.get("truck_number"),
                                    "tare_weight": str(single_new_data.get("tare_weight")),
                                    "gross_weight": str(single_new_data.get("gross_weight")),
                                    "net_weight": str(single_new_data.get("net_weight"))
                                }
                                listDataFinal.append(dictDataa)
                            json_data = {
                                "do_no": grn_instance.do_no,
                                "dc_date": grn_instance.dispatch_date,
                                "invoice_date": grn_instance.invoice_date,
                                "invoice_no": grn_instance.invoice_no,
                                "sale_date": grn_instance.sale_date,
                                "grade": grn_instance.grade,
                                "dispatch_date": grn_instance.dispatch_date,
                                "mine": grn_instance.mine,
                                "do_qty": grn_instance.do_qty,
                                "table_data": listDataFinal,
                            }
                            headers = {
                                'accept': 'application/json',
                                'Content-Type': 'application/json',
                            }
                            console_logger.debug(params)
                            console_logger.debug(json_data)
                            response = requests.post(f'http://{ip}:7704/creategrnfile', params=params, headers=headers, json=json_data)
                            for single_struct in grn_instance.new_data:
                                # console_logger.debug(single_struct.get("object_id"))
                                try:
                                    Gmrdata.objects.get(id=ObjectId(single_struct.get("object_id"))).update(
                                        delivery_challan_number=str(single_struct.get("challan_number")),
                                        delivery_challan_date=str(single_struct.get("dispatch_date_time")),
                                        grade=str(single_struct.get("grade_size")),
                                        gross_qty=str(single_struct.get("gross_weight")),
                                        net_qty=str(single_struct.get("net_weight")),
                                        tare_qty=str(single_struct.get("tare_weight")),
                                        vehicle_number=str(single_struct.get("truck_number")),
                                        arv_cum_do_number=str(single_struct.get("sales_doc_no"))
                                    )
                                except DoesNotExist as e:
                                    continue
                        elif mode == "rail":
                            json_data = {
                                "do_no": grn_instance.do_no,
                                "dc_date": grn_instance.dispatch_date,
                                "invoice_date": grn_instance.invoice_date,
                                "invoice_no": grn_instance.invoice_no,
                                "sale_date": grn_instance.sale_date,
                                "grade": grn_instance.grade,
                                "dispatch_date": grn_instance.dispatch_date,
                                "mine": grn_instance.mine,
                                "do_qty": grn_instance.do_qty,
                                "table_data": grn_instance.new_data,
                            }
                            headers = {
                                'accept': 'application/json',
                                'Content-Type': 'application/json',
                            }
                            response = requests.post(f'http://{ip}:7704/creategrnfilerail', params=params, headers=headers, json=json_data)
                            RailData.objects.get(rr_no=do_no).update(
                                avery_rly_data=grn_instance.new_data,
                                grade=grn_instance.grade,
                                mine=grn_instance.mine,
                                rr_qty=grn_instance.do_qty,
                                avery_placement_date=grn_instance.dispatch_date,
                            )

                        html_output = endpoint_to_show_html_struct("Thankyou, This GRN Flow has been accepted!!", username)
                        return html_output


                    console_logger.debug(add_level_no)
                    console_logger.debug(fetchMultilevelApproval.levels)
                    # if add_level_no in fetchMultilevelApproval.levels:
                    if 0 <= add_level_no < len(fetchMultilevelApproval.levels):
                        console_logger.debug("Pass to next")
                        for user_data in fetchMultilevelApproval.levels[add_level_no].user:
                            # user_data = endpoint_to_fetch_single_user_data_using_email(email=email)
                            
                            fetchtabledata = endpoint_to_generate_email_template(new_data=grn_instance.new_data, original_data=grn_instance.original_data, user_data=user_data, do_no=grn_instance.do_no, level_name="grn_approval", level_count=add_level_no, header_data=header_data, edited_by=edited_by, status="Approve", mode=mode)
                            response_code, fetch_email = fetch_email_data()
                            subject = "GRN Flow"
                            to_data = user_data.get("email")
                            # console_logger.debug(to_data)
                            # console_logger.debug(fetchtabledata)
                            body = fetchtabledata

                            sender_email = fetch_email.get("Smtp_user")
                            password = fetch_email.get("Smtp_password") 
                            smtp_host = fetch_email.get("Smtp_host")
                            smtp_port = fetch_email.get("Smtp_port")
                            checkEmailDevelopment = EmailDevelopmentCheck.objects()
                            if checkEmailDevelopment[0].development == "local":
                                send_multiapproval_mail(subject, to_data, body, sender_email, password, smtp_host, smtp_port)
                            elif checkEmailDevelopment[0].development == "prod":
                                params = {
                                    'subject': "GRN Flow",
                                    'to_data': user_data.get("email"),
                                    'body': fetchtabledata,
                                    'sender_email': fetch_email.get("Smtp_user"),
                                    'smtp_host': fetch_email.get("Smtp_host"),
                                    'smtp_port': fetch_email.get("Smtp_port"),
                                }
                                send_multiapproval_email_struct(params=params)
                            # html structure for return
                            html_output = endpoint_to_show_html_struct("Thankyou, This GRN Flow has been accepted!!", username)
                        return html_output
            elif status == "decline":
                if str(level_no) in dataCheck:
                    console_logger.debug("data there")
                    # return {"details": "This level is already accepted"}
                    # html structure for return
                    html_output = endpoint_to_show_html_struct("Thankyou, This GRN level is already accepted!!", username)
                    return html_output
                else:
                    console_logger.debug("data not there")
                    # dictData = {
                    #     str(level_no): [
                    #         {"username": username, "comment": comment, "created_at": datetime.datetime.now(), "status": status}
                    #     ]
                    # }
                    finalData = [{"username": username, "comment": comment, "created_at": datetime.datetime.now(), "status": status}]
                    grn_instance.approvals[str(level_no)] = finalData
                    grn_instance.save()
                    
                    response_code, fetch_email = fetch_email_data()
                    user_data = endpoint_to_fetch_single_user_data_using_username(edited_by)
                    subject = "GRN Flow"
                    to_data = user_data.get("Email")
                    # console_logger.debug(to_data)
                    # console_logger.debug(fetchtabledata)
                    fetchtabledata = endpoint_to_generate_email_template(new_data=grn_instance.new_data, original_data=grn_instance.original_data, user_data=user_data, do_no=grn_instance.do_no, level_name="grn_approval", level_count=add_level_no, header_data=header_data, edited_by=edited_by, status="Decline", mode=mode)
                    html_data = "<h1>Your GRN Flow has been disapprove, Please contact higher Authorities for more.</h1><br><br>"
                    html_data += f"{fetchtabledata}"
                    body = html_data
                    sender_email = fetch_email.get("Smtp_user")
                    password = fetch_email.get("Smtp_password") 
                    smtp_host = fetch_email.get("Smtp_host")
                    smtp_port = fetch_email.get("Smtp_port")
                    checkEmailDevelopment = EmailDevelopmentCheck.objects()
                    if checkEmailDevelopment[0].development == "local":
                        send_multiapproval_mail(subject, to_data, body, sender_email, password, smtp_host, smtp_port)
                    elif checkEmailDevelopment[0].development == "prod":
                        params = {
                            'subject': "GRN Flow",
                            'to_data': user_data.get("Email"),
                            'body': fetchtabledata,
                            'sender_email': fetch_email.get("Smtp_user"),
                            'smtp_host': fetch_email.get("Smtp_host"),
                            'smtp_port': fetch_email.get("Smtp_port"),
                        }
                        send_multiapproval_email_struct(params=params)
                    # html structure for return
                    html_output = endpoint_to_show_html_struct("Thankyou, This GRN Flow has been disabled!!", username)
                    return html_output
        elif dataCheck.get(str(level_no))[0].get("status") == "approve":
            # return {"details": "Already accepted!"}
            # html structure for return
            html_output = endpoint_to_show_html_struct("Thankyou, This Grn is Already Accepted!", username)
            return html_output
        elif dataCheck.get(str(level_no))[0].get("status") == "decline":
            # return {"details": "Not Allowed!"}
            # html structure for return
            # return HTMLResponse(content=html_content, status_code=200)
            html_output = endpoint_to_show_html_struct("Not Allowed!", username)
            return html_output
          
    except Exception as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(f"Exception type: {exc_type}, Filename: {fname}, Line number: {exc_tb.tb_lineno}")
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")
    

def endpoint_to_show_html_struct(text_data, username):
    try:
        html_content = f"""
            <!DOCTYPE html>
            <!--<html>
            <head>
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>GRN Flow</title>
            </head>
            <body>
                <div class="email-container" style="max-width: 600px; margin: 0 auto; background-color: #ffffff; border-radius: 8px; overflow: hidden; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);">
                    <div class="header" style="background-color: #ffffff; text-align: center; padding: 20px;">
                        <img src="http://{ip}/static_server/receipt/report_logo.png" style="max-width: 150px;" alt="GMR">
                    </div>
                    <div class="content" style="padding: 20px; text-align: center;">
                        <p style="color: #333333; font-size: 16px; margin: 0;">HI, {username.upper()}</p>
                        <div class="status" style="background-color: #d4edda; color: #155724; padding: 10px; border-radius: 4px; font-weight: bold; margin-bottom: 20px;">{text_data.upper()}</div>
                    </div>
                    <div class="footer" style="text-align: center; padding: 20px; background-color: #f9f9f9;">
                        <div class="social-icons" style="margin-top: 10px;">
                            <p style="color: #333333; font-size: 16px; margin: 0;">&copy; 2024 GMR. All rights reserved.</p>
                        </div>
                    </div>
                </div>
            </body>
            </html>-->

            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>GRN Flow Confirmation</title>
            </head>
            <body style="font-family: Arial, sans-serif; margin: 0; padding: 0; background-color: #f4f4f9; text-align: center;">
                <div style="max-width: 600px; margin: 50px auto; background: #fff; border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2); overflow: hidden;">
                    <div style="padding: 20px; background: #3f68e1; color: white;">
                        <img src="http://{ip}/static_server/receipt/report_logo.png" alt="GMR Logo" style="max-width: 100px; margin-bottom: 10px;">
                        <h1 style="margin: 0; font-size: 24px;">HI, {username.upper()}</h1>
                    </div>
                    <div style="padding: 30px; background: #e6f7e6; color: #333;">
                        <p style="font-size: 18px; font-weight: bold; margin: 0;">{text_data.upper()}!!</p>
                    </div>
                    <div style="padding: 20px; background: #f4f4f9; color: #666; font-size: 14px;">
                        <p style="font-weight: bold;">&copy; 2024 GMR. All rights reserved.</p>
                    </div>
                </div>
            </body>
            </html>
        """

        return HTMLResponse(content=html_content, status_code=200)
    except Exception as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(f"Exception type: {exc_type}, Filename: {fname}, Line number: {exc_tb.tb_lineno}")
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")

# encode image
def encoded_data(image_path):
    """
    Function that will encode an image to base64
        Parameters
        ----------
        image_path: str
            path of image

        Returns
        -------
        encoded image
    """
    try:
        with open(image_path, "rb") as image_file:
            image_data = image_file.read()
            encoded_data = base64.b64encode(image_data).decode()
        return encoded_data
    except Exception as e:
        console_logger.debug(e)

def endpoint_for_above_table_data(header_data, mode, new_data):
    try:
        if mode == "road":
            header_part = '<table style="width: 100%; border-collapse: collapse; margin-bottom: 20px; border-spacing: 0;">'
            header_part += '<tr><th style="border: 1px solid #ddd; padding: 10px; background-color: #003974; color: #fff; text-align: center;">Do Number</th><th style="border: 1px solid #ddd; padding: 10px; background-color: #003974; color: #fff; text-align: center;">Invoice Date</th><th style="border: 1px solid #ddd; padding: 10px; background-color: #003974; color: #fff; text-align: center;">Invoice Number</th><th style="border: 1px solid #ddd; padding: 10px; background-color: #003974; color: #fff; text-align: center;">Sale Date</th></tr>'
            header_part += f'<tr><td style="width: 25%; background-color: #ffffff; padding: 10px; text-align: center;">{header_data.get("do_no")}</td><td style="width: 25%; background-color: #ffffff; padding: 10px; text-align: center;">{header_data.get("invoice_date")}</td><td style="width: 25%; background-color: #ffffff; padding: 10px; text-align: center;">{header_data.get("invoice_no")}</td><td style="width: 25%; background-color: #ffffff; padding: 10px; text-align: center;">{header_data.get("sale_date")}</td></tr>'
            header_part += f'<tr><th style="border: 1px solid #ddd; padding: 10px; background-color: #003974; color: #fff; text-align: center;">Grade</th><th style="border: 1px solid #ddd; padding: 10px; background-color: #003974; color: #fff; text-align: center;">Dispatch Date</th><th style="border: 1px solid #ddd; padding: 10px; background-color: #003974; color: #fff; text-align: center;">Mine</th><th style="border: 1px solid #ddd; padding: 10px; background-color: #003974; color: #fff; text-align: center;">Do Qty</th></tr>'
            header_part += f'<tr><td style="width: 25%; background-color: #ffffff; padding: 10px; text-align: center;">{header_data.get("grade")}</td><td style="width: 25%; background-color: #ffffff; padding: 10px; text-align: center;">{header_data.get("dispatch_date")}</td><td style="width: 25%; background-color: #ffffff; padding: 10px; text-align: center;">{header_data.get("mine")}</td><td style="width: 25%; background-color: #ffffff; padding: 10px; text-align: center;">{header_data.get("do_qty")}</td></tr></table>'
        elif mode == "rail":
            header_part = '<table style="width: 100%; border-collapse: collapse; margin-bottom: 20px; border-spacing: 0;">'
            header_part += '<tr><th style="border: 1px solid #ddd; padding: 10px; background-color: #003974; color: #fff; text-align: center;">RR Number</th><th style="border: 1px solid #ddd; padding: 10px; background-color: #003974; color: #fff; text-align: center;">Coal Grade</th><th style="border: 1px solid #ddd; padding: 10px; background-color: #003974; color: #fff; text-align: center;">Invoice Date</th><th style="border: 1px solid #ddd; padding: 10px; background-color: #003974; color: #fff; text-align: center;">Invoice Number</th><th style="border: 1px solid #ddd; padding: 10px; background-color: #003974; color: #fff; text-align: center;">Sale Date</th></tr>'
            header_part += f'<tr><td style="width: 25%; background-color: #ffffff; padding: 10px; text-align: center;">{header_data.get("do_no")}</td><td style="width: 25%; background-color: #ffffff; padding: 10px; text-align: center;">{new_data[0]["coal_grade"]}</td><td style="width: 25%; background-color: #ffffff; padding: 10px; text-align: center;">{header_data.get("invoice_date")}</td><td style="width: 25%; background-color: #ffffff; padding: 10px; text-align: center;">{header_data.get("invoice_no")}</td><td style="width: 25%; background-color: #ffffff; padding: 10px; text-align: center;">{header_data.get("sale_date")}</td></tr>'
            header_part += f'<tr><th style="border: 1px solid #ddd; padding: 10px; background-color: #003974; color: #fff; text-align: center;">Grade</th><th style="border: 1px solid #ddd; padding: 10px; background-color: #003974; color: #fff; text-align: center;">Dispatch Date</th><th style="border: 1px solid #ddd; padding: 10px; background-color: #003974; color: #fff; text-align: center;">Mine</th><th style="border: 1px solid #ddd; padding: 10px; background-color: #003974; color: #fff; text-align: center;">Do Qty</th><th style="border: 1px solid #ddd; padding: 10px; background-color: #003974; color: #fff; text-align: center;">PO Number</th></tr>'
            header_part += f'<tr><td style="width: 25%; background-color: #ffffff; padding: 10px; text-align: center;">{header_data.get("grade")}</td><td style="width: 25%; background-color: #ffffff; padding: 10px; text-align: center;">{header_data.get("dispatch_date")}</td><td style="width: 25%; background-color: #ffffff; padding: 10px; text-align: center;">{header_data.get("mine")}</td><td style="width: 25%; background-color: #ffffff; padding: 10px; text-align: center;">{header_data.get("do_qty")}</td><td style="width: 25%; background-color: #ffffff; padding: 10px; text-align: center;">{new_data[0]["po_number"]}</td></tr></table>'
        # console_logger.debug(header_part)
        return header_part
    except Exception as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(f"Exception type: {exc_type}, Filename: {fname}, Line number: {exc_tb.tb_lineno}")
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")

def endpoint_to_show_button_based_condition(do_no, username, level_name, level_count, comment, edited_by, mode, invoice_no):
    try:
        if mode == "road":
            button_part = '<div style="text-align:center;">'
            button_part += f'<a href="http://{ip}:7704/update/level/status?do_no={do_no}&username={username}&status=approve&level_name={level_name}&level_no={level_count}&comment={comment}&invoice_no={invoice_no}&edited_by={edited_by}&mode={mode}" style="display: inline-block; background-color: #003974; color: #ffffff; text-decoration: none; padding: 10px 20px; border-radius: 5px; font-size: 16px; margin-right: 10px">Approve</a>'
            button_part += f'<a href="http://{ip}:7704/update/level/status?do_no={do_no}&username={username}&status=decline&level_name={level_name}&level_no={level_count}&comment={comment}&invoice_no={invoice_no}&edited_by={edited_by}&mode={mode}" style="display: inline-block; background-color: #003974; color: #ffffff; text-decoration: none; padding: 10px 20px; border-radius: 5px; font-size: 16px;">Decline</a>'
            button_part += '</div>'
        elif mode == "rail":
            button_part = '<div style="text-align:center;">'
            button_part += f'<a href="http://{ip}:7704/update/level/status?do_no={do_no}&username={username}&status=approve&level_name={level_name}&level_no={level_count}&comment={comment}&invoice_no={invoice_no}&edited_by={edited_by}&mode={mode}" style="display: inline-block; background-color: #003974; color: #ffffff; text-decoration: none; padding: 10px 20px; border-radius: 5px; font-size: 16px; margin-right: 10px;">Approve</a>'
            button_part += f'<a href="http://{ip}:7704/update/level/status?do_no={do_no}&username={username}&status=decline&level_name={level_name}&level_no={level_count}&comment={comment}&invoice_no={invoice_no}&edited_by={edited_by}&mode={mode}" style="display: inline-block; background-color: #003974; color: #ffffff; text-decoration: none; padding: 10px 20px; border-radius: 5px; font-size: 16px;">Decline</a>'
            button_part += '</div>'
        return button_part
    except Exception as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(f"Exception type: {exc_type}, Filename: {fname}, Line number: {exc_tb.tb_lineno}")
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")

    


def endpoint_to_generate_email_template(new_data, original_data, user_data, do_no, level_name, level_count, header_data, edited_by, status, mode):
    try:
        comment = ""
        username = user_data.get("username")
        table_data = endpoint_to_make_data_on_table(new_data=new_data, original_data=original_data, mode=mode, do_no=do_no)
        header_part = endpoint_for_above_table_data(header_data=header_data, mode=mode, new_data=new_data)
        button_part = endpoint_to_show_button_based_condition(do_no=do_no, username=username, level_name=level_name, level_count=level_count, comment=comment, edited_by=edited_by, mode=mode, invoice_no=header_data.get('invoice_no'))
        if status == "Approve":
            html_template = f"""
                <!DOCTYPE html>
                <html lang="en">
                <head>
                    <meta charset="UTF-8">
                    <meta name="viewport" content="width=device-width, initial-scale=1.0">
                    <title>GRN Summary</title>
                </head>
                <body style="font-family: Arial, sans-serif; margin: 0; padding: 0; background-color: #f4f4f4;">
                    <div style="width: 80%; margin: 20px auto; background-color: #fff; border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); padding: 20px;">

                        <div style="display:flex; justify-content: center; align-items: center;">
                            <div style="width:33.33vw;"></div>
                            <div style="width:33.33vw;">
                                <img src="cid:image1" alt="Company Logo 1" style="max-width:60px; margin:0 10px; text-align:center; display:block; margin:auto;">
                            </div>
                            <div style="width:33.33vw;" sty>
                                <img src="cid:image2" alt="Company Logo 2" style="max-width:60px; margin:0 10px;  display:block; float:right;">
                            </div>
                        </div>


                        <div style="text-align: center; padding: 10px;">
                            <h1 style="margin: 0; font-size: 24px; color: #2a68af;">Hello, {username}</h1>
                            <p style="font-size: 14px; color: #666;">Here's A Quick Summary Of GRN:</p>
                        </div>
                
                        <div style="margin-top: 20px;">
                            <div style="float: right;">
                                <div style="display: inline-block; vertical-align: middle;">
                                    <div class="icon_div"> 
                                        <div class="box" style="width: 1px; height: 1px; border: 1px solid #cf9336; padding: 10px; margin: 10px; background-color: #fef7ec;">
                                        </div>
                                    </div>
                                </div>
                                <div style="display: inline-block; vertical-align: middle;">
                                    <span style="color: #58666e; font-size: 17px; font-weight: bold;">Modified Data</span>
                                </div>
                            </div>
                            {header_part}
                        </div>
                        <div style="margin-top: 20px;">
                            {table_data}
                        </div>
                        <div style="display:flex; justify-content: center; align-items: center;">
                            <div style="width:33.33vw;"></div>
                            <div style="width:33.33vw; text-align: center;">
                                {button_part}
                            </div>
                            <div style="width:33.33vw;"></div>
                        </div>
                
                        <div class="footer" style="background-color: #003974; color: #fff; text-align: center; margin-top: 20px; padding: 8px;">
                            <p style="font-weight: bold;">&copy; 2024 GMR. All rights reserved.</p>
                        </div>
                    </div>
                </body>
                </html>
                """
        elif status == "Decline":
            html_template = f"""
                <!DOCTYPE html>
                <html lang="en">
                <head>
                    <meta charset="UTF-8">
                    <meta name="viewport" content="width=device-width, initial-scale=1.0">
                    <title>GRN Summary</title>
                </head>
                <body style="font-family: Arial, sans-serif; margin: 0; padding: 0; background-color: #f4f4f4;">
                    <div style="width: 80%; margin: 20px auto; background-color: #fff; border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); padding: 20px;">

                        <div style="display:flex; justify-content: center; align-items: center;">
                            <div style="width:33.33vw;"></div>
                            <div style="width:33.33vw;">
                                <img src="cid:image1" alt="Company Logo 1" style="max-width:60px; margin:0 10px; text-align:center; display:block; margin:auto;">
                            </div>
                            <div style="width:33.33vw;" sty>
                                <img src="cid:image2" alt="Company Logo 2" style="max-width:60px; margin:0 10px;  display:block; float:right;">
                            </div>
                        </div>


                        <div style="text-align: center; padding: 10px;">
                            <h1 style="margin: 0; font-size: 24px; color: #2a68af;">Hello, {username}</h1>
                            <p style="font-size: 14px; color: #666;">Here's A Quick Summary Of GRN:</p>
                        </div>
                
                        <div style="margin-top: 20px;">
                            <div style="float: right;">
                                <div style="display: inline-block; vertical-align: middle;">
                                    <div class="icon_div"> 
                                        <div class="box" style="width: 1px; height: 1px; border: 1px solid #cf9336; padding: 10px; margin: 10px; background-color: #fef7ec;">
                                        </div>
                                    </div>
                                </div>
                                <div style="display: inline-block; vertical-align: middle;">
                                    <span style="color: #58666e; font-size: 17px; font-weight: bold;">Modified Data</span>
                                </div>
                            </div>
                            {header_part}
                        </div>
                        <div style="margin-top: 20px;">
                            {table_data}
                        </div>
                        <div class="footer" style="background-color: #003974; color: #fff; text-align: center; margin-top: 20px; padding: 8px;">
                            <p style="font-weight: bold;">&copy; 2024 GMR. All rights reserved.</p>
                        </div>
                    </div>
                </body>
                </html>
            """
        file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
        return html_template
    except Exception as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(f"Exception type: {exc_type}, Filename: {fname}, Line number: {exc_tb.tb_lineno}")
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")


@router.get("/test/multiapproval", tags=["Approval list"])
def endpoint_to_test_multiapproval(response: Response):
    try:
        levels = 2
        grn_instance = Grn.objects.get(do_no="1140010201", invoice_no="9230023207")
        fetchMultilevelApproval = MultiApproval.objects.get(approval_name="grn_approval")
        if len(fetchMultilevelApproval.levels) == levels:
            json_data = {
                "do_no": grn_instance.do_no,
                "dc_date": grn_instance.dispatch_date,
                "invoice_date": grn_instance.invoice_date,
                "invoice_no": grn_instance.invoice_no,
                "sale_date": grn_instance.sale_date,
                "grade": grn_instance.grade,
                "dispatch_date": grn_instance.dispatch_date,
                "mine": grn_instance.mine,
                "do_qty": grn_instance.do_qty,
                "table_data": []
                }
            headers = {
                'accept': 'application/json',
                'Content-Type': 'application/json',
            }

            response = requests.post(f'http://{ip}:7704/creategrnfile', headers=headers, json=json_data)
            console_logger.debug(response.text)
        else:
            console_logger.info(f"Levels do not match: {len(fetchMultilevelApproval.levels)} != {levels}")
    except Exception as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(f"Exception type: {exc_type}, Filename: {fname}, Line number: {exc_tb.tb_lineno}")
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")


@router.post("/update/grntax", tags=["Approval list"])
def endpoint_to_update_grntax(response: Response, data: grnUpdateTax):
    try:
        payload=data.dict()
        try:
            checkGrn = Grn.objects.get(do_no=payload.get("do_no"), invoice_no=payload.get("invoice_no"))
            response.status_code = 409
            return {"details": "GRN already booked"}
        except DoesNotExist as e:
            Grn(
                do_no=payload.get("do_no"),
                invoice_date= payload.get("invoice_date"),
                invoice_no=payload.get("invoice_no"),
                sale_date = payload.get("sale_date"),
                grade = payload.get("grade"),
                dispatch_date = payload.get("dispatch_date"),
                mine = payload.get("mine"),
                do_qty = payload.get("do_qty"),
                # header_data=payload.get("header_data"),
                original_data=payload.get("original_data"),
                new_data=payload.get("new_data"),
                approvals=payload.get("approvals"),
                changed_by=payload.get("changed_by"),
                basic_price = payload.get("particulars").get("basic_price"),
                sizing_charges = payload.get("particulars").get("sizing_charges"),
                stc_charges = payload.get("particulars").get("stc_charges"),
                evac_facility_charge = payload.get("particulars").get("evac_facility_charge"),
                royalty_charges = payload.get("particulars").get("royalty_charges"),
                nmet_charges = payload.get("particulars").get("nmet_charges"),
                imf = payload.get("particulars").get("imf"),
                cgst = payload.get("particulars").get("cgst"),
                sgst = payload.get("particulars").get("sgst"),
                gst_comp_cess = payload.get("particulars").get("gst_comp_cess"),
                gross_bill_value = payload.get("particulars").get("gross_bill_value"),
                net_value = payload.get("particulars").get("net_value"),
                total_amount = payload.get("particulars").get("total_amount"),
                mode="road",
                type_consumer=payload.get("type_consumer")
            ).save()
        
        level_count = 0
        fetchMultilevelApproval = MultiApproval.objects.get(approval_name="grn_approval")
        if not fetchMultilevelApproval.bypass_level:
            for user_data in fetchMultilevelApproval.levels[level_count].user:
                header_data = {
                    "do_no":payload.get("do_no"),
                    "invoice_date":payload.get("invoice_date"),
                    "invoice_no":payload.get("invoice_no"),
                    "sale_date":payload.get("sale_date"),
                    "grade":payload.get("grade"),
                    "dispatch_date":payload.get("dispatch_date"),
                    "mine":payload.get("mine"),
                    "do_qty":payload.get("do_qty"),
                }
                fetchtabledata = endpoint_to_generate_email_template(new_data=payload.get("new_data"), original_data=payload.get("original_data"), user_data=user_data, do_no=payload.get("do_no"), level_name="grn_approval", level_count=level_count, header_data=header_data, edited_by=payload.get("changed_by"), status="Approve", mode="road")
                response_code, fetch_email = fetch_email_data()
                subject = "GRN Flow"
                to_data = user_data.get("email")
                body = fetchtabledata

                sender_email = fetch_email.get("Smtp_user")
                password = fetch_email.get("Smtp_password") 
                smtp_host = fetch_email.get("Smtp_host")
                smtp_port = fetch_email.get("Smtp_port")
                checkEmailDevelopment = EmailDevelopmentCheck.objects()
                if checkEmailDevelopment[0].development == "local":
                    send_multiapproval_mail(subject, to_data, body, sender_email, password, smtp_host, smtp_port)
                elif checkEmailDevelopment[0].development == "prod":
                    params = {
                        'subject': "GRN Flow",
                        'to_data': user_data.get("emzil"),
                        'body': fetchtabledata,
                        'sender_email': fetch_email.get("Smtp_user"),
                        'smtp_host': fetch_email.get("Smtp_host"),
                        'smtp_port': fetch_email.get("Smtp_port"),
                    }
                    send_multiapproval_email_struct(params=params)
                    
        else:
            console_logger.debug("bypass level true")
            level = len(fetchMultilevelApproval.levels)
            console_logger.debug(level-1)
            level_count_data = level-1
            for user_data in fetchMultilevelApproval.levels[-1].user:
                console_logger.debug(user_data)
                # user_data = endpoint_to_fetch_single_user_data_using_email(email=email)
                header_data = {
                    "do_no":payload.get("do_no"),
                    "invoice_date":payload.get("invoice_date"),
                    "invoice_no":payload.get("invoice_no"),
                    "sale_date":payload.get("sale_date"),
                    "grade":payload.get("grade"),
                    "dispatch_date":payload.get("dispatch_date"),
                    "mine":payload.get("mine"),
                    "do_qty":payload.get("do_qty"),
                }
                fetchtabledata = endpoint_to_generate_email_template(new_data=payload.get("new_data"), original_data=payload.get("original_data"), user_data=user_data, do_no=payload.get("do_no"), level_name="grn_approval", level_count=level_count_data, header_data=header_data, edited_by=payload.get("changed_by"), status="Approve", mode="road")
                response_code, fetch_email = fetch_email_data()
                subject = "GRN Flow"
                to_data = user_data.get("email")
                body = fetchtabledata

                sender_email = fetch_email.get("Smtp_user")
                password = fetch_email.get("Smtp_password") 
                smtp_host = fetch_email.get("Smtp_host")
                smtp_port = fetch_email.get("Smtp_port")
                checkEmailDevelopment = EmailDevelopmentCheck.objects()
                if checkEmailDevelopment[0].development == "local":
                    send_multiapproval_mail(subject, to_data, body, sender_email, password, smtp_host, smtp_port)
                elif checkEmailDevelopment[0].development == "prod":
                    params = {
                        'subject': "GRN Flow",
                        'to_data': user_data.get("email"),
                        'body': fetchtabledata,
                        'sender_email': fetch_email.get("Smtp_user"),
                        'smtp_host': fetch_email.get("Smtp_host"),
                        'smtp_port': fetch_email.get("Smtp_port"),
                    }
                    send_multiapproval_email_struct(params=params)

        return {"details": "success"}
    except Exception as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(f"Exception type: {exc_type}, Filename: {fname}, Line number: {exc_tb.tb_lineno}")
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")

@router.post("/multiapproval-email-test")
def send_multiapproval_email_struct(params:dict):
    try:
        url = f"http://{ip}/api/v1/host/send-email-multiapproval/"

        headers = {'Content-Type': 'application/json'}

        payload = json.dumps(params)
        response = requests.request("POST", url, headers=headers, data=payload)
    except Exception as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(f"Exception type: {exc_type}, Filename: {fname}, Line number: {exc_tb.tb_lineno}")
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")


def is_user_approved(user, data2):
    for approval_data in data2:
        for key, approval_list in approval_data.items():
            for approval in approval_list:
                if approval["username"] == user["username"] and approval["status"] == "approve":
                    return True
    return False


def get_current_level(data, invoice_no):
    try:
        # Filter data for the specific invoice_no
        filtered_data = [entry for entry in data if entry['invoice_no'] == invoice_no]
        # Sort levels dynamically based on their names (e.g., "level1", "level2", etc.)
        levels = sorted(
            (key for entry in filtered_data for key in entry if key.startswith('level')),
            key=lambda x: int(x[5:])  # Extract and sort by the number in "levelX"
        )
        # Determine the current level
        for level in levels:
            for entry in filtered_data:
                if level in entry and entry[level]:  # If the level is True
                    break
            else:
                return level  # Return the current level where the condition is False
        
        return levels[-1]  # If all levels are True, return the highest level
    except Exception as e:
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

@router.get("/fetch/grntax", tags=["Approval list"])
def endpoint_to_fetch_grntax(response: Response, currentPage: Optional[int] = None,
                perPage: Optional[int] = None,
                date: Optional[str] = None,
                search_text: Optional[str] = None,
                start_timestamp: Optional[str] = None,
                end_timestamp: Optional[str] = None,
                mode: Optional[str] = None,
                type: Optional[str] = "display",
                status: Optional[str] = None,
                user_name: Optional[str] = None,
                table_type: Optional[str] = None,
                ):
    try:
        data = {}
        result = {        
                "labels": [],
                "datasets": [],
                "total" : 0,
                "page_size": 15
        }

        if type and type == "display":
            if table_type == "portal":
                page_no = 1
                page_len = result["page_size"]

                if currentPage:
                    page_no = currentPage

                if perPage:
                    page_len = perPage
                    result["page_size"] = perPage

                if date:
                    end =f'{date} 23:59:59'
                    start = f'{date} 00:00:00'
                    
                    data["created_at__gte"] = convert_to_utc_format(start, "%Y-%m-%d %H:%M:%S")
                    data["created_at__lte"] = convert_to_utc_format(end, "%Y-%m-%d %H:%M:%S")

                if start_timestamp:
                    data["created_at__gte"] = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")

                if end_timestamp:
                    data["created_at__lte"] = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")
                
                if search_text:
                    data &= Q(do_no__icontains = search_text) | Q(invoice_no__icontains = search_text)
                
                offset = (page_no - 1) * page_len
                if status == "completed":
                    logs = (
                        Grn.objects(**data, mode=mode, rejected=False)
                        .order_by("do_no", "-created_at")
                        .skip(offset)
                        .limit(page_len)
                    )
                elif status == "pending":
                    logs = (
                        Grn.objects(**data, mode=mode,  rejected=True)
                        .order_by("do_no", "-created_at")
                        .skip(offset)
                        .limit(page_len)
                    )
                fetchMultilevelApproval = MultiApproval.objects.get(approval_name="grn_approval")
                # dictData = {}
                # is_edit_btn_show = False
                is_edit_btn_list = []
                get_list_name = []
                dataDictVal = {"invoice_no": "", "is_btn_active": False}
                if any(logs):
                    for log in logs:
                        payload = log.frpayload()
                        levels = fetchMultilevelApproval.levels
                        grn_approvals = log.approvals
                        #hide show btn start
                        if not grn_approvals:
                            console_logger.debug("inside iff leveelll")
                            # If no approvals, check the first level in MultiApproval DB
                            first_level_users = {user["username"] for user in levels[0]["user"]}
                            if user_name in first_level_users:
                                console_logger.debug("comingggggg")
                                # is_edit_btn_show = True
                                dataDictVal["invoice_no"] = payload.get("invoice_no")
                                dataDictVal["is_btn_active"] = True
                                is_edit_btn_list.append(dataDictVal)
                        else:
                            console_logger.debug("inside else")
                            # Check approvals level by level
                            for level_index, level in enumerate(levels):
                                console_logger.debug(level_index)
                                console_logger.debug(level)
                                level_users = {user["username"] for user in levels[level_index+1]["user"]}
                                approved_users = {
                                    approval["username"]
                                    for approval in grn_approvals.get(str(level_index), [])
                                    if approval["status"] == "approve"
                                }
                                console_logger.debug(approved_users)
                                console_logger.debug(level_users)
                                # If the current level is not fully approved
                                if level_users != approved_users:
                                    console_logger.debug("inisde if level")
                                    # Check if the user is part of this level and can edit
                                    if user_name in level_users and user_name not in approved_users:
                                        # is_edit_btn_show = True
                                        dataDictVal["invoice_no"] = payload.get("invoice_no")
                                        dataDictVal["is_btn_active"] = True
                                        is_edit_btn_list.append(dataDictVal)
                                    break
                        #hide show btn end

                        for level_index, level in enumerate(levels):
                            dictData = {}
                            level_users = {user["username"] for user in level["user"]}
                            console_logger.debug(f"Level {level_index + 1} Users: {level_users}")

                            approved_users = {
                                approval["username"]
                                for approval in grn_approvals.get(str(level_index), [])
                                if approval["status"] == "approve"
                            }
                            console_logger.debug(f"Approved Users for Level {level_index + 1}: {approved_users}")

                            is_level_approved = not level_users.isdisjoint(approved_users)  # True if there is any overlap
                            dictData[f"invoice_no"] = payload.get("invoice_no")
                            dictData[f"level{level_index + 1}"] = True if is_level_approved else False
                            get_list_name.append(dictData)
                            console_logger.debug(f"Level {level_index + 1} Approval Status: {dictData[f'level{level_index + 1}']}")

                        # result["labels"] = list(payload.keys())
                        if mode == "road":
                            do_rr_no = "do_no"
                        elif mode == "rail":
                            do_rr_no = "rr_no"

                        result["labels"] = [
                            do_rr_no,
                            "invoice_date",
                            "invoice_no",
                            "sale_date",
                            "grade",
                            "dispatch_date",
                            "mine",
                            "do_qty",
                            "grn_status",
                            "created_at"
                        ]
                        fetchCurrentLevel = get_current_level(get_list_name, payload["invoice_no"])

                        if get_list_name:
                            payload["grn_status"] = fetchCurrentLevel
                        else:
                            payload["grn_status"] = "level1"
                        if any(d['invoice_no'] == payload["invoice_no"] for d in is_edit_btn_list):
                            console_logger.debug("matched")
                            payload["is_edit_btn_show"] = True
                        else:
                            console_logger.debug("not matched")
                            payload["is_edit_btn_show"] = False
                        result["datasets"].append(payload)
                result["total"] = Grn.objects(**data, mode=mode).count()
                return result
            elif table_type == "approval":
                console_logger.debug("inside approval")
                # approval table type
                page_no = 1
                page_len = result["page_size"]

                if currentPage:
                    page_no = currentPage

                if perPage:
                    page_len = perPage
                    result["page_size"] = perPage

                if date:
                    end =f'{date} 23:59:59'
                    start = f'{date} 00:00:00'
                    
                    data["created_at__gte"] = convert_to_utc_format(start, "%Y-%m-%d %H:%M:%S")
                    data["created_at__lte"] = convert_to_utc_format(end, "%Y-%m-%d %H:%M:%S")

                if start_timestamp:
                    data["created_at__gte"] = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")

                if end_timestamp:
                    data["created_at__lte"] = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")
                
                if search_text:
                    data &= Q(do_no__icontains = search_text) | Q(invoice_no__icontains = search_text)
                
                offset = (page_no - 1) * page_len
                fetchMultilevelApproval = MultiApproval.objects.get(approval_name="grn_approval")
                levels = fetchMultilevelApproval.levels
                if status == "completed":
                    console_logger.debug("here completed")
                    logs = (
                        Grn.objects(**data, mode=mode, rejected=False)
                        .order_by("do_no", "-created_at")
                        .skip(offset)
                        .limit(page_len)
                    )
                    
                    multilevel_data = [single_user["user"] for single_user in levels]

                    grn_data = [single_logs.approvals for single_logs in logs]

                    # Check if each sublist in data1 has at least one approved user in data2
                    resultData = {}
                    for index, user_list in enumerate(multilevel_data):
                        approved_users_in_level = [user for user in user_list if is_user_approved(user, grn_data)]
                        if approved_users_in_level:
                            resultData[f"level{index + 1}"] = "approve"
                        else:
                            resultData[f"level{index + 1}"] = "unapprove"

                    console_logger.debug(resultData)
                    final_result = all(value == "approve" for value in resultData.values())
                    console_logger.debug(final_result)
                    if final_result:
                        if mode == "road":
                            if any(logs):
                                for log in logs:
                                    payload = log.frpayload()
                                    # result["labels"] = list(payload.keys())
                                    if mode == "road":
                                        do_rr_no = "do_no"
                                    elif mode == "rail":
                                        do_rr_no = "rr_no"

                                    result["labels"] = [
                                        do_rr_no,
                                        "invoice_date",
                                        "invoice_no",
                                        "sale_date",
                                        "grade",
                                        "dispatch_date",
                                        "mine",
                                        "do_qty",
                                        "created_at"
                                    ]
                                    # checkGrnStatus = [key for key, value in dictData.items() if value == True]
                                    # if checkGrnStatus:
                                    #     payload["grn_status"] = checkGrnStatus[-1]
                                    # else:
                                    #     payload["grn_status"] = "level1"
                                    result["datasets"].append(payload)
                            result["total"] = Grn.objects(**data, mode=mode).count()
                            return result 
                        elif mode =="rail":
                            # inside rail
                            console_logger.debug("inside rail")
                            is_edit_btn_list = []
                            get_list_name = []
                            dataDictVal = {"invoice_no": "", "is_btn_active": False}
                            
                            if any(logs):
                                for log in logs:
                                    payload = log.frpayload()
                                    grn_approvals = log.approvals

                                    console_logger.debug(levels)
                                    for level_index, level in enumerate(levels):
                                        dictData = {}
                                        level_users = {user["username"] for user in level["user"]}
                                        console_logger.debug(f"Level {level_index + 1} Users: {level_users}")

                                        approved_users = {
                                            approval["username"]
                                            for approval in grn_approvals.get(str(level_index), [])
                                            if approval["status"] == "approve"
                                        }
                                        console_logger.debug(f"Approved Users for Level {level_index + 1}: {approved_users}")
                                        is_level_approved = not level_users.isdisjoint(approved_users)  # True if there is any overlap
                                        dictData[f"invoice_no"] = payload.get("invoice_no")
                                        dictData[f"level{level_index + 1}"] = True if is_level_approved else False
                                        get_list_name.append(dictData)
                                        console_logger.debug(f"Level {level_index + 1} Approval Status: {dictData[f'level{level_index + 1}']}")
                                    do_rr_no = "rr_no"

                                    result["labels"] = [
                                        do_rr_no,
                                        "invoice_date",
                                        "invoice_no",
                                        "sale_date",
                                        "grade",
                                        "dispatch_date",
                                        "mine",
                                        "do_qty",
                                        "grn_status",
                                        "created_at"
                                    ]
                                    # Filter data for the given invoice_no
                                    filtered_data = [entry for entry in get_list_name if entry['invoice_no'] == payload.get("invoice_no")]

                                    # Check if all levels are True for the given invoice_no
                                    is_completed = all(value is True for entry in filtered_data for key, value in entry.items() if key.startswith('level'))
                                    console_logger.debug(payload.get("invoice_no"))
                                    console_logger.debug(is_completed)
                                    if is_completed:
                                        fetchCurrentLevel = get_current_level(get_list_name, payload.get("invoice_no"))
                                        if get_list_name:
                                            payload["grn_status"] = fetchCurrentLevel
                                        else:
                                            payload["grn_status"] = "level1"
                                        if any(d['invoice_no'] == payload.get("invoice_no") for d in is_edit_btn_list):
                                            payload["is_edit_btn_show"] = True
                                        else:
                                            payload["is_edit_btn_show"] = False
                                        result["datasets"].append(payload)
                            result["total"] = Grn.objects(**data, mode=mode).count()
                            return result
                    else:
                        return result
                elif status == "pending":
                    # inside approval pending
                    console_logger.debug("approval pending")
                    logs = (
                        Grn.objects(**data, mode=mode)
                        .order_by("do_no", "-created_at")
                        .skip(offset)
                        .limit(page_len)
                    )
                    multilevel_data = [single_user["user"] for single_user in levels]

                    grn_data = [single_logs.approvals for single_logs in logs]
                    
                    resultData = {}
                    for index, user_list in enumerate(multilevel_data):
                        approved_users_in_level = [user for user in user_list if is_user_approved(user, grn_data)]
                        if approved_users_in_level:
                            resultData[f"level{index + 1}"] = "approve"
                        else:
                            resultData[f"level{index + 1}"] = "unapprove"

                    console_logger.debug(resultData)
                    final_result = all(value == "approve" for value in resultData.values())
                    console_logger.debug(final_result)
                    # dictData = {}
                    is_edit_btn_list = []
                    get_list_name = []
                    dataDictVal = {"invoice_no": "", "is_btn_active": False}
                    if not final_result:
                        console_logger.debug("inside not final result")
                        if any(logs):
                            for log in logs:
                                payload = log.frpayload()
                                grn_approvals = log.approvals

                                #hide show btn start
                                if not grn_approvals:
                                    # If no approvals, check the first level in MultiApproval DB
                                    first_level_users = {user["username"] for user in levels[0]["user"]}
                                    if user_name in first_level_users:
                                        console_logger.debug("comingggggg")
                                        # is_edit_btn_show = True
                                        dataDictVal["invoice_no"] = payload.get("invoice_no")
                                        dataDictVal["is_btn_active"] = True
                                        is_edit_btn_list.append(dataDictVal)
                                else:
                                    console_logger.debug("inside else")
                                    # Check approvals level by level
                                    for level_index, level in enumerate(levels):
                                        level_users = {user["username"] for user in levels[level_index+1]["user"]}
                                        approved_users = {
                                            approval["username"]
                                            for approval in grn_approvals.get(str(level_index), [])
                                            if approval["status"] == "approve"
                                        }
                                        # If the current level is not fully approved
                                        if level_users != approved_users:
                                            # Check if the user is part of this level and can edit
                                            if user_name in level_users and user_name not in approved_users:
                                                dataDictVal["invoice_no"] = payload.get("invoice_no")
                                                dataDictVal["is_btn_active"] = True
                                                is_edit_btn_list.append(dataDictVal)
                                            break
                                #hide show btn end

                                console_logger.debug(levels)
                                for level_index, level in enumerate(levels):
                                    dictData = {}
                                    level_users = {user["username"] for user in level["user"]}
                                    console_logger.debug(f"Level {level_index + 1} Users: {level_users}")

                                    approved_users = {
                                        approval["username"]
                                        for approval in grn_approvals.get(str(level_index), [])
                                        if approval["status"] == "approve"
                                    }
                                    console_logger.debug(f"Approved Users for Level {level_index + 1}: {approved_users}")
                                    is_level_approved = not level_users.isdisjoint(approved_users)  # True if there is any overlap
                                    dictData[f"invoice_no"] = payload.get("invoice_no")
                                    dictData[f"level{level_index + 1}"] = True if is_level_approved else False
                                    get_list_name.append(dictData)
                                    console_logger.debug(f"Level {level_index + 1} Approval Status: {dictData[f'level{level_index + 1}']}")

                                if mode == "road":
                                    do_rr_no = "do_no"
                                elif mode == "rail":
                                    do_rr_no = "rr_no"

                                result["labels"] = [
                                    do_rr_no,
                                    "invoice_date",
                                    "invoice_no",
                                    "sale_date",
                                    "grade",
                                    "dispatch_date",
                                    "mine",
                                    "do_qty",
                                    "grn_status",
                                    "created_at"
                                ]
                                fetchCurrentLevel = get_current_level(get_list_name, payload["invoice_no"])
                                if get_list_name:
                                    payload["grn_status"] = fetchCurrentLevel
                                else:
                                    payload["grn_status"] = "level1"
                                if any(d['invoice_no'] == payload["invoice_no"] for d in is_edit_btn_list):
                                    payload["is_edit_btn_show"] = True
                                else:
                                    payload["is_edit_btn_show"] = False
                                result["datasets"].append(payload)
                        result["total"] = Grn.objects(**data, mode=mode).count()
                        return result 
                    else:
                        # inside rail pending
                        console_logger.debug("inisde elseeeeee rail pending")
                        is_edit_btn_list = []
                        get_list_name = []
                        dataDictVal = {"invoice_no": "", "is_btn_active": False}
                        
                        if any(logs):
                            for log in logs:
                                payload = log.frpayload()
                                grn_approvals = log.approvals

                                console_logger.debug(levels)
                                for level_index, level in enumerate(levels):
                                    dictData = {}
                                    level_users = {user["username"] for user in level["user"]}
                                    console_logger.debug(f"Level {level_index + 1} Users: {level_users}")

                                    approved_users = {
                                        approval["username"]
                                        for approval in grn_approvals.get(str(level_index), [])
                                        if approval["status"] == "approve"
                                    }
                                    console_logger.debug(f"Approved Users for Level {level_index + 1}: {approved_users}")
                                    is_level_approved = not level_users.isdisjoint(approved_users)  # True if there is any overlap
                                    dictData[f"invoice_no"] = payload.get("invoice_no")
                                    dictData[f"level{level_index + 1}"] = True if is_level_approved else False
                                    get_list_name.append(dictData)
                                    console_logger.debug(f"Level {level_index + 1} Approval Status: {dictData[f'level{level_index + 1}']}")
                                do_rr_no = "rr_no"

                                result["labels"] = [
                                    do_rr_no,
                                    "invoice_date",
                                    "invoice_no",
                                    "sale_date",
                                    "grade",
                                    "dispatch_date",
                                    "mine",
                                    "do_qty",
                                    "grn_status",
                                    "created_at"
                                ]
                                # Filter data for the given invoice_no
                                filtered_data = [entry for entry in get_list_name if entry['invoice_no'] == payload.get("invoice_no")]

                                # Check if all levels are True for the given invoice_no
                                is_completed = all(value is False for entry in filtered_data for key, value in entry.items() if key.startswith('level'))
                                console_logger.debug(payload.get("invoice_no"))
                                console_logger.debug(is_completed)
                                if is_completed:
                                    fetchCurrentLevel = get_current_level(get_list_name, payload.get("invoice_no"))
                                    if get_list_name:
                                        payload["grn_status"] = fetchCurrentLevel
                                    else:
                                        payload["grn_status"] = "level1"
                                    if any(d['invoice_no'] == payload.get("invoice_no") for d in is_edit_btn_list):
                                        payload["is_edit_btn_show"] = True
                                    else:
                                        payload["is_edit_btn_show"] = False
                                    result["datasets"].append(payload)
                        result["total"] = Grn.objects(**data, mode=mode).count()
                        return result
        elif type and type == "download":
            del type

            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            if start_timestamp:
                data["created_at__gte"] = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")

            if end_timestamp:
                data["created_at__lte"] = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M")

            usecase_data = Grn.objects(**data, mode=mode).order_by("-created_at")
            count = len(usecase_data)
            path = None
            logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
            if usecase_data:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "GRN_Tax_Report_{}.xlsx".format(
                            datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format()
                    cell_format2.set_bold()
                    cell_format2.set_font_size(10)
                    cell_format2.set_align("center")
                    cell_format2.set_align("vcenter")
                    cell_format2.set_text_wrap(True)

                    header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
                    date_format = workbook.add_format({'align': 'left', 'font_size': 12, "bold": True})
                    report_name_format = workbook.add_format({'align': 'left', 'font_size': 12, "bold": True})


                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)
                    cell_format = workbook.add_format()
                    cell_format.set_font_size(10)
                    cell_format.set_align("center")
                    cell_format.set_align("vcenter")
                    cell_format.set_text_wrap(True)


                    worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                    
                    main_header = "GMR Warora Energy Limited"  # Set your main header text here
                    worksheet.merge_range("A1:O1", main_header, header_format)  # Merge cells A1 to H1 for the header
                    
                    worksheet.write("A2", f"Date: {datetime.datetime.now().strftime('%d-%m-%Y')}", date_format)
                    worksheet.merge_range("C2:H2", f"Report Name: Mine Wise Table", report_name_format)


                    headers = [
                        "Sr.No",
                        "PO No",
                        "DO No",
                        "Mines Name",
                        "Vehicle No.",
                        "Total Net Amount",
                        "Gross Wt. as per challan (MT)",
                        "Tare Wt. as per challan (MT)",
                        "Net Wt. as per challan (MT)",
                        "Gross Wt. as per actual (MT)",
                        "Tare Wt. as per actual (MT)",
                        "Net Wt. as per actual (MT)",
                        "Vehicle In Time",
                        "Transit Loss",
                        "LOT",
                        "Line Item"
                    ]

                    for index, header in enumerate(headers):
                        worksheet.write(2, index, header, cell_format2)
                    for row, query in enumerate(usecase_data,start=3):
                        result = query.payload()
                        worksheet.write(row, 0, count, cell_format)
                        worksheet.write(row, 1, str(result["PO_No"]), cell_format)
                        worksheet.write(row, 2, str(result["DO_No"]), cell_format)
                        worksheet.write(row, 3, str(result["Mines_Name"]), cell_format)
                        worksheet.write(row, 4, str(result["vehicle_number"]), cell_format)
                        worksheet.write(row, 5, str(result["Total_net_amount"]), cell_format)
                        worksheet.write(row, 6, float(result["Challan_Gross_Wt(MT)"]), cell_format)
                        worksheet.write(row, 7, float(result["Challan_Tare_Wt(MT)"]), cell_format)
                        worksheet.write(row, 8, float(result["Challan_Net_Wt(MT)"]), cell_format)
                        worksheet.write(row, 9, float(result["GWEL_Gross_Wt(MT)"]), cell_format)
                        worksheet.write(row, 10, float(result["GWEL_Tare_Wt(MT)"]), cell_format)
                        worksheet.write(row, 11, float(result["GWEL_Net_Wt(MT)"]), cell_format)
                        worksheet.write(row, 12, str(result["Vehicle_in_time"]), cell_format)
                        worksheet.write(row, 13, float(result["Transit_Loss"]), cell_format)
                        worksheet.write(row, 14, str(result["LOT"]), cell_format)
                        worksheet.write(row, 15, str(result["Line_Item"]), cell_format)
                        count-=1
                        
                    workbook.close()
                    console_logger.debug("Successfully {} report generated".format(service_id))
                    console_logger.debug("sent data {}".format(path))

                    return {
                            "Type": "Minewise_road_journey_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                            }
                
                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            else:
                console_logger.error("No data found")
                return {
                        "Type": "Minewise_road_journey_download_event",
                        "Datatype": "Report",
                        "File_Path": path,
                        }

    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/insert/raildataplacementdate", tags=["Rail Data"])
def endpoint_to_update_averyplacement_date(response: Response, start_date= str, end_date=str):
    try:
        fetchRailData = RailData.objects(placement_date__gte=start_date, placement_date__lte=end_date)
        for singleraildata in fetchRailData:
            if singleraildata.avery_placement_date is None:
                placement_date = datetime.datetime.strptime(singleraildata.placement_date, "%Y-%m-%dT%H:%M")
                # Add two days
                new_date = placement_date + datetime.timedelta(days=2)
                final_date = new_date.strftime("%Y-%m-%dT%H:%M")
                singleraildata.avery_placement_date = str(new_date)
            if singleraildata.avery_completion_date is None:
                console_logger.debug("inside avery completion date")
                completion_date = datetime.datetime.strptime(singleraildata.completion_date, "%Y-%m-%dT%H:%M")

                # Add two days
                new_date_completetion = completion_date + datetime.timedelta(days=2)
                final_date = new_date_completetion.strftime("%Y-%m-%dT%H:%M")
                singleraildata.avery_completion_date = str(new_date_completetion)
            singleraildata.save()
        return {"detail": "success"}
    except Exception as e:
        response.status_code = 400
        console_logger.debug(e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/update/thirdparty/excel", tags=["Coal Testing"])
async def endpoint_to_update_third_party_data(response: Response, file: UploadFile = File(...)):
    try:
        if file is None:
            return {"error": "No file Uploaded!"}
        
        contents = await file.read()
        if not contents:
            response.status_code = 400
            return {"error": "Uploaded file is empty!"}

        if file.filename.endswith(".xlsx"):
            # file saving start
            date = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{date}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)
            file_extension = file.filename.split(".")[-1]
            file_name = f'thirdparty_excel_{datetime.datetime.now().strftime("%Y-%m-%d:%H:%M")}.{file_extension}'
            full_path = os.path.join(os.getcwd(), target_directory, file_name)
            with open(full_path, "wb") as file_object:
                file_object.write(contents)
            # file saving end

            excel_data = pd.read_excel(BytesIO(contents))
            data_excel_fetch = json.loads(excel_data.to_json(orient="records"))
            for single_data in data_excel_fetch:
                try:
                    fetchReceiptQualityAnalysis = RecieptCoalQualityAnalysis.objects.get(sample_id=str(single_data.get("sample_id")), sample_no=str(single_data.get("sample_no")))
                    fetchReceiptQualityAnalysis.update(
                        thirdparty_report_date=single_data.get("thirdparty_report_date"),
                        thirdparty_reference_no=single_data.get("thirdparty_reference_no"),
                        thirdparty_sample_date=single_data.get("thirdparty_sample_date"),
                        thirdparty_arb_tm=single_data.get("thirdparty_arb_tm"),
                        thirdparty_arb_vm=single_data.get("thirdparty_arb_vm"),
                        thirdparty_arb_ash=single_data.get("thirdparty_arb_ash"),
                        thirdparty_arb_fc=single_data.get("thirdparty_arb_fc"),
                        thirdparty_arb_gcv=single_data.get("thirdparty_arb_gcv"),
                        thirdparty_adb_im=single_data.get("thirdparty_adb_im"),
                        thirdparty_adb_vm=single_data.get("thirdparty_adb_vm"),
                        thirdparty_adb_ash=single_data.get("thirdparty_adb_ash"),
                        thirdparty_adb_fc=single_data.get("thirdparty_adb_fc"),
                        thirdparty_adb_gcv=single_data.get("thirdparty_adb_gcv"),
                        thirdparty_gcv_grade=single_data.get("thirdparty_gcv_grade"),
                        thirdparty_created_date=str(datetime.datetime.now())
                    )
                except DoesNotExist as e:
                    continue
                #     add_data_excel = SapRecordsRcr(
                #         # slno=single_data["Slno"] if single_data["Slno"] else None,
                #         # source=single_data["source"],
                #         # mine_name=single_data["Mines Name"],
                #         sap_po=str(single_data["SAP PO"]) if single_data["SAP PO"] else None,
                #         do_date=str(single_data["SAP PO Date"]) if single_data["SAP PO Date"] else None,
                #         line_item=str(single_data["Line Item"]) if single_data["Line Item"] else None,
                #         do_no=str(single_data["DO No"]) if single_data["DO No"] else None,
                #         # do_qty=str(single_data["DO QTY"]),
                #         # rake_no=single_data["DO/RR Qty"],
                #         # start_date=single_data["DO Start Date"],
                #         # end_date=single_data["DO End Date"],
                #         # grade=single_data["Grade"]
                #     )
                #     add_data_excel.save()

        return {"detail": "success"}
    except KeyError as e:
        raise HTTPException(status_code=404, detail="Key Error")
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- Sap Excel Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/fetch/recipt_quality_analysis_graph", tags=["Coal Quality"])
def endpoint_to_create_graph_receipt_quality_analysis(response: Response, month_date: str):
    try:
        if month_date:
            final_month_date = month_date.replace(' ', '')
            start_date = f'{final_month_date}-01'
            startd_date=datetime.datetime.strptime(f"{start_date}T00:00","%Y-%m-%dT%H:%M")
            end_date = (datetime.datetime.strptime(start_date, "%Y-%m-%d") + relativedelta(day=31)).strftime("%Y-%m-%d")
            
        fetchReceiptData = RecieptCoalQualityAnalysis.objects(plant_analysis_date__gte=start_date, plant_analysis_date__lte=end_date)
        sample_id_list = []
        plant_arb_gcv_list = []
        thirdparty_arb_gcv_list = []
        for single_data in fetchReceiptData:
            if single_data.thirdparty_arb_gcv is not None:
                sample_id_list.append(single_data.sample_id)
                plant_arb_gcv_list.append(single_data.plant_arb_gcv)
                thirdparty_arb_gcv_list.append(single_data.thirdparty_arb_gcv)
            
        chart_data = {
            "data": {
                "labels": sample_id_list,  # Financial year as the label
                "datasets": [
                    {
                        "label": "Plant Arb GCV",
                        "data": plant_arb_gcv_list,
                        "borderColor": "rgba(75, 192, 192, 1)",
                        "backgroundColor": "rgba(75, 192, 192, 0.2)",
                        "fill": False
                    },
                    {
                        "label": "Thirdparty Arb GCV",
                        "data": thirdparty_arb_gcv_list,
                        "borderColor": "rgba(153, 102, 255, 1)",
                        "backgroundColor": "rgba(153, 102, 255, 0.2)",
                        "fill": False
                    }
                ]
            }
        }
        return chart_data
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- Sap Excel Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

    
@router.get("/fetch/igi_analysis_graph", tags=["Coal Quality"])
def endpoint_to_create_graph_igi(response: Response, month_date: str):
    try:
        if month_date:
            final_month_date = month_date.replace(' ', '')
            start_date = f'{final_month_date}-01'
            startd_date=datetime.datetime.strptime(f"{start_date}T00:00","%Y-%m-%dT%H:%M")
            end_date = (datetime.datetime.strptime(start_date, "%Y-%m-%d") + relativedelta(day=31)).strftime("%Y-%m-%d")
            
        fetchReceiptData = minesamplequalityanalysis.objects(plant_analysis_date__gte=start_date, plant_analysis_date__lte=end_date)
        sample_id_list = []
        plant_arb_gcv_list = []
        thirdparty_arb_gcv_list = []
        for single_data in fetchReceiptData:
            if single_data.mine_thirdparty_adb_gcv is not None:
                sample_id_list.append(single_data.rr_no)
                plant_arb_gcv_list.append(single_data.plant_adb_gcv)
                thirdparty_arb_gcv_list.append(single_data.mine_thirdparty_adb_gcv)
            
        chart_data = {
            "data": {
                "labels": sample_id_list,  # Financial year as the label
                "datasets": [
                    {
                        "label": "Plant Arb GCV",
                        "data": plant_arb_gcv_list,
                        "borderColor": "rgba(75, 192, 192, 1)",
                        "backgroundColor": "rgba(75, 192, 192, 0.2)",
                        "fill": False
                    },
                    {
                        "label": "Thirdparty Arb GCV",
                        "data": thirdparty_arb_gcv_list,
                        "borderColor": "rgba(153, 102, 255, 1)",
                        "backgroundColor": "rgba(153, 102, 255, 0.2)",
                        "fill": False
                    }
                ]
            }
        }
        return chart_data
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- igi_analysis_graph Excel Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/insert/tablesubject", tags=["Table Subject"])
def endpoint_to_insert_table_subject(response: Response, data: TableSubjectData):
    try:
        multyData = data.dict()
        try:
            fetchTableName = tableSubject.objects.get(table_name=multyData.get("table_name"))
            fetchTableName.update(
                table_subject=multyData.get("table_subject")
            )
        except DoesNotExist as e:
            tableSubject(table_name=multyData.get("table_name"), table_subject=multyData.get("table_subject")).save()
        return {"detail": "success"}
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- Table Subject Excel Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

    
@router.get("/fetch/tablesubject", tags=["Table Subject"])
def endpoint_to_fetch_table_subject(response: Response):
    try:
        listData = []
        fetchTableSubject = tableSubject.objects()
        for single_data in fetchTableSubject:
            listData.append(single_data.payload())
        return listData
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- Table Subject Excel Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/fetch/singletablesubject", tags=["Table Subject"])
def endpoint_to_fetch_single_table_subject(response: Response, table_name: str):
    try:
        fetchTableSinglename = tableSubject.objects.get(table_name=table_name)
        if fetchTableSinglename:
            return fetchTableSinglename.payload()
        else:
            response.status_code = 404
            return  {"status":"failed"}
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- Table Subject Excel Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e

    
@router.delete("/delete/singletablesubject", tags=["Table Subject"])
def endpoint_to_delete_single_table_subject(response: Response, table_name: str):
    try:
        fetchTableSingleName  = tableSubject.objects.get(table_name=table_name)
        if fetchTableSingleName:
            fetchTableSingleName.delete()
        return {"details": "success"}
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- Table Subject Excel Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/exportemail", tags=['Table Export Email'])
def endpoint_to_export_table_data_to_email(response: Response, data: TableExportData):
    try:
        payload = data.dict()
        # road coal journey table
        # start_date = "2024-11-18T00:00"
        # end_date = "2024-11-18T00:00"
        response_code, fetch_email = fetch_email_data()
        checkEmailDevelopment = EmailDevelopmentCheck.objects()
        if payload.get("table_name") == "road_coal_journey_table":
            headers = {
                'accept': 'application/json',
            }
            params = {
                # 'filter_data': ["Sr.No.", "Mines_Name", "Type_of_consumer", "PO_No", "Line_Item", "Delivery_Challan_No", "DO_No", "LOT", "vehicle_number", "Challan_Gross_Wt(MT)", "Challan_Tare_Wt(MT)", "Challan_Net_Wt(MT)", "GWEL_Gross_Wt(MT)", "GWEL_Tare_Wt(MT)", "GWEL_Net_Wt(MT)", "GWEL_Gross_Time", "GWEL_Tare_Time", "Transit_Loss", "Transporter_LR_No", "Transporter_LR_Date", "Total_net_amount", "Vehicle_in_time", "Vehicle_out_time", "TAT_difference", "PO_Date", "DO_Qty", "Weightment_Date", "Weightment_Time", "Vehicle_Chassis_No", "Fitness_Expiry", "Driver_Name", "Gate_Pass_No", "Total_net_amount", "Eway_bill_No" ],
                'filter_data': payload.get("filter_data"),
                'start_timestamp': f'{payload.get("start_date")}',
                'end_timestamp': f'{payload.get("end_date")}',
                'type': 'download',
            }
            response_fetch = requests.get(f'http://{ip}:7704/road_journey_table_new_filter', params=params, headers=headers)
            if response_fetch.status_code == 200:
                finalData = json.loads(response_fetch.text)
                if response_code == 200:
                    console_logger.debug(finalData.get("File_Path"))
                    if finalData.get("File_Path") is None:
                        response.status_code = 404
                        return  {"status":"No data found"}
                    elif finalData.get("File_Path") is not None:
                        console_logger.debug(f"{os.path.join(os.getcwd())}/{finalData.get('File_Path')}")
                        subject = f"{payload.get('subject')}"
                        body = f"{payload.get('message')}"
                        if checkEmailDevelopment[0].development == "local":
                            console_logger.debug("inside local")
                            send_email(fetch_email.get("Smtp_user"), 
                                        subject, 
                                        fetch_email.get("Smtp_password"), 
                                        fetch_email.get("Smtp_host"), 
                                        fetch_email.get("Smtp_port"), 
                                        payload.get("to"), 
                                        body, 
                                        f"{os.path.join(os.getcwd())}/{finalData.get('File_Path')}", 
                                        payload.get("cc"),
                                        payload.get("bcc"))
                        elif checkEmailDevelopment[0].development == "prod":
                            console_logger.debug("inside prod")
                            send_data = {
                                "sender_email": fetch_email.get("Smtp_user"),
                                "subject": subject,
                                "password": fetch_email.get("Smtp_password"),
                                "smtp_host": fetch_email.get("Smtp_host"),
                                "smtp_port": fetch_email.get("Smtp_port"),
                                "receiver_email": payload.get("to"),
                                "body": body,
                                "file_path": f"{os.path.join(os.getcwd())}/{finalData.get('File_Path')}",
                                "cc_list": payload.get("cc"),
                                "bcc_list": payload.get("bcc")
                            }
                            generate_email(Response, email=send_data)
        elif payload.get("table_name") == "rail_coal_journey_pending_table":
            fetchrailcoalJourney = endpoint_to_fetch_railway_data(Response, start_timestamp=payload.get("start_date"), end_timestamp=payload.get("end_date"), type="download")
            console_logger.debug(fetchrailcoalJourney.get("File_Path"))
            if response_code == 200:
                if fetchrailcoalJourney.get("File_Path") is None:
                    response.status_code = 404
                    return  {"status":"No data found"}
                elif fetchrailcoalJourney.get("File_Path") is not None:
                    subject = f"{payload.get('subject')}"
                    body = f"{payload.get('message')}"
                    if checkEmailDevelopment[0].development == "local":
                        console_logger.debug("inside local")
                        send_email(fetch_email.get("Smtp_user"), 
                                   subject, 
                                   fetch_email.get("Smtp_password"), 
                                   fetch_email.get("Smtp_host"), 
                                   fetch_email.get("Smtp_port"), 
                                   payload.get("to"), 
                                   body, 
                                   f"{os.path.join(os.getcwd())}/{fetchrailcoalJourney.get('File_Path')}", 
                                   payload.get("cc"),
                                   payload.get("bcc"))
                    elif checkEmailDevelopment[0].development == "prod":
                        console_logger.debug("inside prod")
                        send_data = {
                            "sender_email": fetch_email.get("Smtp_user"),
                            "subject": subject,
                            "password": fetch_email.get("Smtp_password"),
                            "smtp_host": fetch_email.get("Smtp_host"),
                            "smtp_port": fetch_email.get("Smtp_port"),
                            "receiver_email": payload.get("to"),
                            "body": body,
                            "file_path": f"{os.path.join(os.getcwd())}/{fetchrailcoalJourney.get('File_Path')}",
                            "cc_list": payload.get("cc"),
                            "bcc_list": payload.get("bcc")
                        }
                        generate_email(Response, email=send_data)
        elif payload.get("table_name") == "rail_coal_journey_completed_table":
            fetchrailcoalJourneyCompleted = endpoint_to_fetch_railway_data_avery(Response, start_timestamp=payload.get("start_date"), end_timestamp=payload.get("end_date"), type="download")
            console_logger.debug(fetchrailcoalJourneyCompleted.get("File_Path"))
            if response_code == 200:
                if fetchrailcoalJourneyCompleted.get("File_Path") is None:
                    response.status_code = 404
                    return  {"status":"No data found"}
                elif fetchrailcoalJourneyCompleted.get("File_Path") is not None:
                    subject = f"{payload.get('subject')}"
                    body = f"{payload.get('message')}"
                    if checkEmailDevelopment[0].development == "local":
                        console_logger.debug("inside local")
                        send_email(fetch_email.get("Smtp_user"), 
                                   subject, 
                                   fetch_email.get("Smtp_password"), 
                                   fetch_email.get("Smtp_host"), 
                                   fetch_email.get("Smtp_port"), 
                                   payload.get("to"), 
                                   body, 
                                   f"{os.path.join(os.getcwd())}/{fetchrailcoalJourneyCompleted.get('File_Path')}", 
                                   payload.get("cc"),
                                   payload.get("bcc"))
                    elif checkEmailDevelopment[0].development == "prod":
                        console_logger.debug("inside prod")
                        send_data = {
                            "sender_email": fetch_email.get("Smtp_user"),
                            "subject": subject,
                            "password": fetch_email.get("Smtp_password"),
                            "smtp_host": fetch_email.get("Smtp_host"),
                            "smtp_port": fetch_email.get("Smtp_port"),
                            "receiver_email": payload.get("to"),
                            "body": body,
                            "file_path": f"{os.path.join(os.getcwd())}/{fetchrailcoalJourneyCompleted.get('File_Path')}",
                            "cc_list": payload.get("cc"),
                            "bcc_list": payload.get("bcc")
                        }
                        generate_email(Response, email=send_data)
        elif payload.get("table_name") == "rcr_rail_coal_journey_pending_table":
            fetch_rcr_rail_pending = endpoint_to_fetch_saprcr_data(Response, start_timestamp=payload.get("start_date"), end_timestamp=payload.get("end_date"), type="download")
            console_logger.debug(fetch_rcr_rail_pending.get("File_Path"))
            if response_code == 200:
                if fetch_rcr_rail_pending.get("File_Path") is None:
                    response.status_code = 404
                    return  {"status":"No data found"}
                elif fetch_rcr_rail_pending.get("File_Path") is not None:
                    subject = f"{payload.get('subject')}"
                    body = f"{payload.get('message')}"
                    if checkEmailDevelopment[0].development == "local":
                        console_logger.debug("inside local")
                        send_email(fetch_email.get("Smtp_user"), 
                                   subject, 
                                   fetch_email.get("Smtp_password"), 
                                   fetch_email.get("Smtp_host"), 
                                   fetch_email.get("Smtp_port"), 
                                   payload.get("to"), 
                                   body, 
                                   f"{os.path.join(os.getcwd())}/{fetch_rcr_rail_pending.get('File_Path')}", 
                                   payload.get("cc"),
                                   payload.get("bcc"))
                    elif checkEmailDevelopment[0].development == "prod":
                        console_logger.debug("inside prod")
                        send_data = {
                            "sender_email": fetch_email.get("Smtp_user"),
                            "subject": subject,
                            "password": fetch_email.get("Smtp_password"),
                            "smtp_host": fetch_email.get("Smtp_host"),
                            "smtp_port": fetch_email.get("Smtp_port"),
                            "receiver_email": payload.get("to"),
                            "body": body,
                            "file_path": f"{os.path.join(os.getcwd())}/{fetch_rcr_rail_pending.get('File_Path')}",
                            "cc_list": payload.get("cc"),
                            "bcc_list": payload.get("bcc")
                        }
                        generate_email(Response, email=send_data)
        elif payload.get("table_name") == "rcr_rail_coal_journey_completed_table":
            fetch_rcr_rail_completed = endpoint_to_fetch_avery_rcr_data(Response, start_timestamp=payload.get("start_date"), end_timestamp=payload.get("end_date"), type="download")
            console_logger.debug(fetch_rcr_rail_completed.get("File_Path"))
            if response_code == 200:
                if fetch_rcr_rail_completed.get("File_Path") is None:
                    response.status_code = 404
                    return  {"status":"No data found"}
                elif fetch_rcr_rail_completed.get("File_Path") is not None:
                    subject = f"{payload.get('subject')}"
                    body = f"{payload.get('message')}"
                    if checkEmailDevelopment[0].development == "local":
                        console_logger.debug("inside local")
                        send_email(fetch_email.get("Smtp_user"), 
                                subject, 
                                fetch_email.get("Smtp_password"), 
                                fetch_email.get("Smtp_host"), 
                                fetch_email.get("Smtp_port"), 
                                payload.get("to"), 
                                body, 
                                f"{os.path.join(os.getcwd())}/{fetch_rcr_rail_completed.get('File_Path')}", 
                                payload.get("cc"),
                                payload.get("bcc"))
                    elif checkEmailDevelopment[0].development == "prod":
                        console_logger.debug("inside prod")
                        send_data = {
                            "sender_email": fetch_email.get("Smtp_user"),
                            "subject": subject,
                            "password": fetch_email.get("Smtp_password"),
                            "smtp_host": fetch_email.get("Smtp_host"),
                            "smtp_port": fetch_email.get("Smtp_port"),
                            "receiver_email": payload.get("to"),
                            "body": body,
                            "file_path": f"{os.path.join(os.getcwd())}/{fetch_rcr_rail_completed.get('File_Path')}",
                            "cc_list": payload.get("cc"),
                            "bcc_list": payload.get("bcc")
                        }
                        generate_email(Response, email=send_data)   
        elif payload.get("table_name") == "rcr_road_journey_table":
            fetch_rcr_road_journey = endpoint_to_fetch_rcr_road_data(Response, start_timestamp=payload.get("start_date"), end_timestamp=payload.get("end_date"), type="download")
            console_logger.debug(fetch_rcr_road_journey.get("File_Path"))
            if response_code == 200:
                if fetch_rcr_road_journey.get("File_Path") is None:
                    response.status_code = 404
                    return  {"status":"No data found"}
                elif fetch_rcr_road_journey.get("File_Path") is not None:
                    subject = f"{payload.get('subject')}"
                    body = f"{payload.get('message')}"
                    if checkEmailDevelopment[0].development == "local":
                        console_logger.debug("inside local")
                        send_email(fetch_email.get("Smtp_user"), 
                                subject, 
                                fetch_email.get("Smtp_password"), 
                                fetch_email.get("Smtp_host"), 
                                fetch_email.get("Smtp_port"), 
                                payload.get("to"), 
                                body, 
                                f"{os.path.join(os.getcwd())}/{fetch_rcr_road_journey.get('File_Path')}", 
                                payload.get("cc"),
                                payload.get("bcc"))
                    elif checkEmailDevelopment[0].development == "prod":
                        console_logger.debug("inside prod")
                        send_data = {
                            "sender_email": fetch_email.get("Smtp_user"),
                            "subject": subject,
                            "password": fetch_email.get("Smtp_password"),
                            "smtp_host": fetch_email.get("Smtp_host"),
                            "smtp_port": fetch_email.get("Smtp_port"),
                            "receiver_email": payload.get("to"),
                            "body": body,
                            "file_path": f"{os.path.join(os.getcwd())}/{fetch_rcr_road_journey.get('File_Path')}",
                            "cc_list": payload.get("cc"),
                            "bcc_list": payload.get("bcc")
                        }
                        generate_email(Response, email=send_data)
        elif payload.get("table_name") == "road_coal_statement":
            fetch_road_coal_statement = endpoint_to_fetch_coal_statement(Response, type="download")
            console_logger.debug(fetch_road_coal_statement.get("File_Path"))
            if response_code == 200:
                if fetch_road_coal_statement.get("File_Path") is None:
                    response.status_code = 404
                    return  {"status":"No data found"}
                elif fetch_road_coal_statement.get("File_Path") is not None:
                    subject = f"{payload.get('subject')}"
                    body = f"{payload.get('message')}"
                    if checkEmailDevelopment[0].development == "local":
                        console_logger.debug("inside local")
                        send_email(fetch_email.get("Smtp_user"), 
                                subject, 
                                fetch_email.get("Smtp_password"), 
                                fetch_email.get("Smtp_host"), 
                                fetch_email.get("Smtp_port"), 
                                payload.get("to"), 
                                body, 
                                f"{os.path.join(os.getcwd())}/{fetch_road_coal_statement.get('File_Path')}", 
                                payload.get("cc"),
                                payload.get("bcc"))
                    elif checkEmailDevelopment[0].development == "prod":
                        console_logger.debug("inside prod")
                        send_data = {
                            "sender_email": fetch_email.get("Smtp_user"),
                            "subject": subject,
                            "password": fetch_email.get("Smtp_password"),
                            "smtp_host": fetch_email.get("Smtp_host"),
                            "smtp_port": fetch_email.get("Smtp_port"),
                            "receiver_email": payload.get("to"),
                            "body": body,
                            "file_path": f"{os.path.join(os.getcwd())}/{fetch_road_coal_statement.get('File_Path')}",
                            "cc_list": payload.get("cc"),
                            "bcc_list": payload.get("bcc")
                        }
                        generate_email(Response, email=send_data)
        elif payload.get("table_name") == "road_coal_logistics_report":
            # start_date = "2024-09-11"
            fetch_road_coal_logistics_report = coal_logistics_report_test(Response, specified_date=payload.get("start_date"), type="download")
            if response_code == 200:
                if fetch_road_coal_logistics_report.get("File_Path") is None:
                    response.status_code = 404
                    return  {"status":"No data found"}
                elif fetch_road_coal_logistics_report.get("File_Path") is not None:
                    subject = f"{payload.get('subject')}"
                    body = f"{payload.get('message')}"
                    if checkEmailDevelopment[0].development == "local":
                        console_logger.debug("inside local")
                        send_email(fetch_email.get("Smtp_user"), 
                                subject, 
                                fetch_email.get("Smtp_password"), 
                                fetch_email.get("Smtp_host"), 
                                fetch_email.get("Smtp_port"), 
                                payload.get("to"), 
                                body, 
                                f"{os.path.join(os.getcwd())}/{fetch_road_coal_logistics_report.get('File_Path')}", 
                                payload.get("cc"),
                                payload.get("bcc"))
                    elif checkEmailDevelopment[0].development == "prod":
                        console_logger.debug("inside prod")
                        send_data = {
                            "sender_email": fetch_email.get("Smtp_user"),
                            "subject": subject,
                            "password": fetch_email.get("Smtp_password"),
                            "smtp_host": fetch_email.get("Smtp_host"),
                            "smtp_port": fetch_email.get("Smtp_port"),
                            "receiver_email": payload.get("to"),
                            "body": body,
                            "file_path": f"{os.path.join(os.getcwd())}/{fetch_road_coal_logistics_report.get('File_Path')}",
                            "cc_list": payload.get("cc"),
                            "bcc_list": payload.get("bcc")
                        }
                        generate_email(Response, email=send_data)
        elif payload.get("table_name") == "daily_road_coal_logistics_report":
            # start_date = "2024-09-11"
            generateReportData = generate_gmr_report(Response, payload.get("start_date"), "All")
            console_logger.debug(f"{os.path.join(os.getcwd())}/{generateReportData}")
            if response_code == 200:
                if generateReportData:
                    subject = f"{payload.get('subject')}"
                    body = f"{payload.get('message')}"
                    if checkEmailDevelopment[0].development == "local":
                        console_logger.debug("inside local")
                        send_email(fetch_email.get("Smtp_user"), 
                                subject, 
                                fetch_email.get("Smtp_password"), 
                                fetch_email.get("Smtp_host"), 
                                fetch_email.get("Smtp_port"), 
                                payload.get("to"), 
                                body, 
                                f"{os.path.join(os.getcwd())}/{generateReportData}",
                                payload.get("cc"),
                                payload.get("bcc"))
                    elif checkEmailDevelopment[0].development == "prod":
                        console_logger.debug("inside prod")
                        send_data = {
                            "sender_email": fetch_email.get("Smtp_user"),
                            "subject": subject,
                            "password": fetch_email.get("Smtp_password"),
                            "smtp_host": fetch_email.get("Smtp_host"),
                            "smtp_port": fetch_email.get("Smtp_port"),
                            "receiver_email": payload.get("to"),
                            "body": body,
                            "file_path": f"{os.path.join(os.getcwd())}/{generateReportData}",
                            "cc_list": payload.get("cc"),
                            "bcc_list": payload.get("bcc")
                        }
                        generate_email(Response, email=send_data)
        elif payload.get("table_name") == "rail_coal_logistics_report":
            # start_date = "2024-09-11"
            fetch_rail_coal_statement = coal_logistics_report_train(Response, specified_date=payload.get("start_date"), type="download")
            console_logger.debug(fetch_rail_coal_statement.get("File_Path"))
            if response_code == 200:
                if fetch_rail_coal_statement.get("File_Path") is None:
                    response.status_code = 404
                    return  {"status":"No data found"}
                elif fetch_rail_coal_statement.get("File_Path") is not None:
                    subject = f"{payload.get('subject')}"
                    body = f"{payload.get('message')}"
                    if checkEmailDevelopment[0].development == "local":
                        console_logger.debug("inside local")
                        send_email(fetch_email.get("Smtp_user"), 
                                subject, 
                                fetch_email.get("Smtp_password"), 
                                fetch_email.get("Smtp_host"), 
                                fetch_email.get("Smtp_port"), 
                                payload.get("to"), 
                                body, 
                                f"{os.path.join(os.getcwd())}/{fetch_rail_coal_statement.get('File_Path')}", 
                                payload.get("cc"),
                                payload.get("bcc"))
                    elif checkEmailDevelopment[0].development == "prod":
                        console_logger.debug("inside prod")
                        send_data = {
                            "sender_email": fetch_email.get("Smtp_user"),
                            "subject": subject,
                            "password": fetch_email.get("Smtp_password"),
                            "smtp_host": fetch_email.get("Smtp_host"),
                            "smtp_port": fetch_email.get("Smtp_port"),
                            "receiver_email": payload.get("to"),
                            "body": body,
                            "file_path": f"{os.path.join(os.getcwd())}/{fetch_rail_coal_statement.get('File_Path')}",
                            "cc_list": payload.get("cc"),
                            "bcc_list": payload.get("bcc")
                        }
                        generate_email(Response, email=send_data)
        elif payload.get("table_name") == "rcf_road_table":
            fetch_rcf_road_table = get_cmpl_recovery(Response, start_timestamp=payload.get("start_date"), end_timestamp=payload.get("end_date"), type="download")
            console_logger.debug(fetch_rcf_road_table.get("File_Path"))
            if response_code == 200:
                if fetch_rcf_road_table.get("File_Path") is None:
                    response.status_code = 404
                    return  {"status":"No data found"}
                elif fetch_rcf_road_table.get("File_Path") is not None:
                    subject = f"{payload.get('subject')}"
                    body = f"{payload.get('message')}"
                    if checkEmailDevelopment[0].development == "local":
                        console_logger.debug("inside local")
                        send_email(fetch_email.get("Smtp_user"), 
                                subject, 
                                fetch_email.get("Smtp_password"), 
                                fetch_email.get("Smtp_host"), 
                                fetch_email.get("Smtp_port"), 
                                payload.get("to"), 
                                body, 
                                f"{os.path.join(os.getcwd())}/{fetch_rcf_road_table.get('File_Path')}", 
                                payload.get("cc"),
                                payload.get("bcc"))
                    elif checkEmailDevelopment[0].development == "prod":
                        console_logger.debug("inside prod")
                        send_data = {
                            "sender_email": fetch_email.get("Smtp_user"),
                            "subject": subject,
                            "password": fetch_email.get("Smtp_password"),
                            "smtp_host": fetch_email.get("Smtp_host"),
                            "smtp_port": fetch_email.get("Smtp_port"),
                            "receiver_email": payload.get("to"),
                            "body": body,
                            "file_path": f"{os.path.join(os.getcwd())}/{fetch_rcf_road_table.get('File_Path')}",
                            "cc_list": payload.get("cc"),
                            "bcc_list": payload.get("bcc")
                        }
                        generate_email(Response, email=send_data)
        elif payload.get("table_name") == "rcf_rail_table":
            # take input as 2024-09-01T00:00
            month = datetime.datetime.strptime(payload.get("start_date"), "%Y-%m-%dT%H:%M").strftime("%Y-%m")
            fetch_rcf_rail_table = get_rcf_losses(Response, month=month, type="download")
            console_logger.debug(fetch_rcf_rail_table.get("File_Path"))
            if response_code == 200:
                if fetch_rcf_rail_table.get("File_Path") is None:
                    response.status_code = 404
                    return  {"status":"No data found"}
                elif fetch_rcf_rail_table.get("File_Path") is not None:
                    subject = f"{payload.get('subject')}"
                    body = f"{payload.get('message')}"
                    if checkEmailDevelopment[0].development == "local":
                        console_logger.debug("inside local")
                        send_email(fetch_email.get("Smtp_user"), 
                                subject, 
                                fetch_email.get("Smtp_password"), 
                                fetch_email.get("Smtp_host"), 
                                fetch_email.get("Smtp_port"), 
                                payload.get("to"), 
                                body, 
                                f"{os.path.join(os.getcwd())}/{fetch_rcf_rail_table.get('File_Path')}", 
                                payload.get("cc"),
                                payload.get("bcc"))
                    elif checkEmailDevelopment[0].development == "prod":
                        console_logger.debug("inside prod")
                        send_data = {
                            "sender_email": fetch_email.get("Smtp_user"),
                            "subject": subject,
                            "password": fetch_email.get("Smtp_password"),
                            "smtp_host": fetch_email.get("Smtp_host"),
                            "smtp_port": fetch_email.get("Smtp_port"),
                            "receiver_email": payload.get("to"),
                            "body": body,
                            "file_path": f"{os.path.join(os.getcwd())}/{fetch_rcf_rail_table.get('File_Path')}",
                            "cc_list": payload.get("cc"),
                            "bcc_list": payload.get("bcc")
                        }
                        generate_email(Response, email=send_data)
        elif payload.get("table_name") == "receipt_quality_analysis_road":
            fetch_receipt_quality_road_table = coal_wcl_test_table(Response, start_timestamp=payload.get("start_date"), end_timestamp=payload.get("end_date"), filter_type=payload.get("filter_type"), type="download")
            console_logger.debug(fetch_receipt_quality_road_table.get("File_Path"))
            if response_code == 200:
                if fetch_receipt_quality_road_table.get("File_Path") is None:
                    response.status_code = 404
                    return  {"status":"No data found"}
                elif fetch_receipt_quality_road_table.get("File_Path") is not None:
                    subject = f"{payload.get('subject')}"
                    body = f"{payload.get('message')}"
                    if checkEmailDevelopment[0].development == "local":
                        console_logger.debug("inside local")
                        send_email(fetch_email.get("Smtp_user"), 
                                subject, 
                                fetch_email.get("Smtp_password"), 
                                fetch_email.get("Smtp_host"), 
                                fetch_email.get("Smtp_port"), 
                                payload.get("to"), 
                                body, 
                                f"{os.path.join(os.getcwd())}/{fetch_receipt_quality_road_table.get('File_Path')}", 
                                payload.get("cc"),
                                payload.get("bcc"))
                    elif checkEmailDevelopment[0].development == "prod":
                        console_logger.debug("inside prod")
                        send_data = {
                            "sender_email": fetch_email.get("Smtp_user"),
                            "subject": subject,
                            "password": fetch_email.get("Smtp_password"),
                            "smtp_host": fetch_email.get("Smtp_host"),
                            "smtp_port": fetch_email.get("Smtp_port"),
                            "receiver_email": payload.get("to"),
                            "body": body,
                            "file_path": f"{os.path.join(os.getcwd())}/{fetch_receipt_quality_road_table.get('File_Path')}",
                            "cc_list": payload.get("cc"),
                            "bcc_list": payload.get("bcc")
                        }
                        generate_email(Response, email=send_data)
        elif payload.get("table_name") == "receipt_quality_analysis_rail":
            fetch_receipt_quality_rail_table = coal_secl_test_table(Response, start_timestamp=payload.get("start_date"), end_timestamp=payload.get("end_date"), filter_type=payload.get("filter_type"), type="download")
            console_logger.debug(fetch_receipt_quality_rail_table.get("File_Path"))
            if response_code == 200:
                if fetch_receipt_quality_rail_table.get("File_Path") is None:
                    response.status_code = 404
                    return  {"status":"No data found"}
                elif fetch_receipt_quality_rail_table.get("File_Path") is not None:
                    subject = f"{payload.get('subject')}"
                    body = f"{payload.get('message')}"
                    if checkEmailDevelopment[0].development == "local":
                        console_logger.debug("inside local")
                        send_email(fetch_email.get("Smtp_user"), 
                                subject, 
                                fetch_email.get("Smtp_password"), 
                                fetch_email.get("Smtp_host"), 
                                fetch_email.get("Smtp_port"), 
                                payload.get("to"), 
                                body, 
                                f"{os.path.join(os.getcwd())}/{fetch_receipt_quality_rail_table.get('File_Path')}", 
                                payload.get("cc"),
                                payload.get("bcc"))
                    elif checkEmailDevelopment[0].development == "prod":
                        console_logger.debug("inside prod")
                        send_data = {
                            "sender_email": fetch_email.get("Smtp_user"),
                            "subject": subject,
                            "password": fetch_email.get("Smtp_password"),
                            "smtp_host": fetch_email.get("Smtp_host"),
                            "smtp_port": fetch_email.get("Smtp_port"),
                            "receiver_email": payload.get("to"),
                            "body": body,
                            "file_path": f"{os.path.join(os.getcwd())}/{fetch_receipt_quality_rail_table.get('File_Path')}",
                            "cc_list": payload.get("cc"),
                            "bcc_list": payload.get("bcc")
                        }
                        generate_email(Response, email=send_data)
        elif payload.get("table_name") == "mine_third_party_quality_analysis_igi":
            fetch_mine_third_party_quality_analysis = endpoint_to_fetch_minesamplequalityanalysis(Response, start_timestamp=payload.get("start_date"), end_timestamp=payload.get("end_date"), type="download")
            console_logger.debug(fetch_mine_third_party_quality_analysis.get("File_Path"))
            if response_code == 200:
                if fetch_mine_third_party_quality_analysis.get("File_Path") is None:
                    response.status_code = 404
                    return  {"status":"No data found"}
                elif fetch_mine_third_party_quality_analysis.get("File_Path") is not None:
                    subject = f"{payload.get('subject')}"
                    body = f"{payload.get('message')}"
                    if checkEmailDevelopment[0].development == "local":
                        console_logger.debug("inside local")
                        send_email(fetch_email.get("Smtp_user"), 
                                subject, 
                                fetch_email.get("Smtp_password"), 
                                fetch_email.get("Smtp_host"), 
                                fetch_email.get("Smtp_port"), 
                                payload.get("to"), 
                                body, 
                                f"{os.path.join(os.getcwd())}/{fetch_mine_third_party_quality_analysis.get('File_Path')}", 
                                payload.get("cc"),
                                payload.get("bcc"))
                    elif checkEmailDevelopment[0].development == "prod":
                        console_logger.debug("inside prod")
                        send_data = {
                            "sender_email": fetch_email.get("Smtp_user"),
                            "subject": subject,
                            "password": fetch_email.get("Smtp_password"),
                            "smtp_host": fetch_email.get("Smtp_host"),
                            "smtp_port": fetch_email.get("Smtp_port"),
                            "receiver_email": payload.get("to"),
                            "body": body,
                            "file_path": f"{os.path.join(os.getcwd())}/{fetch_mine_third_party_quality_analysis.get('File_Path')}",
                            "cc_list": payload.get("cc"),
                            "bcc_list": payload.get("bcc")
                        }
                        generate_email(Response, email=send_data)

        elif payload.get("table_name") == "bunker_analysis_table":
            fetch_bunker_analysis_table = fetch_bunker_data(Response, start_timestamp=payload.get("start_date"), end_timestamp=payload.get("end_date"), type="download")
            console_logger.debug(fetch_bunker_analysis_table.get("File_Path"))
            if response_code == 200:
                if fetch_bunker_analysis_table.get("File_Path") is None:
                    response.status_code = 404
                    return  {"status":"No data found"}
                elif fetch_bunker_analysis_table.get("File_Path") is not None:
                    subject = f"{payload.get('subject')}"
                    body = f"{payload.get('message')}"
                    if checkEmailDevelopment[0].development == "local":
                        console_logger.debug("inside local")
                        send_email(fetch_email.get("Smtp_user"), 
                                subject, 
                                fetch_email.get("Smtp_password"), 
                                fetch_email.get("Smtp_host"), 
                                fetch_email.get("Smtp_port"), 
                                payload.get("to"), 
                                body, 
                                f"{os.path.join(os.getcwd())}/{fetch_bunker_analysis_table.get('File_Path')}", 
                                payload.get("cc"),
                                payload.get("bcc"))
                    elif checkEmailDevelopment[0].development == "prod":
                        console_logger.debug("inside prod")
                        send_data = {
                            "sender_email": fetch_email.get("Smtp_user"),
                            "subject": subject,
                            "password": fetch_email.get("Smtp_password"),
                            "smtp_host": fetch_email.get("Smtp_host"),
                            "smtp_port": fetch_email.get("Smtp_port"),
                            "receiver_email": payload.get("to"),
                            "body": body,
                            "file_path": f"{os.path.join(os.getcwd())}/{fetch_bunker_analysis_table.get('File_Path')}",
                            "cc_list": payload.get("cc"),
                            "bcc_list": payload.get("bcc")
                        }
                        generate_email(Response, email=send_data)
        
        elif payload.get("table_name") == "coal_analysis":
            fetch_coal_analysis_table = get_overall_analysis(Response, start_timestamp=payload.get("start_date"), end_timestamp=payload.get("end_date"), type="download")
            console_logger.debug(fetch_coal_analysis_table.get("File_Path"))
            if response_code == 200:
                if fetch_coal_analysis_table.get("File_Path") is None:
                    response.status_code = 404
                    return  {"status":"No data found"}
                elif fetch_coal_analysis_table.get("File_Path") is not None:
                    subject = f"{payload.get('subject')}"
                    body = f"{payload.get('message')}"
                    if checkEmailDevelopment[0].development == "local":
                        console_logger.debug("inside local")
                        send_email(fetch_email.get("Smtp_user"), 
                                subject, 
                                fetch_email.get("Smtp_password"), 
                                fetch_email.get("Smtp_host"), 
                                fetch_email.get("Smtp_port"), 
                                payload.get("to"), 
                                body, 
                                f"{os.path.join(os.getcwd())}/{fetch_coal_analysis_table.get('File_Path')}", 
                                payload.get("cc"),
                                payload.get("bcc"))
                    elif checkEmailDevelopment[0].development == "prod":
                        console_logger.debug("inside prod")
                        send_data = {
                            "sender_email": fetch_email.get("Smtp_user"),
                            "subject": subject,
                            "password": fetch_email.get("Smtp_password"),
                            "smtp_host": fetch_email.get("Smtp_host"),
                            "smtp_port": fetch_email.get("Smtp_port"),
                            "receiver_email": payload.get("to"),
                            "body": body,
                            "file_path": f"{os.path.join(os.getcwd())}/{fetch_coal_analysis_table.get('File_Path')}",
                            "cc_list": payload.get("cc"),
                            "bcc_list": payload.get("bcc")
                        }
                        generate_email(Response, email=send_data)
        elif payload.get("table_name") == "coal_gcv_analysis":
            fetch_coal_gcv_analysis_table = endpoint_to_fetch_gcv_analysis(Response, type="download")
            console_logger.debug(fetch_coal_gcv_analysis_table.get("File_Path"))
            if response_code == 200:
                if fetch_coal_gcv_analysis_table.get("File_Path") is None:
                    response.status_code = 404
                    return  {"status":"No data found"}
                elif fetch_coal_gcv_analysis_table.get("File_Path") is not None:
                    subject = f"{payload.get('subject')}"
                    body = f"{payload.get('message')}"
                    if checkEmailDevelopment[0].development == "local":
                        console_logger.debug("inside local")
                        send_email(fetch_email.get("Smtp_user"), 
                                subject, 
                                fetch_email.get("Smtp_password"), 
                                fetch_email.get("Smtp_host"), 
                                fetch_email.get("Smtp_port"), 
                                payload.get("to"), 
                                body, 
                                f"{os.path.join(os.getcwd())}/{fetch_coal_gcv_analysis_table.get('File_Path')}", 
                                payload.get("cc"),
                                payload.get("bcc"))
                    elif checkEmailDevelopment[0].development == "prod":
                        console_logger.debug("inside prod")
                        send_data = {
                            "sender_email": fetch_email.get("Smtp_user"),
                            "subject": subject,
                            "password": fetch_email.get("Smtp_password"),
                            "smtp_host": fetch_email.get("Smtp_host"),
                            "smtp_port": fetch_email.get("Smtp_port"),
                            "receiver_email": payload.get("to"),
                            "body": body,
                            "file_path": f"{os.path.join(os.getcwd())}/{fetch_coal_gcv_analysis_table.get('File_Path')}",
                            "cc_list": payload.get("cc"),
                            "bcc_list": payload.get("bcc")
                        }
                        generate_email(Response, email=send_data)

        # return {"detail": "success"}
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- Table Subject Excel Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/fetch/grnstatus", tags=['Approval list'])
def endpoint_to_fetch_grn_status(response: Response, user_name: Optional[str]=None, invoice_no: Optional[str]=None, do_no: Optional[str]=None, mode: Optional[str]=None, rr_no: Optional[str]=None):
    try:
        try:
            if mode == "rail":
                fetchGrnStatus = Grn.objects.get(do_no = rr_no)
                # dataCheck = grn_instance.approvals
                # fetchGrnStatus = Grn.objects.get(invoice_no = invoice_no, do_no = rr_no)
                fetchRailData = RailData.objects.get(rr_no=rr_no)
            else:
                fetchGrnStatus = Grn.objects.get(invoice_no = invoice_no, do_no = do_no)
                # dataCheck = grn_instance.approvals
            header_data = {
                "do_no": fetchGrnStatus.do_no,
                "invoice_no": fetchGrnStatus.invoice_no,
                "sale_date": fetchGrnStatus.sale_date,
                "grade": fetchGrnStatus.grade,
                "dispatch_date": fetchGrnStatus.dispatch_date,
                "mine": fetchGrnStatus.mine,
                "do_qty": fetchGrnStatus.do_qty,
            }
            if mode == "road":
                billing_data = {
                    "basic_price": fetchGrnStatus.basic_price,
                    "sizing_charges": fetchGrnStatus.sizing_charges,
                    "stc_charges": fetchGrnStatus.stc_charges,
                    "evac_facility_charge": fetchGrnStatus.evac_facility_charge,
                    "royalty_charges": fetchGrnStatus.royalty_charges,
                    "nmet_charges": fetchGrnStatus.nmet_charges,
                    "imf": fetchGrnStatus.imf,
                    "cgst": fetchGrnStatus.cgst,
                    "sgst": fetchGrnStatus.sgst,
                    "gst_comp_cess": fetchGrnStatus.gst_comp_cess,
                    "gross_bill_value": fetchGrnStatus.gross_bill_value,
                    "net_value": fetchGrnStatus.net_value,
                    "total_amount": fetchGrnStatus.total_amount,
                }
            elif mode == "rail":
                billing_data = {
                    "sizing_charges_amount": fetchGrnStatus.sizing_charges_amount,
                    "evac_facility_charge_amount": fetchGrnStatus.evac_facility_charge_amount,
                    "royalty_charges_amount": fetchGrnStatus.royalty_charges_amount,
                    "nmet_amount": fetchGrnStatus.nmet_amount,
                    "dmf_amount": fetchGrnStatus.dmf_amount,
                    "adho_sanrachna_vikas_amount": fetchGrnStatus.adho_sanrachna_vikas_amount,
                    "pariyavaran_upkar_amount": fetchGrnStatus.pariyavaran_upkar_amount,
                    "assessable_value_amount": fetchGrnStatus.assessable_value_amount,
                    "igst_amount": fetchGrnStatus.igst_amount,
                    "gst_comp_cess_amount": fetchGrnStatus.gst_comp_cess_amount,
                    "gross_bill_value_amount": fetchGrnStatus.gross_bill_value_amount,
                    "less_underloading_charges_amount": fetchGrnStatus.less_underloading_charges_amount,
                    # "total_gwel_net": fetchGrnStatus.total_gwel_net,
                    "net_value_amtotal_amountount": fetchGrnStatus.total_amount
                }


            fetchMultiapproval = MultiApproval.objects.get(approval_name="grn_approval")
            mPayload = fetchMultiapproval.payload()
            # check user is at which level for multiaaproval
            key_index_multiapproval = next(
                (index for index, group in enumerate(mPayload.get("levels")) if any(user['username'] == user_name for user in group['user'])),
                None
            )
            console_logger.debug(mPayload.get("bypass_level"))
            # when bypass_level is true i am copying user_level value to grn_level
            if mPayload.get("bypass_level"):
                checkUser = next((item for item in mPayload.get("levels")[-1].get('user') if item["username"] == user_name), False)
                if checkUser:
                    # empty
                    final_grn_index = key_index_multiapproval
                else:
                    final_grn_index = None
            else:
                console_logger.debug("inside else")
                key_index_grn = next((single_data_val for single_data_val in fetchGrnStatus.approvals for single_under_data in fetchGrnStatus.approvals[single_data_val]), None)
                console_logger.debug(key_index_grn)
                if key_index_grn == None:
                    final_grn_index = 0
                else:
                    final_grn_index = int(key_index_grn) + 1
                    
            if mode == "road":
                original_list_data = []
                for single_original_data in fetchGrnStatus.original_data:
                    if single_original_data.get("actual_net_qty"):
                        transist_loss = float(single_original_data.get("net_weight")) - float(single_original_data.get("actual_net_qty"))
                    else:
                        transist_loss = float(single_original_data.get("net_weight")) - 0
                    original_data = {
                        "object_id": single_original_data.get("object_id"),
                        "sales_doc_no": single_original_data.get("sales_doc_no"),
                        "dispatch_date_time": single_original_data.get("dispatch_date_time"),
                        "challan_number": single_original_data.get("challan_number"),
                        "grade_size": single_original_data.get("grade_size"),
                        "truck_number": single_original_data.get("truck_number"),
                        "challan_gross_weight": single_original_data.get("gross_weight"),
                        "challan_tare_weight": single_original_data.get("tare_weight"),
                        "challan_net_weight": single_original_data.get("net_weight"),
                        "gwel_gross_weight": single_original_data.get("actual_gross_qty"),
                        "gwel_tare_weight": single_original_data.get("actual_tare_qty"),
                        "gwel_net_weight": single_original_data.get("actual_net_qty"),
                        "transist_loss": round(transist_loss, 2),
                    }
                    original_list_data.append(original_data)
                
                new_list_data = []
                for single_new_data in fetchGrnStatus.new_data:
                    newDictData = {
                        'challan_number': single_new_data.get('challan_number'), 
                        'delivery_doc_no': single_new_data.get('delivery_doc_no'), 
                        'dispatch_date_time': single_new_data.get('dispatch_date_time'), 
                        'grade_size': single_new_data.get('grade_size'), 
                        'challan_tare_weight': single_new_data.get('tare_weight'), 
                        'challan_gross_weight': single_new_data.get('gross_weight'), 
                        'challan_net_weight': single_new_data.get('net_weight'), 
                        'sales_doc_no': single_new_data.get('sales_doc_no'), 
                        'ship_to_party': single_new_data.get('ship_to_party'), 
                        'truck_number': single_new_data.get('truck_number'),
                        'object_id': single_new_data.get('object_id')
                    }
                    new_list_data.append(newDictData)

                # if dataCheck.get(str(payload.get("level_no")))[0].get("status") == "approve":
                    

                return {
                    "header_data": header_data,
                    "billing_data": billing_data,
                    "original_data": original_list_data,
                    "new_data": new_list_data,
                    "user_level": key_index_multiapproval,
                    "grn_level": final_grn_index,
                }
            elif mode == "rail":
                new_list_data = []
                for single_new_data in fetchGrnStatus.new_data:
                    newDictData = {
                        'indexing': single_new_data.get('indexing'), 
                        'wagon_owner': single_new_data.get('wagon_owner'), 
                        'wagon_type': single_new_data.get('wagon_type'), 
                        'wagon_no': single_new_data.get('wagon_no'), 
                        'coal_grade': single_new_data.get('coal_grade'), 
                        'mode': single_new_data.get('mode'), 
                        'po_number': single_new_data.get('po_number'), 
                        'rake_id': single_new_data.get('rake_id'), 
                        'rake_no': single_new_data.get('rake_no'), 
                        'ser_no': single_new_data.get('ser_no'),
                        'tip_enddate': single_new_data.get('tip_enddate'),
                        'tip_endtime': single_new_data.get('tip_endtime'),
                        'tip_startdate': single_new_data.get('tip_startdate'),
                        'tip_starttime': single_new_data.get('tip_starttime'),
                        'tipple_time': single_new_data.get('tipple_time'),
                        'wagon_cc': single_new_data.get('wagon_cc'),
                        'wagon_id': single_new_data.get('wagon_id'),
                        'wagon_no_avery': single_new_data.get('wagon_no_avery'),
                        'wagon_type_avery': single_new_data.get('wagon_type_avery'),
                        'gwel_gross_wt': single_new_data.get('gwel_gross_wt'),
                        'gwel_net_wt': single_new_data.get('gwel_net_wt'),
                        'gwel_tare_wt': single_new_data.get('gwel_tare_wt'),
                    }
                    secl_data_mapping = {data.wagon_no: data for data in fetchRailData.secl_rly_data}
                    wagon_no = single_new_data.get("wagon_no")
                    secl_data = secl_data_mapping.get(wagon_no)
                    if single_new_data.get("gwel_net_wt") is not None and secl_data.secl_net_wt is not None:
                        secl_net_wt = secl_data.secl_net_wt.encode('ascii', 'ignore') if secl_data.secl_net_wt else b"0"
                        gwel_net_wt = single_new_data.get("gwel_net_wt").encode('ascii', 'ignore') if single_new_data.get("gwel_net_wt") else b"0"
                        weight_diff = round(float(secl_net_wt) - float(gwel_net_wt), 2)
                    else:
                        secl_net_wt = secl_data.secl_net_wt.encode('ascii', 'ignore') if secl_data.secl_net_wt else b"0"
                        weight_diff = float(secl_net_wt)
                    newDictData["gwel_gross_wt"] = single_new_data.get("gwel_gross_wt")
                    newDictData["gwel_tare_wt"] = single_new_data.get("gwel_tare_wt")
                    newDictData["gwel_net_wt"] = single_new_data.get("gwel_net_wt")
                    newDictData["transist_loss"] = weight_diff
                    new_list_data.append(newDictData)

                seclList = []
                for single_secl_data in fetchRailData.secl_rly_data:
                    newSeclDict = {
                        'indexing': single_secl_data.indexing, 
                        'wagon_owner': single_secl_data.wagon_owner, 
                        'wagon_type': single_secl_data.wagon_type, 
                        'wagon_no': single_secl_data.wagon_no, 
                        'secl_cc_wt': single_secl_data.secl_cc_wt, 
                        'secl_gross_wt': single_secl_data.secl_gross_wt, 
                        'secl_tare_wt': single_secl_data.secl_tare_wt, 
                        'secl_net_wt': single_secl_data.secl_net_wt, 
                        'secl_ol_wt': single_secl_data.secl_ol_wt, 
                        'secl_ul_wt': single_secl_data.secl_ul_wt,
                        'secl_chargable_wt': single_secl_data.secl_chargable_wt,
                        'rly_cc_wt': single_secl_data.rly_cc_wt,
                        'rly_gross_wt': single_secl_data.rly_gross_wt,
                        'rly_tare_wt': single_secl_data.rly_tare_wt,
                        'rly_net_wt': single_secl_data.rly_net_wt,
                        'rly_permissible_cc_wt': single_secl_data.rly_permissible_cc_wt,
                        'rly_ol_wt': single_secl_data.rly_ol_wt,
                        'rly_norm_rate': single_secl_data.rly_norm_rate,
                        'rly_pun_rate': single_secl_data.rly_pun_rate,
                        'rly_chargable_wt': single_secl_data.rly_chargable_wt,
                        'rly_sliding_adjustment': single_secl_data.rly_sliding_adjustment,
                    }
                    seclList.append(newSeclDict)
                
                return {
                    "header_data": header_data,
                    "billing_data": billing_data,
                    "new_data": new_list_data,
                    "secl_data": seclList,
                    "user_level": key_index_multiapproval,
                    "grn_level": final_grn_index,
                    "po_date": fetchRailData.po_date,
                    "line_item": fetchRailData.line_item,
                    "source": fetchRailData.source,
                    "placement_date": fetchRailData.placement_date,
                    "completion_date": fetchRailData.completion_date,
                    "avery_placement_date": fetchRailData.avery_placement_date,
                    "avery_completion_date": fetchRailData.avery_completion_date,
                    "drawn_date": fetchRailData.drawn_date,
                    "total_ul_wt": fetchRailData.total_ul_wt,
                    "boxes_supplied": fetchRailData.boxes_supplied,
                    "total_secl_gross_wt": fetchRailData.total_secl_gross_wt,
                    "total_secl_tare_wt": fetchRailData.total_secl_tare_wt,
                    "total_secl_net_wt": fetchRailData.total_secl_net_wt,
                    "total_secl_ol_wt": fetchRailData.total_secl_ol_wt,
                    "boxes_loaded": fetchRailData.boxes_loaded,
                    "total_rly_gross_wt": fetchRailData.total_rly_gross_wt,
                    "total_rly_tare_wt": fetchRailData.total_rly_tare_wt,
                    "total_rly_net_wt": fetchRailData.total_rly_net_wt,
                    "total_rly_ol_wt": fetchRailData.total_rly_ol_wt,
                    "total_secl_chargable_wt": fetchRailData.total_secl_chargable_wt,
                    "total_rly_chargable_wt": fetchRailData.total_rly_chargable_wt,
                    "freight": fetchRailData.freight,
                    "gst": fetchRailData.gst,
                    "pola": fetchRailData.pola,
                    "total_freight": fetchRailData.total_freight,
                    "source_type": fetchRailData.source_type,
                    "month": fetchRailData.month,
                    "rr_date": fetchRailData.rr_date,
                    "siding": fetchRailData.siding,
                    "mine": fetchRailData.mine,
                    "grade": fetchRailData.grade,
                    "po_amount": fetchRailData.po_amount,
                    "rake_no": fetchRailData.rake_no,
                    "GWEL_received_wagons": fetchRailData.GWEL_received_wagons,
                    "GWEL_pending_wagons": fetchRailData.GWEL_pending_wagons,
                    "GWEL_Total_gwel_gross": fetchRailData.Total_gwel_gross,
                    "GWEL_Total_gwel_tare": fetchRailData.Total_gwel_tare,
                    "GWEL_Total_gwel_net": fetchRailData.Total_gwel_net,
                }
        except DoesNotExist as e:
            pass
        response.status_code = 404
        return {"details": "No Data Found"}
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- GRN Status Update Excel Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/grnstatusupdate", tags=["Approval list"])
def endpoint_to_update_grn_status_update(response: Response, data: grnupdateStatusData):
    try:
        payload = data.dict()
        grn_instance = Grn.objects.get(do_no=payload.get("do_no"), invoice_no=payload.get("invoice_no"))
        dataCheck = grn_instance.approvals
        # add_level_no = int(payload.get("level_no")) + 1
        header_data = {
            "do_no":grn_instance.do_no,
            "invoice_date":grn_instance.invoice_date,
            "invoice_no":grn_instance.invoice_no,
            "sale_date":grn_instance.sale_date,
            "grade":grn_instance.grade,
            "dispatch_date":grn_instance.dispatch_date,
            "mine":grn_instance.mine,
            "do_qty":grn_instance.do_qty,
        }
        if not dataCheck.get(str(payload.get("level_no"))):
            if payload.get("status") == "approve":
                # making rejected false if it is rejected previous start
                if grn_instance.rejected:
                    grn_instance.update(
                        rejected=False
                    )
                # making rejected false if it is rejected previous end
                if str(payload.get("level_no")) in dataCheck:
                    console_logger.debug("data there")
                    response.status_code = 202
                    return {"details": "This level is already accepted"}
                else:
                    console_logger.debug("data not there")
                    finalData = [{"username": payload.get("user_name"), "comment": payload.get("comment"), "created_at": datetime.datetime.now(), "status": payload.get("status")}]
                    grn_instance.approvals[str(payload.get("level_no"))] = finalData
                    grn_instance.save()
                    fetchMultilevelApproval = MultiApproval.objects.get(approval_name="grn_approval")
                    if len(fetchMultilevelApproval.levels) == int(payload.get("level_no"))+1:
                        console_logger.debug("inside last data")
                        params = {
                            'invoice_no': grn_instance.invoice_no,
                        }
                        if payload.get("mode") == "road":
                            listGrnTable = []
                            for single_data_grn in grn_instance.new_data:
                                dictDataGrn = {
                                    "sales_doc_no": single_data_grn.get("sales_doc_no"),
                                    "dispatch_date_time": single_data_grn.get("dispatch_date_time"),
                                    "challan_number": single_data_grn.get("challan_number"),
                                    "grade_size": single_data_grn.get("grade_size"),
                                    "truck_number": str(single_data_grn.get("truck_number")),
                                    "tare_weight": str(single_data_grn.get("tare_weight")),
                                    "gross_weight": str(single_data_grn.get("gross_weight")),
                                    "net_weight": str(single_data_grn.get("net_weight"))
                                }
                                listGrnTable.append(dictDataGrn)

                            json_data = {
                                "do_no": grn_instance.do_no,
                                "dc_date": grn_instance.dispatch_date,
                                "invoice_date": grn_instance.invoice_date,
                                "invoice_no": grn_instance.invoice_no,
                                "sale_date": grn_instance.sale_date,
                                "grade": grn_instance.grade,
                                "dispatch_date": grn_instance.dispatch_date,
                                "mine": grn_instance.mine,
                                "do_qty": grn_instance.do_qty,
                                "table_data": listGrnTable,
                                }
                            headers = {
                                'accept': 'application/json',
                                'Content-Type': 'application/json',
                            }
                            response = requests.post(f'http://{ip}:7704/creategrnfile', params=params, headers=headers, json=json_data)
                            console_logger.debug(response)
                            for single_struct in grn_instance.new_data:
                                if single_struct.get("object_id") != None:
                                    Gmrdata.objects.get(id=ObjectId(single_struct.get("object_id"))).update(
                                        delivery_challan_number=str(single_struct.get("challan_number")),
                                        delivery_challan_date=str(single_struct.get("dispatch_date_time")),
                                        grade=str(single_struct.get("grade_size")),
                                        gross_qty=str(single_struct.get("gross_weight")),
                                        net_qty=str(single_struct.get("net_weight")),
                                        tare_qty=str(single_struct.get("tare_weight")),
                                        vehicle_number=str(single_struct.get("truck_number")),
                                        arv_cum_do_number=str(single_struct.get("sales_doc_no"))
                                    )
                        elif payload.get("mode") == "rail":
                            json_data = {
                                "do_no": grn_instance.do_no,
                                "dc_date": grn_instance.dispatch_date,
                                "invoice_date": grn_instance.invoice_date,
                                "invoice_no": grn_instance.invoice_no,
                                "sale_date": grn_instance.sale_date,
                                "grade": grn_instance.grade,
                                "dispatch_date": grn_instance.dispatch_date,
                                "mine": grn_instance.mine,
                                "do_qty": grn_instance.do_qty,
                                "table_data": grn_instance.new_data,
                            }
                            headers = {
                                'accept': 'application/json',
                                'Content-Type': 'application/json',
                            }
                            response = requests.post(f'http://{ip}:7704/creategrnfilerail', params=params, headers=headers, json=json_data)
                            RailData.objects.get(rr_no=grn_instance.do_no).update(
                                avery_rly_data=grn_instance.new_data,
                                grade=grn_instance.grade,
                                mine=grn_instance.mine,
                                rr_qty=grn_instance.do_qty,
                                avery_placement_date=grn_instance.dispatch_date,
                            )
                    return {"details": "Grn Flow Accepted"}
            elif payload.get("status") == "decline":
                # added just to remove approvals while rejected start
                grn_instance.update(
                    approvals={},
                    rejected=True
                )
                return {"details": "Grn Flow Rejected"}
                # grn_instance.approvals = {}
                # grn_instance.rejected = True
                # grn_instance.save()
                # added just to remove approvals while rejected end
                
                # if str(payload.get("level_no")) in dataCheck:
                #     console_logger.debug("data there")
                #     response.status_code = 403
                #     return {"details": "This level is already accepted"}
                # else:
                #     console_logger.debug("data not there")
                #     finalData = [{"username": payload.get("user_name"), "comment": payload.get("comment"), "created_at": datetime.datetime.now(), "status": payload.get("status")}]
                #     grn_instance.approvals[str(payload.get("level_no"))] = finalData
                #     grn_instance.save()
                #     return {"details": "Grn Flow has been disabled"}
        elif dataCheck.get(str(payload.get("level_no")))[0].get("status") == "approve":
            response.status_code = 403
            return {"details": "Grn is already accepted!"}
        elif dataCheck.get(str(payload.get("level_no")))[0].get("status") == "decline":
            response.status_code = 403
            return {"details": "Not Allowed!"}
                    
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- GRN Status Update Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.post("/insert/cmpl", tags=["cmpl"])
def endpoint_to_insert_cmpl_data(response: Response, data: cmplInput):
    try:
        payload = data.dict()
        try:
            fetchCmplData = cmplData.objects.get(lrno=payload.get("lrno"))
            fetchCmplData.update(
                tno = int(payload.get("tno")),
                companycode = payload.get("companycode"),
                financialyearcode = payload.get("financialyearcode"),
                locationcode = payload.get("locationcode"),
                lrno = payload.get("lrno"),
                lrdate = payload.get("lrdate"),
                partycode = payload.get("partycode"),
                source_location_tno = payload.get("source_location_tno"),
                consignor_code = payload.get("consignor_code"),
                destination_location_tno = payload.get("destination_location_tno"),
                consigneecode = payload.get("consigneecode"),
                vehicle_no = payload.get("vehicle_no"),
                freightamount = float(payload.get("freightamount")),
                item_code = payload.get("item_code"),
                nos = int(payload.get("nos")),
                quantity1 = int(payload.get("quantity1")),
                quantity2 = int(payload.get("quantity2")),
                invoice_no = payload.get("invoice_no"),
                invoice_date = payload.get("invoice_date"),
                consignor_name = payload.get("consignor_name"),
                consignor_address = payload.get("consignor_address"),
                consignor_citycode = payload.get("consignor_citycode"),
                consignor_statecode =  payload.get("consignor_statecode"),
                consignor_phoneno = payload.get("consignor_phoneno"),
                consignee_name = payload.get("consignee_name"),
                consignee_address = payload.get("consignee_address"),
                consignee_citycode = payload.get("consignee_citycode"),
                consignee_statecode =  payload.get("consignee_statecode"),
                consignee_phoneno = payload.get("consignee_phoneno"),
                invoice_amount = float(payload.get("invoice_amount")),
                challon_no = payload.get("challon_no"),
                challan_date = payload.get("challan_date"),
                driver_name = payload.get("driver_name"),
                driver_licenseno = payload.get("driver_licenseno"),
                eway_billno = payload.get("eway_billno"),
                eway_billdate = payload.get("eway_billdate"),
                balance_qty = int(payload.get("balance_qty")),
                do_qty = int(payload.get("do_qty")),
                delivery_order_tno = int(payload.get("delivery_order_tno"))
            )
        except DoesNotExist as e:
            cmplData(
                tno = int(payload.get("tno")),
                companycode = payload.get("companycode"),
                financialyearcode = payload.get("financialyearcode"),
                locationcode = payload.get("locationcode"),
                lrno = payload.get("lrno"),
                lrdate = payload.get("lrdate"),
                partycode = payload.get("partycode"),
                source_location_tno = payload.get("source_location_tno"),
                consignor_code = payload.get("consignor_code"),
                destination_location_tno = payload.get("destination_location_tno"),
                consigneecode = payload.get("consigneecode"),
                vehicle_no = payload.get("vehicle_no"),
                freightamount = float(payload.get("freightamount")),
                item_code = payload.get("item_code"),
                nos = int(payload.get("nos")),
                quantity1 = int(payload.get("quantity1")),
                quantity2 = int(payload.get("quantity2")),
                invoice_no = payload.get("invoice_no"),
                invoice_date = payload.get("invoice_date"),
                consignor_name = payload.get("consignor_name"),
                consignor_address = payload.get("consignor_address"),
                consignor_citycode = payload.get("consignor_citycode"),
                consignor_statecode =  payload.get("consignor_statecode"),
                consignor_phoneno = payload.get("consignor_phoneno"),
                consignee_name = payload.get("consignee_name"),
                consignee_address = payload.get("consignee_address"),
                consignee_citycode = payload.get("consignee_citycode"),
                consignee_statecode =  payload.get("consignee_statecode"),
                consignee_phoneno = payload.get("consignee_phoneno"),
                invoice_amount = float(payload.get("invoice_amount")),
                challon_no = payload.get("challon_no"),
                challan_date = payload.get("challan_date"),
                driver_name = payload.get("driver_name"),
                driver_licenseno = payload.get("driver_licenseno"),
                eway_billno = payload.get("eway_billno"),
                eway_billdate = payload.get("eway_billdate"),
                balance_qty = int(payload.get("balance_qty")),
                do_qty = int(payload.get("do_qty")),
                delivery_order_tno = int(payload.get("delivery_order_tno")),
            ).save()
        return {"details": "Data Inserted Successfully"}
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- Insert Cmpl Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/fetch/railsinglegrn", tags=['Rail Map'])
def endpoint_to_fetch_rail_single_data_to_fetch_for_grn(response: Response, rr_no: str):
    try:
        DataExecutionsHandler = DataExecutions()
        fetchDataRail = DataExecutionsHandler.findSingleRailDataThroughRRNo(rr_no=rr_no, response=response)
        return fetchDataRail
    except Exception as e:
        success = False
        response.status_code = 400
        console_logger.debug("----- Rail Single Grn Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e


@router.post("/update/rail/grn", tags={'Grn Status'})
def endpoint_to_update_rail_data_on_grn_db(response: Response, data: railGrnPost):
    try:
        mpayload = data.dict()
        try:
            checkGrn = Grn.objects.get(do_no=mpayload.get("rr_no"), invoice_no=mpayload.get("invoice_no"))
            response.status_code = 409
            return {"details": "GRN already booked"}
        except DoesNotExist as e:
            Grn(
                do_no=mpayload.get("rr_no"),
                invoice_date= mpayload.get("invoice_date"),
                invoice_no=mpayload.get("invoice_no"),
                sale_date = mpayload.get("sale_date"),
                grade = mpayload.get("grade"),
                dispatch_date = mpayload.get("avery_placement_date"),
                mine = mpayload.get("mine"),
                do_qty = mpayload.get("rr_qty"),
                # header_data=mpayload.get("header_data"),
                # original_data=mpayload.get("original_data"),
                new_data=mpayload.get("new_data"),
                approvals=mpayload.get("approvals"),
                changed_by=mpayload.get("changed_by"),
                sizing_charges_amount=mpayload.get("sizing_charges"),
                evac_facility_charge_amount=mpayload.get("evac_facility_charge"),
                royalty_charges_amount=mpayload.get("royality_charges"),
                nmet_amount=mpayload.get("nmet_charges"),
                dmf_amount=mpayload.get("dmf"),
                adho_sanrachna_vikas_amount=mpayload.get("adho_sanrachna_vikas"),
                pariyavaran_upkar_amount=mpayload.get("pariyavaran_upkar"),
                assessable_value_amount=mpayload.get("assessable_value"),
                igst_amount=mpayload.get("igst"),
                gst_comp_cess_amount=mpayload.get("gst_comp_cess"),
                gross_bill_value_amount=mpayload.get("gross_bill_value"),
                less_underloading_charges_amount=mpayload.get("less_underloading_charges"),
                net_value_amount=mpayload.get("net_value"),
                total_amount=mpayload.get("total_amount"),
                mode="rail",
                source_type=mpayload.get("source_type"),
            ).save()

        level_count = 0
        fetchMultilevelApproval = MultiApproval.objects.get(approval_name="grn_approval")
        if not fetchMultilevelApproval.bypass_level:
            for user_data in fetchMultilevelApproval.levels[level_count].user:
                header_data = {
                    "do_no":mpayload.get("rr_no"),
                    "invoice_date":mpayload.get("invoice_date"),
                    "invoice_no":mpayload.get("invoice_no"),
                    "sale_date":mpayload.get("sale_date"),
                    "grade":mpayload.get("grade"),
                    "dispatch_date":mpayload.get("avery_placement_date"),
                    "mine":mpayload.get("mine"),
                    "do_qty":mpayload.get("rr_qty"),
                }
                fetchtabledata = endpoint_to_generate_email_template(new_data=mpayload.get("new_data"), original_data="", user_data=user_data, do_no=mpayload.get("rr_no"), level_name="grn_approval", level_count=level_count, header_data=header_data, edited_by=mpayload.get("changed_by"), status="Approve", mode="rail")
                response_code, fetch_email = fetch_email_data()
                subject = "GRN Flow"
                to_data = user_data.get("email")
                body = fetchtabledata

                sender_email = fetch_email.get("Smtp_user")
                password = fetch_email.get("Smtp_password") 
                smtp_host = fetch_email.get("Smtp_host")
                smtp_port = fetch_email.get("Smtp_port")
                checkEmailDevelopment = EmailDevelopmentCheck.objects()
                if checkEmailDevelopment[0].development == "local":
                    send_multiapproval_mail(subject, to_data, body, sender_email, password, smtp_host, smtp_port)
                elif checkEmailDevelopment[0].development == "prod":
                    params = {
                        'subject': "GRN Flow",
                        'to_data': user_data.get("emzil"),
                        'body': fetchtabledata,
                        'sender_email': fetch_email.get("Smtp_user"),
                        'smtp_host': fetch_email.get("Smtp_host"),
                        'smtp_port': fetch_email.get("Smtp_port"),
                    }
                    send_multiapproval_email_struct(params=params)
                    
        else:
            console_logger.debug("bypass level true")
            level = len(fetchMultilevelApproval.levels)
            console_logger.debug(level-1)
            level_count_data = level-1
            for user_data in fetchMultilevelApproval.levels[-1].user:
                console_logger.debug(user_data)
                # user_data = endpoint_to_fetch_single_user_data_using_email(email=email)
                header_data = {
                    "do_no":mpayload.get("rr_no"),
                    "invoice_date":mpayload.get("invoice_date"),
                    "invoice_no":mpayload.get("invoice_no"),
                    "sale_date":mpayload.get("sale_date"),
                    "grade":mpayload.get("grade"),
                    "dispatch_date":mpayload.get("avery_placement_date"),
                    "mine":mpayload.get("mine"),
                    "do_qty":mpayload.get("rr_qty"),
                }
                fetchtabledata = endpoint_to_generate_email_template(new_data=mpayload.get("new_data"), original_data="", user_data=user_data, do_no=mpayload.get("rr_no"), level_name="grn_approval", level_count=level_count_data, header_data=header_data, edited_by=mpayload.get("changed_by"), status="Approve", mode="rail")
                response_code, fetch_email = fetch_email_data()
                subject = "GRN Flow"
                to_data = user_data.get("email")
                body = fetchtabledata

                sender_email = fetch_email.get("Smtp_user")
                password = fetch_email.get("Smtp_password") 
                smtp_host = fetch_email.get("Smtp_host")
                smtp_port = fetch_email.get("Smtp_port")
                checkEmailDevelopment = EmailDevelopmentCheck.objects()
                if checkEmailDevelopment[0].development == "local":
                    send_multiapproval_mail(subject, to_data, body, sender_email, password, smtp_host, smtp_port)
                elif checkEmailDevelopment[0].development == "prod":
                    params = {
                        'subject': "GRN Flow",
                        'to_data': user_data.get("email"),
                        'body': fetchtabledata,
                        'sender_email': fetch_email.get("Smtp_user"),
                        'smtp_host': fetch_email.get("Smtp_host"),
                        'smtp_port': fetch_email.get("Smtp_port"),
                    }
                    send_multiapproval_email_struct(params=params)
        return {"details": "success"}
    except Exception as e:
        success = False
        response.status_code = 400
        console_logger.debug("----- Rail Single Grn Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e

    
@router.get("/fetch/rail/grn", tags=["Rail Map"])
def endpoint_to_fetch_railway_grn_data(response: Response, currentPage: Optional[int] = None, perPage: Optional[int] = None, search_text: Optional[str] = None, start_timestamp: Optional[str] = None, end_timestamp: Optional[str] = None, month_date: Optional[str] = None, type: Optional[str] = "display", status: Optional[str] = None):
    try:
        result = {        
                "labels": [],
                "datasets": [],
                "total" : 0,
                "page_size": 15
        }
        if type and type == "display":
            page_no = 1
            page_len = result["page_size"]

            if currentPage:
                page_no = currentPage

            if perPage:
                page_len = perPage
                result["page_size"] = perPage

            data = Q()

            # based on condition for timestamp playing with & and | 
            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(avery_placement_date__gte = f"{start_date}")

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M","Asia/Kolkata")
                data &= Q(avery_placement_date__lte = f"{end_date}")

            if search_text:
                if search_text.isdigit():
                    data &= Q(rr_no__icontains=search_text) | Q(po_no__icontains=search_text)
                else:
                    data &= (Q(source__icontains=search_text))

            if month_date:
                # data &= Q(avery_placement_date__icontains=month_date)
                start_date = f'{month_date}-01'
                startd_date=datetime.datetime.strptime(f"{start_date}T00:00","%Y-%m-%dT%H:%M")
                end_date = (datetime.datetime.strptime(start_date, "%Y-%m-%d") + relativedelta(day=31)).strftime("%Y-%m-%d")
                data &= Q(avery_placement_date__gte = startd_date.strftime("%Y-%m-%dT%H:%M"))
                data &= Q(avery_placement_date__lte = f"{end_date}T23:59")


            offset = (page_no - 1) * page_len
            filtered_total_count = 0
            logs = (
                    RailData.objects(data, avery_placement_date__ne=None)
                    .order_by("-avery_placement_date")
                ) 
            if status == "pending":
                listData = []
                if any(logs):
                    for log in logs:
                        mPayload = log.simplepayload()
                        try:
                            checkGrnData = Grn.objects.get(do_no=mPayload.get("rr_no"))
                        except DoesNotExist:
                            try:
                                if mPayload.get("po_date"):
                                    if 10 < len(mPayload.get("po_date")) <= 13:
                                        mPayload["po_date"] = str(datetime.datetime.fromtimestamp(int(mPayload["po_date"]) / 1000).strftime("%Y-%m-%d"))
                                # if len(mPayload.get("po_date")) > 10:
                                #     mPayload["po_date"] = str(datetime.datetime.fromtimestamp(int(mPayload["po_date"]) / 1000).strftime("%Y-%m-%d"))
                                checkSapRecordsdata = sapRecordsRail.objects.get(rr_no=mPayload.get("rr_no"))
                                if checkSapRecordsdata.invoice_no is not None:
                                    result["labels"] = list(mPayload.keys())
                                    listData.append(mPayload)
                            except DoesNotExist:
                                pass

                    start_idx = (page_no - 1) * page_len
                    end_idx = start_idx + page_len
                    paginated_data = listData[start_idx:end_idx]
                    result["datasets"] = paginated_data
                    result["total"] = len(listData)
            elif status == "completed":
                listData = []
                if any(logs):
                    for log in logs:
                        mPayload = log.simplepayload()
                        try:
                            checkGrnData = Grn.objects.get(do_no=mPayload.get("rr_no"))
                            try:
                                if mPayload.get("po_date"):
                                    if 10 < len(mPayload.get("po_date")) <= 13:
                                        mPayload["po_date"] = str(datetime.datetime.fromtimestamp(int(mPayload["po_date"]) / 1000).strftime("%Y-%m-%d"))
                                # if len(mPayload.get("po_date")) > 10:
                                #     mPayload["po_date"] = str(datetime.datetime.fromtimestamp(int(mPayload["po_date"]) / 1000).strftime("%Y-%m-%d"))
                                checkSapRecordsdata = sapRecordsRail.objects.get(rr_no=mPayload.get("rr_no"))
                                if checkSapRecordsdata.invoice_no is not None:
                                    result["labels"] = list(mPayload.keys())
                                    listData.append(mPayload)
                            except DoesNotExist:
                                pass
                        except DoesNotExist as e:
                            pass
                        

                    start_idx = (page_no - 1) * page_len
                    end_idx = start_idx + page_len
                    paginated_data = listData[start_idx:end_idx]
                    result["datasets"] = paginated_data
                    result["total"] = len(listData)
            return result
        elif type and type == "download":
            del type

            file = str(datetime.datetime.now().strftime("%d-%m-%Y"))
            target_directory = f"static_server/gmr_ai/{file}"
            os.umask(0)
            os.makedirs(target_directory, exist_ok=True, mode=0o777)

            # Constructing the base for query
            data = Q()

            if start_timestamp:
                start_date = convert_to_utc_format(start_timestamp, "%Y-%m-%dT%H:%M")
                data &= Q(avery_placement_date__gte = f"{start_date}")

            if end_timestamp:
                end_date = convert_to_utc_format(end_timestamp, "%Y-%m-%dT%H:%M","Asia/Kolkata",False)
                data &= Q(avery_placement_date__lte = f"{end_date}")
            
            if search_text:
                if search_text.isdigit():
                    data &= Q(arv_cum_do_number__icontains = search_text) | Q(delivery_challan_number__icontains = search_text)
                else:
                    data &= Q(vehicle_number__icontains = search_text)

            usecase_data = RailData.objects(data, avery_placement_date__ne=None).order_by("-created_at")
            count = len(usecase_data)
            path = None
            logo_path = f"{os.path.join(os.getcwd(), 'static_server/receipt/report_logo.png')}"
            if usecase_data:
                try:
                    path = os.path.join(
                        "static_server",
                        "gmr_ai",
                        file,
                        "Rail_Report_{}.xlsx".format(
                            datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
                        ),
                    )
                    filename = os.path.join(os.getcwd(), path)
                    workbook = xlsxwriter.Workbook(filename)
                    workbook.use_zip64()
                    cell_format2 = workbook.add_format()
                    cell_format2.set_bold()
                    cell_format2.set_font_size(10)
                    cell_format2.set_align("center")
                    cell_format2.set_align("vcenter")
                    cell_format2.set_text_wrap(True)
                    cell_format2.set_border(1)

                    header_format = workbook.add_format({'bold': True, 'font_size': 40, 'align': 'center'})
                    date_format = workbook.add_format({'align': 'center', 'font_size': 12, "bold": True})
                    report_name_format = workbook.add_format({'align': 'center', 'font_size': 15, "bold": True})

                    header_format.set_align("vcenter")
                    date_format.set_align("vcenter")
                    report_name_format.set_align("vcenter")
                    header_format.set_border(1)
                    date_format.set_border(1)
                    report_name_format.set_border(1)

                    worksheet = workbook.add_worksheet()
                    worksheet.set_column("A:AZ", 20)
                    worksheet.set_default_row(50)
                    cell_format = workbook.add_format()
                    cell_format.set_font_size(10)
                    cell_format.set_align("center")
                    cell_format.set_align("vcenter")
                    cell_format.set_text_wrap(True)
                    cell_format.set_border(1)

                    worksheet.insert_image('A1', logo_path, {'x_scale': 0.3, 'y_scale': 0.3})
                    
                    # Merge cells for the main header and place it in the center
                    main_header = "GMR Warora Energy Limited"  # Set your main header text here
                    worksheet.merge_range("A1:AC1", main_header, header_format)  # Merge cells A1 to H1 for the header
                    
                    # Write the current date on the left side (A2)
                    worksheet.write("A2", f"Date: {datetime.datetime.now().strftime('%d-%m-%Y')}", date_format)
                    worksheet.merge_range("C2:AC2", f"Rail Coal Journey", report_name_format)

                    headers = [
                        "Sr.No",
                        "RR No",
                        "RR Qty",
                        "Po No",
                        "Po Date",
                        "Line Item",
                        "Source",
                        "Placement Date",
                        "Completion Date",
                        "Drawn Date",
                        "Total ul wt",
                        "Boxes Supplied",
                        "Total Secl Gross Wt",
                        "Total Secl Tare Wt",
                        "Total Secl Net Wt",
                        "Total Secl Ol Wt",
                        "Boxes Loaded",
                        "Total Rly Gross Wt",
                        "Total Rly_Tare Wt",
                        "Total Rly Net Wt",
                        "Total Rly Ol Wt",
                        "Total Secl Chargable Wt",
                        "Total Rly Chargable Wt",
                        "Freight",
                        "Gst",
                        "Pola",
                        "Total Freight",
                        "Source Type",
                        "Created At"
                    ]
                   
                    for index, header in enumerate(headers):
                        worksheet.write(2, index, header, cell_format2)

                    for row, query in enumerate(usecase_data, start=3):
                        result = query.simplepayload()
                        try:
                            if result.get("po_date"):
                                if 10 < len(result.get("po_date")) <= 13:
                                    result["po_date"] = str(datetime.datetime.fromtimestamp(int(result["po_date"]) / 1000).strftime("%Y-%m-%d"))
                            checkSapRecordsdata = sapRecordsRail.objects.get(rr_no=result["rr_no"])
                            if checkSapRecordsdata.invoice_no is not None:
                                worksheet.write(row, 0, count, cell_format)     
                                worksheet.write(row, 1, str(result["rr_no"]), cell_format)                      
                                worksheet.write(row, 2, str(result["rr_qty"]), cell_format)                      
                                worksheet.write(row, 3, str(result["po_no"]), cell_format)                      
                                worksheet.write(row, 4, str(result["po_date"]), cell_format)                      
                                worksheet.write(row, 5, str(result["line_item"]), cell_format)                      
                                worksheet.write(row, 6, str(result["source"]), cell_format)                      
                                worksheet.write(row, 7, str(result["placement_date"]), cell_format)                      
                                worksheet.write(row, 8, str(result["completion_date"]), cell_format)                      
                                worksheet.write(row, 9, str(result["drawn_date"]), cell_format)                      
                                worksheet.write(row, 10, float(result["total_ul_wt"]), cell_format)                      
                                worksheet.write(row, 11, str(result["boxes_supplied"]), cell_format)                      
                                worksheet.write(row, 12, float(result["total_secl_gross_wt"]), cell_format)                      
                                worksheet.write(row, 13, float(result["total_secl_tare_wt"]), cell_format)                      
                                worksheet.write(row, 14, float(result["total_secl_net_wt"]), cell_format)                      
                                worksheet.write(row, 15, float(result["total_secl_ol_wt"]), cell_format)                      
                                worksheet.write(row, 16, str(result["boxes_loaded"]), cell_format)                      
                                worksheet.write(row, 17, float(result["total_rly_gross_wt"]), cell_format)                      
                                worksheet.write(row, 18, float(result["total_rly_tare_wt"]), cell_format)                      
                                worksheet.write(row, 19, float(result["total_rly_net_wt"]), cell_format)                      
                                worksheet.write(row, 20, float(result["total_rly_ol_wt"]), cell_format)                      
                                worksheet.write(row, 21, float(result["total_secl_chargable_wt"]), cell_format)                      
                                worksheet.write(row, 22, float(result["total_rly_chargable_wt"]), cell_format)                      
                                worksheet.write(row, 23, float(result["freight"]), cell_format)                      
                                worksheet.write(row, 24, float(result["gst"]), cell_format)
                                if result.get("pola") == "Not found":                    
                                    worksheet.write(row, 25, str(result["pola"]), cell_format)
                                elif result.get("pola") == "":
                                    worksheet.write(row, 25, "N/A", cell_format)
                                else:
                                    worksheet.write(row, 25, float(result["pola"]), cell_format)             
                                worksheet.write(row, 26, float(result["total_freight"]), cell_format)                      
                                worksheet.write(row, 27, str(result["source_type"]), cell_format)                      
                                worksheet.write(row, 28, str(result["created_at"]), cell_format)                   
                            
                                count-=1
                        except DoesNotExist as e:
                            pass
                        
                    workbook.close()
                    console_logger.debug("Successfully {} report generated".format(service_id))
                    console_logger.debug("sent data {}".format(path))

                    return {
                            "Type": "gmr_rail_journey_download_event",
                            "Datatype": "Report",
                            "File_Path": path,
                            }
                except Exception as e:
                    console_logger.debug(e)
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                    console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
                    console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
            else:
                console_logger.error("No data found")
                return {
                        "Type": "gmr_rail_journey_download_event",
                        "Datatype": "Report",
                        "File_Path": path,
                        }
    except Exception as e:
        console_logger.debug("----- Fetch Report Name Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return e


@router.get("/form15automation", tags=['Form 15'])
def endpoint_to_automate_form15_data(month: Optional[str] = None):
    try:
        start_date = f"{month}-01"
        format_date = "%Y-%m-%d"
        startd_date = datetime.datetime.strptime(start_date, format_date)
        end_date = startd_date + relativedelta( day=31)
        endd_date = end_date.replace(hour=23, minute=59, second=59)
        fetchMultyGrn = Grn.objects(created_at__gte=startd_date, created_at__lte=endd_date)
        linkage_total_amt = 0
        spl_eauction = 0
        shakti_b3 = 0
        shakti_b8 = 0
        for single_data_grn in fetchMultyGrn:
            mPayload = single_data_grn.payload()
            console_logger.debug(mPayload.get("mode"))
            if mPayload.get("mode") == "road":
                console_logger.debug(mPayload.get("type_consumer"))
                if mPayload.get("type_consumer") == "WCL FSA Coal":
                    console_logger.debug("WCL FSA Coal")
                    if mPayload.get("total_amount"):
                        linkage_total_amt += mPayload.get("total_amount")
                elif any(item in mPayload.get("type_consumer") for item in ["WCL E Auction Coal", "WCL Shakti B(viii) Coal"]):
                    console_logger.debug("WCL")
                    if mPayload.get("total_amount"):
                        spl_eauction += mPayload.get("total_amount")
                elif mPayload.get("type_consumer").startswith("WCL Shakti B(iii)"):
                    console_logger.debug("inside wcl shakti b3")
                    console_logger.debug(mPayload.get("total_amount"))
                    if mPayload.get("total_amount"):
                        shakti_b3 += mPayload.get("total_amount")
                elif mPayload.get("type_consumer").startswith("WCL Shakti B(viii)"):
                    console_logger.debug("inside wcl shakti b8")
                    if mPayload.get("total_amount"):
                        shakti_b8 += mPayload.get("total_amount")
            elif mPayload.get("mode") == "rail":
                if mPayload.get("source_type") == "SECL Linkage(U1)":
                    if mPayload.get("net_value_amount"):
                        linkage_total_amt += mPayload.get("net_value_amount")
                elif mPayload.get("source_type") == "WCL E-Auction Linkage(U1)":
                    if mPayload.get("net_value_amount"):
                        spl_eauction += mPayload.get("net_value_amount")
                elif mPayload.get("source_type").startswith("WCL Shakti B(iii)"):
                    if mPayload.get("net_value_amount"):
                        shakti_b3 += mPayload.get("net_value_amount")
                elif mPayload.get("source_type").startswith("WCL Shakti B(viii)"):
                    if mPayload.get("net_value_amount"):
                        shakti_b8 += mPayload.get("net_value_amount")    

        console_logger.debug(linkage_total_amt)
        console_logger.debug(spl_eauction)
        console_logger.debug(shakti_b3)
        console_logger.debug(shakti_b8)
        try:
            checkForm15Data = Form15Data.objects.get(month__gte=startd_date, month__lte=endd_date)
            if checkForm15Data:
                qty_supplied_dict = {
                    "remark": "GRN Quantity",
                    "uom": "MT",
                    "mou_coal": 0,
                    "linkage": linkage_total_amt,
                    "aiwib_washery": 0,
                    "open_mkt": 0,
                    "spot_eauction": 0,
                    "spl_for_eauction": spl_eauction,
                    "imported": 0,
                    "total": linkage_total_amt+spl_eauction,
                    "shakti_b": shakti_b3,
                    "shakti_b3": shakti_b8,
                    "particular": "Quantity of Coal/Lignite supplied by Coal/Lignite Company",
                }
                checkForm15Data.update(
                    qty_supplied=qty_supplied_dict
                )
            checkForm15Data.reload()
            # coal supplied update start
            if checkForm15Data.adj_qty and checkForm15Data.qty_supplied:
                # coal_supplied_dict = {
                #     "remark": "",
                #     "uom": "MT",
                #     "mou_coal": checkForm15Data.adj_qty.mou_coal-checkForm15Data.qty_supplied.mou_coal,
                #     "linkage": checkForm15Data.adj_qty.linkage-checkForm15Data.qty_supplied.linkage,
                #     "aiwib_washery": checkForm15Data.adj_qty.aiwib_washery-checkForm15Data.qty_supplied.aiwib_washery,
                #     "open_mkt": checkForm15Data.adj_qty.open_mkt-checkForm15Data.qty_supplied.open_mkt,
                #     "spot_eauction": checkForm15Data.adj_qty.spot_eauction-checkForm15Data.qty_supplied.spot_eauction,
                #     "spl_for_eauction": checkForm15Data.adj_qty.spl_for_eauction-checkForm15Data.qty_supplied.spl_for_eauction,
                #     "imported": checkForm15Data.adj_qty.imported-checkForm15Data.qty_supplied.imported,
                #     "total": checkForm15Data.adj_qty.total-checkForm15Data.qty_supplied.total,
                #     "shakti_b": checkForm15Data.adj_qty.shakti_b-checkForm15Data.qty_supplied.shakti_b,
                #     "shakti_b3": checkForm15Data.adj_qty.shakti_b3-checkForm15Data.qty_supplied.shakti_b3,
                #     "particular": "Coal Supplied by Coal Lignite company (3+4)",
                # }
                coal_supplied_dict = {
                    "remark": "",
                    "uom": "MT",
                    "mou_coal": checkForm15Data.adj_qty.mou_coal - checkForm15Data.qty_supplied.mou_coal if checkForm15Data.qty_supplied.mou_coal >= 0 else checkForm15Data.adj_qty.mou_coal + abs(checkForm15Data.qty_supplied.mou_coal),
                    "linkage": checkForm15Data.adj_qty.linkage - checkForm15Data.qty_supplied.linkage if checkForm15Data.qty_supplied.linkage >= 0 else checkForm15Data.adj_qty.linkage + abs(checkForm15Data.qty_supplied.linkage),
                    "aiwib_washery": checkForm15Data.adj_qty.aiwib_washery - checkForm15Data.qty_supplied.aiwib_washery if checkForm15Data.qty_supplied.aiwib_washery >= 0 else checkForm15Data.adj_qty.aiwib_washery + abs(checkForm15Data.qty_supplied.aiwib_washery),
                    "open_mkt": checkForm15Data.adj_qty.open_mkt - checkForm15Data.qty_supplied.open_mkt if checkForm15Data.qty_supplied.open_mkt >= 0 else checkForm15Data.adj_qty.open_mkt + abs(checkForm15Data.qty_supplied.open_mkt),
                    "spot_eauction": checkForm15Data.adj_qty.spot_eauction - checkForm15Data.qty_supplied.spot_eauction if checkForm15Data.qty_supplied.spot_eauction >= 0 else checkForm15Data.adj_qty.spot_eauction + abs(checkForm15Data.qty_supplied.spot_eauction),
                    "spl_for_eauction": checkForm15Data.adj_qty.spl_for_eauction - checkForm15Data.qty_supplied.spl_for_eauction if checkForm15Data.qty_supplied.spl_for_eauction >= 0 else checkForm15Data.adj_qty.spl_for_eauction + abs(checkForm15Data.qty_supplied.spl_for_eauction),
                    "imported": checkForm15Data.adj_qty.imported - checkForm15Data.qty_supplied.imported if checkForm15Data.qty_supplied.imported >= 0 else checkForm15Data.adj_qty.imported + abs(checkForm15Data.qty_supplied.imported),
                    "total": checkForm15Data.adj_qty.total - checkForm15Data.qty_supplied.total if checkForm15Data.qty_supplied.total >= 0 else checkForm15Data.adj_qty.total + abs(checkForm15Data.qty_supplied.total),
                    "shakti_b": checkForm15Data.adj_qty.shakti_b - checkForm15Data.qty_supplied.shakti_b if checkForm15Data.qty_supplied.shakti_b >= 0 else checkForm15Data.adj_qty.shakti_b + abs(checkForm15Data.qty_supplied.shakti_b),
                    "shakti_b3": checkForm15Data.adj_qty.shakti_b3 - checkForm15Data.qty_supplied.shakti_b3 if checkForm15Data.qty_supplied.shakti_b3 >= 0 else checkForm15Data.adj_qty.shakti_b3 + abs(checkForm15Data.qty_supplied.shakti_b3),
                    "particular": "Coal Supplied by Coal Lignite company (3+4)",
                }
                checkForm15Data.update(
                    coal_supplied=coal_supplied_dict
                )
            checkForm15Data.reload()
            # coal supplied update end
            if checkForm15Data.coal_supplied and checkForm15Data.norm_transit_loss:
                console_logger.debug("inside net supplied")
                net_supplied_dict = {
                    "remark": "",
                    "uom": "MT",
                    "mou_coal": checkForm15Data.coal_supplied.mou_coal - checkForm15Data.norm_transit_loss.mou_coal if checkForm15Data.norm_transit_loss.mou_coal >= 0 else checkForm15Data.coal_supplied.mou_coal + abs(checkForm15Data.norm_transit_loss.mou_coal),
                    "linkage": checkForm15Data.coal_supplied.linkage - checkForm15Data.norm_transit_loss.linkage if checkForm15Data.norm_transit_loss.linkage >= 0 else checkForm15Data.coal_supplied.linkage + abs(checkForm15Data.norm_transit_loss.linkage),
                    "aiwib_washery": checkForm15Data.coal_supplied.aiwib_washery - checkForm15Data.norm_transit_loss.aiwib_washery if checkForm15Data.norm_transit_loss.aiwib_washery >= 0 else checkForm15Data.coal_supplied.aiwib_washery + abs(checkForm15Data.norm_transit_loss.aiwib_washery),
                    "open_mkt": checkForm15Data.coal_supplied.open_mkt - checkForm15Data.norm_transit_loss.open_mkt if checkForm15Data.norm_transit_loss.open_mkt >= 0 else checkForm15Data.coal_supplied.open_mkt + abs(checkForm15Data.norm_transit_loss.open_mkt),
                    "spot_eauction": checkForm15Data.coal_supplied.spot_eauction - checkForm15Data.norm_transit_loss.spot_eauction if checkForm15Data.norm_transit_loss.spot_eauction >= 0 else checkForm15Data.coal_supplied.spot_eauction + abs(checkForm15Data.norm_transit_loss.spot_eauction),
                    "spl_for_eauction": checkForm15Data.coal_supplied.spl_for_eauction - checkForm15Data.norm_transit_loss.spl_for_eauction if checkForm15Data.norm_transit_loss.spl_for_eauction >= 0 else checkForm15Data.coal_supplied.spl_for_eauction + abs(checkForm15Data.norm_transit_loss.spl_for_eauction),
                    "imported": checkForm15Data.coal_supplied.imported - checkForm15Data.norm_transit_loss.imported if checkForm15Data.norm_transit_loss.imported >= 0 else checkForm15Data.coal_supplied.imported + abs(checkForm15Data.norm_transit_loss.imported),
                    "total": checkForm15Data.coal_supplied.total - checkForm15Data.norm_transit_loss.total if checkForm15Data.norm_transit_loss.total >= 0 else checkForm15Data.coal_supplied.total + abs(checkForm15Data.norm_transit_loss.total),
                    "shakti_b": checkForm15Data.coal_supplied.shakti_b - checkForm15Data.norm_transit_loss.shakti_b if checkForm15Data.norm_transit_loss.shakti_b >= 0 else checkForm15Data.coal_supplied.shakti_b + abs(checkForm15Data.norm_transit_loss.shakti_b),
                    "shakti_b3": checkForm15Data.coal_supplied.shakti_b3 - checkForm15Data.norm_transit_loss.shakti_b3 if checkForm15Data.norm_transit_loss.shakti_b3 >= 0 else checkForm15Data.coal_supplied.shakti_b3 + abs(checkForm15Data.norm_transit_loss.shakti_b3),
                    "particular": "Net Coal/Lignite Supplied (5-6)",
                }
                checkForm15Data.update(
                    net_supplied=net_supplied_dict
                )

            checkForm15Data.reload()
            if checkForm15Data.amt_charged and checkForm15Data.adj_amt and checkForm15Data.unloading_charges:
                # - then +
                total_amt_charged_dict = {
                    "remark": "",
                    "uom": "MT",
                    "mou_coal": checkForm15Data.amt_charged.mou_coal - checkForm15Data.adj_amt.mou_coal + checkForm15Data.unloading_charges.mou_coal if checkForm15Data.adj_amt.mou_coal >= 0 else checkForm15Data.coal_supplied.mou_coal + abs(checkForm15Data.adj_amt.mou_coal),
                    "linkage": checkForm15Data.amt_charged.linkage - checkForm15Data.adj_amt.linkage + checkForm15Data.unloading_charges.linkage if checkForm15Data.adj_amt.linkage >= 0 else checkForm15Data.amt_charged.linkage + abs(checkForm15Data.adj_amt.linkage),
                    "aiwib_washery": checkForm15Data.amt_charged.aiwib_washery - checkForm15Data.adj_amt.aiwib_washery + checkForm15Data.unloading_charges.aiwib_washery if checkForm15Data.adj_amt.aiwib_washery >= 0 else checkForm15Data.amt_charged.aiwib_washery + abs(checkForm15Data.adj_amt.aiwib_washery),
                    "open_mkt": checkForm15Data.amt_charged.open_mkt - checkForm15Data.adj_amt.open_mkt + checkForm15Data.unloading_charges.open_mkt if checkForm15Data.adj_amt.open_mkt >= 0 else checkForm15Data.amt_charged.open_mkt + abs(checkForm15Data.adj_amt.open_mkt),
                    "spot_eauction": checkForm15Data.amt_charged.spot_eauction - checkForm15Data.adj_amt.spot_eauction + checkForm15Data.unloading_charges.spot_eauction if checkForm15Data.adj_amt.spot_eauction >= 0 else checkForm15Data.amt_charged.spot_eauction + abs(checkForm15Data.adj_amt.spot_eauction),
                    "spl_for_eauction": checkForm15Data.amt_charged.spl_for_eauction - checkForm15Data.adj_amt.spl_for_eauction + checkForm15Data.unloading_charges.spl_for_eauction if checkForm15Data.adj_amt.spl_for_eauction >= 0 else checkForm15Data.amt_charged.spl_for_eauction + abs(checkForm15Data.adj_amt.spl_for_eauction),
                    "imported": checkForm15Data.amt_charged.imported - checkForm15Data.adj_amt.imported + checkForm15Data.unloading_charges.imported if checkForm15Data.adj_amt.imported >= 0 else checkForm15Data.amt_charged.imported + abs(checkForm15Data.adj_amt.imported),
                    "total": checkForm15Data.amt_charged.total - checkForm15Data.adj_amt.total + checkForm15Data.unloading_charges.total if checkForm15Data.adj_amt.total >= 0 else checkForm15Data.amt_charged.total + abs(checkForm15Data.adj_amt.total),
                    "shakti_b": checkForm15Data.amt_charged.shakti_b - checkForm15Data.adj_amt.shakti_b + checkForm15Data.unloading_charges.shakti_b if checkForm15Data.adj_amt.shakti_b >= 0 else checkForm15Data.amt_charged.shakti_b + abs(checkForm15Data.adj_amt.shakti_b),
                    "shakti_b3": checkForm15Data.amt_charged.shakti_b3 - checkForm15Data.adj_amt.shakti_b3 + checkForm15Data.unloading_charges.shakti_b3 if checkForm15Data.adj_amt.shakti_b3 >= 0 else checkForm15Data.amt_charged.shakti_b3 + abs(checkForm15Data.adj_amt.shakti_b3),
                    "particular": "Total amount Charged (8+9+10)",
                }

                # total_amt_charged_dict = {
                #     "remark": "",
                #     "uom": "MT",
                #     "mou_coal": checkForm15Data.amt_charged.mou_coal + checkForm15Data.adj_amt.mou_coal + checkForm15Data.unloading_charges.mou_coal,
                #     "linkage": checkForm15Data.amt_charged.linkage + checkForm15Data.adj_amt.linkage + checkForm15Data.unloading_charges.linkage,
                #     "aiwib_washery": checkForm15Data.amt_charged.aiwib_washery + checkForm15Data.adj_amt.aiwib_washery + checkForm15Data.unloading_charges.aiwib_washery,
                #     "open_mkt": checkForm15Data.amt_charged.open_mkt + checkForm15Data.adj_amt.open_mkt + checkForm15Data.unloading_charges.open_mkt,
                #     "spot_eauction": checkForm15Data.amt_charged.spot_eauction + checkForm15Data.adj_amt.spot_eauction + checkForm15Data.unloading_charges.spot_eauction,
                #     "spl_for_eauction": checkForm15Data.amt_charged.spl_for_eauction + checkForm15Data.adj_amt.spl_for_eauction + checkForm15Data.unloading_charges.spl_for_eauction,
                #     "imported": checkForm15Data.amt_charged.imported + checkForm15Data.adj_amt.imported + checkForm15Data.unloading_charges.imported,
                #     "total": checkForm15Data.amt_charged.total + checkForm15Data.adj_amt.total + checkForm15Data.unloading_charges.total,
                #     "shakti_b": checkForm15Data.amt_charged.shakti_b + checkForm15Data.adj_amt.shakti_b + checkForm15Data.unloading_charges.shakti_b,
                #     "shakti_b3": checkForm15Data.amt_charged.shakti_b3 + checkForm15Data.adj_amt.shakti_b3 + checkForm15Data.unloading_charges.shakti_b3,
                #     "particular": "Total amount Charged (8+9+10)",
                # }
                checkForm15Data.update(
                    total_amt_charged=total_amt_charged_dict
                )

            checkForm15Data.reload()
            if checkForm15Data.trans_charges and checkForm15Data.adj_trans_charges and checkForm15Data.demurrage and checkForm15Data.diesel_cost:
                total_trans_charges_dict = {
                    "remark": "",
                    "uom": "(Rs.)",
                    "mou_coal": checkForm15Data.trans_charges.mou_coal - checkForm15Data.adj_trans_charges.mou_coal + checkForm15Data.demurrage.mou_coal + checkForm15Data.diesel_cost.mou_coal if checkForm15Data.adj_trans_charges.mou_coal >= 0 else checkForm15Data.coal_supplied.mou_coal + abs(checkForm15Data.adj_trans_charges.mou_coal),
                    "linkage": checkForm15Data.trans_charges.linkage - checkForm15Data.adj_trans_charges.linkage + checkForm15Data.demurrage.linkage + checkForm15Data.diesel_cost.linkage if checkForm15Data.adj_trans_charges.linkage >= 0 else checkForm15Data.trans_charges.linkage + abs(checkForm15Data.adj_trans_charges.linkage),
                    "aiwib_washery": checkForm15Data.trans_charges.aiwib_washery - checkForm15Data.adj_trans_charges.aiwib_washery + checkForm15Data.demurrage.aiwib_washery + checkForm15Data.diesel_cost.aiwib_washery if checkForm15Data.adj_trans_charges.aiwib_washery >= 0 else checkForm15Data.trans_charges.aiwib_washery + abs(checkForm15Data.adj_trans_charges.aiwib_washery),
                    "open_mkt": checkForm15Data.trans_charges.open_mkt - checkForm15Data.adj_trans_charges.open_mkt + checkForm15Data.demurrage.open_mkt + checkForm15Data.diesel_cost.open_mkt if checkForm15Data.adj_trans_charges.open_mkt >= 0 else checkForm15Data.trans_charges.open_mkt + abs(checkForm15Data.adj_trans_charges.open_mkt),
                    "spot_eauction": checkForm15Data.trans_charges.spot_eauction - checkForm15Data.adj_trans_charges.spot_eauction + checkForm15Data.demurrage.spot_eauction + checkForm15Data.diesel_cost.spot_eauction if checkForm15Data.adj_trans_charges.spot_eauction >= 0 else checkForm15Data.trans_charges.spot_eauction + abs(checkForm15Data.adj_trans_charges.spot_eauction),
                    "spl_for_eauction": checkForm15Data.trans_charges.spl_for_eauction - checkForm15Data.adj_trans_charges.spl_for_eauction + checkForm15Data.demurrage.spl_for_eauction + checkForm15Data.diesel_cost.spl_for_eauction if checkForm15Data.adj_trans_charges.spl_for_eauction >= 0 else checkForm15Data.trans_charges.spl_for_eauction + abs(checkForm15Data.adj_trans_charges.spl_for_eauction),
                    "imported": checkForm15Data.trans_charges.imported - checkForm15Data.adj_trans_charges.imported + checkForm15Data.demurrage.imported + checkForm15Data.diesel_cost.imported if checkForm15Data.adj_trans_charges.imported >= 0 else checkForm15Data.trans_charges.imported + abs(checkForm15Data.adj_trans_charges.imported),
                    "total": checkForm15Data.trans_charges.total - checkForm15Data.adj_trans_charges.total + checkForm15Data.demurrage.total + checkForm15Data.diesel_cost.total if checkForm15Data.adj_trans_charges.total >= 0 else checkForm15Data.trans_charges.total + abs(checkForm15Data.adj_trans_charges.total),
                    "shakti_b": checkForm15Data.trans_charges.shakti_b - checkForm15Data.adj_trans_charges.shakti_b + checkForm15Data.demurrage.shakti_b + checkForm15Data.diesel_cost.shakti_b if checkForm15Data.adj_trans_charges.shakti_b >= 0 else checkForm15Data.trans_charges.shakti_b + abs(checkForm15Data.adj_trans_charges.shakti_b),
                    "shakti_b3": checkForm15Data.trans_charges.shakti_b3 - checkForm15Data.adj_trans_charges.shakti_b3 + checkForm15Data.demurrage.shakti_b3 + checkForm15Data.diesel_cost.shakti_b3 if checkForm15Data.adj_trans_charges.shakti_b3 >= 0 else checkForm15Data.trans_charges.shakti_b3 + abs(checkForm15Data.adj_trans_charges.shakti_b3),
                    "particular": "Total transportation charges (12+/-13+14+15)",
                }
                checkForm15Data.update(
                    total_trans_charges=total_trans_charges_dict
                )
            checkForm15Data.reload()
            if checkForm15Data.total_amt_charged and checkForm15Data.total_trans_charges:
                total_amt_incl_trans = {
                    "remark": "",
                    "uom": "(Rs.)",
                    "mou_coal": checkForm15Data.total_amt_charged.mou_coal - checkForm15Data.total_trans_charges.mou_coal if checkForm15Data.total_trans_charges.mou_coal >= 0 else checkForm15Data.total_amt_charged.mou_coal + abs(checkForm15Data.total_trans_charges.mou_coal),
                    "linkage": checkForm15Data.total_amt_charged.linkage - checkForm15Data.total_trans_charges.linkage if checkForm15Data.total_trans_charges.linkage >= 0 else checkForm15Data.total_amt_charged.linkage + abs(checkForm15Data.total_trans_charges.linkage),
                    "aiwib_washery": checkForm15Data.total_amt_charged.aiwib_washery - checkForm15Data.total_trans_charges.aiwib_washery if checkForm15Data.total_trans_charges.aiwib_washery >= 0 else checkForm15Data.total_amt_charged.aiwib_washery + abs(checkForm15Data.total_trans_charges.aiwib_washery),
                    "open_mkt": checkForm15Data.total_amt_charged.open_mkt - checkForm15Data.total_trans_charges.open_mkt if checkForm15Data.total_trans_charges.open_mkt >= 0 else checkForm15Data.total_amt_charged.open_mkt + abs(checkForm15Data.total_trans_charges.open_mkt),
                    "spot_eauction": checkForm15Data.total_amt_charged.spot_eauction - checkForm15Data.total_trans_charges.spot_eauction if checkForm15Data.total_trans_charges.spot_eauction >= 0 else checkForm15Data.total_amt_charged.spot_eauction + abs(checkForm15Data.total_trans_charges.spot_eauction),
                    "spl_for_eauction": checkForm15Data.total_amt_charged.spl_for_eauction - checkForm15Data.total_trans_charges.spl_for_eauction if checkForm15Data.total_trans_charges.spl_for_eauction >= 0 else checkForm15Data.total_amt_charged.spl_for_eauction + abs(checkForm15Data.total_trans_charges.spl_for_eauction),
                    "imported": checkForm15Data.total_amt_charged.imported - checkForm15Data.total_trans_charges.imported if checkForm15Data.total_trans_charges.imported >= 0 else checkForm15Data.total_amt_charged.imported + abs(checkForm15Data.total_trans_charges.imported),
                    "total": checkForm15Data.total_amt_charged.total - checkForm15Data.total_trans_charges.total if checkForm15Data.total_trans_charges.total >= 0 else checkForm15Data.total_amt_charged.total + abs(checkForm15Data.total_trans_charges.total),
                    "shakti_b": checkForm15Data.total_amt_charged.shakti_b - checkForm15Data.total_trans_charges.shakti_b if checkForm15Data.total_trans_charges.shakti_b >= 0 else checkForm15Data.total_amt_charged.shakti_b + abs(checkForm15Data.total_trans_charges.shakti_b),
                    "shakti_b3": checkForm15Data.total_amt_charged.shakti_b3 - checkForm15Data.total_trans_charges.shakti_b3 if checkForm15Data.total_trans_charges.shakti_b3 >= 0 else checkForm15Data.total_amt_charged.shakti_b3 + abs(checkForm15Data.total_trans_charges.shakti_b3),
                    "particular": "Total amount charged for Coal/lignite supplied including transportation (11+16)",
                }

                checkForm15Data.update(
                    total_amt_incl_trans=total_amt_incl_trans
                )
            checkForm15Data.reload()
            if checkForm15Data.osd_month and checkForm15Data.net_supplied:
                total_qty_at_station_dict = {
                    "remark": "",
                    "uom": "(Rs.)",
                    "mou_coal": checkForm15Data.osd_month.mou_coal - checkForm15Data.net_supplied.mou_coal if checkForm15Data.net_supplied.mou_coal >= 0 else checkForm15Data.osd_month.mou_coal + abs(checkForm15Data.net_supplied.mou_coal),
                    "linkage": checkForm15Data.osd_month.linkage - checkForm15Data.net_supplied.linkage if checkForm15Data.net_supplied.linkage >= 0 else checkForm15Data.osd_month.linkage + abs(checkForm15Data.net_supplied.linkage),
                    "aiwib_washery": checkForm15Data.osd_month.aiwib_washery - checkForm15Data.net_supplied.aiwib_washery if checkForm15Data.net_supplied.aiwib_washery >= 0 else checkForm15Data.osd_month.aiwib_washery + abs(checkForm15Data.net_supplied.aiwib_washery),
                    "open_mkt": checkForm15Data.osd_month.open_mkt - checkForm15Data.net_supplied.open_mkt if checkForm15Data.net_supplied.open_mkt >= 0 else checkForm15Data.osd_month.open_mkt + abs(checkForm15Data.net_supplied.open_mkt),
                    "spot_eauction": checkForm15Data.osd_month.spot_eauction - checkForm15Data.net_supplied.spot_eauction if checkForm15Data.net_supplied.spot_eauction >= 0 else checkForm15Data.osd_month.spot_eauction + abs(checkForm15Data.net_supplied.spot_eauction),
                    "spl_for_eauction": checkForm15Data.osd_month.spl_for_eauction - checkForm15Data.net_supplied.spl_for_eauction if checkForm15Data.net_supplied.spl_for_eauction >= 0 else checkForm15Data.osd_month.spl_for_eauction + abs(checkForm15Data.net_supplied.spl_for_eauction),
                    "imported": checkForm15Data.osd_month.imported - checkForm15Data.net_supplied.imported if checkForm15Data.net_supplied.imported >= 0 else checkForm15Data.osd_month.imported + abs(checkForm15Data.net_supplied.imported),
                    "total": checkForm15Data.osd_month.total - checkForm15Data.net_supplied.total if checkForm15Data.net_supplied.total >= 0 else checkForm15Data.osd_month.total + abs(checkForm15Data.net_supplied.total),
                    "shakti_b": checkForm15Data.osd_month.shakti_b - checkForm15Data.net_supplied.shakti_b if checkForm15Data.net_supplied.shakti_b >= 0 else checkForm15Data.osd_month.shakti_b + abs(checkForm15Data.net_supplied.shakti_b),
                    "shakti_b3": checkForm15Data.osd_month.shakti_b3 - checkForm15Data.net_supplied.shakti_b3 if checkForm15Data.net_supplied.shakti_b3 >= 0 else checkForm15Data.osd_month.shakti_b3 + abs(checkForm15Data.net_supplied.shakti_b3),
                    "particular": "Quantity of coal at station for the month (1+7)",
                }

                checkForm15Data.update(
                    qty_at_station=total_qty_at_station_dict
                )
            checkForm15Data.reload()
            # vos_month
            try:
                previous_month_start = startd_date - relativedelta(months=1)
                prev_startd_date = previous_month_start
                prev_endd_date = prev_startd_date + relativedelta(day=31)
                prev_endd_date = prev_endd_date.replace(hour=23, minute=59, second=59)

                checkForm15Data_vos = Form15Data.objects.get(month__gte=prev_startd_date, month__lte=prev_endd_date)
                if checkForm15Data_vos.closing_coal_stock_value:
                    vos_month_dict = {
                        "remark": "",
                        "uom": "(Rs.)",
                        "mou_coal": checkForm15Data_vos.closing_coal_stock_value.mou_coal,
                        "linkage": checkForm15Data_vos.closing_coal_stock_value.linkage,
                        "aiwib_washery": checkForm15Data_vos.closing_coal_stock_value.aiwib_washery,
                        "open_mkt": checkForm15Data_vos.closing_coal_stock_value.open_mkt,
                        "spot_eauction": checkForm15Data_vos.closing_coal_stock_value.spot_eauction,
                        "spl_for_eauction": checkForm15Data_vos.closing_coal_stock_value.spl_for_eauction,
                        "imported": checkForm15Data_vos.closing_coal_stock_value.imported,
                        "total": checkForm15Data_vos.closing_coal_stock_value.total,
                        "shakti_b": checkForm15Data_vos.closing_coal_stock_value.shakti_b,
                        "shakti_b3": checkForm15Data_vos.closing_coal_stock_value.shakti_b3,
                        "particular": "Value of opening stock as on 1st Day of the Month",
                    }

                    checkForm15Data.update(
                        vos_month=vos_month_dict
                    )
            except DoesNotExist as e:
                vos_month_dict = {
                    "remark": "",
                    "uom": "(Rs.)",
                    "mou_coal": 0,
                    "linkage": 0,
                    "aiwib_washery": 0,
                    "open_mkt": 0,
                    "spot_eauction": 0,
                    "spl_for_eauction": 0,
                    "imported": 0,
                    "total": 0,
                    "shakti_b": 0,
                    "shakti_b3": 0,
                    "particular": "Value of opening stock as on 1st Day of the Month",
                }
                checkForm15Data.update(
                    vos_month=vos_month_dict
                )
            
            checkForm15Data.reload()
            if checkForm15Data.vos_month and checkForm15Data.total_amt_incl_trans:
                total_amt_for_coal_dict = {
                    "remark": "",
                    "uom": "(Rs.)",
                    "mou_coal": checkForm15Data.vos_month.mou_coal - checkForm15Data.total_amt_incl_trans.mou_coal if checkForm15Data.total_amt_incl_trans.mou_coal >= 0 else checkForm15Data.vos_month.mou_coal + abs(checkForm15Data.total_amt_incl_trans.mou_coal),
                    "linkage": checkForm15Data.vos_month.linkage - checkForm15Data.total_amt_incl_trans.linkage if checkForm15Data.total_amt_incl_trans.linkage >= 0 else checkForm15Data.vos_month.linkage + abs(checkForm15Data.total_amt_incl_trans.linkage),
                    "aiwib_washery": checkForm15Data.vos_month.aiwib_washery - checkForm15Data.total_amt_incl_trans.aiwib_washery if checkForm15Data.total_amt_incl_trans.aiwib_washery >= 0 else checkForm15Data.vos_month.aiwib_washery + abs(checkForm15Data.total_amt_incl_trans.aiwib_washery),
                    "open_mkt": checkForm15Data.vos_month.open_mkt - checkForm15Data.total_amt_incl_trans.open_mkt if checkForm15Data.total_amt_incl_trans.open_mkt >= 0 else checkForm15Data.vos_month.open_mkt + abs(checkForm15Data.total_amt_incl_trans.open_mkt),
                    "spot_eauction": checkForm15Data.vos_month.spot_eauction - checkForm15Data.total_amt_incl_trans.spot_eauction if checkForm15Data.total_amt_incl_trans.spot_eauction >= 0 else checkForm15Data.vos_month.spot_eauction + abs(checkForm15Data.total_amt_incl_trans.spot_eauction),
                    "spl_for_eauction": checkForm15Data.vos_month.spl_for_eauction - checkForm15Data.total_amt_incl_trans.spl_for_eauction if checkForm15Data.total_amt_incl_trans.spl_for_eauction >= 0 else checkForm15Data.vos_month.spl_for_eauction + abs(checkForm15Data.total_amt_incl_trans.spl_for_eauction),
                    "imported": checkForm15Data.vos_month.imported - checkForm15Data.total_amt_incl_trans.imported if checkForm15Data.total_amt_incl_trans.imported >= 0 else checkForm15Data.vos_month.imported + abs(checkForm15Data.total_amt_incl_trans.imported),
                    "total": checkForm15Data.vos_month.total - checkForm15Data.total_amt_incl_trans.total if checkForm15Data.total_amt_incl_trans.total >= 0 else checkForm15Data.vos_month.total + abs(checkForm15Data.total_amt_incl_trans.total),
                    "shakti_b": checkForm15Data.vos_month.shakti_b - checkForm15Data.total_amt_incl_trans.shakti_b if checkForm15Data.total_amt_incl_trans.shakti_b >= 0 else checkForm15Data.vos_month.shakti_b + abs(checkForm15Data.total_amt_incl_trans.shakti_b),
                    "shakti_b3": checkForm15Data.vos_month.shakti_b3 - checkForm15Data.total_amt_incl_trans.shakti_b3 if checkForm15Data.total_amt_incl_trans.shakti_b3 >= 0 else checkForm15Data.vos_month.shakti_b3 + abs(checkForm15Data.total_amt_incl_trans.shakti_b3),
                    "particular": "Total amount charged for coal (2+17)",
                }

                checkForm15Data.update(
                    total_amt_for_coal=total_amt_for_coal_dict
                )
            checkForm15Data.reload()
            if checkForm15Data.total_amt_for_coal and checkForm15Data.qty_at_station:
                landed_cost_dict = {
                    "remark": "",
                    "uom": "Rs/MT",
                    "mou_coal": safe_divide(checkForm15Data.qty_at_station.mou_coal, checkForm15Data.total_amt_for_coal.mou_coal),
                    "linkage": safe_divide(checkForm15Data.qty_at_station.linkage, checkForm15Data.total_amt_for_coal.linkage),
                    "aiwib_washery": safe_divide(checkForm15Data.qty_at_station.aiwib_washery, checkForm15Data.total_amt_for_coal.aiwib_washery),
                    "open_mkt": safe_divide(checkForm15Data.qty_at_station.open_mkt, checkForm15Data.total_amt_for_coal.open_mkt),
                    "spot_eauction": safe_divide(checkForm15Data.qty_at_station.spot_eauction, checkForm15Data.total_amt_for_coal.spot_eauction),
                    "spl_for_eauction": safe_divide(checkForm15Data.qty_at_station.spl_for_eauction, checkForm15Data.total_amt_for_coal.spl_for_eauction),
                    "imported": safe_divide(checkForm15Data.qty_at_station.imported, checkForm15Data.total_amt_for_coal.imported),
                    "total": safe_divide(checkForm15Data.qty_at_station.total, checkForm15Data.total_amt_for_coal.total),
                    "shakti_b": safe_divide(checkForm15Data.qty_at_station.shakti_b, checkForm15Data.total_amt_for_coal.shakti_b),
                    "shakti_b3": safe_divide(checkForm15Data.qty_at_station.shakti_b3, checkForm15Data.total_amt_for_coal.shakti_b3),
                    "particular": "Landed cost of coal (19/18)",
                }

                checkForm15Data.update(
                    landed_cost=landed_cost_dict
                )
            checkForm15Data.reload()
            if checkForm15Data.qty_consumed and checkForm15Data.landed_cost:
                value_consumed_dict = {
                    "remark": "",
                    "uom": "Rs",
                    "mou_coal": checkForm15Data.landed_cost.mou_coal * checkForm15Data.qty_consumed.mou_coal,
                    "linkage": checkForm15Data.landed_cost.linkage * checkForm15Data.qty_consumed.linkage,
                    "aiwib_washery": checkForm15Data.landed_cost.aiwib_washery * checkForm15Data.qty_consumed.aiwib_washery,
                    "open_mkt": checkForm15Data.landed_cost.open_mkt * checkForm15Data.qty_consumed.open_mkt,
                    "spot_eauction": checkForm15Data.landed_cost.spot_eauction * checkForm15Data.qty_consumed.spot_eauction,
                    "spl_for_eauction": checkForm15Data.landed_cost.spl_for_eauction * checkForm15Data.qty_consumed.spl_for_eauction,
                    "imported": checkForm15Data.landed_cost.imported * checkForm15Data.qty_consumed.imported,
                    "total": checkForm15Data.landed_cost.total * checkForm15Data.qty_consumed.total,
                    "shakti_b": checkForm15Data.landed_cost.shakti_b * checkForm15Data.qty_consumed.shakti_b,
                    "shakti_b3": checkForm15Data.landed_cost.shakti_b3 * checkForm15Data.qty_consumed.shakti_b3,
                    "particular": "Value of coal Consumed (20*21)",
                }
                checkForm15Data.update(
                    value_consumed=value_consumed_dict
                )
            checkForm15Data.reload()
            if checkForm15Data.osd_month and checkForm15Data.net_supplied and checkForm15Data.qty_consumed:
                closing_coal_stock_dict = {
                    "remark": "",
                    "uom": "",
                    "mou_coal": checkForm15Data.osd_month.mou_coal + checkForm15Data.net_supplied.mou_coal - checkForm15Data.qty_consumed.mou_coal,
                    "linkage": checkForm15Data.osd_month.linkage + checkForm15Data.net_supplied.linkage - checkForm15Data.qty_consumed.linkage,
                    "aiwib_washery": checkForm15Data.osd_month.aiwib_washery + checkForm15Data.net_supplied.aiwib_washery - checkForm15Data.qty_consumed.aiwib_washery,
                    "open_mkt": checkForm15Data.osd_month.open_mkt + checkForm15Data.net_supplied.open_mkt - checkForm15Data.qty_consumed.open_mkt,
                    "spot_eauction": checkForm15Data.osd_month.spot_eauction + checkForm15Data.net_supplied.spot_eauction - checkForm15Data.qty_consumed.spot_eauction,
                    "spl_for_eauction": checkForm15Data.osd_month.spl_for_eauction + checkForm15Data.net_supplied.spl_for_eauction - checkForm15Data.qty_consumed.spl_for_eauction,
                    "imported": checkForm15Data.osd_month.imported + checkForm15Data.net_supplied.imported - checkForm15Data.qty_consumed.imported,
                    "total": checkForm15Data.osd_month.total + checkForm15Data.net_supplied.total - checkForm15Data.qty_consumed.total,
                    "shakti_b": checkForm15Data.osd_month.shakti_b + checkForm15Data.net_supplied.shakti_b - checkForm15Data.qty_consumed.shakti_b,
                    "shakti_b3": checkForm15Data.osd_month.shakti_b3 + checkForm15Data.net_supplied.shakti_b3 - checkForm15Data.qty_consumed.shakti_b3,
                    "particular": "Closing stock of coal as on last Day of the Month",
                }
                checkForm15Data.update(
                    closing_coal_stock=closing_coal_stock_dict
                )
            checkForm15Data.reload()
            if checkForm15Data.closing_coal_stock and checkForm15Data.landed_cost:
                closing_coal_stock_value_dict = {
                    "remark": "",
                    "uom": "",
                    "mou_coal": checkForm15Data.closing_coal_stock.mou_coal * checkForm15Data.landed_cost.mou_coal,
                    "linkage": checkForm15Data.closing_coal_stock.linkage * checkForm15Data.landed_cost.linkage,
                    "aiwib_washery": checkForm15Data.closing_coal_stock.aiwib_washery * checkForm15Data.landed_cost.aiwib_washery,
                    "open_mkt": checkForm15Data.closing_coal_stock.open_mkt * checkForm15Data.landed_cost.open_mkt,
                    "spot_eauction": checkForm15Data.closing_coal_stock.spot_eauction * checkForm15Data.landed_cost.spot_eauction,
                    "spl_for_eauction": checkForm15Data.closing_coal_stock.spl_for_eauction * checkForm15Data.landed_cost.spl_for_eauction,
                    "imported": checkForm15Data.closing_coal_stock.imported * checkForm15Data.landed_cost.imported,
                    "total": checkForm15Data.closing_coal_stock.total * checkForm15Data.landed_cost.total,
                    "shakti_b": checkForm15Data.closing_coal_stock.shakti_b * checkForm15Data.landed_cost.shakti_b,
                    "shakti_b3": checkForm15Data.closing_coal_stock.shakti_b3 * checkForm15Data.landed_cost.shakti_b3,
                    "particular": "Closing stock of coal as on last Day of the Month",
                }
                checkForm15Data.update(
                    closing_coal_stock_value=closing_coal_stock_value_dict
                )
            checkForm15Data.reload()
            return {"details": "success"}
        except DoesNotExist as e:
            return JSONResponse(content={"details":"No Data Found"}, status_code=404)
    except Exception as e:
        success = False
        # response.status_code = 400
        console_logger.debug("----- Form 15 Automation Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e


@router.post("/insert/sappo/excel", tags=["Sap PO"])
async def endpoint_to_insert_sapop_data(response: Response):
    try:
        # directory = "/home/diycam/sap_ftp/PO"
        folder_path=os.path.join(os.getcwd(),"sap_ftp","PO")
        files=os.listdir(folder_path)
        for filename in files:
            # f_name = UploadedFile.filename
            file_path = os.path.join(folder_path, filename)
            excel_data = pd.read_excel(file_path)
            data_excel_fetch = json.loads(excel_data.to_json(orient="records"))
            
            for single_data_excel in data_excel_fetch:
                data_excel_fetch_final = {x.replace(' ', ''): v for x, v in single_data_excel.items()}
                if str(data_excel_fetch_final.get("DoNumber")).strip():
                    try:
                        fetchPoData = POdata.objects.get(do_number=str(data_excel_fetch_final.get("DoNumber")).strip())
                        fetchPoData.update(
                            po_number=data_excel_fetch_final.get("PoNumber"),
                            line_item=data_excel_fetch_final.get("PoLineItem"),
                            transport_code=data_excel_fetch_final.get("TransportationCode"),
                            transporter_name=data_excel_fetch_final.get("TransporterName").strip(),
                            material_code=data_excel_fetch_final.get("MaterialCode"),
                            material_description=data_excel_fetch_final.get("MaterialDescription").strip(),
                            plant_code=data_excel_fetch_final.get("PlantCode"),
                            storage_location=data_excel_fetch_final.get("StorageLocation").strip(),
                            valuation_type=data_excel_fetch_final.get("ValuationType").strip(),
                            po_open_quantity=data_excel_fetch_final.get("POOpenQty"),
                            uom=data_excel_fetch_final.get("UOM").strip()
                        )
                    except DoesNotExist as e:
                        POdata(
                            po_number=data_excel_fetch_final.get("PoNumber"),
                            line_item=data_excel_fetch_final.get("PoLineItem"),
                            do_number=str(data_excel_fetch_final.get("DoNumber")).strip(),
                            transport_code=data_excel_fetch_final.get("TransportationCode"),
                            transporter_name=data_excel_fetch_final.get("TransporterName").strip(),
                            material_code=data_excel_fetch_final.get("MaterialCode"),
                            material_description=data_excel_fetch_final.get("MaterialDescription").strip(),
                            plant_code=data_excel_fetch_final.get("PlantCode"),
                            storage_location=data_excel_fetch_final.get("StorageLocation").strip(),
                            valuation_type=data_excel_fetch_final.get("ValuationType").strip(),
                            po_open_quantity=data_excel_fetch_final.get("POOpenQty"),
                            uom=data_excel_fetch_final.get("UOM").strip()
                        ).save()
                    
                    try:
                        fetchSapRecordsdata = SapRecords.objects.get(do_no=str(data_excel_fetch_final.get("DoNumber")).strip())
                        fetchSapRecordsdata.update(
                            sap_po=str(data_excel_fetch_final.get("PoNumber")),
                            line_item=str(data_excel_fetch_final.get("PoLineItem")),
                            transport_code=str(data_excel_fetch_final.get("TransportationCode")),
                            transport_name=str(data_excel_fetch_final.get("TransporterName")).strip(),
                            material_code=str(data_excel_fetch_final.get("MaterialCode")),
                            material_description=str(data_excel_fetch_final.get("MaterialDescription")).strip(),
                            plant_code=str(data_excel_fetch_final.get("PlantCode")),
                            storage_location=str(data_excel_fetch_final.get("StorageLocation")).strip(),
                            valuation_type=str(data_excel_fetch_final.get("ValuationType")).strip(),
                            po_open_quantity=str(data_excel_fetch_final.get("POOpenQty")),
                            uom=str(data_excel_fetch_final.get("UOM")).strip()
                        )
                    except DoesNotExist as e:
                        pass
                    
                    try:
                        fetchSapRecordsdata = sapRecordsRail.objects.get(rr_no=str(data_excel_fetch_final.get("DoNumber")).strip())
                        fetchSapRecordsdata.update(
                            sap_po=str(data_excel_fetch_final.get("PoNumber")),
                            line_item=str(data_excel_fetch_final.get("PoLineItem")),
                            transport_code=str(data_excel_fetch_final.get("TransportationCode")),
                            transport_name=str(data_excel_fetch_final.get("TransporterName")).strip(),
                            material_code=str(data_excel_fetch_final.get("MaterialCode")),
                            material_description=str(data_excel_fetch_final.get("MaterialDescription")).strip(),
                            plant_code=str(data_excel_fetch_final.get("PlantCode")),
                            storage_location=str(data_excel_fetch_final.get("StorageLocation")).strip(),
                            valuation_type=str(data_excel_fetch_final.get("ValuationType")).strip(),
                            po_open_quantity=str(data_excel_fetch_final.get("POOpenQty")),
                            uom=str(data_excel_fetch_final.get("UOM")).strip()
                        )
                    except DoesNotExist as e:
                        pass

                    try:
                        fetchSapRecordsRoaddata = SapRecordsRcrRoad.objects.get(do_no=str(data_excel_fetch_final.get("DoNumber")).strip())
                        fetchSapRecordsdata.update(
                            sap_po=str(data_excel_fetch_final.get("PoNumber")),
                            line_item=str(data_excel_fetch_final.get("PoLineItem")),
                            transport_code=str(data_excel_fetch_final.get("TransportationCode")),
                            transport_name=str(data_excel_fetch_final.get("TransporterName")).strip(),
                            material_code=str(data_excel_fetch_final.get("MaterialCode")),
                            material_description=str(data_excel_fetch_final.get("MaterialDescription")).strip(),
                            plant_code=str(data_excel_fetch_final.get("PlantCode")),
                            storage_location=str(data_excel_fetch_final.get("StorageLocation")).strip(),
                            valuation_type=str(data_excel_fetch_final.get("ValuationType")).strip(),
                            po_open_quantity=str(data_excel_fetch_final.get("POOpenQty")),
                            uom=str(data_excel_fetch_final.get("UOM")).strip()
                        )
                    except DoesNotExist as e:
                        pass
        return {"detail": "success"}
    except KeyError as e:
        raise HTTPException(status_code=404, detail="Key Error")
    except Exception as e:
        response.status_code = 400
        console_logger.debug("----- IGI Excel Error -----", e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        return {"error": str(e)}


@router.get("/findupdatereciptcoalquality", tags=["Coal Testing"])
def find_update_receipt_coal_quality(do_rr_no: Optional[str] = None):
    try:
        if not do_rr_no:
            return {"error": "do_rr_no is required"}
        found_data = {}
        # found_data = None
        try:
            check_sap_records = SapRecords.objects.get(do_no=do_rr_no)
            console_logger.debug(check_sap_records.consumer_type)
            found_data['consumer_type'] = check_sap_records.consumer_type
            found_data['mine'] = check_sap_records.mine_name
            console_logger.debug("found in sap records road")
            # return {"message": "Record found in SapRecords", "data": check_sap_records}
        except DoesNotExist as e:
            pass

        try:
            check_rail_data = RailData.objects.get(rr_no=do_rr_no)
            console_logger.debug(check_rail_data.source_type)
            found_data["consumer_type"] = check_rail_data.source
            found_data["mine"] = check_rail_data.source_type
            console_logger.debug("found in rail")
        except DoesNotExist as e:
            pass

        try:
            check_sap_records_rcr_road = SapRecordsRcrRoad.objects.get(do_no=do_rr_no)
            console_logger.debug(check_sap_records_rcr_road.consumer_type)
            # found_data = check_sap_records_rcr_road.consumer_type
            found_data['consumer_type'] = check_sap_records.consumer_type
            found_data['mine'] = check_sap_records.mine_name
            console_logger.debug("found in sap records rcr road")
        except DoesNotExist as e:
            pass

        try:
            check_rcr_rail_data = RcrData.objects.get(rr_no=do_rr_no)
            console_logger.debug(check_rcr_rail_data.source_type)
            found_data["consumer_type"] = check_rcr_rail_data.source
            found_data["mine"] = check_rcr_rail_data.source_type
            console_logger.debug("found in sap records rcr rail")
        except DoesNotExist as e:
            pass
        console_logger.debug(found_data)
        if found_data:
            return found_data
        # else:
        #     response.status_code = 404
        #     return {"error": "No records found for the given do_rr_no"}

    except Exception as e:
        console_logger.debug("----- IGI Excel Error -----", exc_info=True)
        return {"error": str(e)}


@router.get("/gmr/removeduplicate", tags=["Duplicate"])
def endpoint_to_remove_duplicate_from_gmrdb_using_challan_no(response: Response):
    try:
        fetchGmrData = Gmrdata.objects()
        fetchGmrHistoricData = gmrdataHistoric.objects()

        for single_gmr_data in fetchGmrData:
            for single_historic_gmr_data in fetchGmrHistoricData:
                if single_gmr_data.delivery_challan_number==single_historic_gmr_data.delivery_challan_number:
                    console_logger.debug(f"Matched Challan Number: {single_gmr_data.arv_cum_do_number}")
        return {"detail": "success"}
    except Exception as e:
        success = False
        console_logger.debug("----- Gmr Duplicate Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e


@router.post("/insert/aopstatic", tags=["Aop"])
def endpoint_to_insert_aop_static_data(response: Response, data: aopStatic):
    try:
        payload = data.dict()
        Aop(
            percentage=payload.get("percentage"),
            gcv_crushing=payload.get("gcv_crushing"),
            qty_saving=payload.get("qty_saving"),
            month=payload.get("month")
        ).save()
    except Exception as e:
        success = False
        console_logger.debug("----- Gmr Duplicate Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e

def endpoint_to_get_all_months_in_between(coal_plant, years):
    try:
        dates = [years.get("start_date"), years.get("end_date")]
        start, end = [datetime.datetime.strptime(_, "%Y-%m-%d") for _ in dates]
        Datafetch = OrderedDict(((start + timedelta(_)).strftime(r"%b_%y"), None) for _ in range((end - start).days)).keys()

        typeConsumer = {}
        for single_list_data in list(Datafetch):
            if coal_plant in typeConsumer:
                typeConsumer['coal_plant'] = coal_plant
                typeConsumer[single_list_data] = {"target": 0, "achieved": 0}
        return typeConsumer

    except Exception as e:
        success = False
        console_logger.debug("----- Gmr Duplicate Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e    

@router.get("/fetch/aop", tags=["Aop"])
def endpoint_to_fetch_aop_data(response: Response):
    try:
        financial_year = get_financial_year(datetime.date.today().strftime("%Y-%m-%d"))
        console_logger.debug(financial_year)
        fetchGmrData = Gmrdata.objects(GWEL_Tare_Time__gte=financial_year.get("start_date"), GWEL_Tare_Time__lte=financial_year.get("end_date"))
        dictData = {"type": "", "reports": []}
        listData = []
        typeConsumer = {}

        

        for single_data_gmr in fetchGmrData:
            coal_plant = single_data_gmr.type_consumer
            fetchMonthData = endpoint_to_get_all_months_in_between(coal_plant, financial_year)
            console_logger.debug(fetchMonthData)  
        
    except Exception as e:
        success = False
        console_logger.debug("----- Gmr Duplicate Error -----",e)
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        console_logger.debug(exc_type, fname, exc_tb.tb_lineno)
        console_logger.debug("Error {} on line {} ".format(e, sys.exc_info()[-1].tb_lineno))
        success = e


#  x------------------------------    Scheduler To Tigger Coal API's    ------------------------------------x


params = UsecaseParameters.objects.first()


if not params:
    testing_hr, testing_min = "00", "00"
    consumption_hr, consumption_min = "00", "00"

if params:
    testing_scheduler = None
    try:
        gmr_dict = params.Parameters.get('gmr_api', {})
        roi_dict = gmr_dict.get('roi1', {})
        testing_dict = roi_dict.get('Coal Testing Scheduler', {})
        if testing_dict:
            testing_scheduler = testing_dict.get("time")
    except AttributeError:
        console_logger.error("Error accessing nested dictionary for testing_scheduler.")
        testing_scheduler = None

    if testing_scheduler is not None:
        console_logger.debug(f"---- Coal Testing Schedular ----  {testing_scheduler}")
        testing_hr, testing_min = testing_scheduler.split(":")

    # consumption_scheduler = None
    # try:
    #     gmr_dict = params.Parameters.get('gmr_api', {})
    #     roi_dict = gmr_dict.get('roi1', {})
    #     consumption_dict = roi_dict.get('Coal Consumption Scheduler', {})
    #     if consumption_dict:
    #         consumption_scheduler = consumption_dict.get("time")
    # except AttributeError:
    #     console_logger.error("Error accessing nested dictionary for consumption_scheduler.")
    #     consumption_scheduler = None

    # if consumption_scheduler is not None:
    #     console_logger.debug(f"---- Coal Consumption Schedular ----     {consumption_scheduler}")
    #     consumption_hr, consumption_min = consumption_scheduler.split(":")


console_logger.debug(f"---- Coal Testing Hr ----          {testing_hr}")
console_logger.debug(f"---- Coal Testing Min ----         {testing_min}")

# Time format for parsing and formatting time
time_format = "%H:%M"
# Time to subtract: 5 hours and 30 minutes
time_to_subtract = datetime.timedelta(hours=5, minutes=30)



# backgroundTaskHandler.run_job(task_name="save consumption data",
#                                 func=extract_historian_data,
#                                 trigger="cron",
#                                 **{"day": "*", "hour": "*", "minute": 0})


# shift_time = "22:00"
# Adata = datetime.datetime.strptime(shift_time, time_format)
# shift_time_ist = Adata - time_to_subtract

# coal_shift_hh, coal_shift_mm = shift_time_ist.strftime(time_format).split(":")
# console_logger.debug(coal_shift_hh)
# console_logger.debug(coal_shift_mm)
# backgroundTaskHandler.run_job(task_name="save testing data",
#                                 func=coal_test,
#                                 trigger="cron",
#                                 **{"day": "*", "hour": coal_shift_hh, "minute": coal_shift_mm})

# gcv_shift_time = "22:30"
# Bdata = datetime.datetime.strptime(gcv_shift_time, time_format)
# gcv_shift_time_ist = Bdata - time_to_subtract

# gcv_shift_hh, gcv_shift_mm = gcv_shift_time_ist.strftime(time_format).split(":")
# console_logger.debug(gcv_shift_hh)
# console_logger.debug(gcv_shift_mm)
# backgroundTaskHandler.run_job(task_name="update coal gcv data",
#                                 func=endpoint_to_fetch_coal_quality_gcv,
#                                 trigger="cron",
#                                 **{"day": "*", "hour": gcv_shift_hh, "minute": gcv_shift_mm})

                                


# backgroundTaskHandler.run_job(task_name="save testing data", func=coal_test, trigger="cron", **{"day": "*", "second": 2})
                                


# fetchShiftSchedule = shiftScheduler.objects(report_name="bunker_db_schedule")
# for single_shift in fetchShiftSchedule:
#     console_logger.debug(single_shift.shift_name)
#     console_logger.debug(single_shift.start_shift_time)
#     console_logger.debug(single_shift.end_shift_time)
#     # Parse end_shift_time
#     end_shift_time = datetime.datetime.strptime(single_shift.end_shift_time, time_format)
#     # Adjust for timezone by subtracting the specified duration
#     end_shift_time_ist = end_shift_time - time_to_subtract
#     # Convert the adjusted time back to hours and minutes
#     end_shift_hh, end_shift_mm = end_shift_time_ist.strftime(time_format).split(":")
#     # Schedule the background task
#     backgroundTaskHandler.run_job(
#         task_name=single_shift.shift_name,
#         func=bunker_scheduler,
#         trigger="cron",
#         **{"day": "*", "hour": end_shift_hh, "minute": end_shift_mm}, 
#         func_kwargs={
#             "shift_name": single_shift.shift_name, 
#             "start_time": single_shift.start_shift_time, 
#             "end_time": single_shift.end_shift_time
#         }
#     )                              


if __name__ == "__main__":
    usecase_handler_object.handler.run(ip=server_ip, port=server_port)
    usecase_handler_object.handler.send_status(True)
    pre_processing()
    import uvicorn
    uvicorn.run("main:router", reload=True, host="0.0.0.0", port=7704)
    # sched.add_job(scheduled_job, "interval", seconds=10)
    # sched.start()